# hdf5

> Official HDF5Â® Library Repository

## Repository Info

- **Stars:** 867
- **Forks:** 322
- **Language:** C
- **License:** Other
- **Topics:** c, cpp, database, fortran, hdf, hdf5, java, library, nosql
- **Source:** `https://github.com/HDFGroup/hdf5`
- **Branch:** `develop`
- **Commit:** `d1efeba7be60`
- **Last Commit:** 2025-12-31 11:44:08 -0600
- **Commits:** 1
- **Extracted:** 2026-01-01T15:16:36.329032


## Directory Structure

```
hdf5/
â”œâ”€â”€ .devcontainer/
â”‚   â”œâ”€â”€ devcontainer.json
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ noop.txt
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ .well-known/
â”‚   â”‚   â”œâ”€â”€ funding-manifest-urls
â”‚   â”‚   â””â”€â”€ funding.json
â”‚   â”œâ”€â”€ actions/
â”‚   â”‚   â””â”€â”€ setup-jextract/
â”‚   â”‚       â””â”€â”€ action.yml
â”‚   â”œâ”€â”€ ISSUE_TEMPLATE/
â”‚   â”‚   â”œâ”€â”€ blank-issue.md
â”‚   â”‚   â”œâ”€â”€ bug_report.md
â”‚   â”‚   â”œâ”€â”€ config.yml
â”‚   â”‚   â””â”€â”€ feature-request.md
â”‚   â”œâ”€â”€ PULL_REQUEST_TEMPLATE/
â”‚   â”‚   â””â”€â”€ pull_request_template.md
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ generate-index-html.sh
â”‚   â”‚   â”œâ”€â”€ test-java-implementations.sh
â”‚   â”‚   â”œâ”€â”€ test-maven-consumer.sh
â”‚   â”‚   â””â”€â”€ validate-maven-artifacts.sh
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â”œâ”€â”€ abi-report.yml
â”‚   â”‚   â”œâ”€â”€ analysis.yml
â”‚   â”‚   â”œâ”€â”€ aocc.yml
â”‚   â”‚   â”œâ”€â”€ arm-main.yml
â”‚   â”‚   â”œâ”€â”€ bintest.yml
â”‚   â”‚   â”œâ”€â”€ build-aws-c-s3.yml
â”‚   â”‚   â”œâ”€â”€ build_mpich_source.yml
â”‚   â”‚   â”œâ”€â”€ build_openmpi_source.yml
â”‚   â”‚   â”œâ”€â”€ call-workflows.yml
â”‚   â”‚   â”œâ”€â”€ clang-format-check.yml
â”‚   â”‚   â”œâ”€â”€ clang-format-fix.yml
â”‚   â”‚   â”œâ”€â”€ codeql.yml
â”‚   â”‚   â”œâ”€â”€ codespell.yml
â”‚   â”‚   â”œâ”€â”€ cross-compile.yml
â”‚   â”‚   â”œâ”€â”€ ctest.yml
â”‚   â”‚   â”œâ”€â”€ cve.yml
â”‚   â”‚   â”œâ”€â”€ cygwin.yml
â”‚   â”‚   â”œâ”€â”€ daily-build.yml
â”‚   â”‚   â”œâ”€â”€ daily-schedule.yml
â”‚   â”‚   â”œâ”€â”€ dev.yml
â”‚   â”‚   â”œâ”€â”€ freebsd.yml
â”‚   â”‚   â”œâ”€â”€ h5py.yml
â”‚   â”‚   â”œâ”€â”€ hdfeos5.yml
â”‚   â”‚   â”œâ”€â”€ i386.yml
â”‚   â”‚   â”œâ”€â”€ intel.yml
â”‚   â”‚   â”œâ”€â”€ java-examples-maven-test.yml
â”‚   â”‚   â”œâ”€â”€ java-implementation-test.yml
â”‚   â”‚   â”œâ”€â”€ julia.yml
â”‚   â”‚   â”œâ”€â”€ linkchecker.yml
â”‚   â”‚   â”œâ”€â”€ macos-26-matrix.yml
â”‚   â”‚   â”œâ”€â”€ main-par-spc.yml
â”‚   â”‚   â”œâ”€â”€ main-par.yml
â”‚   â”‚   â”œâ”€â”€ main-spc.yml
â”‚   â”‚   â”œâ”€â”€ main-static.yml
â”‚   â”‚   â”œâ”€â”€ main.yml
â”‚   â”‚   â”œâ”€â”€ markdown-link-check.yml
â”‚   â”‚   â”œâ”€â”€ markdown_config.json
â”‚   â”‚   â”œâ”€â”€ maven-build-test.yml
â”‚   â”‚   â”œâ”€â”€ maven-deploy.yml
â”‚   â”‚   â”œâ”€â”€ maven-staging.yml
â”‚   â”‚   â”œâ”€â”€ msys2.yml
â”‚   â”‚   â”œâ”€â”€ netcdf.yml
â”‚   â”‚   â”œâ”€â”€ nvhpc.yml
â”‚   â”‚   â”œâ”€â”€ openbsd.yml
â”‚   â”‚   â”œâ”€â”€ par-script.yml
â”‚   â”‚   â”œâ”€â”€ par-source.yml
â”‚   â”‚   â”œâ”€â”€ publish-branch.yml
â”‚   â”‚   â”œâ”€â”€ publish-release.yml
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ release-files.yml
â”‚   â”‚   â”œâ”€â”€ release.yml
â”‚   â”‚   â”œâ”€â”€ remove-files.yml
â”‚   â”‚   â”œâ”€â”€ scorecard.yml
â”‚   â”‚   â”œâ”€â”€ script.yml
â”‚   â”‚   â”œâ”€â”€ tarball.yml
â”‚   â”‚   â”œâ”€â”€ test-binary-installation.yml
â”‚   â”‚   â”œâ”€â”€ test-maven-deployment.yml
â”‚   â”‚   â”œâ”€â”€ test-maven-packages.yml
â”‚   â”‚   â”œâ”€â”€ testxpr.yml
â”‚   â”‚   â”œâ”€â”€ update-progress.py
â”‚   â”‚   â”œâ”€â”€ update-progress.yml
â”‚   â”‚   â”œâ”€â”€ vfd-main.yml
â”‚   â”‚   â”œâ”€â”€ vfd-ros3.yml
â”‚   â”‚   â”œâ”€â”€ vfd-subfiling.yml
â”‚   â”‚   â”œâ”€â”€ vfd.yml
â”‚   â”‚   â”œâ”€â”€ vol.yml
â”‚   â”‚   â”œâ”€â”€ vol_adios2.yml
â”‚   â”‚   â”œâ”€â”€ vol_async.yml
â”‚   â”‚   â”œâ”€â”€ vol_cache.yml
â”‚   â”‚   â”œâ”€â”€ vol_ext_passthru.yml
â”‚   â”‚   â”œâ”€â”€ vol_log.yml
â”‚   â”‚   â””â”€â”€ vol_rest.yml
â”‚   â”œâ”€â”€ CODEOWNERS
â”‚   â”œâ”€â”€ dependabot.yml
â”‚   â””â”€â”€ FUNDING.yml
â”œâ”€â”€ bin/
â”‚   â”œâ”€â”€ batch/
â”‚   â”‚   â”œâ”€â”€ ctest.qsub.in.cmake
â”‚   â”‚   â”œâ”€â”€ ctest_parallel.cmake.in
â”‚   â”‚   â”œâ”€â”€ ctest_serial.cmake.in
â”‚   â”‚   â”œâ”€â”€ ctestP.lsf.in.cmake
â”‚   â”‚   â”œâ”€â”€ ctestP.sl.in.cmake
â”‚   â”‚   â”œâ”€â”€ ctestS.lsf.in.cmake
â”‚   â”‚   â”œâ”€â”€ ctestS.sl.in.cmake
â”‚   â”‚   â”œâ”€â”€ ray_ctestP.lsf.in.cmake
â”‚   â”‚   â”œâ”€â”€ ray_ctestS.lsf.in.cmake
â”‚   â”‚   â””â”€â”€ raybsub
â”‚   â”œâ”€â”€ pkgscrpts/
â”‚   â”‚   â””â”€â”€ h5rmflags
â”‚   â”œâ”€â”€ checkapi
â”‚   â”œâ”€â”€ chkcopyright
â”‚   â”œâ”€â”€ debug-ohdr
â”‚   â”œâ”€â”€ format_source
â”‚   â”œâ”€â”€ genparser
â”‚   â”œâ”€â”€ h5vers
â”‚   â”œâ”€â”€ iostats
â”‚   â”œâ”€â”€ make_err
â”‚   â”œâ”€â”€ make_overflow
â”‚   â”œâ”€â”€ make_vers
â”‚   â”œâ”€â”€ output_filter.sh
â”‚   â”œâ”€â”€ process_source.sh
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ release
â”‚   â”œâ”€â”€ runbkgprog
â”‚   â”œâ”€â”€ trace
â”‚   â””â”€â”€ warnhist
â”œâ”€â”€ c++/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ header_files/
â”‚   â”‚   â”‚   â”œâ”€â”€ filelist.xml
â”‚   â”‚   â”‚   â”œâ”€â”€ hdf_logo.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ help.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ image001.jpg
â”‚   â”‚   â”‚   â””â”€â”€ image002.jpg
â”‚   â”‚   â”œâ”€â”€ C2Cppfunction_map.htm
â”‚   â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”‚   â”œâ”€â”€ H5AbstractDs.cpp
â”‚   â”‚   â”œâ”€â”€ H5AbstractDs.h
â”‚   â”‚   â”œâ”€â”€ H5Alltypes.h
â”‚   â”‚   â”œâ”€â”€ H5ArrayType.cpp
â”‚   â”‚   â”œâ”€â”€ H5ArrayType.h
â”‚   â”‚   â”œâ”€â”€ H5AtomType.cpp
â”‚   â”‚   â”œâ”€â”€ H5AtomType.h
â”‚   â”‚   â”œâ”€â”€ H5Attribute.cpp
â”‚   â”‚   â”œâ”€â”€ H5Attribute.h
â”‚   â”‚   â”œâ”€â”€ H5Classes.h
â”‚   â”‚   â”œâ”€â”€ H5CommonFG.cpp
â”‚   â”‚   â”œâ”€â”€ H5CommonFG.h
â”‚   â”‚   â”œâ”€â”€ H5CompType.cpp
â”‚   â”‚   â”œâ”€â”€ H5CompType.h
â”‚   â”‚   â”œâ”€â”€ H5Cpp.h
â”‚   â”‚   â”œâ”€â”€ H5DaccProp.cpp
â”‚   â”‚   â”œâ”€â”€ H5DaccProp.h
â”‚   â”‚   â”œâ”€â”€ H5DataSet.cpp
â”‚   â”‚   â”œâ”€â”€ H5DataSet.h
â”‚   â”‚   â”œâ”€â”€ H5DataSpace.cpp
â”‚   â”‚   â”œâ”€â”€ H5DataSpace.h
â”‚   â”‚   â”œâ”€â”€ H5DataType.cpp
â”‚   â”‚   â”œâ”€â”€ H5DataType.h
â”‚   â”‚   â”œâ”€â”€ H5DcreatProp.cpp
â”‚   â”‚   â”œâ”€â”€ H5DcreatProp.h
â”‚   â”‚   â”œâ”€â”€ H5DxferProp.cpp
â”‚   â”‚   â”œâ”€â”€ H5DxferProp.h
â”‚   â”‚   â”œâ”€â”€ H5EnumType.cpp
â”‚   â”‚   â”œâ”€â”€ H5EnumType.h
â”‚   â”‚   â”œâ”€â”€ H5Exception.cpp
â”‚   â”‚   â”œâ”€â”€ H5Exception.h
â”‚   â”‚   â”œâ”€â”€ H5FaccProp.cpp
â”‚   â”‚   â”œâ”€â”€ H5FaccProp.h
â”‚   â”‚   â”œâ”€â”€ H5FcreatProp.cpp
â”‚   â”‚   â”œâ”€â”€ H5FcreatProp.h
â”‚   â”‚   â”œâ”€â”€ H5File.cpp
â”‚   â”‚   â”œâ”€â”€ H5File.h
â”‚   â”‚   â”œâ”€â”€ H5FloatType.cpp
â”‚   â”‚   â”œâ”€â”€ H5FloatType.h
â”‚   â”‚   â”œâ”€â”€ H5Group.cpp
â”‚   â”‚   â”œâ”€â”€ H5Group.h
â”‚   â”‚   â”œâ”€â”€ H5IdComponent.cpp
â”‚   â”‚   â”œâ”€â”€ H5IdComponent.h
â”‚   â”‚   â”œâ”€â”€ H5Include.h
â”‚   â”‚   â”œâ”€â”€ H5IntType.cpp
â”‚   â”‚   â”œâ”€â”€ H5IntType.h
â”‚   â”‚   â”œâ”€â”€ H5LaccProp.cpp
â”‚   â”‚   â”œâ”€â”€ H5LaccProp.h
â”‚   â”‚   â”œâ”€â”€ H5LcreatProp.cpp
â”‚   â”‚   â”œâ”€â”€ H5LcreatProp.h
â”‚   â”‚   â”œâ”€â”€ H5Library.cpp
â”‚   â”‚   â”œâ”€â”€ H5Library.h
â”‚   â”‚   â”œâ”€â”€ H5Location.cpp
â”‚   â”‚   â”œâ”€â”€ H5Location.h
â”‚   â”‚   â”œâ”€â”€ H5Object.cpp
â”‚   â”‚   â”œâ”€â”€ H5Object.h
â”‚   â”‚   â”œâ”€â”€ H5OcreatProp.cpp
â”‚   â”‚   â”œâ”€â”€ H5OcreatProp.h
â”‚   â”‚   â”œâ”€â”€ H5PredType.cpp
... (truncated)
```

## File Statistics

- **Files Processed:** 500
- **Files Skipped:** 0


## README

HDF5 version 2.0.1 currently under development

> [!WARNING]
> **Heads Up: HDF5 Dropped Autotools March 10th**
>
> It's happenedâ€”the day we've all been dreadingâ€”or eagerly anticipating, depending on your perspective. Yes, we have switched to CMake-only builds in HDF5.
>
> The [PR stripping all autotools](https://github.com/HDFGroup/hdf5/pull/5308) was merged into the "develop" branch on **March 10, 2025**. Starting with HDF5 2.0, *only* the CMake build system is supported.

![HDF5 Logo][u3]

[![develop cmake build status](https://img.shields.io/github/actions/workflow/status/HDFGroup/hdf5/call-workflows.yml?branch=develop&label=HDF5%20develop%20CMake%20CI)](https://github.com/HDFGroup/hdf5/actions/workflows/call-workflows.yml?query=branch%3Adevelop)
[![HDF5 develop daily build status](https://img.shields.io/github/actions/workflow/status/HDFGroup/hdf5/daily-schedule.yml?branch=develop&label=HDF5%20develop%20daily%20build)](https://github.com/HDFGroup/hdf5/actions/workflows/daily-schedule.yml?query=branch%3Adevelop)
[![HDF-EOS5 build status](https://img.shields.io/github/actions/workflow/status/HDFGroup/hdf5/hdfeos5.yml?branch=develop&label=HDF-EOS5)](https://github.com/HDFGroup/hdf5/actions/workflows/hdfeos5.yml?query=branch%3Adevelop)
[![netCDF build status](https://img.shields.io/github/actions/workflow/status/HDFGroup/hdf5/netcdf.yml?branch=develop&label=netCDF)](https://github.com/HDFGroup/hdf5/actions/workflows/netcdf.yml?query=branch%3Adevelop)
[![h5py build status](https://img.shields.io/github/actions/workflow/status/HDFGroup/hdf5/h5py.yml?branch=develop&label=h5py)](https://github.com/HDFGroup/hdf5/actions/workflows/h5py.yml?query=branch%3Adevelop)
[![CVE regression](https://img.shields.io/github/actions/workflow/status/HDFGroup/hdf5/cve.yml?branch=develop&label=CVE)](https://github.com/HDFGroup/hdf5/actions/workflows/cve.yml?query=branch%3Adevelop)
[![HDF5 VOL connectors build status](https://img.shields.io/github/actions/workflow/status/HDFGroup/hdf5/vol.yml?branch=develop&label=HDF5-VOL)](https://github.com/HDFGroup/hdf5/actions/workflows/vol.yml?query=branch%3Adevelop)
[![HDF5 VFD build status](https://img.shields.io/github/actions/workflow/status/HDFGroup/hdf5/vfd.yml?branch=develop&label=HDF5-VFD)](https://github.com/HDFGroup/hdf5/actions/workflows/vfd.yml?query=branch%3Adevelop)
[![BSD](https://img.shields.io/badge/License-BSD-blue.svg)](https://github.com/HDFGroup/hdf5/blob/develop/LICENSE)
[![OSS-Fuzz Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/hdf5.svg)](https://oss-fuzz-build-logs.storage.googleapis.com/index.html#hdf5)
[![Link Checker Status](https://github.com/HDFGroup/hdf5/actions/workflows/linkchecker.yml/badge.svg)](https://github.com/HDFGroup/hdf5/actions/workflows/linkchecker.yml)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17808614.svg)](https://doi.org/10.5281/zenodo.17808614)

[HPC configure/build/test results](https://my.cdash.org/index.php?project=HDF5)

*Please refer to the release_docs/INSTALL file for installation/usage instructions.*

This repository contains a high-performance library's source code and a file format
specification that implements the HDF5Â® data model. The model has been adopted across
many industries, and this implementation has become a de facto data management standard
in science, engineering, and research communities worldwide.

The HDF Group is the developer, maintainer, and steward of HDF5 software. Find more
information about The HDF Group, the HDF5 Community, and other HDF5 software projects,
tools, and services at [The HDF Group's website](https://www.hdfgroup.org/). 

DOCUMENTATION
-------------
Documentation for all HDF software is available at:

   https://support.hdfgroup.org/documentation/index.html

The latest documentation for the HDF5 library can be found at:

   https://support.hdfgroup.org/documentation/hdf5/latest

See the [CHANGELOG.md][u1] file in the [release_docs/][u4] directory for information specific
to the features and updates included in this release of the library.

Several more files are located within the [release_docs/][u4] directory with specific
details for several common platforms and configurations.
- INSTALL - Start Here. General instructions for compiling and installing the library or using an installed library
- INSTALL_CMAKE - instructions for building with CMake (Kitware.com)
- README_HPC.md - instructions for building and configuring Parallel HDF5 on HPC systems
- INSTALL_Windows and INSTALL_Cygwin - MS Windows installations.
- USING_HDF5_CMake - Build and Install HDF5 Applications with CMake
- USING_CMake_Examples - Build and Test HDF5 Examples with CMake



HELP AND SUPPORT
----------------
The HDF Group staffs a free Help Desk accessible at [https://help.hdfgroup.org](https://help.hdfgroup.org) and also monitors the [Forum](https://forum.hdfgroup.org). Our free support service is community-based and handled as time allows. Weâ€™ll do our best to respond to your question as soon as possible, but please note that response times may vary depending on the complexity of the issue and staff availability.

If you're interested in guaranteed response and resolution times, a dedicated technical account manager, and more benefits (all while supporting the open-source work of The HDF Group), please check out [Priority Support](https://www.hdfgroup.org/solutions/priority-support/).



FORUM and NEWS
--------------
The [HDF Forum](https://forum.hdfgroup.org) is provided for public announcements, technical questions, and discussions
of interest to the general HDF5 Community.

   - News and Announcements
   https://forum.hdfgroup.org/c/news-and-announcements-from-the-hdf-group

   - HDF5 Topics
   https://forum.hdfgroup.org/c/hdf5

These forums are provided as an open and public service for searching and reading.
Posting requires completing a simple registration and allows one to join in the
conversation.  Please read the [instructions](https://forum.hdfgroup.org/t/quickstart-guide-welcome-to-the-new-hdf-forum
) for more information on how to get started.

RELEASE SCHEDULE
----------------

![HDF5 release schedule][u2] 

HDF5 does not follow a regular release schedule. Instead, updates are based on the
introduction of new features and the resolution of bugs. However, we aim to have at
least one annual release for each maintenance branch.

| Release | New Features |
| ------- | ------------ |
| 2.0.0 | Drop Autotools support, drop the HDF5 <--> GIF tools, add complex number support, update library defaults (cache sizes, etc.) |
| FUTURE | Multi-threaded HDF5, crashproofing / metadata journaling, Full (VFD) SWMR, encryption, digital signatures, sparse datasets, improved storage for variable-length datatypes, better Unicode support (especially on Windows) |

### Release Progress

[![Release Progress](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/HDFGroup-Bot/0ad2eabb63b28eb90d69f5e5b2c1496f/raw/release-progress-hdf5.json)](https://github.com/orgs/HDFGroup/projects/39/views/24)

The badge above shows the current progress of release-blocking issues with colors that reflect completion status:

- **ðŸŸ¢ Green (90%+)**:  Readying for Deployment - most blockers completed
- **ðŸŸ¡ Yellow (60-89%)**:  Nearing Completion - on track for release
- **ðŸŸ  Orange (40-59%)**:  In Development - attention needed
- **ðŸ”´ Red (<40%)**:  Initial Phase - significant blockers remain

Click the badge to view the detailed project board with current release-blocking issues.

SNAPSHOTS, PREVIOUS RELEASES AND SOURCE CODE
--------------------------------------------
Periodically development code snapshots are provided at the following URL:

   https://github.com/HDFGroup/hdf5/releases/tag/snapshot

Source packages for current and previous releases are located at:

   [Latest HDF5 release](https://github.com/HDFGroup/hdf5/releases)
   [Previous releases](https://support.hdfgroup.org/archive/support/ftp/HDF5/releases/index.html)

Maven artifacts for Java bindings and examples are available at:

   GitHub Packages:
   https://maven.pkg.github.com/HDFGroup/hdf5

   Maven Central (coming soon):
   https://central.sonatype.com/artifact/org.hdfgroup/hdf5-java

Java Examples Maven Integration:
   - **org.hdfgroup:hdf5-java** - HDF5 Java bindings with platform-specific JARs (linux-x86_64, windows-x86_64, macos-x86_64, macos-aarch64)
   - **org.hdfgroup:hdf5-java-examples** - Complete collection of Java examples (platform-independent)
   - Cross-platform CI/CD testing and deployment
   - Comprehensive Maven integration with automated testing
   - See HDF5Examples/JAVA/README-MAVEN.md for complete usage instructions

Development code is available at our Github location:

   https://github.com/HDFGroup/hdf5.git

[u1]: https://github.com/HDFGroup/hdf5/blob/develop/release_docs/CHANGELOG.md
[u2]: https://github.com/HDFGroup/hdf5/blob/develop/release_docs/img/release-schedule.png
[u3]: https://github.com/HDFGroup/hdf5/blob/develop/doxygen/img/HDF5.png
[u4]: https://github.com/HDFGroup/hdf5/blob/develop/release_docs



## Source Files

### `.github/workflows/README.md`

```markdown
# Workflows for Testing
The workflows are contained in .yml files and use the callable workflow method.
Workflows can be triggered from PR, creation or merge, and on a scheduled timer.
There are a few that only get triggered manually.

## Scheduled Workflows
- daily-schedule.yml executes the daily-build.yml which first checks that there are changes
    * tarball.yml to create a source.zip and source.tar.gz
    * cygwin.yml to test on cygwin
    * script.yml to test and report to my.cdash
    * par-script.yml to test with released MPI and report to my.cdash
    * par-source.yml to test with MPI default branch and report to my.cdash
    * analysis.yml to test with LEAK and ADDRESS sanitizers and report to my.cdash
    * ctest.yml to create signed binaries with commit hash in the name
    * abi-report.yml to compare ABI to the last released binaries
    * release-files.yml uploads new binaries to snapshots
    * remove-files.yml remove previous binaries
- h5py.yml executes Python tests for h5py
- markdown-link-check.yml checks the links in markdown files
- scorecard.yml executes code-scanning and uploads to Github dashboard
- vfd.yml executes vfd-main.yml with combos of Release and Debug
    * vfd-subfiling.yml configures, builds, and tests MPI with subfiling feature
- vol.yml calls the following workflows
    * vol_rest.yml tests the REST VOL connector
    * vol_ext_passthru.yml tests the external passthrough VOL connector
    * vol_async.yml tests the asynchronous I/O VOL connector
    * vol_cache.yml tests the cache VOL connector
    * vol_adios2.yml tests the ADIOS2 VOL connector
    * vol_log.yml tests the Log-based VOL connector

## Manual Only Workflows
- publish-branch.yml publishes a local folder to the support.hdfgroup bucket
- publish-release.yml publishes release binaries to the support.hdfgroup bucket
- release.yml creates binaries for an official release or snapshot
    * tarball.yml to create a source.zip and source.tar.gz
    * ctest.yml to create signed binaries
    * abi-report.yml to compare ABI to last released binaries
    * maven-staging.yml to generate and test Maven artifacts with Java examples across all platforms
    * maven-deploy.yml to deploy Maven artifacts to repositories
    * release-files.yml uploads new binaries to releases page
- java-examples-maven-test.yml comprehensive Java examples testing with Maven artifacts

## Triggered Workflows
- clang-format-check.yml runs clang-format and reports issues
- call-workflows.yml
- codespell.yml checks spelling
- cve.yml executes test_hdf5_cve.sh script
- hdfeos5.yml configures and builds HDF5 then tests HDF-EOS5
- linkchecker.yml verifies the links in generated doxygen files
- netcdf.yml configures and builds HDF5 then tests NetCDF

## Workflows called by call-workflows.yml
- main-spc.yml configure, build, and test HDF5 with:
    * API default v1_6
    * API default v1_8
    * API default v1_10
    * API default v1_12
    * API default v1_14
    * API default v2_0
    * using system zlib
    * using zlibng
    * using no filters
    * in debug mode and -Werror compiler option
    * in release mode and -Werror compiler option
    * with minimum CMake Version 3.18
- main.yml configure, build, test, and package HDF5 on Ubuntu, macOS, and Windows
- main-static.yml configure, build, test static only HDF5 on Ubuntu, macOS, and Windows
- bintest.yml test binary packages created by main.yml
- main-par.yml configure, build, and test HDF5 with openmpi
- main-par-spc.yml configure, build, and test HDF5 with HDF5_ENABLE_WARNINGS_AS_ERRORS=ON
- intel.yml configure, build, and test HDF5 with Intel OneAPI on Linux and Windows
- nvhpc.yml configure, build, and test HDF5 with nvhpc
- aocc.yml configure, build, and test HDF5 with AOCC and OpenMPI
- testxpr.yml configure, build, and test HDF5 with HDF_TEST_EXPRESS=0
- julia.yml configure and build HDF5, then test Julia hdf5 source
- msys2.yml configure, build, and test HDF5 on mingw32, mingw64, ucrt64, clang64
- i386.yml configure, build, and test HDF5 on 32-bit Linux
```

### `HDF5Examples/C/TUTR/README`

```
                               HDF5 Examples

This directory contains example programs for the installed APIs and scripts to 
compile and run them.  Examples in the c and hl/c subdirectories are always 
installed, and those in fortran, hl/fortran, c++ and hl/c++ will be installed 
when fortran or c++ are enabled.

Running the run-all-ex.sh script in this directory will run the scripts and in 
turn the examples in all the subdirectories where examples are installed.  The 
scripts can also be run individually.  The appropriate compile scripts in the 
bin directory for this install will be used by default to compile and link the 
example programs.  Note that h5redeploy must be run if these binaries are 
copied or extracted in a directory other than the one where they were initially 
installed.  Compile scripts from other locations can be used by setting an 
environment variable prefix to the path of the directory containing the bin 
directory with the compile scripts h5cc, h5fc, etc.  For example, export 
prefix=/usr/local/hdf5 to use h5cc, h5fc, etc. in /usr/local/hdf5/bin.
```

### `HDF5Examples/README.md`

```markdown
HDF5 Examples

*Please refer to the Using_CMake.txt file for CMake instructions.*

Note that this HDF5Examples directory structure is also provided as a stand-alone project
distributed with library binaries as well as compiled and tested during a HDF5 Library build.

This repository contains a high-performance library's example code that demonstrate the HDF5Â® data
model API. The HDF5Â® data model has been adopted across
many industries and this implementation has become a de facto data management standard
in science, engineering, and research communities worldwide.

The HDF Group is the developer, maintainer, and steward of HDF5 software. Find more
information about The HDF Group, the HDF5 Community, and other HDF5 software projects,
tools, and services at [The HDF Group's website](https://www.hdfgroup.org/).

We suggest using the presets method with CMake for building the examples. However, if you prefer to use the
h5cc pkg-config wrappers, you can use the following commands to build the examples:

    export HDF5_HOME="hdf5 installation root"
    export PKG_CONFIG_PATH="$HDF5_HOME/lib/pkgconfig"
    export LD_LIBRARY_PATH="$HDF5_HOME/lib:$LD_LIBRARY_PATH"
    export PATH="$HDF5_HOME/bin:$PATH"

Then, you can compile the examples with:

    h5cc -o example1 example1.c
    h5c++ -o example2 example2.cpp
    h5fc -o example3 example3.f90

For Java examples with Maven integration, see the JAVA/README-MAVEN.md file for complete instructions on using the `org.hdfgroup:hdf5-java-examples` Maven artifact.

The test-pc.sh script can test the examples with the h5*cc pkg-config wrappers with:
    cd \<path to examples\>
    export HDF5_HOME="hdf5 installation root"; sh ./test-pc.sh \<path to examples\> \<path to build dir\> .
    Notice that period (.) at the end of the command is important, it tells the script to use the current
    directory as the source directory.

HELP AND SUPPORT
----------------
Information regarding Help Desk and Support services is available at

   https://help.hdfgroup.org 



FORUM and NEWS
--------------
The [HDF Forum](https://forum.hdfgroup.org) is provided for public announcements and discussions
of interest to the general HDF5 Community.

   - News and Announcements
   https://forum.hdfgroup.org/c/news-and-announcements-from-the-hdf-group

   - HDF5 Topics
   https://forum.hdfgroup.org/c/hdf5

These forums are provided as an open and public service for searching and reading.
Posting requires completing a simple registration and allows one to join in the
conversation.  Please read the [instructions](https://forum.hdfgroup.org/t/quickstart-guide-welcome-to-the-new-hdf-forum
) pertaining to the Forum's use and configuration.


HDF5 SNAPSHOTS, PREVIOUS RELEASES AND SOURCE CODE
--------------------------------------------
Full Documentation and Programming Resources for this HDF5 can be found at

   https://support.hdfgroup.org/documentation/index.html

Periodically development code snapshots are provided at the following URL:

   https://github.com/HDFGroup/hdf5/releases/tag/snapshot

Source packages for current and previous releases are located at:

   [Latest HDF5 release](https://github.com/HDFGroup/hdf5/releases)
   [Previous releases](https://support.hdfgroup.org/archive/support/ftp/HDF5/releases/index.html)

Development code is available at our Github location:

   https://github.com/HDFGroup/hdf5.git
```

### `bin/README.md`

```markdown
# Scripts in `bin` and their purpose

|Program|Purpose|
|-------|-------|
|`checkapi`|Checks if public API calls are used in internal functions|
|`chkcopyright`|Checks if files have appropriate copyright statements|
|`debug-ohdr`|Examines debug output from `H5O_open/close` to look for open objects|
|`format_source`|Runs `clang-format` over the source files, applying our rules|
|`genparser`|Creates the flex/bison-based parser files in the high-level library|
|`h5cc.in`|Input file from which h5cc is created|
|`h5redeploy.in`|Input file from which h5redeploy is created|
|`h5vers`|Updates the library version number|
|`make_err`|Generates the H5E header files|
|`make_vers`|Generates H5version.h|
|`make_overflow`|Generates H5overflow.h|
|`output_filter`|Used in the tools test code to strip extraneous output before we diff files|
|`runbkprog`|Used by CMake to run test programs in the background|
|`trace`|Updates `H5ARG_TRACE` macros in H5ES\_insert() calls|
|`warnhist`|Generates compiler warning statistics for gcc/clang when fed output of make|

## TODO

* chkcopyright is currently semi-broken as it doesn't handle the full variety of copyright headers we need. We're leaving it in place, though, in the hopes that someone will update it in the future.
* Extending warnhist to better understand the output of additional compilers/languages would be nice.
```

### `config/README.md`

```markdown
# The `config` directory

## Intro

HDF5 can be configured using CMake.

Configuration information for the HDF5 library and tools is
specific to the repository folders. Each subdirectory of the project
has its own CMake build and test files. Basic library configuration will generally
be found in the root's `CMakeLists.txt` with support for macros and settings
in this config directory.


This directory contains a few important things:

* Support files for optional components (in `cmake`)
* Compiler and platform parameters (in `flags`)
* Warning files (in `*-warnings` directories)
* Toolchain files (in `toolchain`)
* Sanitizer files (in `sanitizer`)
* Example install scripts (in `examples`)
* Installation support files (in `install`)

CMake is documented in the following in the root's release_docs folder files:

* INSTALL
* INSTALL_CMake.txt
* USING_HDF5_CMake.txt
* USING_HDF5_VS.txt
* INSTALL_Windows.txt
* USING_CMake_Examples.txt
```

### `config/sanitizer/README.md`

```markdown
# CMake Scripts <!-- omit in toc -->

[![pipeline status](https://git.stabletec.com/other/cmake-scripts/badges/main/pipeline.svg)](https://git.stabletec.com/other/cmake-scripts/commits/main)
[![license](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://git.stabletec.com/other/cmake-scripts/blob/main/LICENSE)

This is a collection of quite useful scripts that expand the possibilities for building software with CMake, by making some things easier and otherwise adding new build types

- [Sanitizer Builds `sanitizers.cmake`](#sanitizer-builds-sanitizerscmake)
- [Code Coverage `code-coverage.cmake`](#code-coverage-code-coveragecmake)
  - [Added Targets](#added-targets)
  - [Usage](#usage)
    - [Example 1 - All targets instrumented](#example-1---all-targets-instrumented)
      - [1a - Via global command](#1a---via-global-command)
      - [1b - Via target commands](#1b---via-target-commands)
    - [Example 2: Target instrumented, but with regex pattern of files to be excluded from report](#example-2-target-instrumented-but-with-regex-pattern-of-files-to-be-excluded-from-report)
    - [Example 3: Target added to the 'ccov' and 'ccov-all' targets](#example-3-target-added-to-the-ccov-and-ccov-all-targets)
- [AFL Fuzzing Instrumentation `afl-fuzzing.cmake`](#afl-fuzzing-instrumentation-afl-fuzzingcmake)
  - [Usage](#usage-1)
- [Dependency Graph `dependency-graph.cmake`](#dependency-graph-dependency-graphcmake)
  - [Required Arguments](#required-arguments)
    - [OUTPUT\_TYPE *STR*](#output_type-str)
  - [Optional Arguments](#optional-arguments)
    - [ADD\_TO\_DEP\_GRAPH](#add_to_dep_graph)
    - [TARGET\_NAME *STR*](#target_name-str)
    - [OUTPUT\_DIR *STR*](#output_dir-str)
- [Tools `tools.cmake`](#tools-toolscmake)
  - [clang-tidy](#clang-tidy)
  - [include-what-you-use](#include-what-you-use)
  - [cppcheck](#cppcheck)
- [Formatting `formatting.cmake`](#formatting-formattingcmake)
  - [clang-format](#clang-format)
  - [cmake-format](#cmake-format)

## Sanitizer Builds [`sanitizers.cmake`](sanitizers.cmake)

Sanitizers are tools that perform checks during a programâ€™s runtime and returns issues, and as such, along with unit testing, code coverage and static analysis, is another tool to add to the programmers toolbox. And of course, like the previous tools, are tragically simple to add into any project using CMake, allowing any project and developer to quickly and easily use.

A quick rundown of the tools available, and what they do:
- [LeakSanitizer](https://clang.llvm.org/docs/LeakSanitizer.html) detects memory leaks, or issues where memory is allocated and never deallocated, causing programs to slowly consume more and more memory, eventually leading to a crash.
- [AddressSanitizer](https://clang.llvm.org/docs/AddressSanitizer.html) is a fast memory error detector. It is useful for detecting most issues dealing with memory, such as:
    - Out of bounds accesses to heap, stack, global
    - Use after free
    - Use after return
    - Use after scope
    - Double-free, invalid free
    - Memory leaks (using LeakSanitizer)
- [ThreadSanitizer](https://clang.llvm.org/docs/ThreadSanitizer.html) detects data races for multi-threaded code.
- [UndefinedBehaviourSanitizer](https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html) detects the use of various features of C/C++ that are explicitly listed as resulting in undefined behaviour. Most notably:
    - Using misaligned or null pointer.
    - Signed integer overflow
    - Conversion to, from, or between floating-point types which would overflow the destination
    - Division by zero
    - Unreachable code
- [MemorySanitizer](https://clang.llvm.org/docs/MemorySanitizer.html) detects uninitialized reads.
- [Control Flow Integrity](https://clang.llvm.org/docs/ControlFlowIntegrity.html) is designed to detect certain forms of undefined behaviour that can potentially allow attackers to subvert the program's control flow.

These are used by declaring the `HDF5_USE_SANITIZER` CMake variable as string containing any of:
- Address
- Memory
- MemoryWithOrigins
- Undefined
- Thread
- Leak
- CFI

Multiple values are allowed, e.g. `-DHDF5_USE_SANITIZER=Address,Leak` but some sanitizers cannot be combined together, e.g.`-DHDF5_USE_SANITIZER=Address,Memory` will result in configuration error. The delimiter character is not required and `-DHDF5_USE_SANITIZER=AddressLeak` would work as well.

## Code Coverage [`code-coverage.cmake`](code-coverage.cmake)

> In computer science, test coverage is a measure used to describe the degree to which the source code of a program is executed when a particular test suite runs. A program with high test coverage, measured as a percentage, has had more of its source code executed during testing, which suggests it has a lower chance of containing undetected software bugs compared to a program with low test coverage. Many different metrics can be used to calculate test coverage; some of the most basic are the percentage of program subroutines and the percentage of program statements called during execution of the test suite. 
>
> [Wikipedia, Code Coverage](https://en.wikipedia.org/wiki/Code_coverage)

Code coverage is the detailing of, during the execution of a binary, which regions, functions, or lines of code are *actually* executed. This can be used in a number of ways, from figuring out areas that automated testing is lacking or not touching, to giving a user an instrumented binary to determine which areas of code are used most/least to determine which areas to focus on. Although this does come with the caveat that coverage is no guarantee of good testing, just of what code has been.

Coverage here is supported on both GCC and Clang. GCC requires the `lcov` program, and Clang requires `llvm-cov` and `llvm-profdata`, often provided with the llvm toolchain.

To enable, turn on the `CODE_COVERAGE` variable.

### Added Targets

- GCOV/LCOV:
    - ccov : Generates HTML code coverage report for every target added with 'AUTO' parameter.
    - ccov-${TARGET_NAME} : Generates HTML code coverage report for the associated named target.
    - ccov-all : Generates HTML code coverage report, merging every target added with 'ALL' parameter into a single detailed report.
    - ccov-all-capture : Generates an all-merged.info file, for use with coverage dashboards (e.g. codecov.io, coveralls).
- LLVM-COV:
    - ccov : Generates HTML code coverage report for every target added with 'AUTO' parameter.
    - ccov-report : Generates HTML code coverage report for every target added with 'AUTO' parameter.
    - ccov-${TARGET_NAME} : Generates HTML code coverage report.
    - ccov-rpt-${TARGET_NAME} : Prints to command line summary per-file coverage information.
    - ccov-show-${TARGET_NAME} : Prints to command line detailed per-line coverage information.
    - ccov-all : Generates HTML code coverage report, merging every target added with 'ALL' parameter into a single detailed report.
    - ccov-all-report : Prints summary per-file coverage information for every target added with ALL' parameter to the command line.

### Usage

To enable any code coverage instrumentation/targets, the single CMake option of `CODE_COVERAGE` needs to be set to 'ON', either by GUI, ccmake, or on the command line ie `-DCODE_COVERAGE=ON`.

From this point, there are two primary methods for adding instrumentation to targets:
1. A blanket instrumentation by calling `add_code_coverage()`, where all targets in that directory and all subdirectories are automatically instrumented.
2. Per-target instrumentation by calling `target_code_coverage(<TARGET_NAME>)`, where the target is given and thus only that target is instrumented. This applies to both libraries and executables.

To add coverage targets, such as calling `make ccov` to generate the actual coverage information for perusal or consumption, call `target_code_coverage(<TARGET_NAME>)` on an *executable* target.

**NOTE:** For more options, please check the actual [`code-coverage.cmake`](code-coverage.cmake) file.

#### Example 1 - All targets instrumented

In this case, the coverage information reported will will be that of the `theLib` library target and `theExe` executable.

##### 1a - Via global command

```
add_code_coverage() # Adds instrumentation to all targets

add_library(theLib lib.cpp)

add_executable(theExe main.cpp)
target_link_libraries(theExe PRIVATE theLib)
target_code_coverage(theExe) # As an executable target, adds the 'ccov-theExe' target (instrumentation already added via global anyways) for generating code coverage reports.
```

##### 1b - Via target commands

```
add_library(theLib lib.cpp)
target_code_coverage(theLib) # As a library target, adds coverage instrumentation but no targets.

add_executable(theExe main.cpp)
target_link_libraries(theExe PRIVATE theLib)
target_code_coverage(theExe) # As an executable target, adds the 'ccov-theExe' target and instrumentation for generating code coverage reports.
```

#### Example 2: Target instrumented, but with regex pattern of files to be excluded from report

```
add_executable(theExe main.cpp non_covered.cpp)
target_code_coverage(theExe EXCLUDE non_covered.cpp) # As an executable target, the reports will exclude the non_covered.cpp file.
```

#### Example 3: Target added to the 'ccov' and 'ccov-all' targets

```
add_code_coverage_all_targets(EXCLUDE test/*) # Adds the 'ccov-all' target set and sets it to exclude all files in test/ folders.

add_executable(theExe main.cpp non_covered.cpp)
target_code_coverage(theExe AUTO ALL EXCLUDE non_covered.cpp test/*) # As an executable target, adds to the 'ccov' and ccov-all' targets, and the reports will exclude the non-covered.cpp file, and any files in a test/ folder.
```

## AFL Fuzzing Instrumentation [`afl-fuzzing.cmake`](afl-fuzzing.cmake)

> American fuzzy lop is a security-oriented fuzzer that employs a novel type of compile-time instrumentation and genetic algorithms to automatically discover clean, interesting test cases that trigger new internal states in the targeted binary. This substantially improves the functional coverage for the fuzzed code. The compact synthesized corpora produced by the tool are also useful for seeding other, more labor- or resource-intensive testing regimes down the road.
>
> [american fuzzy lop](https://lcamtuf.coredump.cx/afl/)

NOTE: This actually works based off the still-developed daughter project [AFL++](https://aflplus.plus/).

### Usage

To enable the use of AFL instrumentation, this file needs to be included into the CMake scripts at any point *before* any of the compilers are setup by CMake, typically at/before the first call to project(), or any part before compiler detection/validation occurs. This is since CMake does not support changing the compiler after it has been set:

```
cmake_minimum_required(VERSION 3.4)
include(cmake/afl-fuzzing.cmake)
project(Example C CXX)
```

Using `-DAFL=ON` will search for and switch to the AFL++ compiler wrappers that will instrument builds, or error if it cannot.

Using `-DAFL_MODE=<MODE>` will attempt to use the specified  instrumentation type, see [here](https://github.com/AFLplusplus/AFLplusplus/blob/stable/docs/fuzzing_in_depth.md). Options are:
- LTO
- LLVM
- GCC-PLUGIN
- CLANG
- GCC

Using `-DAFL_ENV_OPTIONS=<...;...>` allows adding any number of AFL++'s instrumentation enabled via environment variables, and these will be prefixed to the build calls (see `afl-cc -hh`).

As an example, a CMake configuration such as this:
```cmake .. -DAFL_MODE=LTO -DAFL_ENV_OPTIONS=AFL_LLVM_THREADSAFE_INST=1;AFL_LLVM_LAF_ALL=1```
would result in build commands such as this:
```AFL_LLVM_THREADSAFE_INST=1 AFL_LLVM_LAF_ALL=1 afl-clang-lto --afl-lto <...>```

## Compiler Options

Allows for easy use of some pre-made compiler options for the major compilers.

Using `-DENABLE_ALL_WARNINGS=ON` will enable almost all of the warnings available for a compiler:

| Compiler | Options       |
| :------- | :------------ |
| MSVC     | /W4           |
| GCC      | -Wall -Wextra |
| Clang    | -Wall -Wextra |

Using `-DENABLE_EFFECTIVE_CXX=ON` adds the `-Weffc++` for both GCC and clang.

Using `-DGENERATE_DEPENDENCY_DATA=ON` generates `.d` files along with regular object files on a per-source file basis on GCC/Clang compilers. These files contains the list of all header files used during compilation of that compilation unit.

## Dependency Graph [`dependency-graph.cmake`](dependency-graph.cmake)

CMake, with the dot application available, will build a visual representation of the library/executable dependencies.

### Required Arguments

#### OUTPUT_TYPE *STR*
The type of output of `dot` to produce. Can be whatever `dot` itself supports (eg. png, ps, pdf).

### Optional Arguments

#### ADD_TO_DEP_GRAPH
If specified, add this generated target to be a dependency of the more general `dep-graph` target.

#### TARGET_NAME *STR*
The name to give the doc target. (Default: doc-${PROJECT_NAME})

#### OUTPUT_DIR *STR*
The directory to place the generated output

## Tools [`tools.cmake`](tools.cmake)

The three tools in this are used via two provided functions each, for example for clang-tidy:
```
add_executable(big_test)

clang_tidy()

# Sources provided here are run with clang-tidy with no options
add_executable(test2 main2.cpp)
target_sources(big_test test2.c test2.cpp)

clang_tidy(-header-filter='${CMAKE_SOURCE_DIR}/*')

# Sources provided here are run with clang-tidy with the header-filter options provided to it from above
add_execuable(test1 main1.cpp)
target_sources(big_test test1.c test1.cpp)

reset_clang_tidy()

# Sources provided here are not run with clang-tidy at all
add_executable(test3 main3.cpp)
target_sources(big_test test3.c test3.cpp)

clang_tidy()

# Sources provided here are run with clang-tidy with no options
add_executable(test4 main4.cpp)
target_sources(big_test test4.c test4.cpp)
```

### clang-tidy

> clang-tidy is a clang-based C++ â€œlinterâ€ tool. Its purpose is to provide an extensible framework for diagnosing and fixing typical programming errors, like style violations, interface misuse, or bugs that can be deduced via static analysis. clang-tidy is modular and provides a convenient interface for writing new checks.
>
> [clang-tidy](https://clang.llvm.org/extra/clang-tidy/)

To use, add the `clang_tidy()` macro, with the arguments being the options passed to the clang-tidy call in the form of `clang-tidy ${ARGS}`. The settings used with clang-tidy can be changed by calling `clang_tidy()` macro again. It can be turned off by calling the `reset_clang_tidy()` macro.

### include-what-you-use

> "Include what you use" means this: for every symbol (type, function variable, or macro) that you use in foo.cc, either foo.cc or foo.h should #include a .h file that exports the declaration of that symbol. The include-what-you-use tool is a program that can be built with the clang libraries in order to analyze #includes of source files to find include-what-you-use violations, and suggest fixes for them.
>
> The main goal of include-what-you-use is to remove superfluous #includes. It does this both by figuring out what #includes are not actually needed for this file (for both .cc and .h files), and replacing #includes with forward-declares when possible.
>
> [include-what-you-use](https://include-what-you-use.org/)

To use, add the `include_what_you_use()` macro, with the arguments being the options passed to the include_what_you_use call in the form of `include-what-you-use ${ARGS}`. The settings used with include-what-you-use can be changed by calling `include_what_you_use()` macro again. It can be turned off by calling the `reset_include_what_you_use()` macro.

### cppcheck

> Cppcheck is a static analysis tool for C/C++ code. It provides unique code analysis to detect bugs and focuses on detecting undefined behaviour and dangerous coding constructs. The goal is to have very few false positives. Cppcheck is designed to be able to analyze your C/C++ code even if it has non-standard syntax (common in embedded projects). 
> 
> [cppcheck](http://cppcheck.net/)

To use, add the `cppcheck()` macro, with the arguments being the options passed to the cppcheck call in the form of `cppcheck ${ARGS}`. The settings used with iwyu can be changed by calling `cppcheck()` macro again. It can be turned off by calling the `reset_cppcheck()` macro.

## Formatting [`formatting.cmake`](formatting.cmake)

### clang-format

Allows to automatically perform code formatting using the clang-format program, by calling an easy-to-use target ala `make format`. It requires a target name, and the list of files to format. As well, if the target name is the name of another target, then all files associated with that target will be added, and the target name changed to be `format_<TARGET>`. As well, any targets otherwise listed with the files will also have their files imported for formatting.

```
file(GLOB_RECURSE ALL_CODE_FILES
    ${PROJECT_SOURCE_DIR}/src/*.[ch]pp
    ${PROJECT_SOURCE_DIR}/src/*.[ch]
    ${PROJECT_SOURCE_DIR}/include/*.[h]pp
    ${PROJECT_SOURCE_DIR}/include/*.[h]
    ${PROJECT_SOURCE_DIR}/example/*.[ch]pp
    ${PROJECT_SOURCE_DIR}/example/*.[ch]
)

clang_format(TARGET_NAME ${ALL_CODE_FILES})
```

### cmake-format

Similar to the clang-format above, creates a target `cmake-format` when the `cmake_format(<FILES>)` function is defined in CMake scripts, and any <FILES> passed in will be formatted by the cmake-format program, if it is found.

```
file(GLOB_RECURSE CMAKE_FILES
    CMakeLists.txt
)

cmake_format(TARGET_NAME ${CMAKE_FILES})
```
```

### `fortran/src/README.md`

```markdown
Information about the Fortran APIs
===================================

This directory contains Fortran APIs for HDF5 Library functionality.
A complete list of implemented Fortran subroutines can be found in the HDF5
Reference Manual.

About the source code organization
----------------------------------

The Fortran APIs are organized in modules parallel to the HDF5 Interfaces.
Each module is in a separate file with the name H5\*ff.F90.  Corresponding C
stubs are in the H5\*f.c files.  For example, the Fortran File APIs are in
the file H5Fff.F90, and the corresponding C stubs are in the file H5Ff.c.

Each module contains Fortran definitions of the constants, interfaces to
the subroutines if needed, and the subroutines themselves.

It is crucial for users to use constant names in their programs instead
of the numerical values, as the constant names have values which are
subject to change without notice.

Quick overview of the Fortran APIs
----------------------------------

*  An in-depth description of each Fortran API and its parameters can
   be found in the HDF5 Reference Manual. They tend to be summarized
   from the C descriptions.

*  The Fortran APIs come in the form of Fortran subroutines.

*  Each Fortran subroutine name is derived from the corresponding C function
   name by adding "_f" to the name.  For example, the name of the C function
   to create an HDF5 file is H5Fcreate;  the corresponding Fortran subroutine
   is h5fcreate_f.

*  The parameter list for each Fortran subroutine usually has two more parameters
   than the corresponding C function.  These additional parameters typically hold
   the return value and an error code.  The order of the Fortran subroutine
   parameters may differ from the order of the C function parameters.

   The Fortran subroutine parameters are usually listed in the following order:

      * required input parameters,
      * output parameters, including return value and error code, and
         optional input parameters.

   For example, the C function to create a dataset has the following
   prototype:

       hid_t H5Dcreate2(hid_it loc_id, char *name, hid_t type_id,
             hid_t space_id, hid_t link_creation_prp, hid_t dset_creation_prp,
             hid_t dset_access_prop);

   The corresponding Fortran subroutine has the following form:

       SUBROUTINE h5dcreate_f(loc_id, name, type_id, space_id, dset_id, &
       hdferr, dset_creation_prp, link_creation_prp, dset_access_prop)

   The first four parameters of the Fortran subroutine correspond to the
   parameters of the C function. The fifth parameter, dset_id, is an output
   parameter containing a valid dataset identifier, and the sixth
   output parameter, hdferr, indicates successful completion.
   The error code descriptions can be found in the subroutine descriptions
   of the Reference Manual. The last three input parameters are optional
   and can be omitted, in which case default values will be used.

*  Parameters to the Fortran subroutines typically include
   predefined datatypes (see the build-time generated file
   H5fortran_types.F90 for a complete listing):

        INTEGER(HID_T)      compares with hid_t type in HDF5 C APIs
        INTEGER(HSIZE_T)    compares with hsize_t in HDF5 C APIs
        INTEGER(HSSIZE_T)   compares with hssize_t in HDF5 C APIs
        INTEGER(SIZE_T)     compares with the C size_t type

   These integer types usually correspond to 4- or 8-byte integers,
   depending on the Fortran compiler and corresponding HDF5
   C library definitions.

*  Before calling HDF5 Fortran subroutines, each Fortran application must initialize Fortran datatypes
   by calling the h5open_f subroutine. After all calls to the HDF5 Fortran Library, the application
   should call the h5close_f subroutine.

*  All public APIs for the Reference Manual have Doxygen descriptions and should use parameter aliases when possible.

*  When a C application reads data stored by a Fortran program, the data appears
   to be transposed. This is because C and Fortran have different storage orders
   (row-major and column-major, respectively). For instance, if a Fortran program
   writes a 4x6 two-dimensional dataset to a file, a C program will read it into
   memory as a 6x4 two-dimensional dataset. The HDF5 C utilities h5dump and h5ls
   display transposed data if it was written from a Fortran program.

*  It is important to note that in Fortran, the indexing of arrays starts at 1.


FOR DEVELOPERS
==============

* The build system generates APIs compatible with Fortran 90 to handle the backward compatibility
  of the older F90 APIs. During the configuration process, the build system determines all
  valid integer and real KINDs, as well as the maximum decimal precision for reals and floats
  in both Fortran and C.  To determine all the available kinds, the Fortran program
  *PROGRAM FC_AVAIL_KINDS* is used, which is located in aclocal_fc.f90. The available KINDs
  are stored in H5config_f.inc, a file processed during configuration time from
  H5config_f.inc.in. Each program in m4/aclocal_fc.f90 is enclosed with a
  "!---START-----+" line and a "!---END-------+" line, which are used as markers to
  isolate each test program for the build systems.

  The valid KINDs for integers and reals that are stored in H5config_f.inc are used in the H5_buildiface.F90 file located in the fortran/src directory. During the build process, H5_buildiface.F90 generates all the valid F90 KIND interfaces for the following APIs: h5awrite_f, h5aread_f, h5dwrite_f, h5dread_f, h5pset_ï¬ll_value_f, h5pget_ï¬ll_value_f, h5pset_f, h5pget_f, h5pregister_f, and h5pinsert_f. These APIs can handle up to and including rank seven arrays for all the found KINDs. Again, it's important to note that no new Fortran APIs should be added to H5_buildiface.F90 since new Fortran APIs should not use F90 specification but should instead use F2003. The source file generated by H5_buildiface.F90 is H5_gen.F90, which is the Fortran module H5_GEN, Figure 1. This module is included in the HDF5 module HDF5.F90.

![Figure 1: During the configure and build phases, Fortran files are generated and compiled. This overview explains the flow steps of the build process.](./FortBuildFlow.svg)

Procedure to add a new function
--------------------------------

> [!IMPORTANT]  
> The use of C stubs (H5\*f.c) is no longer recommended. The C APIs should now be called from Fortran wrappers. C wrappers description exists for maintenance purposes and to create and understand alternative development options.

1. Edit the fortran/src/H5\*ff.F90 file
2. Edit the fortran/src/H5\*f.c file
3. Edit the fortran/src/H5f90proto.h file
4. Add the new function to fortran/src/hdf5_fortrandll.def.in

Procedure for passing C variables to Fortran
---------------------------------------------

(1) Find the C struct name you are interested in:

        (a) src/H5public.h if it is a generic type, i.e. H5_*
            or
        (b) src/H5*public.h if is a specific type, i.e. H5*_

(2) Put that structure into an array that will be passed to Fortran in:

        (a) fortran/src/H5_f.c (add to the h5init_flags_c subroutine)
        (b) edit fortran/src/H5f90proto.h and edit h5init_flags_c interface call

(3) Edit the function call in fortran/src/H5_ff.F90

        (a) edit the call:  FUNCTION h5init_flags_c
        (b) edit h5init_flags_c call in h5open_f to match the number of arguments being passed

(4) Add the size of the array and array to fortran/src/H5f90global.F90, it must match the size found in H5_f.c

> [!NOTE]  
> To add a default C value argument, do steps (2a) and (4).


Procedure for adding a new file to the repository
--------------------------------------------------

Add the name of the file to the:
    (1) CMakeLists.txt located in the same directory as the new file.
```

### `release_docs/README.md`

```markdown
# The `release_docs` directory

## Intro

This directory contains instructions for building and using the library as
well as the HDF5 history files.

## HISTORY files

The `HISTORY` files contain the history of this branch of HDF5. They fall into
three categories.

### HISTORY-\[VERSION 1\]-\[VERSION 2\].txt

These files are created when we release a new major version and include all
the changes that were made to the `develop` branch while creating a major release.

### HISTORY-\[VERSION\].txt

This file contains the changes that were made to a maintenance branch since
it split off from `develop`. It will also be found in the `develop` branch
when experimental releases have been created.

### CHANGELOG.md (formerly RELEASE.txt)

This is the changelog for the current version of the library.

For a MAJOR release (or in `develop`) this files lists all the changes since the
last major version. For a MINOR release (or in a maintenance branch), this file
lists all the changes since the last release in the maintenance branch.

Examples:

* The file for HDF5 1.14.0 includes all the changes since HDF5 1.12.0
* The file for HDF5 1.10.9 includes all the changes since HDF5 1.10.8
* The file in `develop` includes all the changes since the last major release
* The file in `hdf5_1_14` includes all the changes since the last minor HDF5 1.14 release

Note that we make no effort to bring maintenance branch `HISTORY` files back to
develop. If you want to compare, say, 1.10.4 with 1.12.3, you'd have to get
the history files from those releases and compare them by hand.

## Creating new releases

### MAJOR release

* If there were experimental releases, merge the experimental `HISTORY` file
  and the current `CHANGELOG.md` by category to create a separate, unified
  file that ignores the experimental releases. Don't check this in yet or
  clobber any existing `HISTORY`/`RELEASE` files, but put it someplace handy for
  use in later steps.

* Create the new maintenance branch

In develop:
* Create the new `HISTORY-\[VERSION 1\]-\[VERSION 2\].txt` file
    * If there is an experimental `HISTORY` file, add `CHANGELOG.md` to the beginning of it and use that
    * Otherwise, start with `CHANGELOG.md`
    * Add the introduction boilerplate like in the other `HISTORY` files (TOC, etc.)
* Delete any experimental `HISTORY` file
* Clear out `CHANGELOG.md`

Note that we're KEEPING any experimental release history information in the
`HISTORY-\[VERSION 1\]-\[VERSION 2\].txt` file, so do NOT use the merged file in
the above steps!

In the new maintenance branch:
* Create the new `HISTORY-\[VERSION\].txt` file
    * If there is an experimental `HISTORY` file use the combined file you created earlier
    * Otherwise, start with `CHANGELOG.md`
    * Add the introduction boilerplate like in the other `HISTORY` files (TOC, etc.)
* Delete any experimental `HISTORY` file
* Clear out `CHANGELOG.md`

* Create the new release branch

In the new release branch:
* If there were experimental releases, use the combined file you created earlier as `CHANGELOG.md`
* Otherwise the `CHANGELOG.md` will be used as-is

### MINOR release

* Create the release branch

In the maintenance branch:
* Add the contents of `CHANGELOG.md` to the beginnnig of `HISTORY-\[VERSION\].txt`
* Clear out `CHANGELOG.md`

### EXPERIMENTAL release

* Add the contents of `CHANGELOG.md` to the beginnnig of `HISTORY-\[VERSION\].txt`
* Clear out `CHANGELOG.md`

## INSTALL files

These files include instructions for building and installing HDF5 on various
platforms.

## USING files

These files document how to build HDF5 applications with an installed HDF5
library.

## Java Examples Maven Integration

The HDF5 Java examples are now integrated with Maven deployment to provide
complete example applications that work with HDF5 Java library Maven artifacts:

### Maven Artifacts

HDF5 provides two Java implementation artifacts:

* **`org.hdfgroup:hdf5-java-ffm`** - FFM (Foreign Function & Memory) implementation
  - Default for Java 25+, modern native access via Project Panama
  - Platform-specific JARs: linux-x86_64, windows-x86_64, macos-x86_64, macos-aarch64

* **`org.hdfgroup:hdf5-java-jni`** - JNI (Java Native Interface) implementation
  - Available for all Java versions (11+)
  - Platform-specific JARs: linux-x86_64, windows-x86_64, macos-x86_64, macos-aarch64

* **`org.hdfgroup:hdf5-java-examples`** - Complete collection of 62 Java examples
  - Platform-independent JAR with educational examples
  - Covers all major HDF5 functionality: datasets, datatypes, groups, tutorials
  - Compatible with both FFM and JNI implementations

Both implementations use the same `hdf.hdf5lib.*` package structure for seamless migration.

### FFM Test Suite

**Running FFM Tests:**
```bash
ctest -R "JUnitFFM" -V           # All FFM tests
ctest -R "JUnit-TestH5Affm" -V   # Attribute tests
ctest -R "JUnit-TestH5Pffm" -V   # Property tests
ctest -R "JUnit-TestH5Tffm" -V   # Datatype tests
```

**Test Location:** `java/jtest/TestH5*ffm.java`

### Features
* **Dual Implementation Support:** Choose FFM for modern Java or JNI for compatibility
* **Cross-Platform CI/CD:** Automated testing across all supported platforms
* **Fork-Based Testing:** Complete validation framework for testing on repository forks
* **Dynamic Repository Support:** Workflows adapt automatically to any GitHub repository
* **Maven Integration:** Seamless integration with standard Maven dependency management
* **Documentation:** See `HDF5Examples/JAVA/README-MAVEN.md` for complete usage instructions

### GitHub Packages Deployment

Using FFM implementation (Java 25+):
```xml
<dependency>
    <groupId>org.hdfgroup</groupId>
    <artifactId>hdf5-java-ffm</artifactId>
    <version>2.0.0-3</version>
    <classifier>linux-x86_64</classifier>
</dependency>
```

Using JNI implementation (Java 11+):
```xml
<dependency>
    <groupId>org.hdfgroup</groupId>
    <artifactId>hdf5-java-jni</artifactId>
    <version>2.0.0-3</version>
    <classifier>linux-x86_64</classifier>
</dependency>
```

The Java examples demonstrate proper usage of HDF5 Java bindings in real-world
scenarios and serve as templates for developing HDF5-based Java applications.
```

### `test/API/README.md`

```markdown
# HDF5 API Tests

This directory contains several test applications that exercise HDF5's
public API and serve as regression tests for HDF5 [VOL Connectors](https://support.hdfgroup.org/documentation/hdf5/latest/_h5_v_l__u_g.html).

## Build Process and options

These HDF5 API tests are enabled by default, but can be disabled by passing the
`-DHDF5_TEST_API=OFF` option to CMake. The following build options are available
to influence how the API tests get built:

### CMake

To set an option, it should be prepended with `-D` when passed to the `cmake` command.
For example,

    cmake -DHDF5_TEST_API=OFF ..

`HDF5_TEST_API` (Default: `ON`) - Determines whether the API tests will be built.

`HDF5_TEST_API_INSTALL` (Default: `OFF`) - Determines whether the API tests should be installed
on the system.

`HDF5_TEST_API_ENABLE_ASYNC` (Default: `OFF`) - Determines whether tests for HDF5's asynchronous
I/O capabilities should be enabled. Note that the "native" HDF5 VOL connector doesn't support
this functionality, so these tests are directed towards VOL connectors that do.

`HDF5_TEST_ENABLE_DRIVER` (Default: `OFF`) - Determines whether the API test driver program should
be built. This driver program is useful when a VOL connector relies upon a server executable
(as well as possible additional executables) in order to function. The driver program can be
supplied with a server executable and server/client arguments to use when running the API
tests.

`HDF5_TEST_API_SERVER` (Default: empty string) - If `HDF5_TEST_ENABLE_DRIVER` is set to `ON`, this
option should be edited to point to the server executable that the driver program should attempt
to launch before running the API tests.

### Autotools

Currently unsupported

### Usage

These API tests currently only support usage with the native HDF5 VOL connector and HDF5 VOL
connectors that can be loaded dynamically as a plugin. For information on how to build a VOL
connector in this manner, refer to section 2.3 of the [HDF5 VOL Connector Author Guide](https://support.hdfgroup.org/documentation/hdf5/latest/_v_o_l__connector.html).

If an HDF5 VOL connector has been built alongside the library, these API tests can be used
for testing that VOL connector at the same time that HDF5 is test. For more information on
how to build a VOL connector alongside the HDF5 library, refer to
[Building and testing HDF5 VOL connectors with CMake FetchContent](https://support.hdfgroup.org/documentation/hdf5/latest/_c_make_vols.html).

These API tests can also be used to test an HDF5 VOL connector that is external to the library.
For convenience, the `HDF5_TEST_API_INSTALL` option can be used to install these tests on the
system where other HDF5 executables (such as `h5dump`) are installed. To run these tests manually
with your VOL connector, set the following two environment variables:

`HDF5_VOL_CONNECTOR` - This environment variable should be set to the name chosen for the VOL connector
to be used. For example, HDF5's DAOS VOL connector uses the name "[daos](https://github.com/HDFGroup/vol-daos/blob/v1.2.0/src/daos_vol.h#L30)"
and would therefore set:

    HDF5_VOL_CONNECTOR=daos

`HDF5_PLUGIN_PATH` - This environment variable should be set to the directory that contains the built
library for the VOL connector to be used.

Once these are set, the HDF5 API tests will attempt to automatically load the specified VOL connector
and use it when running tests. If HDF5 is unable to locate or load the VOL connector specified, it
will fall back to running the tests with the native HDF5 VOL connector and an error similar to the
following will appear in the test output:

    HDF5-DIAG: Error detected in HDF5 (X.XX.X) MPI-process 0:
      #000: /home/user/git/hdf5/src/H5.c line 1010 in H5open(): library initialization failed
        major: Function entry/exit
        minor: Unable to initialize object
      #001: /home/user/git/hdf5/src/H5.c line 277 in H5_init_library(): unable to initialize vol interface
        major: Function entry/exit
        minor: Unable to initialize object
      #002: /home/user/git/hdf5/src/H5VLint.c line 199 in H5VL_init_phase2(): unable to set default VOL connector
        major: Virtual Object Layer
        minor: Can't set value
      #003: /home/user/git/hdf5/src/H5VLint.c line 429 in H5VL__set_def_conn(): can't register connector
        major: Virtual Object Layer
        minor: Unable to register new ID
      #004: /home/user/git/hdf5/src/H5VLint.c line 1321 in H5VL__register_connector_by_name(): unable to load VOL connector
        major: Virtual Object Layer
        minor: Unable to initialize object

### Help and Support

For help with building or using the HDF5 API tests, please contact the [HDF Help Desk](https://help.hdfgroup.org/).
```

### `tools/test/h5repack/testfiles/README`

```
h5repack_nested_8bit_enum_deflated.h5:
h5repack_nested_8bit_enum.h5:
    enuberated 8bit type nested in compound type.  Original file provided
    by a user (HDFFV-8667) as a test file. Used h5copy to extract only the
    Compound type dataset. The non-deflated version is produced by h5repack.
```

### `HDF5Examples/JAVA/README-MAVEN.md`

```markdown
# HDF5 Java Examples Maven Integration

This directory contains Java examples demonstrating the usage of HDF5 Java bindings, organized into categories and deployable as a Maven artifact.

## Directory Structure

```
HDF5Examples/JAVA/
â”œâ”€â”€ H5D/          # Dataset operations examples
â”œâ”€â”€ H5T/          # Datatype operations examples
â”œâ”€â”€ H5G/          # Group operations examples
â”œâ”€â”€ TUTR/         # Tutorial examples
â”œâ”€â”€ pom-examples.xml.in    # Maven POM template for examples
â”œâ”€â”€ CMakeLists.txt         # CMake configuration
â””â”€â”€ README-MAVEN.md        # This file
```

## Maven Artifact Usage

### Using Examples as Dependency

```xml
<dependency>
    <groupId>org.hdfgroup</groupId>
    <artifactId>hdf5-java-examples</artifactId>
    <version>2.0.0</version>
</dependency>
```

### Platform-Specific Dependencies

The examples depend on platform-specific HDF5 Java libraries:

```xml
<!-- Linux -->
<dependency>
    <groupId>org.hdfgroup</groupId>
    <artifactId>hdf5-java</artifactId>
    <version>2.0.0</version>
    <classifier>linux-x86_64</classifier>
</dependency>

<!-- Windows -->
<dependency>
    <groupId>org.hdfgroup</groupId>
    <artifactId>hdf5-java</artifactId>
    <version>2.0.0</version>
    <classifier>windows-x86_64</classifier>
</dependency>

<!-- macOS Intel -->
<dependency>
    <groupId>org.hdfgroup</groupId>
    <artifactId>hdf5-java</artifactId>
    <version>2.0.0</version>
    <classifier>macos-x86_64</classifier>
</dependency>

<!-- macOS Apple Silicon -->
<dependency>
    <groupId>org.hdfgroup</groupId>
    <artifactId>hdf5-java</artifactId>
    <version>2.0.0</version>
    <classifier>macos-aarch64</classifier>
</dependency>
```

## Building Examples with Maven

### Compile All Examples

```bash
cd HDF5Examples/JAVA
mvn compile -f pom-examples.xml
```

### Run Representative Examples

```bash
mvn test -Prun-examples -f pom-examples.xml
```

### Create Examples JAR

```bash
mvn package -f pom-examples.xml
```

This creates:
- `hdf5-java-examples-{version}.jar` - Compiled examples
- `hdf5-java-examples-{version}-sources.jar` - Source code
- `hdf5-java-examples-{version}-javadoc.jar` - Documentation

## Testing Maven Artifacts

Two standalone scripts are provided to test HDF5 Maven artifacts against the examples in this directory:

### test-maven-jni.sh - Test JNI Implementation

Tests the JNI (Java Native Interface) implementation, compatible with Java 11+.

**Usage:**
```bash
./test-maven-jni.sh [VERSION] [REPOSITORY_URL] [BUILD_DIR]
```

**Examples:**
```bash
# Test latest snapshot from HDFGroup
./test-maven-jni.sh 2.0.1-SNAPSHOT

# Test specific version from custom repository
./test-maven-jni.sh 2.0.0 https://maven.pkg.github.com/myorg/hdf5

# Use custom build directory
./test-maven-jni.sh 2.0.1-SNAPSHOT https://maven.pkg.github.com/HDFGroup/hdf5 /tmp/test
```

**What it does:**
1. Downloads `hdf5-java-jni` artifact from Maven repository
2. Verifies JAR contains HDF5 classes (not just dependencies)
3. Compiles all 55 HDF5 v2.0+ examples from `compat/` subdirectories
4. Executes 12 comprehensive tests covering major HDF5 features
5. Reports results with detailed pass/fail summary

**Prerequisites:**
- Java 21 or later (class version 65.0)
- Maven 3.6.0 or later
- GitHub authentication (for GitHub Packages)
- Optional: HDF5 native libraries or `HDF5_HOME` for execution tests

### test-maven-ffm.sh - Test FFM Implementation

Tests the FFM (Foreign Function & Memory) implementation, requires Java 25+.

**Usage:**
```bash
./test-maven-ffm.sh [VERSION] [REPOSITORY_URL] [BUILD_DIR]
```

**Examples:**
```bash
# Test FFM snapshot
./test-maven-ffm.sh 2.0.1-SNAPSHOT

# Test specific version
./test-maven-ffm.sh 2.0.0-3 https://maven.pkg.github.com/HDFGroup/hdf5
```

**What it does:**
1. Downloads `hdf5-java-ffm` artifact from Maven repository
2. Verifies JAR contains FFM bindings (`org.hdfgroup.javahdf5.*`)
3. Compiles 52 HDF5 v2.0+ examples from `compat/` subdirectories
4. Executes 12 comprehensive tests covering major HDF5 features
5. Reports results with detailed pass/fail summary

**Note:** 3 callback-based examples are excluded (H5Ex_G_Visit, H5Ex_G_Intermediate, H5Ex_G_Traverse) as FFM callback handling differs from JNI and these examples have not yet been adapted.

**Prerequisites:**
- Java 25 or later (class version 69.0)
- Maven 3.6.0 or later
- GitHub authentication (for GitHub Packages)
- Optional: HDF5 native libraries or `HDF5_HOME` for execution tests

### Build Directory Pattern

Both scripts use a separate build directory to keep the source tree clean:

**Default locations:**
- JNI: `HDF5Examples/JAVA/build/maven-test-jni/`
- FFM: `HDF5Examples/JAVA/build/maven-test-ffm/`

**Generated files:**
```
build/
â”œâ”€â”€ maven-test-jni/
â”‚   â”œâ”€â”€ pom-examples.xml       # Generated Maven POM
â”‚   â”œâ”€â”€ target/                # Compiled classes
â”‚   â”‚   â””â”€â”€ classes/
â”‚   â””â”€â”€ *.h5                   # Output HDF5 files
â””â”€â”€ maven-test-ffm/
    â”œâ”€â”€ pom-examples.xml
    â”œâ”€â”€ target/
    â””â”€â”€ *.h5
```

**Benefits:**
- âœ… Source tree stays clean (no generated files)
- âœ… Easy cleanup: `rm -rf build/`
- âœ… Multiple parallel tests possible
- âœ… CMake-like out-of-source build pattern

### GitHub Authentication

For testing artifacts from GitHub Packages, authentication is required:

**Option 1: GitHub CLI (Recommended)**
```bash
gh auth login
gh auth refresh --scopes read:packages
```

Scripts automatically detect GitHub CLI authentication.

**Option 2: Maven settings.xml**
```bash
# Scripts can create settings.xml automatically if gh is authenticated
# Or create manually:
cat > ~/.m2/settings.xml <<EOF
<settings>
  <servers>
    <server>
      <id>github-hdfgroup-hdf5</id>
      <username>YOUR_GITHUB_USERNAME</username>
      <password>YOUR_GITHUB_TOKEN</password>
    </server>
  </servers>
</settings>
EOF
```

### Running Additional Examples

After initial test succeeds, you can run more examples:

**JNI:**
```bash
cd build/maven-test-jni
mvn exec:java -Dexec.mainClass="H5Ex_T_String" -f pom-examples.xml
```

**FFM:**
```bash
cd build/maven-test-ffm
mvn exec:java -Dexec.mainClass="H5Ex_T_String" -f pom-examples.xml
```

### Cleanup

**Remove single test build:**
```bash
rm -rf build/maven-test-jni
rm -rf build/maven-test-ffm
```

**Remove all test builds:**
```bash
rm -rf build/
```

**Clean with Maven (keeps directory structure):**
```bash
mvn clean -f build/maven-test-jni/pom-examples.xml
mvn clean -f build/maven-test-ffm/pom-examples.xml
```

### Troubleshooting Test Scripts

**"Failed to download artifact"**
- Check GitHub authentication: `gh auth status`
- Verify repository URL is correct
- Ensure version exists in repository

**"JAR does not contain HDF5 classes"**
- Indicates incomplete Maven artifact (build issue)
- This is what the verification step catches!
- Report to maintainers if public artifact is incomplete

**"Java version too old"**
- JNI requires Java 11+
- FFM requires Java 25+
- Check: `java -version`

**"UnsatisfiedLinkError: no hdf5_java"**
- This is expected during Maven-only testing
- Indicates JAR structure is correct
- Native libraries would be needed for full execution

## Example Categories

### H5D - Dataset Operations
- Basic read/write operations
- Chunking and compression
- External storage
- Fill values and allocation
- Filters (gzip, checksum, nbit, etc.)

### H5T - Datatype Operations
- Array datatypes
- Compound datatypes
- Enumerated datatypes
- Opaque datatypes
- String handling
- Variable-length datatypes

### H5G - Group Operations
- Creating and managing groups
- Group iteration
- Intermediate group creation
- Group hierarchy traversal

### TUTR - Tutorial Examples
- Step-by-step learning examples
- Basic concepts demonstration
- Progressive complexity

## CI/CD Integration

The examples are automatically tested in CI:

1. **Compilation Testing**: All examples must compile successfully
2. **Execution Testing**: Examples are run and output validated
3. **Cross-Platform Testing**: Tested on Linux, Windows, and macOS
4. **Maven Integration Testing**: Tests against staging Maven artifacts

### Maven-Only Testing Behavior

**Expected Native Library Errors**: During Maven-only testing (without HDF5 installation), examples will compile successfully but fail at runtime with:
```
UnsatisfiedLinkError: no hdf5_java in java.library.path
```

This is **expected behavior** and indicates:
- âœ… **JAR structure is correct**
- âœ… **Dependencies resolve properly**
- âœ… **Compilation succeeds**
- âš ï¸ **Native HDF5 libraries not available** (expected in Maven-only environment)

### Running Examples Successfully

To actually execute examples (not just compile them), you need HDF5 native libraries installed:

#### Option 1: Install HDF5 from Package Manager (Recommended)

**Linux (Ubuntu/Debian):**
```bash
sudo apt-get update
sudo apt-get install libhdf5-dev hdf5-tools
```

**Linux (Fedora/RHEL):**
```bash
sudo dnf install hdf5 hdf5-devel
```

**macOS (Homebrew):**
```bash
brew install hdf5
```

**Windows:**
- Download pre-built binaries from [HDF Group Downloads](https://www.hdfgroup.org/downloads/hdf5/)
- Set `HDF5_HOME` environment variable to installation directory
- Alternatively, add HDF5 `bin` directory to system PATH

#### Option 2: Build HDF5 from Source

Build HDF5 with Java support enabled:

```bash
# Clone HDF5 repository
git clone https://github.com/HDFGroup/hdf5.git
cd hdf5

# Build with Java (JNI)
cmake --preset ci-StdShar-GNUC --fresh
cmake --build build/ci-StdShar-GNUC
sudo cmake --install build/ci-StdShar-GNUC

# Or build with Java (FFM) - requires Java 25+
cmake --preset ci-StdShar-GNUC-FFM --fresh
cmake --build build/ci-StdShar-GNUC-FFM
sudo cmake --install build/ci-StdShar-GNUC-FFM
```

#### Option 3: Set HDF5_HOME (Recommended for Custom Installations)

If HDF5 is installed in a non-standard location, set `HDF5_HOME`:

**Linux/macOS:**
```bash
# Point to HDF5 installation directory
export HDF5_HOME=/path/to/hdf5/installation

# Then run examples (scripts automatically find libraries)
cd HDF5Examples/JAVA
./test-maven-jni.sh 2.0.1-SNAPSHOT

# Or run Maven directly
cd build/maven-test-jni
mvn exec:java -Dexec.mainClass="H5Ex_D_ReadWrite" -f pom-examples.xml
```

**Windows (PowerShell):**
```powershell
# Set HDF5_HOME environment variable
$env:HDF5_HOME = "C:\path\to\hdf5\installation"

# Run Maven examples
cd build\maven-test-jni
mvn exec:java -Dexec.mainClass="H5Ex_D_ReadWrite" -f pom-examples.xml
```

**Windows (CMD):**
```cmd
REM Set HDF5_HOME environment variable
set HDF5_HOME=C:\path\to\hdf5\installation

REM Run Maven examples
cd build\maven-test-jni
mvn exec:java -Dexec.mainClass="H5Ex_D_ReadWrite" -f pom-examples.xml
```

**Note:** The test scripts automatically add `${HDF5_HOME}/lib` (Unix) or `%HDF5_HOME%\bin` (Windows) to the library path.

#### Option 4: Specify Library Path in Java (Advanced)

**Note:** This is an advanced option. Prefer using `HDF5_HOME` (Option 3) instead.

```bash
# Run with explicit library path
java -Djava.library.path=/path/to/hdf5/lib \
     -cp "target/classes:~/.m2/repository/org/hdfgroup/hdf5-java-jni/2.0.1-SNAPSHOT/*" \
     H5Ex_D_ReadWrite
```

#### Verify Native Libraries Are Found

After installing HDF5, verify the libraries are accessible:

**Linux:**
```bash
# Check library is in system path
ldconfig -p | grep hdf5

# Or find library location
find /usr -name "libhdf5.so*" 2>/dev/null
```

**macOS:**
```bash
# Check library location
find /usr/local -name "libhdf5*.dylib" 2>/dev/null
```

**Windows:**
```cmd
# Check PATH includes HDF5 bin directory
echo %PATH%

# Verify DLL exists
where hdf5.dll
```

#### Running Examples After Library Installation

Once native libraries are installed, examples should run successfully:

**JNI Examples:**
```bash
cd build/maven-test-jni
mvn exec:java -Dexec.mainClass="H5Ex_D_ReadWrite" -f pom-examples.xml

# Expected output:
# Dataset successfully created and written
# Data read from dataset: [1, 2, 3, 4, ...]
```

**FFM Examples:**
```bash
cd build/maven-test-ffm
mvn exec:java -Dexec.mainClass="H5Ex_D_ReadWrite" -f pom-examples.xml

# Expected output:
# Dataset successfully created and written
# Data read from dataset: [1, 2, 3, 4, ...]
```

#### Why Maven Artifacts Don't Include Native Libraries

Maven artifacts contain only:
- âœ… Java bytecode (.class files)
- âœ… Java source code (in -sources.jar)
- âœ… Javadoc (in -javadoc.jar)

They do **not** include:
- âŒ Native shared libraries (.so, .dll, .dylib)
- âŒ Platform-specific binaries

**Reason:** Native libraries are platform-specific and typically hundreds of MB. Maven artifacts should be small (~2-5 MB) and platform-independent where possible. The JNI/FFM bindings provide the Java interface, but you must install the native HDF5 libraries separately.

### Pattern-Based Output Validation

Examples are validated using pattern matching for:
- **Success patterns**: `dataset|datatype|group|success|created|written|read`
- **Expected failures**: `UnsatisfiedLinkError.*hdf5_java.*java.library.path` (Maven-only testing)
- **Unexpected failures**: Other errors indicating JAR or compilation issues

### Non-Blocking Failures

- Individual example failures don't block CI
- Native library errors are treated as **expected** in Maven-only testing
- Multi-platform failures for the same example trigger alerts
- Results are uploaded as artifacts for debugging

## Development Workflow

### Adding New Examples

1. Add `.java` file to appropriate category directory
2. Update CMakeLists.txt if needed
3. Examples are automatically discovered by Maven and CI

### Testing Changes

```bash
# Test specific category
cd H5D && javac -cp "../../../maven-artifacts/*.jar" *.java

# Run example
java -cp ".:../../../maven-artifacts/*" H5Ex_D_ReadWrite
```

### Expected Output Files

Expected outputs for validation are stored in version control:
- `tfiles/min_hdf_version/H5Ex_D_ReadWrite.txt`
- Pattern-based validation for flexibility
- Platform-specific outputs handled automatically

## Deployment

Examples are deployed alongside main HDF5 Maven artifacts:

1. Built during Maven staging workflow
2. Tested in dedicated Java examples workflow
3. Deployed to GitHub Packages
4. Available for Maven Central deployment

## Usage in Projects

### Quick Start

```java
import hdf.hdf5lib.H5;
import hdf.hdf5lib.HDF5Constants;

// Use examples as reference
// Source code available in JAR resources at examples/
```

### Maven Archetype (Future)

```bash
mvn archetype:generate \
  -DgroupId=com.example \
  -DartifactId=my-hdf5-project \
  -DarchetypeGroupId=org.hdfgroup \
  -DarchetypeArtifactId=hdf5-java-archetype
```

## Troubleshooting

### Common Issues

1. **Platform Mismatch**: Ensure correct classifier for your platform
2. **Native Library Path**: HDF5 native libraries loaded automatically
3. **Java Version**: Requires Java 11 or higher

### Debug Information

Examples JAR includes manifest entries:
- `HDF5-Version`: HDF5 library version
- `HDF5-Platform`: Target platform
- `Examples-Count`: Number of included examples

## Support

- GitHub Issues: https://github.com/HDFGroup/hdf5/issues
- Documentation: https://support.hdfgroup.org/documentation/
- Examples Source: Included in JAR resources
```

### `release_docs/README_HPC.md`

```markdown
# Installation Instructions for Parallel HDF5

## 1. Overview

This file contains instructions for installing parallel HDF5 (PHDF5) using
CMake. The document covers:

- **Section 1:** Requirements and prerequisites
- **Section 2:** Obtaining HDF5 source
- **Section 3:** Quick start instructions
- **Section 4:** Automated builds with ctest (HPC systems)
- **Section 5:** Manual CMake configuration
- **Section 6:** Cross-compiling for HPC hardware
- **Section 7:** Running parallel tests
- **Appendix A:** Sample MPI-IO programs

### 1.1. Requirements

PHDF5 requires:

- CMake version 3.26 or greater
- An MPI compiler with MPI-IO support
- A POSIX compliant parallel file system (see References)

You should first consult with your system support staff for information on:

- How to compile an MPI program
- How to run an MPI application
- How to access the parallel file system

Sample MPI-IO C and Fortran programs are provided in Appendix A. Use them to
test your MPI compiler and parallel file system before building HDF5.

### 1.2. Prerequisites for HPC Systems

When building on HPC systems:

1. **Create a working directory** accessible from compute nodes for running tests.
   Use a scratch space or parallel file system - **NOT** your home directory, as
   this typically causes test failures.

2. **Load required modules:**
   - Desired compiler modules (and set CC, FC, CXX if needed)
   - CMake version 3.26 or greater
   - MPI implementation module

3. **For Cray and other systems with recommend compiler wrappers,** set compiler environment variables AFTER loading modules:

   ```bash
   export CC=cc
   export FC=ftn
   export CXX=CC
   ```

### 1.3. Further Help

For installation help, post questions to the HDF Forum or HDF Support:

- **HDF Forum:** <https://forum.hdfgroup.org/>
- **HDF Support:** <https://support.hdfgroup.org/>

Include the output of `uname -a` and the contents of `CMakeCache.txt` and
`CMakeError.log` from your build directory, and the loaded modules if applicable.

---

## 2. Obtaining HDF5 Source

Obtain HDF5 source code from the HDF5 repository or from a release tar file:

```bash
git clone https://github.com/HDFGroup/hdf5.git [-b branch] [directory]
```

If no branch is specified, the `develop` branch will be checked out.
If no directory is specified, source will be in the `hdf5` directory.

For release or snapshot tar files, extract them to your working directory.

> **Note:** When using the ctest automated build method (Section 4), the source
> directory should be named `hdf5-<version>`, where version uses format `1.xx.xx`
> or `2.xx.xx`. Use `bin/h5vers` to determine the version string if needed.

---

## 3. Quick Start Instructions

### 3.1. Using CMake Presets (Recommended for General Builds)

For building with CMake 3.26 or greater using presets:

```bash
cd hdf5
cmake --workflow --preset ci-StdShar-GNUC --fresh
```

> **Note:** Standard presets do not enable parallel by default. To enable parallel
> support, you need to create a custom preset in `CMakeUserPresets.json` that sets
> `-DHDF5_ENABLE_PARALLEL=ON`, or use the standard CMake build approach below.

### 3.2. Standard CMake Build (Recommended for Parallel)

For a basic parallel build:

```bash
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Release \
      -DHDF5_ENABLE_PARALLEL=ON \
      -DBUILD_SHARED_LIBS=ON \
      -DBUILD_TESTING=ON \
      ..
cmake --build . --config Release
ctest . -C Release
cmake --install . --prefix /path/to/install
```

### 3.3. Specifying MPI Compiler

CMake can usually find MPI automatically. To specify explicitly:

```bash
cmake -DCMAKE_C_COMPILER=mpicc \
      -DCMAKE_Fortran_COMPILER=mpif90 \
      -DHDF5_ENABLE_PARALLEL=ON \
      ..
```

Or use MPI compiler wrappers:

```bash
export CC=mpicc
export FC=mpif90
cmake -DHDF5_ENABLE_PARALLEL=ON ..
```

### 3.4. Important Configuration Options

| Option | Description |
|--------|-------------|
| `-DHDF5_ENABLE_PARALLEL=ON` | Enable parallel HDF5 (required) |
| `-DBUILD_SHARED_LIBS=ON` | Build shared libraries |
| `-DBUILD_STATIC_LIBS=ON` | Build static libraries |
| `-DHDF5_BUILD_FORTRAN=ON` | Build Fortran interface |
| `-DHDF5_BUILD_CPP_LIB=OFF` | C++ disabled in parallel builds |
| `-DHDF5_ENABLE_THREADSAFE=OFF` | Thread safety disabled in parallel builds |
| `-DHDF5_ENABLE_SUBFILING_VFD=ON` | Enable subfiling VFD (parallel I/O optimization) |
| `-DMPIEXEC_EXECUTABLE=mpiexec` | MPI launcher executable |
| `-DMPIEXEC_NUMPROC_FLAG=-n` | MPI flag for number of processes |
| `-DMPIEXEC_MAX_NUMPROCS=6` | Number of processes for tests |

> **Note:** Some MPI implementations (e.g., OpenMPI 4.0+) disallow oversubscribing
> by default. Set `MPIEXEC_MAX_NUMPROCS` to available processors or fewer, or add
> appropriate flags (e.g., `--oversubscribe` for OpenMPI).

---

## 4. Automated Builds with ctest (HPC Systems)

The ctest command provides automated configure, build, test, and package
workflow for HPC systems with batch schedulers.

### 4.1. Setup Steps

1. Rename source directory to `hdf5-<version>` (e.g., `hdf5-2.0.0`)

2. Copy or link these CMake scripts to your working directory:
   - `hdf5-<version>/config/cmake/scripts/HDF5config.cmake`
   - `hdf5-<version>/config/cmake/scripts/CTestScript.cmake`
   - `hdf5-<version>/config/cmake/scripts/HDF5options.cmake`

3. Your working directory should contain:
   ```
   CTestScript.cmake
   HDF5config.cmake
   HDF5options.cmake
   hdf5-<version>/
   ```

### 4.2. Running ctest

Basic ctest command:

```bash
ctest -S HDF5config.cmake,BUILD_GENERATOR=Unix -C Release -V -O hdf5.log
```

For parallel builds on HPC systems with batch schedulers:

```bash
ctest -S HDF5config.cmake,HPC=sbatch,MPI=true -C Release -V -O hdf5.log
```

#### Available HPC Options

Add after `HDF5config.cmake,` separated by commas:

| Option | Description |
|--------|-------------|
| `HPC=sbatch` | Use SLURM batch system |
| `HPC=bsub` | Use LSF batch system |
| `MPI=true` | Enable parallel (disables C++, Java, threadsafe) |
| `LOCAL_BATCH_SCRIPT_ARGS="--account=<acct>"` | Supply batch job account information |

#### Examples

**SLURM system with parallel:**

```bash
ctest -S HDF5config.cmake,HPC=sbatch,MPI=true \
      -C Release -V -O hdf5.log
```

Use `-VV` instead of `-V` for more detailed logging.

---

## 5. Manual CMake Configuration

If the automated ctest approach is not suitable, you can manually configure
and build HDF5.

### 5.1. Create Build Directory

```bash
mkdir build && cd build
```

### 5.2. Run CMake Configure

Example for parallel build with Fortran on HPC system:

```bash
cmake \
  -C ../hdf5-<version>/config/cmake/cacheinit.cmake \
  -DCMAKE_BUILD_TYPE:STRING=Release \
  -DCMAKE_INSTALL_PREFIX:PATH=/install/path \
  -DHDF5_ENABLE_PARALLEL:BOOL=ON \
  -DHDF5_BUILD_FORTRAN:BOOL=ON \
  -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
  -DHDF5_BUILD_JAVA:BOOL=OFF \
  -DHDF5_ENABLE_THREADSAFE:BOOL=OFF \
  -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF \
  -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF \
  -DMPIEXEC_EXECUTABLE:STRING=srun \
  -DMPIEXEC_NUMPROC_FLAG:STRING=-n \
  -DMPIEXEC_MAX_NUMPROCS:STRING=6 \
  -G"Unix Makefiles" \
  ../hdf5-<version>
```

### 5.3. Build

```bash
cmake --build . --config Release -j 8
```

### 5.4. Test

For systems where you can run MPI directly:

```bash
ctest . -C Release
```

For batch systems, create and submit batch scripts (see section 7.3).

### 5.5. Install

```bash
cmake --install . --prefix /install/path
```

---

## 6. Cross-Compiling for HPC Hardware

### 6.1. Overview

Cross-compiling is the process of building software on one system architecture (like a login node) to be run on a different architecture (like a compute node). This section provides a historical overview of how this was done on systems that are no longer in service.

### 6.2. Historical Example: Knights Landing (KNL) on Cray XC40

A common historical use case was compiling for Intel Knights Landing (KNL) nodes on Cray XC40 systems, such as the retired Mutrino and Cori machines. These supercomputers had login nodes with a standard CPU architecture (e.g., Haswell) but used the different KNL architecture for their compute nodes.

To build software for KNL, a "module swapping" technique was required. The build process involved:
1. Loading the compiler module for the login node architecture (e.g., `craype-haswell`) to configure the project.

2. Switching to the compiler module for the compute node architecture (e.g., `craype-mic-knl`) before starting the actual compilation.

This process was managed by special CMake toolchain files and custom batch scripts, which were often automated within the `ctest` framework.

### 6.3. Cross-Compilation on Current Systems

The specific hardware (Cray XC40, KNL) and the build procedures described above are historical and no longer in use.

While cross-compilation is less common on many modern, homogeneous HPC clusters, it is still a necessary technique for advanced architectures, such as systems with different processor types or accelerators (e.g., GPUs).

**If you need assistance with cross-compiling for a current HPC system, please contact the facility administrators or The HDF Group (Section 1.3).**

---

## 7. Running Parallel Tests

### 7.1. Test Directory Location

The parallel test suite (`testpar/`) contains tests for Parallel HDF5 and MPI-IO.

By default, tests use the current directory for test files. To specify a
different location (e.g., parallel file system):

```bash
export HDF5_PARAPREFIX=/scratch/username/hdf5test
ctest . -C Release
```

> **Important:** This is critical for performance - avoid using NFS or home
> directories for parallel testing.

### 7.2. Important Test Notes

#### t_mpi

Tests basic MPI-IO features used by Parallel HDF5. Returns non-zero
exit code if required features fail.

**Exception:** Testing files >2GB will print informational messages but not fail,
as HDF5 can use alternative file drivers (e.g., family driver) to handle size limits.

#### t_cache

Performs many small I/O requests. May run slowly on NFS or other
non-parallel file systems. Set `HDF5_PARAPREFIX` to a parallel file system if
this test is slow.

#### Test Express Level

Controls test thoroughness:

```bash
export HDF5_TEST_EXPRESS=3    # Quick tests (default)
export HDF5_TEST_EXPRESS=0    # Exhaustive tests
```

#### Test Timeout

Default is 1200 seconds (20 minutes). Modify in CMake with:

```bash
-DDART_TESTING_TIMEOUT=3600
```

### 7.3. Running Tests on Batch Systems

**For SLURM systems:**

```bash
sbatch -C quad,cache build/bin/batch/ctestS.sl    # Serial tests
sbatch -C quad,cache build/bin/batch/ctestP.sl    # Parallel tests
```

**For LSF systems:**

```bash
bsub build/bin/batch/ctestS.lsf    # Serial tests
bsub build/bin/batch/ctestP.lsf    # Parallel tests
```

Batch scripts are generated during CMake configuration in the build directory.

### 7.4. Running Specific Test Categories

To run specific test suites:

```bash
ctest -R "H5TEST"              # Core library tests
ctest -R "MPI_TEST"            # Parallel/MPI tests
ctest -R "CPP|FORTRAN"         # C++ and Fortran tests
ctest -E "MPI_TEST"            # Exclude parallel tests
ctest --parallel 4             # Run 4 tests in parallel
```

---

## 8. Known Platform Notes

### 8.1. Linux Systems

For MPICH on Linux, ensure >2GB file support by configuring MPICH with:

```bash
-cflags="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64"
```

This is available on Linux kernels 2.4 and greater.

### 8.2. Cray Systems

- Use `CC=cc`, `FC=ftn`, `CXX=CC` after loading compiler modules
- Unload `craype-hugepages2M` if loaded (**Note**: This is situational advice and is not a universal rule, but it may be a valid troubleshooting step if you encounter memory-related performance issues or allocation errors.)
- Disable shared libraries if encountering linking issues:
  ```bash
  -DBUILD_SHARED_LIBS=OFF
  ```

### 8.3. OpenMPI

OpenMPI 4.0+ disallows oversubscribing by default. Either:

- Set `MPIEXEC_MAX_NUMPROCS` to actual processor count, or
- Add `--oversubscribe` flag: `-DMPIEXEC_PREFLAGS=--oversubscribe`

---

## References

**POSIX Compliant:** After a write() to a regular file has successfully
returned, any successful read() from each byte position modified by that
write() will return the data that was written. A subsequent write() to the
same byte will overwrite the file data. If a read() can be proven by any
means [e.g., MPI_Barrier()] to occur after a write() of that data, it must
reflect that write(), even if calls are made by different processes.

> Lewin, D. (1994). "POSIX Programmer's Guide (pg. 513-4)". O'Reilly &
> Associates.

---

## Appendix A. Sample MPI-IO Programs

Here are sample MPI-IO C and Fortran programs to test your MPI compiler and
parallel file system before building HDF5. The programs assume they run in
a parallel file system (create test files in current directory). For more
examples, please refer to the following directories:
HDF5Examples/C/H5PAR and HDF5Examples/FORTRAN/H5PAR

### Example Compiling and Running

```bash
mpicc Sample_mpio.c -o c.out
mpiexec -np 4 c.out

mpif90 Sample_mpio.f90 -o f.out
mpiexec -np 4 f.out
```

### Sample_mpio.c

```c
/* Simple MPI-IO program testing if a parallel file can be created.
 * Default filename can be specified via first program argument.
 * Each process writes something, then reads all data back.
 */

#include <stdio.h>
#include <unistd.h>
#include <mpi.h>
#ifndef MPI_FILE_NULL           /*MPIO may be defined in mpi.h already       */
#   include <mpio.h>
#endif

#define DIMSIZE	10		/* dimension size, avoid powers of 2. */
#define PRINTID printf("Proc %d: ", mpi_rank)

int main(int ac, char **av)
{
    char hostname[128];
    int  mpi_size, mpi_rank;
    MPI_File fh;
    char *filename = "./mpitest.data";
    char mpi_err_str[MPI_MAX_ERROR_STRING];
    int  mpi_err_strlen;
    int  mpi_err;
    char writedata[DIMSIZE], readdata[DIMSIZE];
    char expect_val;
    int  i, irank;
    int  nerrors = 0;		/* number of errors */
    MPI_Offset  mpi_off;
    MPI_Status  mpi_stat;

    MPI_Init(&ac, &av);
    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);

    /* get file name if provided */
    if (ac > 1){
	filename = *++av;
    }
    if (mpi_rank==0){
	printf("Testing simple MPIO program with %d processes accessing file %s\n",
	    mpi_size, filename);
        printf("    (Filename can be specified via program argument)\n");
    }

    /* show the hostname so that we can tell where the processes are running */
    if (gethostname(hostname, 128) < 0){
	PRINTID;
	printf("gethostname failed\n");
	return 1;
    }
    PRINTID;
    printf("hostname=%s\n", hostname);

    if ((mpi_err = MPI_File_open(MPI_COMM_WORLD, filename,
	    MPI_MODE_RDWR | MPI_MODE_CREATE | MPI_MODE_DELETE_ON_CLOSE,
	    MPI_INFO_NULL, &fh))
	    != MPI_SUCCESS){
	MPI_Error_string(mpi_err, mpi_err_str, &mpi_err_strlen);
	PRINTID;
	printf("MPI_File_open failed (%s)\n", mpi_err_str);
	return 1;
    }

    /* each process writes some data */
    for (i=0; i < DIMSIZE; i++)
	writedata[i] = mpi_rank*DIMSIZE + i;
    mpi_off = mpi_rank*DIMSIZE;
    if ((mpi_err = MPI_File_write_at(fh, mpi_off, writedata, DIMSIZE, MPI_BYTE,
	    &mpi_stat))
	    != MPI_SUCCESS){
	MPI_Error_string(mpi_err, mpi_err_str, &mpi_err_strlen);
	PRINTID;
	printf("MPI_File_write_at offset(%ld), bytes (%d), failed (%s)\n",
		(long) mpi_off, (int) DIMSIZE, mpi_err_str);
	return 1;
    };

    /* make sure all processes has done writing. */
    MPI_Barrier(MPI_COMM_WORLD);

    /* each process reads all data and verify. */
    for (irank=0; irank < mpi_size; irank++){
	mpi_off = irank*DIMSIZE;
	if ((mpi_err = MPI_File_read_at(fh, mpi_off, readdata, DIMSIZE, MPI_BYTE,
		&mpi_stat))
		!= MPI_SUCCESS){
	    MPI_Error_string(mpi_err, mpi_err_str, &mpi_err_strlen);
	    PRINTID;
	    printf("MPI_File_read_at offset(%ld), bytes (%d), failed (%s)\n",
		    (long) mpi_off, (int) DIMSIZE, mpi_err_str);
	    return 1;
	};
	for (i=0; i < DIMSIZE; i++){
	    expect_val = irank*DIMSIZE + i;
	    if (readdata[i] != expect_val){
		PRINTID;
		printf("read data[%d:%d] got %d, expect %d\n", irank, i,
			readdata[i], expect_val);
		nerrors++;
	    }
	}
    }
    if (nerrors)
	return 1;

    MPI_File_close(&fh);

    PRINTID;
    printf("all tests passed\n");

    MPI_Finalize();
    return 0;
}
```

### Sample_mpio.f90

```fortran
!
! The following example demonstrates how to create and close a parallel
! file using MPI-IO calls.
!
! USE MPI is the proper way to bring in MPI definitions but many
! MPI Fortran compiler supports the pseudo standard of INCLUDE.
! So, HDF5 uses the INCLUDE statement instead.
!

     PROGRAM MPIOEXAMPLE

     USE mpi

     IMPLICIT NONE

     CHARACTER(LEN=80), PARAMETER :: filename = "filef.h5" ! File name
     INTEGER     ::   ierror  ! Error flag
     INTEGER     ::   fh      ! File handle
     INTEGER     ::   amode   ! File access mode

     call MPI_INIT(ierror)
     amode = MPI_MODE_RDWR + MPI_MODE_CREATE + MPI_MODE_DELETE_ON_CLOSE
     call MPI_FILE_OPEN(MPI_COMM_WORLD, filename, amode, MPI_INFO_NULL, fh, ierror)
     print *, "Trying to create ", filename
     if ( ierror .eq. MPI_SUCCESS ) then
        print *, "MPI_FILE_OPEN succeeded"
        call MPI_FILE_CLOSE(fh, ierror)
     else
        print *, "MPI_FILE_OPEN failed"
     endif

     call MPI_FINALIZE(ierror);
     END PROGRAM
```
```

### `.devcontainer/Dockerfile`

```dockerfile
FROM mcr.microsoft.com/devcontainers/base:ubuntu

RUN apt-get update && apt-get -y install --no-install-recommends \
  build-essential clang cmake cmake-curses-gui default-jdk doxygen gfortran git graphviz \
  less libtool-bin libyajl-dev mpi-default-dev ninja-build pkg-config valgrind wget
```

### `.gitignore`

```
# .gitignore file for HDF5

# Java .classes files
**/java/.classes

# Backup files
# Editor-specific extensions should go in your ~/.gitconfig, but we
# include the standard Unix backup '~' extension since autoheader
# backs up H5config.h.in when it creates a new one and the
# extension is always '~'.
**/*~

# Other files, most of which are created by scripts in bin/
/build*
src/H5config.h.in

/.classpath
.claude/
/CLAUDE.md
/CMakeUserPresets.json
HDF5Examples/CMakeUserPresets.json
/CLAUDE.md
```

### `CODE_OF_CONDUCT.md`

```markdown
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socioeconomic status,
nationality, personal appearance, race, religion, or sexual identity
and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
  advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
  address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
help@hdfgroup.org.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or
permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior,  harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0, available at
https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at
https://www.contributor-covenant.org/translations.
```

### `CONTRIBUTING.md`

```markdown
# How to contribute to HDF5 development

Welcome to the HDF5 development community! This comprehensive guide covers everything you need to know
about contributing to HDF5, from getting started to submitting your changes.

> [!IMPORTANT]
> No contribution can be accepted unless the contributor agrees to the HDF Group's software license terms,
  which can be found in the LICENSE file located in the top source directory of every branch.

## Table of Contents

- [Getting Started](#getting-started)
- [Prerequisites](#prerequisites)
- [Getting the Source Code](#getting-the-source-code)
- [Building for Development](#building-for-development)
- [Source Code Overview](#source-code-overview)
- [Development Conventions](#development-conventions)
- [Contributing Changes](#contributing-changes)
- [Testing](#testing)
- [Documentation](#documentation)
- [Command-Line Tools](#command-line-tools)
- [Checklist for Contributors](#checklist-for-contributors)
- [Getting Help](#getting-help)


---

## Getting Started

The HDF Group welcomes contributions of all kinds, from fixing typos to adding significant features. We are
dedicated to making the contribution process enjoyable and straightforward.

> [!NOTE]
> This guide offers a brief introduction to the HDF5 library and its development procedures. In contrast,
  [An Overview of the HDF5 Library Architecture][u1] aims to provide a comprehensive understanding of the inner workings
  of the HDF5 library by exploring its fundamental principles. It covers the systematic, structural, and organized aspects
  that enable the library to function clearly and effectively. By reviewing this document, readers can gain insights into
  the library's architecture and learn how to use it efficiently. Additionally, it will provide an overview of the various
  approaches used to simplify the understanding of the HDF5 library's operations.

---

## Prerequisites

Before you begin, ensure your development machine has:

### Required Tools
* **A C11-compatible C compiler** (MSVC on Windows is supported).
* **A build system:** **CMake** is required.
* **Perl:** Needed to run build and test scripts, even on Windows.
* **Git:** For version control.
  - If you are new to Git and GitHub, we encourage you to check out
    the [GitHub tutorial](https://guides.github.com/activities/hello-world/), which takes about 10 minutes to complete.

### Recommended Tools
* **clang-format:** For code formatting. The CI system will automatically format pull requests if needed.
* **codespell:** For identifying spelling issues before submission.
* **Doxygen:** For compiling the documentation.

### Optional Components
Depending on which features you want to build or enable:
* A _C++11_-compatible compiler for the C++ wrappers.
* A _Fortran 2003_-compatible compiler for the Fortran wrappers.
* A _Java 8_-compatible compiler for the Java wrappers.
* **Maven** for Java artifact deployment and validation (when `HDF5_ENABLE_MAVEN_DEPLOY=ON`).
* `flex`/`lex` and `bison`/`yacc` if you want to modify the high-level parsers.
* Development versions of **zlib** and **szip** for compression support.
* An MPI-3 compatible MPI library for parallel HDF5 development.
* `curl` and other components for the read-only S3 VFD.

---

## Getting the Source Code

The HDF5 source code is hosted on GitHub:

```bash
git clone https://github.com/HDFGroup/hdf5.git
cd hdf5
```

---

## Building for Development

### Basic CMake Build

CMake is the required build system for all platforms:

1. **Create a build directory:**
   ```bash
   mkdir build && cd build
   ```

2. **Configure the build:**
   ```bash
   cmake -G "Unix Makefiles" -DHDF5_ENABLE_DEVELOPER_MODE=ON ..
   ```
   The `HDF5_ENABLE_DEVELOPER_MODE` option enables debug symbols, warnings as errors, and other developer-friendly settings.

3. **Build the library:**
   ```bash
   make
   ```

### Developer Build Tips

* **Memory Checking:** Use `HDF5_ENABLE_USING_MEMCHECKER:BOOL=ON` when using tools like Valgrind. This disables
                       internal memory pools that can hide memory issues.
* **Developer Warnings:** Enable extra warnings with `HDF5_ENABLE_DEV_WARNINGS:BOOL=ON` (generates significant
                          output but can be useful).
* **Warnings as Errors:** The CI system builds with `-Werror`, so fix all compiler warnings before submitting pull requests.

### Maven Integration Development

For developers working on Java bindings and Maven integration:

* **Enable Maven Support:** Use `HDF5_ENABLE_MAVEN_DEPLOY:BOOL=ON` to enable Maven artifact generation.
* **Snapshot Builds:** Use `HDF5_MAVEN_SNAPSHOT:BOOL=ON` for development builds with `-SNAPSHOT` versions.
* **Maven Presets:** Use Maven-enabled CMake presets for consistent builds:
  ```bash
  # For full builds (includes all components)
  cmake --workflow --preset ci-StdShar-GNUC-Maven-Snapshot --fresh

  # For Java artifact generation only (recommended for Maven development)
  cmake --workflow --preset ci-MinShar-GNUC-Maven-Snapshot --fresh
  ```
* **Artifact Validation:** Test Maven artifacts using `.github/scripts/validate-maven-artifacts.sh` script.
* **Repository Testing:** Use the `maven-staging.yml` workflow for pull request validation.

---

## Source Code Overview

Here's where to find things in the source tree:

* **`src/`**: Main C library source code
* **`test/`**: C library test code
* **`testpar/`**: Parallel C library test code
* **`tools/`**: Command-line tools (h5dump, h5repack, etc.)
* **`HDF5Examples/`**: Library examples including Java examples with Maven integration
* **`hl/`**: High-level library source, tests, and examples
* **`c++/`**: C++ language wrapper
* **`fortran/`**: Fortran language wrapper
* **`java/`**: JNI/Java language wrapper
* **`bin/`**: Build scripts and miscellaneous tools
* **`config/`**: Configuration files for CMake
* **`doxygen/`**: Doxygen build files and documentation
* **`release_docs/`**: Install instructions and release notes
* **`utils/`**: Small utility programs

---

## Development Conventions

### Code Organization: Public, Private, and Package

HDF5 code is organized into *packages* that encapsulate related functionality (e.g., `H5D` for datasets).
Functions have three visibility levels:

* **Public:** User-facing API functions
  * **Format:** `H5Xfoo()` (e.g., `H5Dcreate`)
  * **Headers:** `H5Xpublic.h`

* **Private:** Internal library API, usable across packages
  * **Format:** `H5X_foo()` (one underscore, e.g., `H5D_create`)

* **Package:** Used only within the defining package
  * **Format:** `H5X__foo()` (two underscores, e.g., `H5D__create`)

### Function Structure

HDF5 functions follow a consistent structure for entry/exit and error handling:

```c
/*
 * Function description
 */
herr_t
H5X_do_stuff(/*parameters*/)
{
    /* 1. Variables declared at top */
    void *foo = NULL;
    herr_t ret_value = SUCCEED; /* 2. Return value variable */

    FUNC_ENTER_NOAPI(FAIL) /* 3. Function entry macro */

    HDassert(/*parameter check*/);

    /* 4. Check for errors and goto done */
    if (H5X_other_call() < 0)
        HGOTO_ERROR(H5E_MAJ, H5E_MIN, FAIL, "An error occurred");

done: /* 5. Target for error jumps */
    if (ret_value < 0)
        /* error cleanup */
    /* regular cleanup */

    FUNC_LEAVE_NOAPI(ret_value); /* 6. Function leave macro */
}
```

**Public** functions use `FUNC_ENTER_API`, include `H5TRACE` macros for API tracing, and perform more rigorous parameter checking.

### Error Handling

* Almost all functions return `herr_t` or `hid_t` error codes
* `FUNC_ENTER_*` macros set up error handling stack
* `HGOTO_ERROR` pushes errors onto stack and jumps to cleanup
* `FUNC_LEAVE_*` macros return the result
* Always check return values of functions that can fail

### Platform Independence

HDF5 uses a compatibility layer for platform differences:

* Standard C and POSIX calls are prefixed with `HD` (e.g., `HDmalloc`, `HDopen`)
* `H5private.h` and `H5win32defs.h` map these to platform-specific functions
* This layer is being modernized as C99 and POSIX become universal

### Memory Management

Use HDF5's internal memory management instead of direct `malloc`/`free`:

* **`H5MM`:** General-purpose memory management (recommended for most uses)
* **`H5FL`:** Memory pools for fixed-size, frequently allocated objects (use only when performance testing shows clear benefits)

---

## Contributing Changes

### Workflow

1. **Open a GitHub issue** ([HDF5 Issues](https://github.com/HDFGroup/hdf5/issues))
   - **Required** unless the change is minor (e.g., typo fix).
   - Describe the problem or feature request clearly.

2. **Fork the repository** and create your branch
   - Target the `develop` branch for new features and bug fixes.
   - Use descriptive branch names.

3. **Make your changes**
   - Follow HDF5 coding conventions.
   - Add tests for new functionality or bug fixes.
   - Update documentation as needed.

4. **Build and test thoroughly**
   - Follow build instructions in `release_docs/INSTALL*` files.
   - Ensure all tests pass.

5. **Submit a pull request (PR)**
   - Address any formatting or testing issues reported by CI.
   - Make sure to include the issue that the PR addresses in the description.
   - Work with HDF Group developers to meet acceptance criteria.

### Acceptance Criteria

For a pull request to be accepted, it must satisfy:

* **Clear purpose:** What does it address? How does it benefit the HDF5 community?
* **Proper documentation:** Code must be documented for maintainability.
* **Testing:** Must pass HDF5 regression testing and include appropriate tests.
  - We do not expect you to perform comprehensive testing across multiple platforms
    before we accept the pull request.
* **Compatibility:** Must not compromise HDF5's core principles:
  - 100% backward compatibility (any HDF5 file must remain readable).
    - If your patch's purpose is to modify the HDF5 data model or file format, **please** discuss
      this with us first. File format changes and features required by those changes can be
      introduced only in a new major release.
  - Machine independence (data readable across all platforms).
  - Binary compatibility for maintenance releases (no changes to public APIs/structures).
* **Documentation:** New features must be properly documented. This includes using Doxygen
    and providing information in release documents such as `CHANGELOG.md`.

### Branching Strategy

* **Small features:** Develop in forks of the main repository.
* **Large collaborative work:** Use feature branches named `feature/<feature>` in the main repository.
* Add `BRANCH.md` file explaining branch purpose and contact info for feature branches.

---

## Testing

### Test Structure

HDF5 uses custom testing macros rather than standard frameworks. There are two systems:

#### Modern Testing (`h5test.h`) - Preferred
```c
#include "h5test.h"

static int
test_feature(void)
{
    TESTING("some feature");
    
    /* test code */
    if (error_condition)
        TEST_ERROR;
    
    PASSED();
    return SUCCEED;

error:
    return FAIL;
}
```

#### Legacy Testing (`testhdf5.h`) - Avoid for New Code
Used only by the large `testhdf5` program. Uses global variables and should be avoided.

### Adding New Tests

**All new functionality and bug fixes must include tests.**

1. Add tests to existing test files when appropriate.
2. Create new test programs using `h5test.h` macros.
3. Avoid adding to the `testhdf5` program.
4. Update `CMakeLists.txt` in the `test/` directory.
5. Ensure tests run and pass under CMake.

### Maven Deployment Testing

For contributions involving Maven deployment or Java bindings:

1. **Test Maven Artifacts:** Use the validation script to verify artifact generation:
   ```bash
   # Build with Maven support
   cmake --workflow --preset ci-MinShar-GNUC-Maven-Snapshot --fresh

   # Validate generated artifacts
   .github/scripts/validate-maven-artifacts.sh build/ci-MinShar-GNUC-Maven-Snapshot
   ```

2. **PR Validation:** The `maven-staging.yml` workflow automatically tests Maven artifacts for pull requests when Java-related files are modified.

3. **Multi-Platform Testing:** Verify artifacts generate correctly on all platforms by testing with different Maven presets:
   - Linux: `ci-MinShar-GNUC-Maven-Snapshot`
   - Windows: `ci-MinShar-MSVC-Maven-Snapshot`
   - macOS: `ci-MinShar-Clang-Maven-Snapshot`

4. **Dry Run Testing:** Before deploying to repositories, test deployment permissions using the dry run mode in release workflows.

5. **Java Examples Testing:** The Java examples Maven integration includes comprehensive testing:
   ```bash
   # Test Java examples with Maven artifacts (all platforms)
   gh workflow run maven-staging.yml -f platforms=all-platforms

   # Run dedicated Java examples testing
   gh workflow run java-examples-maven-test.yml -f category=all
   ```
   - **Cross-Platform Validation:** Ensures examples work with platform-specific Maven artifacts
   - **Native Library Error Handling:** Validates JAR structure through expected native library errors
   - **Multi-Platform Coverage:** Tests on Linux, Windows, macOS x86_64, and macOS aarch64

---

## Documentation

### Release Notes

Write release notes for changes that affect users:

#### When to Write Release Notes
- **Required:** User-visible changes in functionality or behavior.
- **Required:** Known problems and user-reported issue fixes.
- **Not required:** Internal code changes, comments, or build process changes.

#### Release Note Format
```
- Title/Problem

  Problem description paragraph explaining the issue and conditions
  where it occurs.

  Solution paragraph describing what was done to resolve the issue
  and any functional impact or workarounds.
```

#### Entry Elements
- **Title:** Categories to help readers identify relevance.
- **Problem:** Clear description of the issue and conditions.
- **Solution:** What was done, functional impact, and any workarounds.

### API Documentation

* **Public functions:** Must have Doxygen markup in `H5Xpublic.h` headers.
* **New features:** Document in user guide content in `H5Xmodule.h` files.
* **Developer docs:** By means of well documented source.

---

## Command-Line Tools

Tools in the `tools/` directory:
- Written in C using only the **public** HDF5 API.
- Organized with central tools library (`tools/lib`) and individual tool directories.
- Use simplified error-handling compared to main library.
- Examples: `h5dump`, `h5diff`, `h5repack`.

---

## Checklist for Contributors

Before submitting your pull request, verify:

### Code
- [ ] Corresponding GitHub issue exists (unless minor change).
- [ ] Follows HDF5 conventions (naming, portability, structure).
- [ ] Applicable to other branches? (document in GitHub issue).
- [ ] Sufficiently documented for maintenance.
- [ ] API changes follow compatibility guidelines.

### Documentation
- [ ] Change described in `release_docs/CHANGELOG.md`.
- [ ] New functions documented with Doxygen in public headers.
- [ ] New features documented for HDF5 community.

### Testing
- [ ] Pull request includes tests.
- [ ] Consider performance impact.

---

## Getting Help

### Resources
* **HDF Forum:** Best place for questions about HDF5 usage and development (on HDF Group website).
* **GitHub Issues:** For bug reports and feature requests.
* **Documentation:** Check existing docs on the HDF Group website.

### Community
The HDF5 community is here to help. Don't hesitate to reach out with questions or for guidance on contributions.

---

Thank you for contributing to HDF5! Your efforts help maintain and improve one of the most widely used data formats today.

[u1]: https://github.com/HDFGroup/arch-doc/blob/main/An_Overview_of_the_HDF5_Library_Architecture.v2.pdf
```

### `HDF5Examples/JAVA/.gitignore`

```
# Maven test script build directories
build/

# Generated POM files (if any escape to root)
pom-examples.xml

# HDF5 output files from examples
*.h5

# Maven target directory (if built in-place)
target/

# Compiled class files
*.class

# IDE files
.idea/
*.iml
.vscode/
.settings/
.classpath
.project
```

### `LICENSE`

```
Copyright Notice and License Terms for
HDF5 (Hierarchical Data Format 5) Software Library and Utilities
-----------------------------------------------------------------------------

HDF5 (Hierarchical Data Format 5) Software Library and Utilities
Copyright 2006 by The HDF Group.

NCSA HDF5 (Hierarchical Data Format 5) Software Library and Utilities
Copyright 1998-2006 by The Board of Trustees of the University of Illinois.

All rights reserved.

This software library and utilities is covered by the 3-clause BSD License.

Redistribution and use in source and binary forms, with or without
modification, are permitted for any purpose (including commercial purposes)
provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice,
   this list of conditions, and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions, and the following disclaimer in the documentation
   and/or materials provided with the distribution.

3. Neither the name of The HDF Group, the name of the University, nor the
   name of any Contributor may be used to endorse or promote products derived
   from this software without specific prior written permission from
   The HDF Group, the University, or the Contributor, respectively.

DISCLAIMER:
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
â€œAS ISâ€ AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

For further details, please refer to the full license text available
at https://opensource.org/licenses/bsd-3-clause

You are under no obligation whatsoever to provide any bug fixes, patches, or
upgrades to the features, functionality or performance of the source code
("Enhancements") to anyone; however, if you choose to make your Enhancements
available either publicly, or directly to The HDF Group, without imposing a
separate written license agreement for such Enhancements, then you hereby
grant the following license: a non-exclusive, royalty-free perpetual license
to install, use, modify, prepare derivative works, incorporate into other
computer software, distribute, and sublicense such enhancements or derivative
works thereof, in binary and source code form.

-----------------------------------------------------------------------------
-----------------------------------------------------------------------------

Contributors:   National Center for Supercomputing Applications (NCSA) at
the University of Illinois, Fortner Software, Unidata Program Center
(netCDF), The Independent JPEG Group (JPEG), Jean-loup Gailly and Mark Adler
(gzip), and Digital Equipment Corporation (DEC).

-----------------------------------------------------------------------------

Portions of HDF5 were developed with support from the Lawrence Berkeley
National Laboratory (LBNL) and the United States Department of Energy
under Prime Contract No. DE-AC02-05CH11231.

-----------------------------------------------------------------------------

Portions of HDF5 were developed with support from Lawrence Livermore
National Laboratory and the United States Department of Energy under
Prime Contract No. DE-AC52-07NA27344.

-----------------------------------------------------------------------------

Portions of HDF5 were developed with support from the University of
California, Lawrence Livermore National Laboratory (UC LLNL).
The following statement applies to those portions of the product and must
be retained in any redistribution of source code, binaries, documentation,
and/or accompanying materials:

   This work was partially produced at the University of California,
   Lawrence Livermore National Laboratory (UC LLNL) under contract
   no. W-7405-ENG-48 (Contract 48) between the U.S. Department of Energy
   (DOE) and The Regents of the University of California (University)
   for the operation of UC LLNL.

   DISCLAIMER:
   THIS WORK WAS PREPARED AS AN ACCOUNT OF WORK SPONSORED BY AN AGENCY OF
   THE UNITED STATES GOVERNMENT. NEITHER THE UNITED STATES GOVERNMENT NOR
   THE UNIVERSITY OF CALIFORNIA NOR ANY OF THEIR EMPLOYEES, MAKES ANY
   WARRANTY, EXPRESS OR IMPLIED, OR ASSUMES ANY LIABILITY OR RESPONSIBILITY
   FOR THE ACCURACY, COMPLETENESS, OR USEFULNESS OF ANY INFORMATION,
   APPARATUS, PRODUCT, OR PROCESS DISCLOSED, OR REPRESENTS THAT ITS USE
   WOULD NOT INFRINGE PRIVATELY- OWNED RIGHTS. REFERENCE HEREIN TO ANY
   SPECIFIC COMMERCIAL PRODUCTS, PROCESS, OR SERVICE BY TRADE NAME,
   TRADEMARK, MANUFACTURER, OR OTHERWISE, DOES NOT NECESSARILY CONSTITUTE
   OR IMPLY ITS ENDORSEMENT, RECOMMENDATION, OR FAVORING BY THE UNITED
   STATES GOVERNMENT OR THE UNIVERSITY OF CALIFORNIA. THE VIEWS AND
   OPINIONS OF AUTHORS EXPRESSED HEREIN DO NOT NECESSARILY STATE OR REFLECT
   THOSE OF THE UNITED STATES GOVERNMENT OR THE UNIVERSITY OF CALIFORNIA,
   AND SHALL NOT BE USED FOR ADVERTISING OR PRODUCT ENDORSEMENT PURPOSES.

-----------------------------------------------------------------------------

Portions of HDF5 were developed with support from the National Science
Foundation under Federal Award No. 2534078.

-----------------------------------------------------------------------------
```

### `SECURITY.md`

```markdown
# Security Policy

## Supported Versions

Security updates are applied only to the latest release.

## Reporting a Vulnerability

If you have discovered a security vulnerability in this project, please report it privately. **Do not disclose it as a public issue.** This gives us time to work with you to fix the issue before public exposure, reducing the chance that the exploit will be used before a patch is released.

Please disclose it at [security advisory](https://github.com/HDFGroup/hdf5/security/advisories/new).

This project is maintained by a team of volunteers on a reasonable-effort basis. As such, vulnerabilities will be disclosed in a best effort base.
```

### `config/sanitizer/LICENSE`

```
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.
```

### `release_docs/CHANGELOG.md`

```markdown
HDF5 version 2.0.1 currently under development

# ðŸ”º HDF5 Changelog
All notable changes to this project will be documented in this file. This document describes the differences between this release and the previous
HDF5 release, platforms tested, and known problems in this release.

For releases prior to version 2.0.0, please see the release.txt file and for more details check the HISTORY*.txt files in the HDF5 source.

# ðŸ”— Quick Links
* [HDF5 documentation](https://support.hdfgroup.org/documentation/hdf5/latest/)
* [Official HDF5 releases](https://support.hdfgroup.org/downloads/index.html)
* [Changes from Release to Release and New Features in the HDF5-2.x.y](https://support.hdfgroup.org/releases/hdf5/documentation/release_specific_info.md)
* [Getting help, questions, or comments](https://github.com/HDFGroup/hdf5#help-and-support)

## ðŸ“– Contents
* [Executive Summary](CHANGELOG.md#execsummary)
* [Breaking Changes](CHANGELOG.md#%EF%B8%8F-breaking-changes)
* [New Features & Improvements](CHANGELOG.md#-new-features--improvements)
* [Bug Fixes](CHANGELOG.md#-bug-fixes)
* [Support for new platforms and languages](CHANGELOG.md#-support-for-new-platforms-and-languages)
* [Platforms Tested](CHANGELOG.md#%EF%B8%8F-platforms-tested)
* [Known Problems](CHANGELOG.md#-known-problems)

# ðŸ”† Executive Summary: HDF5 Version 2.0.1

## Performance Enhancements:


## Significant Advancements:


## Updated Foundation:

> [!IMPORTANT]
>
> - Transitioned to [CMake-only](CHANGELOG.md#cmake) builds, and Autotools is no longer in use.
> - Renamed library state variables, notably `HDF5_ENABLE_PARALLEL` is now `HDF5_PROVIDES_PARALLEL`, see PR [#5716](https://github.com/HDFGroup/hdf5/pull/5716) for more details.
> - The default setting for `H5Fset_libver_bounds` has been updated to set the lower bound to the HDF5 library version 1.8. This change ensures that users can take advantage of the library's optimal performance and the latest features by default. If users need their files to be compatible with older versions of the HDF5 library, they will need to adjust this lower bound manually.

## Enhanced Features:


## Java Enhancements:

  
## Acknowledgements: 

We would like to thank the many HDF5 community members who contributed to HDF5 2.0.

# âš ï¸ Breaking Changes


# ðŸš€ New Features & Improvements

## Configuration

## Library

### Added predefined datatypes for FP6 data

   Predefined datatypes have been added for FP6 data in E2M3 and E3M2 formats (https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf).

   The following new macros have been added:

    - H5T_FLOAT_F6E2M3
    - H5T_FLOAT_F6E3M2

   These macros map to IDs of HDF5 datatypes representing a 6-bit floating-point datatype with 1 sign bit and either 2 exponent bits and 3 mantissa bits (E2M3 format) or 3 exponent bits and 2 mantissa bits (E3M2 format).

   Note that support for a native FP6 datatype has not been added yet. This means that any datatype conversions to/from the new FP6 datatypes will be emulated in software rather than potentially using specialized hardware instructions. Until support for a native FP6 type is added, an application can avoid datatype conversion performance issues if it is sure that the datatype used for in-memory data buffers matches one of the above floating-point formats. In this case, the application can specify one of the above macros for both the file datatype when creating a dataset or attribute and the memory datatype when performing I/O on the dataset or attribute.

   Also note that HDF5 currently has incomplete support for datatype conversions involving non-IEEE floating-point format datatypes. Refer to the 'Known Problems' section for information about datatype conversions with these new datatypes.

### Added predefined datatype for FP4 data

   A predefined datatype has been added for FP4 data in E2M1 format (https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf).

   The following new macro has been added:

    - H5T_FLOAT_F4E2M1

   This macro maps to the ID of an HDF5 datatype representing a 4-bit floating-point datatype with 1 sign bit, 2 exponent bits and 1 mantissa bit.

   Note that support for a native FP4 datatype has not been added yet. This means that any datatype conversions to/from the new FP4 datatype will be emulated in software rather than potentially using specialized hardware instructions. Until support for a native FP4 type is added, an application can avoid datatype conversion performance issues if it is sure that the datatype used for in-memory data buffers matches the above floating-point format. In this case, the application can specify the above macro for both the file datatype when creating a dataset or attribute and the memory datatype when performing I/O on the dataset or attribute.

   Also note that HDF5 currently has incomplete support for datatype conversions involving non-IEEE floating-point format datatypes. Refer to the 'Known Problems' section for information about datatype conversions with these new datatypes.

## Parallel Library

## Fortran Library

## C++ Library

## Java Library

## Tools

## High-Level APIs

## C Packet Table API

## Internal header file

## Documentation


# ðŸª² Bug Fixes

## Library

## Java Library

## Configuration

## Tools

## Performance

## Fortran API

### Added Fortran wrappers for SWMR functionality

   Added four new Fortran wrappers that provide direct access to SWMR (Single Writer Multiple Reader) C APIs:
   - `h5fstart_swmr_write_f` - Enables SWMR writing mode for a file
   - `h5dflush_f`            - Flushes dataset buffers to disk
   - `h5pset_append_flush_f` - Sets append flush property values including optional callback function
   - `h5pget_append_flush_f` - Retrieves append flush property values including callback function

## High-Level Library

## Fortran High-Level APIs

## Documentation

## F90 APIs

## C++ APIs

## Testing

# âœ¨ Support for new platforms and languages

# â˜‘ï¸ Platforms Tested

A table of platforms tested can be seen on the [wiki](https://github.com/HDFGroup/hdf5/wiki/Platforms-Tested).
Current test results are available [here](https://my.cdash.org/index.php?project=HDF5).

# â›” Known Problems

- When performing implicit datatype conversion on specific non-IEEE floating-point format data, HDF5 may improperly convert some data values:

   When performing I/O operations using a non-IEEE floating-point format datatype, HDF5 may improperly convert some data values due to incomplete handling of non-IEEE types. Such types include the following pre-defined datatypes:

    H5T_FLOAT_F8E4M3
    H5T_FLOAT_F8E5M2
    H5T_FLOAT_F6E2M3
    H5T_FLOAT_F6E3M2
    H5T_FLOAT_F4E2M1

   If possible, an application should perform I/O with these datatypes using an in-memory type that matches the specific floating-point format and perform explicit data conversion outside of HDF5, if necessary. Otherwise, read/written values should be verified to be correct.

- When the library detects and builds in support for the _Float16 datatype, an issue has been observed on at least one MacOS 14 system where the library fails to initialize due to not being able to detect the byte order of the _Float16 type [#4310](https://github.com/HDFGroup/hdf5/issues/4310):

     #5: H5Tinit_float.c line 308 in H5T__fix_order(): failed to detect byte order
     major: Datatype
     minor: Unable to initialize object

   If this issue is encountered, support for the _Float16 type can be disabled with a configuration option:

     `CMake: HDF5_ENABLE_NONSTANDARD_FEATURE_FLOAT16=OFF`

- When HDF5 is compiled with NVHPC versions 23.5 - 23.9 (additional versions may also be applicable) and with -O2 (or higher) and -DNDEBUG, test failures occur in the following tests:

   - H5PLUGIN-filter_plugin
   - H5TEST-flush2
   - H5TEST-testhdf5-base
   - MPI_TEST_t_filters_parallel

  Sporadic failures (even with lower -O levels):

   - Java JUnit-TestH5Pfapl
   - Java JUnit-TestH5D

  Also, NVHPC will fail to compile the test/tselect.c test file with a compiler error of `use of undefined value` when the optimization level is -O2 or higher.

   This is confirmed to be a [bug in the nvc compiler](https://forums.developer.nvidia.com/t/hdf5-no-longer-compiles-with-nv-23-9/269045) that has been fixed as of 23.11. If you are using an affected version of the NVidia compiler, the work-around is to set the optimization level to -O1.

- CMake files do not behave correctly with paths containing spaces

   Do not use spaces in paths because the required escaping for handling spaces results in very complex and fragile build files.

- At present, metadata cache images may not be generated by parallel applications. Parallel applications can read files with metadata cache images, but since this is a collective operation, a deadlock is possible if one or more processes do not participate.

- The subsetting option in `ph5diff` currently will fail and should be avoided

   The subsetting option works correctly in serial `h5diff`.

- Flang Fortran compilation will fail (last check version 17) due to not yet implemented: (1) derived type argument passed by value (H5VLff.F90), and (2) support for REAL with KIND = 2 in intrinsic SPACING used in testing.

- Fortran tests HDF5_1_8.F90 and HDF5_F03.F90 will fail with Cray compilers greater than version 16.0 due to a compiler bug. The latest version verified as failing was version 17.0.

- Several tests currently fail on certain platforms:
   MPI_TEST-t_bigio fails with spectrum-mpi on ppc64le platforms.

   MPI_TEST-t_subfiling_vfd and MPI_TEST_EXAMPLES-ph5_subfiling fail with
   cray-mpich on theta and with XL compilers on ppc64le platforms.

- File space may not be released when overwriting or deleting certain nested variable length or reference types.

Known problems in previous releases can be found in the HISTORY*.txt files in the HDF5 source. Please report any new problems found to <a href="mailto:help@hdfgroup.org">help@hdfgroup.org</a>.
```

### `.devcontainer/devcontainer.json`

```json
{
  "name": "HDF5 Developer",
  "build": {
    "context": "..",
    "dockerfile": "Dockerfile"
  },
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "ms-toolsai.jupyter",
        "ms-vscode.cpptools",
        "ms-vscode.cpptools-extension-pack",
        "ms-vscode.live-server",
        "ms-vscode-remote.remote-containers",
        "ms-azuretools.vscode-docker",
        "h5web.vscode-h5web",
        "davidanson.vscode-markdownlint"
      ],
      "settings": {
        "C_Cpp.default.cppStandard": "c++17",
        "C_Cpp.default.cStandard": "c99",
        "terminal.integrated.shell.linux": "/bin/bash"
      }
    }
  }
}
```

### `.devcontainer/noop.txt`

```
This file is copied into the container along with environment.yml* from the
parent folder. This file prevents the Dockerfile COPY instruction from failing
if no environment.yml is found.
```

### `.github/.well-known/funding.json`

```json
{
  "version": "v1.0.0",
  "entity": {
    "type": "organisation",
    "role": "owner",
    "name": "The HDF Group",
    "email": "bizoffice@hdfgroup.org",
    "phone": "217-531-6100",
    "description": "The HDF Group is a non-profit organization with the mission of advancing state-of-the-art open-source data management technologies, ensuring long-term access to the data, and supporting our dedicated and diverse user community.",
    "webpageUrl": {
      "url": "https://hdfgroup.org"
    }
  },
  "projects": [
    {
      "guid": "hdf5-vfdswmr",
      "name": "hdf5-vfdswmr",
      "description": "The primary objective of the Virtual File Driver (VFD) Single-Writer Multiple-Reader (SWMR) is to implement a more modular approach than the current SWMR HDF5 library implementation, thereby reducing maintenance costs and improving overall library efficiency. This allows the HDF5 library to guarantee the maximum time from data writing to reading availability, depending on the underlying file system's performance and the writer's frequency of library calls. Additionally, it extends SWMR compatibility to Network File Systems (NFS) and object storage. This proposal outlines the necessary work for supporting the VFD SWMR framework, as well as other VFDs, since the current process of configuring VFD stacks can be complex and time-consuming. The proposed language parser aims to streamline this process by introducing a straightforward configuration language that allows for flexible and structured configuration through the use of name-value pairs. The grammar and structure of the language will define these name-value pairs, specifying the types for the values, which may include integers, floating-point numbers, quoted strings, binary blobs, and lists of name-value pairs. Notably, the grammar will fully support tree-structured VFD stacks, providing a robust and reliable configuration solution. For parsing and implementation, the language will utilize a recursive descent parser. This parser will adeptly handle the configuration language, with functions dedicated to parsing name-value pairs and lists, and will return success or failure based on validation results.",
      "webpageUrl": {
        "url": "https://hdfgroup.org"
      },
      "repositoryUrl": {
        "url": "https://github.com/HDFGroup/hdf5",
        "wellKnown": "https://github.com/HDFGroup/hdf5/blob/develop/.github/.well-known/funding-manifest-urls"
      },
      "licenses": [
        "spdx:BSD-3-Clause"
      ],
      "tags": [
        "hdf-5",
        "hierarchical-data-format",
        "self-describing",
        "data-format",
        "data-management",
        "data-interoperability",
        "metadata",
        "heterogeneous-data"
      ]
    },
    {
      "guid": "hdf5-utf8",
      "name": "hdf5-utf8",
      "description": "Support for UTF-8 in the HDF5 is essential for modern scientific and data-intensive applications. It ensures data integrity, promotes interoperability, and enables the handling of a wide range of information globally. Scientific data is often processed using various tools and programming languages across different operating systems. As the standard for text on the web and in contemporary operating systems, adopting UTF-8 allows HDF5 to facilitate seamless data exchange between platforms and applications, making data sharing easier and more efficient for the scientific community. This project aims to address high-priority issues related to UTF-8 in the HDF5 library, focusing on fixing bugs associated with UTF-8 filenames and variable-length strings, particularly on Windows. Additionally, enhancements will support UTF-8 in HDF5 tools and enable the introspection of UTF-8 strings within functions and variable-length string properties in the HDF5 library. The work also includes the addition of UTF-8 character testing to the existing Continuous Integration (CI) framework of HDF5. These enhancements will validate HDF5's ability to handle international characters in various components of the library, including filenames. While HDF5 has foundational support for UTF-8, this support is not consistently enforced or tested, which can lead to inconsistencies and bugs, especially when data is transferred across different platforms. The work involves creating a new suite of tests designed explicitly for UTF-8. These tests will account for variations in how operating systems, particularly Windows, handle UTF-8 in filenames and command-line interactions. The new tests will be integrated into the existing GitHub Actions to ensure continuous validation of HDF5's internationalization capabilities. This proactive approach will prevent character encoding issues and enhance the HDF5 library's quality for the global scientific community.",
      "webpageUrl": {
        "url": "https://hdfgroup.org"
      },
      "repositoryUrl": {
        "url": "https://github.com/hdfgroup/hdf5",
        "wellKnown": "https://github.com/hdfgroup/hdf5/blob/develop/.github/.well-known/funding-manifest-urls"
      },
      "licenses": [
        "spdx:BSD-3-Clause"
      ],
      "tags": [
        "hdf-5",
        "utf8",
        "hierarchical-data-format",
        "self-describing",
        "data-format",
        "data-management",
        "data-interoperability",
        "metadata",
        "heterogeneous-data"
      ]
    },
    {
      "guid": "hsds",
      "name": "hsds",
      "description": "HDF Scalable Data Service (HSDS) is a REST-based web service that allows web clients to read from and write to HDF data sources. HSDS can be deployed in the cloud or on-premises, making it ideal for situations where multiple clients need to access data via an HTTP endpoint. This functionality enables HDF to be used by web clients, such as web applications, as well as programs written in any language that can make HTTP requests. HSDS's robust support for multiple storage platforms, including POSIX, AWS S3, and Azure Blob Storage, provides reassurance and flexibility. It can be deployed using Docker, Kubernetes, or as an operating system service (e.g., Unix Daemon). The development of HSDS has been supported by both public and commercial organizations seeking specialized features or improved performance for specific use cases. One <a href='https://github.com/HDFGroup/hsds/blob/master/docs/design/query/chunk_summary.md>proposal</a> has generated significant interest, although there has been no monetary support offered thus far. The idea is to dramatically speed up SQL-like queries over large datasets without the need to build indexes on the datasets. While indexing is a common technique used by many databases, creating indexes for large-scale standard HDF5 datasets (multi-terabyte) is time-consuming and requires substantial additional storage. The chunk summary design aims to address these challenges and has the potential to be a valuable enhancement to the HSDS software package.",
      "webpageUrl": {
        "url": "https://hdfgroup.org"
      },
      "repositoryUrl": {
        "url": "https://github.com/HDFGroup",
        "wellKnown": "https://github.com/HDFGroup/hsds/blob/master/.github/.well-known/funding-manifest-urls"
      },
      "licenses": [
        "spdx:Apache-2.0"
      ],
      "tags": [
        "hdf-server",
        "cloud-native",
        "service-based-access",
        "hierarchical-data-format",
        "data-management",
        "data-interoperability",
        "object-storage-backend",
        "heterogeneous-data"
      ]
    },
    {
      "guid": "cgns",
      "name": "cgns",
      "description": "The Computational Fluid Dynamics (CFD) General Notation System (CGNS) is used worldwide to store and exchange CFD input and output data, facilitating interdisciplinary collaboration and enriching the archival quality of scientific data. CGNS is a CFD community standard that describes the intellectual content required for CFD analysis, as well as open-source software that implements the standard. Having a standardized, well-maintained, well-tested and well-performing CGNS software package encourages its use and popularity in the CFD community. Since its inception in the 1990s, the CGNS Steering Committee has operated as a self-sustaining and largely unfunded group of volunteers from various contributing organizations worldwide. The primary scientific objective of this project is to enhance the high-performance computing (HPC) capabilities of CGNS by examining how it can leverage the recent HPC features of the software stack it relies on, specifically the HDF5 and MPI I/O libraries. We plan to implement the changes made to both CGNS and HDF5 and make these enhancements available in open-source distributions. Furthermore, the project is committed to supporting accepted and essential CGNS Proposals for Extension (CPEX) to advance the next generation of CFD simulations. Lastly, we aim to broaden continuous integration (CI) testing to include a wider variety of operating systems and compilers while also addressing some of the technical debt that has accumulated over the years.",
      "webpageUrl": {
        "url": "https://cgns.org",
        "wellKnown": "https://cgns.org/.well-known/funding-manifest-urls"
      },
      "repositoryUrl": {
        "url": "https://github.com/CGNS/CGNS",
        "wellKnown": "https://github.com/CGNS/CGNS/blob/develop/.github/.well-known/funding-manifest-urls"
      },
      "licenses": [
        "spdx:zlib-acknowledgement"
      ],
      "tags": [
        "library",
        "standard",
        "computational-fluid-dynamics",
        "data-format",
        "aerospace",
        "grids",
        "flow-solutions",
        "hdf",
        "notation"
      ]
    }
  ],
  "funding": {
    "channels": [
      {
        "guid": "mybank",
        "type": "bank",
        "address": "Will accept direct bank transfers. Please e-mail me for details.",
        "description": ""
      }
    ],
    "plans": [
      {
        "guid": "vfdswmr-developer-compensation",
        "status": "active",
        "name": "VFD SWMR Developer Compensation",
        "description": "This will cover the cost of developers working on the hdf5-vfdswmr project.",
        "amount": 50000,
        "currency": "USD",
        "frequency": "one-time",
        "channels": [
          "mybank"
        ]
      },
      {
        "guid": "utf8-developer-compensation",
        "status": "active",
        "name": "UTF8 Developer Compensation",
        "description": "This will cover the cost of developers working on the hdf5-utf8 project.",
        "amount": 10000,
        "currency": "USD",
        "frequency": "one-time",
        "channels": [
          "mybank"
        ]
      },
      {
        "guid": "hsds-developer-compensation",
        "status": "active",
        "name": "HSDS Developer Compensation",
        "description": "This will cover the cost of HSDS developers working on the hsds project.",
        "amount": 10000,
        "currency": "USD",
        "frequency": "one-time",
        "channels": [
          "mybank"
        ]
      },
      {
        "guid": "cgns-developer-compensation",
        "status": "active",
        "name": "CGNS Developer Compensation",
        "description": "This will cover the cost of part-time developers working on the CGNS project.",
        "amount": 45000,
        "currency": "USD",
        "frequency": "one-time",
        "channels": [
          "mybank"
        ]
      },
      {
        "guid": "hdf5-flexible-giving",
        "status": "active",
        "name": "HDF5 Flexible Giving",
        "description": "Support the HDF5 project with a goodwill contribution of any size you deem fit.",
        "amount": 0,
        "currency": "USD",
        "frequency": "one-time",
        "channels": [
          "mybank"
        ]
      },
      {
        "guid": "hsds-flexible-giving",
        "status": "active",
        "name": "HSDS Flexible Giving",
        "description": "Support the HSDS project with a goodwill contribution of any size you deem fit.",
        "amount": 0,
        "currency": "USD",
        "frequency": "one-time",
        "channels": [
          "mybank"
        ]
      },
      {
        "guid": "cgns-flexible-giving",
        "status": "active",
        "name": "CGNS Flexible Giving",
        "description": "Support the CGNS project with a goodwill contribution of any size you deem fit.",
        "amount": 0,
        "currency": "USD",
        "frequency": "one-time",
        "channels": [
          "mybank"
        ]
      }
    ],
    "history": []
  }
}
```

### `.github/FUNDING.yml`

```yaml
# These are supported funding model platforms

github: hdfgroup
```

### `.github/ISSUE_TEMPLATE/blank-issue.md`

```markdown
---
name: Blank issue
about: Create a new issue from scratch
title: ''
labels: ''
assignees: ''

---

***
 â›” **IMPORTANT**  â›”
 Security issues should not be reported here! Instead, please
 report them privately as security vulnerabilities; see for more
 information https://github.com/HDFGroup/hdf5/security/policy
***
```

### `.github/ISSUE_TEMPLATE/bug_report.md`

```markdown
---
name: Bug report
about: Report a problem with HDF5
title: ''
labels: ''
assignees: ''

---

***
 â›” **IMPORTANT**  â›”
 Security issues should not be reported here! Instead, please
 report them privately as security vulnerabilities; see for more
 information https://github.com/HDFGroup/hdf5/security/policy
***

**Describe the bug**
A clear and concise description of what the bug is.

**Expected behavior**
A clear and concise description of what you expected to happen.

**Platform (please complete the following information)**
 - HDF5 version (if building from a maintenance branch, please include the commit hash)
 - OS and version
 - Compiler and version
 - Build system (e.g. CMake version) and generator (e.g. XCode, Ninja)
 - Any configure options you specified
 - MPI library and version (parallel HDF5)

**Additional context**
Add any other context about the problem here.
```

### `.github/ISSUE_TEMPLATE/config.yml`

```yaml
blank_issues_enabled: false
```

### `.github/ISSUE_TEMPLATE/feature-request.md`

```markdown
---
name: Feature request
about: Suggest an improvement to HDF5
title: ''
labels: ''
assignees: ''

---

**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
```

### `.github/PULL_REQUEST_TEMPLATE/pull_request_template.md`

```markdown
## Describe your changes

## Issue ticket number (GitHub or JIRA)

## Checklist before requesting a review
- [ ] My code conforms to the guidelines in CONTRIBUTING.md
- [ ] I made an entry in release_docs/CHANGELOG.md (bug fixes, new features)
- [ ] I added a test (bug fixes, new features)
```

### `.github/actions/setup-jextract/action.yml`

```yaml
name: 'Setup jextract'
description: 'Install jextract for FFM binding generation across all platforms'
inputs:
  java-version:
    description: 'Java version for jextract (24, 25, latest)'
    required: false
    default: '25'

outputs:
  jextract-home:
    description: 'Path to jextract installation'
    value: ${{ steps.setup-jextract.outputs.jextract-home }}
  jextract-version:
    description: 'Version of jextract installed'
    value: ${{ steps.setup-jextract.outputs.jextract-version }}

runs:
  using: 'composite'
  steps:
    - name: Setup jextract (Linux/macOS)
      id: setup-jextract-unix
      if: runner.os != 'Windows'
      shell: bash
      run: |
        echo "Installing jextract for $RUNNER_OS..."

        # Determine platform
        if [[ "$RUNNER_OS" == "macOS" ]]; then
          PLATFORM="macos-x64"
        else
          PLATFORM="linux-x64"
        fi

        # Try different jextract versions (from latest to older)
        # Check https://jdk.java.net/jextract/ for available builds
        JEXTRACT_URLS=(
          "https://download.java.net/java/early_access/jextract/22/6/openjdk-22-jextract+6-47_${PLATFORM}_bin.tar.gz"
          "https://download.java.net/java/early_access/jextract/21/5/openjdk-21-jextract+5-31_${PLATFORM}_bin.tar.gz"
          "https://download.java.net/java/early_access/jextract/20/1/openjdk-20-jextract+1-2_${PLATFORM}_bin.tar.gz"
        )

        mkdir -p $HOME/jextract
        cd $HOME/jextract

        SUCCESS=false
        for URL in "${JEXTRACT_URLS[@]}"; do
          echo "Trying to download from: $URL"
          if curl -L -f -o jextract.tar.gz "$URL" 2>/dev/null; then
            echo "âœ“ Download successful from $URL"
            tar -xzf jextract.tar.gz --strip-components=1
            rm jextract.tar.gz
            SUCCESS=true
            break
          else
            echo "âœ— Failed to download from $URL, trying next..."
          fi
        done

        if [ "$SUCCESS" = false ]; then
          echo "ERROR: Failed to download jextract from any known source"
          echo "Please check https://jdk.java.net/jextract/ for available builds"
          exit 1
        fi

        # Set outputs
        echo "jextract-home=$HOME/jextract" >> $GITHUB_OUTPUT

        # Verify installation
        if $HOME/jextract/bin/jextract --version 2>&1; then
          VERSION=$($HOME/jextract/bin/jextract --version 2>&1 | head -1 || echo "unknown")
          echo "jextract-version=$VERSION" >> $GITHUB_OUTPUT
          echo "âœ“ jextract installed successfully: $VERSION"
        else
          echo "jextract-version=unknown" >> $GITHUB_OUTPUT
          echo "âœ“ jextract installed (version check not supported)"
        fi

    - name: Setup jextract (Windows)
      id: setup-jextract-windows
      if: runner.os == 'Windows'
      shell: pwsh
      run: |
        Write-Host "Installing jextract for Windows..."

        # Try multiple jextract versions (latest to older)
        # Windows now uses .tar.gz format
        $JextractUrls = @(
          "https://download.java.net/java/early_access/jextract/22/6/openjdk-22-jextract+6-47_windows-x64_bin.tar.gz",
          "https://download.java.net/java/early_access/jextract/21/5/openjdk-21-jextract+5-31_windows-x64_bin.tar.gz",
          "https://download.java.net/java/early_access/jextract/20/1/openjdk-20-jextract+1-2_windows-x64_bin.tar.gz"
        )

        $JextractHome = "$env:USERPROFILE\jextract"
        New-Item -ItemType Directory -Force -Path $JextractHome | Out-Null

        $Success = $false
        foreach ($Url in $JextractUrls) {
          Write-Host "Trying to download from: $Url"
          $TarPath = "$JextractHome\jextract.tar.gz"

          try {
            Invoke-WebRequest -Uri $Url -OutFile $TarPath -ErrorAction Stop
            Write-Host "âœ“ Download successful from $Url"

            # Extract using tar (available in Windows 10+)
            $TempExtract = "$JextractHome\temp"
            New-Item -ItemType Directory -Force -Path $TempExtract | Out-Null
            tar -xzf "$TarPath" -C "$TempExtract" 2>&1 | Out-Null

            # Find jextract.bat
            $JextractBat = Get-ChildItem -Path $TempExtract -Filter "jextract.bat" -Recurse -ErrorAction SilentlyContinue | Select-Object -First 1

            if ($JextractBat) {
              Write-Host "Found jextract.bat at: $($JextractBat.FullName)"

              # Move contents to JextractHome
              $JextractRoot = $JextractBat.Directory.Parent.FullName
              Get-ChildItem -Path $JextractRoot | ForEach-Object {
                Move-Item -Path $_.FullName -Destination $JextractHome -Force
              }

              # Clean up
              Remove-Item -Path $TempExtract -Recurse -Force
              Remove-Item -Path $TarPath -Force

              # Verify
              if (Test-Path "$JextractHome\bin\jextract.bat") {
                Write-Host "âœ“ jextract extracted successfully to $JextractHome"
                $Success = $true
                break
              }
            } else {
              Write-Host "âœ— Could not find jextract.bat in extracted files"
              Remove-Item -Path $TempExtract -Recurse -Force -ErrorAction SilentlyContinue
            }
          }
          catch {
            Write-Host "âœ— Failed to download or extract from $Url"
            Write-Host "Error: $_"
          }
        }

        if (-not $Success) {
          Write-Host "ERROR: Failed to download jextract from any known source"
          Write-Host "Please check https://jdk.java.net/jextract/ for available builds"
          exit 1
        }

        # Set outputs
        echo "jextract-home=$JextractHome" >> $env:GITHUB_OUTPUT

        # Verify installation
        try {
          $Version = & "$JextractHome\bin\jextract.bat" --version 2>&1 | Select-Object -First 1
          echo "jextract-version=$Version" >> $env:GITHUB_OUTPUT
          Write-Host "âœ“ jextract installed successfully: $Version"
        } catch {
          echo "jextract-version=unknown" >> $env:GITHUB_OUTPUT
          Write-Host "âœ“ jextract installed (version check not supported)"
        }

    - name: Set environment variables
      id: setup-jextract
      shell: bash
      run: |
        if [[ "$RUNNER_OS" == "Windows" ]]; then
          JEXTRACT_HOME="${{ steps.setup-jextract-windows.outputs.jextract-home }}"
          JEXTRACT_VERSION="${{ steps.setup-jextract-windows.outputs.jextract-version }}"
        else
          JEXTRACT_HOME="${{ steps.setup-jextract-unix.outputs.jextract-home }}"
          JEXTRACT_VERSION="${{ steps.setup-jextract-unix.outputs.jextract-version }}"
        fi

        echo "JEXTRACT_HOME=$JEXTRACT_HOME" >> $GITHUB_ENV
        echo "jextract-home=$JEXTRACT_HOME" >> $GITHUB_OUTPUT
        echo "jextract-version=$JEXTRACT_VERSION" >> $GITHUB_OUTPUT

        # Add to PATH
        if [[ "$RUNNER_OS" == "Windows" ]]; then
          echo "$JEXTRACT_HOME\bin" >> $GITHUB_PATH
        else
          echo "$JEXTRACT_HOME/bin" >> $GITHUB_PATH
        fi

        echo "âœ“ jextract setup complete"
        echo "  JEXTRACT_HOME=$JEXTRACT_HOME"
        echo "  Version: $JEXTRACT_VERSION"
```

### `.github/dependabot.yml`

```yaml
version: 2
updates:
  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "monthly"
    groups:
      github-actions:
        patterns:
          - "*"
```

### `.github/scripts/generate-index-html.sh`

```bash
#!/bin/bash
# Generate index.html files for HDF5 release directories
# Usage: generate-index-html.sh <directory> <title> <description> [parent_url]

set -euo pipefail

DIRECTORY="${1:-}"
TITLE="${2:-Index}"
DESCRIPTION="${3:-}"
PARENT_URL="${4:-../}"

if [ -z "$DIRECTORY" ]; then
    echo "Usage: $0 <directory> <title> <description> [parent_url]"
    exit 1
fi

if [ ! -d "$DIRECTORY" ]; then
    echo "Error: Directory '$DIRECTORY' does not exist"
    exit 1
fi

OUTPUT_FILE="$DIRECTORY/index.html"

# Get file information with size and modification time
generate_file_list() {
    local dir="$1"
    local files=()

    # Find all files and directories (excluding index.html itself)
    while IFS= read -r -d '' item; do
        if [ "$(basename "$item")" != "index.html" ]; then
            files+=("$item")
        fi
    done < <(find "$dir" -maxdepth 1 ! -path "$dir" -print0 | sort -z)

    # Generate HTML list items
    for item in "${files[@]}"; do
        local name=$(basename "$item")
        local rel_path="$name"
        local size=""
        local modified=""
        local item_type="file"

        if [ -d "$item" ]; then
            item_type="dir"
            rel_path="$name/"
            # Count items in directory
            local count=$(find "$item" -maxdepth 1 ! -path "$item" | wc -l)
            size="$count items"
        else
            # Get file size in human-readable format
            size=$(ls -lh "$item" | awk '{print $5}')

            # Get modification time
            if [[ "$OSTYPE" == "darwin"* ]]; then
                modified=$(stat -f "%Sm" -t "%Y-%m-%d %H:%M" "$item")
            else
                modified=$(stat -c "%y" "$item" | cut -d'.' -f1)
            fi
        fi

        # Determine file description based on extension
        local desc=""
        case "$name" in
            *.tar.gz|*.tgz)
                desc="Source tarball"
                ;;
            *.zip)
                if [[ "$name" == *"doxygen"* ]]; then
                    desc="Doxygen documentation"
                elif [[ "$name" == *"win"* ]] || [[ "$name" == *"WIN"* ]]; then
                    desc="Windows binary package"
                else
                    desc="Source archive"
                fi
                ;;
            *.msi)
                desc="Windows installer"
                ;;
            *.exe)
                desc="Windows executable installer"
                ;;
            *.dmg)
                desc="macOS disk image"
                ;;
            *.deb)
                desc="Debian/Ubuntu package"
                ;;
            *.rpm)
                desc="Red Hat/Fedora package"
                ;;
            *abi.reports*)
                desc="ABI compatibility reports"
                ;;
            SHA256*)
                desc="SHA256 checksums"
                ;;
            downloads)
                desc="Release binaries and source code"
                ;;
            documentation)
                desc="API documentation and user guides"
                ;;
            doxygen)
                desc="Doxygen API documentation"
                ;;
            compat_report)
                desc="ABI/API compatibility reports"
                ;;
            *)
                if [ "$item_type" == "dir" ]; then
                    desc="Directory"
                else
                    desc="File"
                fi
                ;;
        esac

        # Output HTML row
        if [ "$item_type" == "dir" ]; then
            echo "      <tr class='dir'>"
            echo "        <td class='name'><a href='$rel_path'>ðŸ“ $name/</a></td>"
            echo "        <td class='size'>$size</td>"
            echo "        <td class='modified'>-</td>"
            echo "        <td class='description'>$desc</td>"
            echo "      </tr>"
        else
            echo "      <tr class='file'>"
            echo "        <td class='name'><a href='$rel_path'>ðŸ“„ $name</a></td>"
            echo "        <td class='size'>$size</td>"
            echo "        <td class='modified'>$modified</td>"
            echo "        <td class='description'>$desc</td>"
            echo "      </tr>"
        fi
    done
}

# Generate the index.html file
cat > "$OUTPUT_FILE" << 'EOF'
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>INDEX_TITLE_PLACEHOLDER</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        header {
            border-bottom: 3px solid #003d7a;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }

        h1 {
            color: #003d7a;
            font-size: 28px;
            margin-bottom: 10px;
        }

        .subtitle {
            color: #666;
            font-size: 16px;
            margin-bottom: 10px;
        }

        .description {
            color: #555;
            font-size: 14px;
            font-style: italic;
        }

        .breadcrumb {
            margin-bottom: 20px;
            font-size: 14px;
        }

        .breadcrumb a {
            color: #0066cc;
            text-decoration: none;
        }

        .breadcrumb a:hover {
            text-decoration: underline;
        }

        .file-list {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }

        .file-list thead {
            background: #f0f0f0;
            border-bottom: 2px solid #003d7a;
        }

        .file-list th {
            text-align: left;
            padding: 12px;
            font-weight: 600;
            color: #003d7a;
        }

        .file-list td {
            padding: 10px 12px;
            border-bottom: 1px solid #eee;
        }

        .file-list tr:hover {
            background: #f8f9fa;
        }

        .file-list tr.dir {
            background: #f9f9ff;
        }

        .file-list tr.dir:hover {
            background: #e8e8ff;
        }

        .file-list a {
            color: #0066cc;
            text-decoration: none;
            font-weight: 500;
        }

        .file-list a:hover {
            text-decoration: underline;
        }

        .name {
            min-width: 300px;
        }

        .size {
            width: 100px;
            text-align: right;
            font-family: monospace;
            color: #666;
        }

        .modified {
            width: 180px;
            font-family: monospace;
            font-size: 13px;
            color: #666;
        }

        .description {
            color: #777;
            font-size: 13px;
        }

        footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #ddd;
            text-align: center;
            color: #666;
            font-size: 13px;
        }

        footer a {
            color: #0066cc;
            text-decoration: none;
        }

        footer a:hover {
            text-decoration: underline;
        }

        .empty-message {
            padding: 40px;
            text-align: center;
            color: #999;
            font-style: italic;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>INDEX_TITLE_PLACEHOLDER</h1>
            <div class="subtitle">HDF5 (Hierarchical Data Format 5) Software Library and Utilities</div>
            <div class="description">INDEX_DESCRIPTION_PLACEHOLDER</div>
        </header>

        <div class="breadcrumb">
            <a href="PARENT_URL_PLACEHOLDER">â¬†ï¸ Parent Directory</a>
        </div>

        <table class="file-list">
            <thead>
                <tr>
                    <th class="name">Name</th>
                    <th class="size">Size</th>
                    <th class="modified">Modified</th>
                    <th class="description">Description</th>
                </tr>
            </thead>
            <tbody>
FILE_LIST_PLACEHOLDER
            </tbody>
        </table>

        <footer>
            <p>
                <a href="https://www.hdfgroup.org/">The HDF Group</a> |
                <a href="https://portal.hdfgroup.org/documentation/index.html">Documentation</a> |
                <a href="https://github.com/HDFGroup/hdf5">GitHub</a>
            </p>
            <p style="margin-top: 10px;">Copyright Â© 2006-2025 by The HDF Group</p>
        </footer>
    </div>
</body>
</html>
EOF

# Replace placeholders
sed -i.bak "s|INDEX_TITLE_PLACEHOLDER|$TITLE|g" "$OUTPUT_FILE"
sed -i.bak "s|INDEX_DESCRIPTION_PLACEHOLDER|$DESCRIPTION|g" "$OUTPUT_FILE"
sed -i.bak "s|PARENT_URL_PLACEHOLDER|$PARENT_URL|g" "$OUTPUT_FILE"

# Generate and insert file list
FILE_LIST=$(generate_file_list "$DIRECTORY")

if [ -z "$FILE_LIST" ]; then
    FILE_LIST="        <tr><td colspan='4' class='empty-message'>No files or directories found</td></tr>"
fi

# Create secure temporary file
TEMP_FILE=$(mktemp) || {
    echo "Error: Failed to create temporary file"
    exit 1
}

# Ensure cleanup on exit
trap 'rm -f "$TEMP_FILE"' EXIT

# Use a different delimiter for sed since the content contains slashes
echo "$FILE_LIST" > "$TEMP_FILE"
sed -i.bak "/FILE_LIST_PLACEHOLDER/r $TEMP_FILE" "$OUTPUT_FILE"
sed -i.bak "/FILE_LIST_PLACEHOLDER/d" "$OUTPUT_FILE"

# Clean up backup files
rm -f "$OUTPUT_FILE.bak"

echo "âœ… Generated index.html at: $OUTPUT_FILE"
```

### `.github/scripts/test-java-implementations.sh`

```bash
#!/bin/bash
# Test script for validating Java FFM and JNI implementations across different Java versions
# Usage: test-java-implementations.sh [java_version] [implementation] [test_mode]

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/../.." && pwd)"

# Default values
JAVA_VERSION="${1:-24}"
IMPLEMENTATION="${2:-auto}"  # auto, ffm, jni
TEST_MODE="${3:-build}"      # build, maven, full

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Test matrix configuration
declare -A JAVA_VERSIONS=(
    ["11"]="JNI only"
    ["17"]="JNI only"
    ["21"]="JNI only"
    ["24"]="JNI default, FFM optional"
    ["25"]="JNI default, FFM optional"
)

declare -A TEST_PRESETS_FFM=(
    ["build"]="ci-StdShar-GNUC-FFM"
    ["maven"]="ci-MinShar-GNUC-Maven-FFM"
)

declare -A TEST_PRESETS_JNI=(
    ["build"]="ci-StdShar-GNUC"
    ["maven"]="ci-MinShar-GNUC-Maven"
)

# Validate Java version support
validate_java_version() {
    local version=$1
    local impl=$2

    if [[ ! ${JAVA_VERSIONS[$version]+_} ]]; then
        log_error "Unsupported Java version: $version"
        log_info "Supported versions: ${!JAVA_VERSIONS[@]}"
        return 1
    fi

    if [[ $version -lt 25 && "$impl" == "ffm" ]]; then
        log_error "FFM implementation requires Java 25, got Java $version"
        return 1
    fi

    log_info "Java $version validation: ${JAVA_VERSIONS[$version]}"
    return 0
}

# Determine implementation based on Java version and user preference
determine_implementation() {
    local version=$1
    local requested=$2

    case "$requested" in
        "auto")
            # JNI is default for HDF5 2.0, regardless of Java version
            echo "jni"
            ;;
        "ffm")
            if [[ $version -ge 25 ]]; then
                echo "ffm"
            else
                log_error "FFM requires Java 25+, got Java $version"
                return 1
            fi
            ;;
        "jni")
            echo "jni"
            ;;
        *)
            log_error "Invalid implementation: $requested (use auto, ffm, or jni)"
            return 1
            ;;
    esac
}

# Create build directory with unique name
create_build_dir() {
    local impl=$1
    local mode=$2

    BUILD_DIR="${PROJECT_ROOT}/build-test-java${JAVA_VERSION}-${impl}-${mode}"

    if [[ -d "$BUILD_DIR" ]]; then
        log_warning "Removing existing build directory: $BUILD_DIR"
        rm -rf "$BUILD_DIR"
    fi

    mkdir -p "$BUILD_DIR"
    log_info "Created build directory: $BUILD_DIR"
}

# Test basic build configuration
test_build_config() {
    local impl=$1
    local preset_key="build"

    log_info "Testing $impl build configuration..."

    if [[ "$impl" == "ffm" ]]; then
        preset=${TEST_PRESETS_FFM[$preset_key]}
    else
        preset=${TEST_PRESETS_JNI[$preset_key]}
    fi

    log_info "Using preset: $preset"

    cd "$PROJECT_ROOT"

    # Configure with preset
    if ! cmake --preset "$preset" -B "$BUILD_DIR"; then
        log_error "CMake configuration failed for $impl implementation"
        return 1
    fi

    # Verify implementation detection
    # Note: CMake uses HDF5_ENABLE_JNI (not HDF5_ENABLE_FFM)
    # JNI enabled (ON or not set) = JNI implementation
    # JNI disabled (OFF) = FFM implementation
    if [ "$impl" = "jni" ]; then
        # For JNI, verify it's not explicitly disabled
        if grep -q "HDF5_ENABLE_JNI:BOOL=OFF" "$BUILD_DIR/CMakeCache.txt"; then
            log_error "Implementation detection failed - expected JNI but found HDF5_ENABLE_JNI=OFF"
            cat "$BUILD_DIR/CMakeCache.txt" | grep "HDF5_ENABLE_JNI" || true
            return 1
        fi
        log_info "JNI implementation verified (HDF5_ENABLE_JNI not OFF)"
    elif [ "$impl" = "ffm" ]; then
        # For FFM, verify JNI is explicitly disabled
        if ! grep -q "HDF5_ENABLE_JNI:BOOL=OFF" "$BUILD_DIR/CMakeCache.txt"; then
            log_error "Implementation detection failed - expected HDF5_ENABLE_JNI=OFF for FFM"
            cat "$BUILD_DIR/CMakeCache.txt" | grep "HDF5_ENABLE_JNI" || true
            return 1
        fi
        log_info "FFM implementation verified (HDF5_ENABLE_JNI=OFF)"
    fi

    log_success "Build configuration test passed for $impl"
    return 0
}

# Test Maven artifact generation
test_maven_artifacts() {
    local impl=$1
    local preset_key="maven"

    log_info "Testing $impl Maven artifact generation..."

    if [[ "$impl" == "ffm" ]]; then
        preset=${TEST_PRESETS_FFM[$preset_key]}
        expected_artifact="hdf5-java-ffm"
    else
        preset=${TEST_PRESETS_JNI[$preset_key]}
        expected_artifact="hdf5-java-jni"
    fi

    cd "$PROJECT_ROOT"

    # Configure with Maven preset
    if ! cmake --preset "$preset" -B "$BUILD_DIR"; then
        log_error "Maven configuration failed for $impl implementation"
        return 1
    fi

    # Build the project
    if ! cmake --build "$BUILD_DIR" --parallel 4; then
        log_error "Build failed for $impl implementation"
        return 1
    fi

    # Verify artifact generation
    jar_pattern="$BUILD_DIR/java/**/target/${expected_artifact}-*.jar"
    if ! ls $jar_pattern 1> /dev/null 2>&1; then
        log_error "Expected JAR artifact not found: $expected_artifact"
        log_info "Looking for JARs in build directory:"
        find "$BUILD_DIR" -name "*.jar" -type f || true
        return 1
    fi

    # Verify JAR manifest
    jar_file=$(ls $jar_pattern | head -1)
    log_info "Checking JAR manifest: $jar_file"

    if ! unzip -q -c "$jar_file" META-INF/MANIFEST.MF | grep -q "HDF5-Java-Implementation: ${impl^^}"; then
        log_error "JAR manifest missing implementation metadata"
        unzip -q -c "$jar_file" META-INF/MANIFEST.MF || true
        return 1
    fi

    log_success "Maven artifact test passed for $impl"
    return 0
}

# Test POM file generation
test_pom_generation() {
    local impl=$1

    log_info "Testing POM file generation for $impl..."

    if [[ "$impl" == "ffm" ]]; then
        expected_artifact="hdf5-java-ffm"
        expected_desc="Java Foreign Function"
    else
        expected_artifact="hdf5-java-jni"
        expected_desc="Java Native Interface"
    fi

    # Find generated POM file
    pom_file=$(find "$BUILD_DIR" -name "pom.xml" -path "*/java/*" | head -1)

    if [[ ! -f "$pom_file" ]]; then
        log_error "POM file not found for $impl implementation"
        return 1
    fi

    log_info "Checking POM file: $pom_file"

    # Verify artifact ID
    if ! grep -q "<artifactId>$expected_artifact</artifactId>" "$pom_file"; then
        log_error "POM artifact ID incorrect - expected $expected_artifact"
        grep "<artifactId>" "$pom_file" || true
        return 1
    fi

    # Verify description
    if ! grep -q "$expected_desc" "$pom_file"; then
        log_error "POM description missing expected text: $expected_desc"
        grep "<description>" "$pom_file" || true
        return 1
    fi

    log_success "POM generation test passed for $impl"
    return 0
}

# Run comprehensive test suite
run_test_suite() {
    local impl=$1
    local mode=$2

    log_info "Running test suite for Java $JAVA_VERSION with $impl implementation (mode: $mode)"

    case "$mode" in
        "build")
            create_build_dir "$impl" "build"
            test_build_config "$impl"
            ;;
        "maven")
            create_build_dir "$impl" "maven"
            test_maven_artifacts "$impl"
            test_pom_generation "$impl"
            ;;
        "full")
            create_build_dir "$impl" "build"
            test_build_config "$impl"

            create_build_dir "$impl" "maven"
            test_maven_artifacts "$impl"
            test_pom_generation "$impl"
            ;;
        *)
            log_error "Invalid test mode: $mode (use build, maven, or full)"
            return 1
            ;;
    esac
}

# Cleanup function
cleanup() {
    if [[ -n "${BUILD_DIR:-}" && -d "$BUILD_DIR" ]]; then
        log_info "Cleaning up build directory: $BUILD_DIR"
        rm -rf "$BUILD_DIR"
    fi
}

# Main execution
main() {
    log_info "Java Implementation Test Suite"
    log_info "=============================="
    log_info "Java Version: $JAVA_VERSION"
    log_info "Implementation: $IMPLEMENTATION"
    log_info "Test Mode: $TEST_MODE"
    log_info ""

    # Validate inputs
    if ! validate_java_version "$JAVA_VERSION" "$IMPLEMENTATION"; then
        exit 1
    fi

    # Determine actual implementation
    actual_impl=$(determine_implementation "$JAVA_VERSION" "$IMPLEMENTATION")
    if [[ $? -ne 0 ]]; then
        exit 1
    fi

    log_info "Selected implementation: $actual_impl"
    log_info ""

    # Set trap for cleanup
    trap cleanup EXIT

    # Run tests
    if run_test_suite "$actual_impl" "$TEST_MODE"; then
        log_success "All tests passed for Java $JAVA_VERSION with $actual_impl implementation!"
        exit 0
    else
        log_error "Tests failed for Java $JAVA_VERSION with $actual_impl implementation"
        exit 1
    fi
}

# Help function
show_help() {
    cat << EOF
Java Implementation Test Suite

Usage: $0 [java_version] [implementation] [test_mode]

Arguments:
  java_version    Java version to test (11, 17, 21, 24, 25) [default: 24] (25+ required for FFM)
  implementation  Implementation to test (auto, ffm, jni) [default: auto]
  test_mode      Test mode (build, maven, full) [default: build]

Examples:
  $0                          # Test Java 25 with auto implementation (JNI - default)
  $0 25 ffm build            # Test Java 25 with FFM (optional), build only
  $0 11 jni maven            # Test Java 11 with JNI, Maven artifacts
  $0 25 auto full            # Test Java 25 with auto selection (JNI), full suite

Test Modes:
  build   - Basic build configuration test
  maven   - Maven artifact generation and validation
  full    - Both build and Maven tests

Supported Matrix:
EOF

    for version in "${!JAVA_VERSIONS[@]}"; do
        echo "  Java $version: ${JAVA_VERSIONS[$version]}"
    done
}

# Check for help request
if [[ "${1:-}" == "-h" || "${1:-}" == "--help" ]]; then
    show_help
    exit 0
fi

# Run main function
main
```

### `.github/scripts/test-maven-consumer.sh`

```bash
#!/bin/bash

# Test script to validate deployed Maven artifacts
# Usage: ./test-maven-consumer.sh [version] [repository-url]

set -e

VERSION="${1:-2.0.0-3}"
REPOSITORY_URL="${2:-https://maven.pkg.github.com/HDFGroup/hdf5}"

echo "=== Testing HDF5 Maven Artifacts ==="
echo "Version: ${VERSION}"
echo "Repository: ${REPOSITORY_URL}"
echo ""

# Create temporary test directory
TEST_DIR=$(mktemp -d)
echo "Test directory: ${TEST_DIR}"
cd "${TEST_DIR}"

# Create a simple Maven test project
cat > pom.xml << EOF
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.hdfgroup.test</groupId>
    <artifactId>hdf5-maven-test</artifactId>
    <version>1.0.0</version>

    <properties>
        <maven.compiler.source>11</maven.compiler.source>
        <maven.compiler.target>11</maven.compiler.target>
        <hdf5.version>${VERSION}</hdf5.version>
    </properties>

    <repositories>
        <repository>
            <id>github-hdf5</id>
            <url>${REPOSITORY_URL}</url>
        </repository>
    </repositories>

    <dependencies>
        <!-- HDF5 Java Library (platform-specific) -->
        <dependency>
            <groupId>org.hdfgroup</groupId>
            <artifactId>hdf5-java</artifactId>
            <version>\${hdf5.version}</version>
            <classifier>linux-x86_64</classifier>
        </dependency>

        <!-- HDF5 Java Examples -->
        <dependency>
            <groupId>org.hdfgroup</groupId>
            <artifactId>hdf5-java-examples</artifactId>
            <version>\${hdf5.version}</version>
        </dependency>
    </dependencies>
</project>
EOF

# Create a simple test class
mkdir -p src/main/java/org/hdfgroup/test
cat > src/main/java/org/hdfgroup/test/TestConsumer.java << 'EOF'
package org.hdfgroup.test;

public class TestConsumer {
    public static void main(String[] args) {
        System.out.println("Testing HDF5 Maven artifact consumption...");

        try {
            // Try to load HDF5 Java classes
            Class.forName("hdf.hdf5lib.H5");
            System.out.println("âœ“ HDF5 Java library classes found");
        } catch (ClassNotFoundException e) {
            System.out.println("âš  HDF5 Java library classes not found: " + e.getMessage());
        }

        System.out.println("âœ“ Maven artifact consumption test completed");
    }
}
EOF

echo "=== Testing Maven Dependency Resolution ==="

# Test dependency resolution
if mvn dependency:resolve -q; then
    echo "âœ“ Maven dependencies resolved successfully"
else
    echo "âŒ Maven dependency resolution failed"
    exit 1
fi

# Test compilation
echo "=== Testing Compilation ==="
if mvn compile -q; then
    echo "âœ“ Compilation successful"
else
    echo "âŒ Compilation failed"
    exit 1
fi

# List resolved dependencies
echo "=== Resolved Dependencies ==="
mvn dependency:list | grep org.hdfgroup || echo "No org.hdfgroup dependencies found"

# Show artifact details
echo "=== Artifact Details ==="
find ~/.m2/repository/org/hdfgroup -name "*.jar" 2>/dev/null | head -10 | while read jar; do
    echo "Found: $(basename "$jar") ($(du -h "$jar" | cut -f1))"
done

echo ""
echo "=== Test Summary ==="
echo "âœ“ Maven artifact consumption test completed successfully"
echo "âœ“ HDF5 Java artifacts are accessible via Maven"
echo "âœ“ Dependencies resolve and compile correctly"
echo ""
echo "Cleanup: rm -rf ${TEST_DIR}"

# Cleanup
cd /
rm -rf "${TEST_DIR}"
```

### `.github/scripts/validate-maven-artifacts.sh`

```bash
#!/bin/bash
#
# Enhanced validation framework for Maven artifacts before deployment
# This script validates JAR files, POM files, and deployment readiness
#

set -euo pipefail

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/../.." && pwd)"
ARTIFACTS_DIR="${1:-./artifacts}"
VALIDATION_LOG="/tmp/maven-validation-$(date +%s).log"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $*" | tee -a "${VALIDATION_LOG}"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $*" | tee -a "${VALIDATION_LOG}"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $*" | tee -a "${VALIDATION_LOG}"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $*" | tee -a "${VALIDATION_LOG}"
}

# Validation counters
VALIDATION_ERRORS=0
VALIDATION_WARNINGS=0

# Error tracking
add_error() {
    VALIDATION_ERRORS=$((VALIDATION_ERRORS + 1))
    log_error "$*"
}

add_warning() {
    VALIDATION_WARNINGS=$((VALIDATION_WARNINGS + 1))
    log_warn "$*"
}

# Java/Maven environment validation
validate_environment() {
    log_info "Validating build environment..."

    # Check Java availability
    if ! command -v java &> /dev/null; then
        add_error "Java is not installed or not in PATH"
        return 1
    fi

    JAVA_VERSION=$(java -version 2>&1 | head -n1 | cut -d'"' -f2)
    log_info "Java version: ${JAVA_VERSION}"

    # Check Maven availability
    if ! command -v mvn &> /dev/null; then
        add_warning "Maven is not installed - some validations will be skipped"
    else
        MVN_VERSION=$(mvn -version | head -n1 | cut -d' ' -f3)
        log_info "Maven version: ${MVN_VERSION}"
    fi

    # Check JAR command
    if ! command -v jar &> /dev/null; then
        add_error "jar command is not available"
        return 1
    fi

    log_success "Environment validation completed"
    return 0
}

# JAR file validation
validate_jar_file() {
    local jar_file="$1"
    local jar_basename
    jar_basename=$(basename "${jar_file}")

    log_info "Validating JAR: ${jar_basename}"

    # Check file exists and is readable
    if [[ ! -f "${jar_file}" ]]; then
        add_error "JAR file not found: ${jar_file}"
        return 1
    fi

    if [[ ! -r "${jar_file}" ]]; then
        add_error "JAR file not readable: ${jar_file}"
        return 1
    fi

    # Check file size (must be > 1KB)
    local file_size
    file_size=$(stat -c%s "${jar_file}" 2>/dev/null || stat -f%z "${jar_file}" 2>/dev/null || echo "0")
    if [[ ${file_size} -lt 1024 ]]; then
        add_error "JAR file too small: ${jar_file} (${file_size} bytes)"
        return 1
    fi
    log_info "JAR size: ${file_size} bytes"

    # Test JAR integrity
    if ! jar tf "${jar_file}" > /dev/null 2>&1; then
        add_error "JAR file is corrupted or invalid: ${jar_file}"
        return 1
    fi

    # Check for required HDF5 Java classes
    local temp_dir
    temp_dir=$(mktemp -d)
    trap "rm -rf '${temp_dir}'" EXIT

    if ! (cd "${temp_dir}" && jar xf "${jar_file}"); then
        add_error "Failed to extract JAR: ${jar_file}"
        rm -rf "${temp_dir}"
        return 1
    fi

    # Check for essential HDF5 classes based on JAR type
    # FFM builds have two separate JARs:
    #   - javahdf5-*.jar: FFM bindings (org/hdfgroup/javahdf5/*)
    #   - jarhdf5-*.jar: Wrapper classes (hdf/hdf5lib/*)
    # JNI builds have single JAR with hdf/hdf5lib/* classes

    if [[ "${jar_basename}" == *"javahdf5"* ]]; then
        # This is the FFM bindings JAR - check for FFM classes
        local ffm_classes=(
            "org/hdfgroup/javahdf5/hdf5_h.class"
        )

        local has_ffm=false
        for class_file in "${ffm_classes[@]}"; do
            if [[ -f "${temp_dir}/${class_file}" ]]; then
                has_ffm=true
                log_info "Found FFM binding class: ${class_file}"
                break
            fi
        done

        if [[ "${has_ffm}" == "false" ]]; then
            add_error "FFM bindings JAR missing required FFM classes (expected org/hdfgroup/javahdf5/hdf5_h.class)"
        fi
    else
        # This is a wrapper/JNI JAR - check for hdf.hdf5lib classes
        local required_classes=(
            "hdf/hdf5lib/H5.class"
            "hdf/hdf5lib/HDF5Constants.class"
            "hdf/hdf5lib/HDFArray.class"
            "hdf/hdf5lib/HDFNativeData.class"
        )

        for class_file in "${required_classes[@]}"; do
            if [[ ! -f "${temp_dir}/${class_file}" ]]; then
                add_error "Missing required class in JAR: ${class_file}"
            fi
        done
    fi

    # Check manifest
    if [[ -f "${temp_dir}/META-INF/MANIFEST.MF" ]]; then
        if grep -q "Enable-Native-Access: ALL-UNNAMED" "${temp_dir}/META-INF/MANIFEST.MF"; then
            log_info "Native access enabled in manifest"
        else
            add_warning "Native access not found in manifest - may cause runtime issues"
        fi
    else
        add_warning "No manifest found in JAR"
    fi

    rm -rf "${temp_dir}"
    log_success "JAR validation completed: ${jar_basename}"
    return 0
}

# POM file validation
validate_pom_file() {
    local pom_file="$1"

    log_info "Validating POM: $(basename "${pom_file}")"

    # Check file exists
    if [[ ! -f "${pom_file}" ]]; then
        add_error "POM file not found: ${pom_file}"
        return 1
    fi

    # Check XML validity
    if command -v xmllint &> /dev/null; then
        if ! xmllint --noout "${pom_file}" 2>/dev/null; then
            add_error "POM file is not valid XML: ${pom_file}"
            return 1
        fi
    else
        add_warning "xmllint not available - skipping XML validation"
    fi

    # Check required Maven coordinates
    if ! grep -q "<groupId>org.hdfgroup</groupId>" "${pom_file}"; then
        add_error "Invalid or missing groupId in POM"
    fi

    if ! grep -qE "<artifactId>hdf5-java(-ffm|-jni)?</artifactId>" "${pom_file}"; then
        add_error "Invalid or missing artifactId in POM (expected hdf5-java, hdf5-java-ffm, or hdf5-java-jni)"
    fi

    # Extract version
    local version
    version=$(grep -o '<version>[^<]*</version>' "${pom_file}" | head -1 | sed 's/<[^>]*>//g' || echo "")
    if [[ -z "${version}" ]]; then
        add_error "No version found in POM"
    else
        log_info "POM version: ${version}"

        # Validate version format
        if [[ ! "${version}" =~ ^[0-9]+\.[0-9]+\.[0-9]+(-[0-9]+)?(-SNAPSHOT)?$ ]]; then
            add_warning "Version format may not comply with Maven conventions: ${version}"
        fi
    fi

    # Check for required sections
    local required_sections=(
        "<name>"
        "<description>"
        "<url>"
        "<licenses>"
        "<developers>"
        "<scm>"
    )

    for section in "${required_sections[@]}"; do
        if ! grep -q "${section}" "${pom_file}"; then
            add_warning "Missing recommended section in POM: ${section}"
        fi
    done

    # Check dependencies
    if grep -q "<dependencies>" "${pom_file}"; then
        log_info "Dependencies section found in POM"
    else
        add_warning "No dependencies section in POM"
    fi

    log_success "POM validation completed"
    return 0
}

# Version consistency validation
validate_version_consistency() {
    local pom_file="$1"
    shift
    local jar_files=("$@")

    log_info "Validating version consistency across artifacts..."

    # Extract version from POM
    local pom_version
    pom_version=$(grep -o '<version>[^<]*</version>' "${pom_file}" | head -1 | sed 's/<[^>]*>//g' || echo "")

    if [[ -z "${pom_version}" ]]; then
        add_error "Cannot extract version from POM for consistency check"
        return 1
    fi

    log_info "POM version: ${pom_version}"

    # Check JAR filenames for version consistency
    for jar_file in "${jar_files[@]}"; do
        local jar_basename
        jar_basename=$(basename "${jar_file}")

        # Extract version from JAR filename (allowing for classifiers)
        local jar_version
        jar_version=$(echo "${jar_basename}" | sed -E 's/.*-([0-9]+\.[0-9]+\.[0-9]+(-[0-9]+)?(-SNAPSHOT)?)(-[^.]+)?\.jar$/\1/' || echo "")

        if [[ -z "${jar_version}" ]]; then
            add_warning "Cannot extract version from JAR filename: ${jar_basename}"
        elif [[ "${jar_version}" != "${pom_version}" ]]; then
            add_error "Version mismatch: POM=${pom_version}, JAR=${jar_version} (${jar_basename})"
        else
            log_info "Version consistency verified: ${jar_basename}"
        fi
    done

    return 0
}

# Platform classifier validation
validate_platform_classifiers() {
    local jar_files=("$@")

    log_info "Validating platform classifiers..."

    local valid_classifiers=(
        "linux-x86_64"
        "windows-x86_64"
        "macos-x86_64"
        "macos-aarch64"
    )

    for jar_file in "${jar_files[@]}"; do
        local jar_basename
        jar_basename=$(basename "${jar_file}")

        # Skip universal JARs (no classifier)
        if [[ ! "${jar_basename}" =~ -[a-z]+-[a-z0-9_]+\.jar$ ]]; then
            log_info "Universal JAR (no classifier): ${jar_basename}"
            continue
        fi

        # Extract classifier
        local classifier
        classifier=$(echo "${jar_basename}" | sed -E 's/.*-([a-z]+-[a-z0-9_]+)\.jar$/\1/' || echo "")

        if [[ -z "${classifier}" ]]; then
            add_warning "Cannot extract classifier from JAR: ${jar_basename}"
            continue
        fi

        # Validate classifier
        local valid=false
        for valid_classifier in "${valid_classifiers[@]}"; do
            if [[ "${classifier}" == "${valid_classifier}" ]]; then
                valid=true
                break
            fi
        done

        if [[ "${valid}" == "true" ]]; then
            log_info "Valid platform classifier: ${classifier} (${jar_basename})"
        else
            add_error "Invalid platform classifier: ${classifier} (${jar_basename})"
        fi
    done

    return 0
}

# Maven dependency simulation
simulate_maven_dependency() {
    local pom_file="$1"

    if ! command -v mvn &> /dev/null; then
        add_warning "Maven not available - skipping dependency simulation"
        return 0
    fi

    log_info "Simulating Maven dependency resolution..."

    # Create temporary Maven project
    local temp_project
    temp_project=$(mktemp -d)
    trap "rm -rf '${temp_project}'" EXIT

    # Extract coordinates from POM
    local group_id artifact_id version
    group_id=$(grep -o '<groupId>[^<]*</groupId>' "${pom_file}" | head -1 | sed 's/<[^>]*>//g' || echo "")
    artifact_id=$(grep -o '<artifactId>[^<]*</artifactId>' "${pom_file}" | head -1 | sed 's/<[^>]*>//g' || echo "")
    version=$(grep -o '<version>[^<]*</version>' "${pom_file}" | head -1 | sed 's/<[^>]*>//g' || echo "")

    # Find the JAR file in the artifacts directory
    local jar_file
    jar_file=$(find "$(dirname "${pom_file}")" -maxdepth 2 -name "${artifact_id}-${version}.jar" -o -name "${artifact_id}-*.jar" | head -1)

    if [ -z "${jar_file}" ]; then
        add_warning "Could not find JAR file for ${artifact_id}:${version} - skipping dependency simulation"
        return 0
    fi

    # Install artifact to local Maven repository first
    log_info "Installing artifact to local Maven repository: ${group_id}:${artifact_id}:${version}"
    if ! mvn install:install-file \
        -Dfile="${jar_file}" \
        -DgroupId="${group_id}" \
        -DartifactId="${artifact_id}" \
        -Dversion="${version}" \
        -Dpackaging=jar \
        -DpomFile="${pom_file}" \
        -q 2>&1 | tee -a "${VALIDATION_LOG}"; then
        add_warning "Failed to install artifact to local Maven repository"
        return 0
    fi

    # Create test POM
    cat > "${temp_project}/pom.xml" << EOF
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
         http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>test</groupId>
    <artifactId>maven-validation-test</artifactId>
    <version>1.0.0</version>
    <dependencies>
        <dependency>
            <groupId>${group_id}</groupId>
            <artifactId>${artifact_id}</artifactId>
            <version>${version}</version>
        </dependency>
    </dependencies>
</project>
EOF

    # Test dependency resolution
    if (cd "${temp_project}" && mvn dependency:resolve -q); then
        log_success "Maven dependency simulation passed"
    else
        add_warning "Maven dependency simulation failed - may indicate packaging issues"
    fi

    rm -rf "${temp_project}"
    return 0
}

# Deployment readiness check
check_deployment_readiness() {
    local artifacts_dir="$1"

    log_info "Checking deployment readiness..."

    # Check for required files
    local jar_files pom_files
    # Only count HDF5 JAR files, exclude dependencies like slf4j
    jar_files=($(find "${artifacts_dir}" -name "*hdf5*.jar" -not -name "*test*" 2>/dev/null || true))
    pom_files=($(find "${artifacts_dir}" -name "pom.xml" 2>/dev/null || true))

    if [[ ${#jar_files[@]} -eq 0 ]]; then
        add_error "No JAR files found in artifacts directory"
        return 1
    fi

    if [[ ${#pom_files[@]} -eq 0 ]]; then
        add_error "No POM files found in artifacts directory"
        return 1
    fi

    log_info "Found ${#jar_files[@]} JAR file(s) and ${#pom_files[@]} POM file(s)"

    # Check environment variables for deployment
    if [[ -z "${MAVEN_USERNAME:-}" ]]; then
        add_warning "MAVEN_USERNAME not set - deployment will fail"
    fi

    if [[ -z "${MAVEN_PASSWORD:-}" ]]; then
        add_warning "MAVEN_PASSWORD not set - deployment will fail"
    fi

    return 0
}

# Generate validation report
generate_report() {
    local artifacts_dir="$1"

    log_info "=== Maven Artifact Validation Report ==="
    log_info "Timestamp: $(date)"
    log_info "Artifacts directory: ${artifacts_dir}"
    log_info "Validation log: ${VALIDATION_LOG}"
    echo

    # Summary
    if [[ ${VALIDATION_ERRORS} -eq 0 ]]; then
        if [[ ${VALIDATION_WARNINGS} -eq 0 ]]; then
            log_success "âœ… All validations passed with no warnings"
        else
            log_warn "âš ï¸  All validations passed with ${VALIDATION_WARNINGS} warning(s)"
        fi
    else
        log_error "âŒ Validation failed with ${VALIDATION_ERRORS} error(s) and ${VALIDATION_WARNINGS} warning(s)"
    fi

    echo
    log_info "Full validation log available at: ${VALIDATION_LOG}"

    return ${VALIDATION_ERRORS}
}

# Main validation function
main() {
    local artifacts_dir="${1:-./artifacts}"

    log_info "Starting Maven artifact validation..."
    log_info "Artifacts directory: ${artifacts_dir}"

    # Check artifacts directory
    if [[ ! -d "${artifacts_dir}" ]]; then
        add_error "Artifacts directory not found: ${artifacts_dir}"
        generate_report "${artifacts_dir}"
        exit 1
    fi

    # Environment validation
    validate_environment

    # Find artifacts
    local jar_files pom_files all_jars
    # Only validate HDF5 JAR files, exclude dependencies like slf4j
    jar_files=($(find "${artifacts_dir}" -name "*hdf5*.jar" -not -name "*test*" 2>/dev/null || true))
    pom_files=($(find "${artifacts_dir}" -name "pom.xml" 2>/dev/null || true))
    all_jars=($(find "${artifacts_dir}" -name "*.jar" 2>/dev/null || true))

    # Log what we found
    log_info "Found ${#all_jars[@]} total JAR file(s), ${#jar_files[@]} HDF5 JAR file(s) to validate"
    if [[ ${#all_jars[@]} -gt ${#jar_files[@]} ]]; then
        log_info "Skipping non-HDF5 JAR files (dependencies like slf4j, etc.)"
        for jar in "${all_jars[@]}"; do
            if [[ ! "$(basename "$jar")" =~ hdf5 ]]; then
                log_info "  Skipping: $(basename "$jar")"
            fi
        done
    fi

    # Basic readiness check
    check_deployment_readiness "${artifacts_dir}"

    # Validate each JAR file
    for jar_file in "${jar_files[@]}"; do
        validate_jar_file "${jar_file}"
    done

    # Validate each POM file
    for pom_file in "${pom_files[@]}"; do
        validate_pom_file "${pom_file}"
    done

    # Version consistency check
    if [[ ${#pom_files[@]} -gt 0 && ${#jar_files[@]} -gt 0 ]]; then
        validate_version_consistency "${pom_files[0]}" "${jar_files[@]}"
    fi

    # Platform classifier validation
    if [[ ${#jar_files[@]} -gt 0 ]]; then
        validate_platform_classifiers "${jar_files[@]}"
    fi

    # Maven dependency simulation
    if [[ ${#pom_files[@]} -gt 0 ]]; then
        simulate_maven_dependency "${pom_files[0]}"
    fi

    # Generate final report
    generate_report "${artifacts_dir}"
    exit ${VALIDATION_ERRORS}
}

# Show usage if no arguments provided
if [[ $# -eq 0 ]]; then
    echo "Usage: $0 <artifacts_directory>"
    echo
    echo "Enhanced validation framework for Maven artifacts before deployment"
    echo
    echo "This script validates:"
    echo "  - JAR file integrity and content"
    echo "  - POM file structure and compliance"
    echo "  - Version consistency across artifacts"
    echo "  - Platform classifier conventions"
    echo "  - Maven dependency resolution simulation"
    echo "  - Deployment readiness"
    echo
    echo "Environment variables:"
    echo "  MAVEN_USERNAME - Maven repository username (optional for validation)"
    echo "  MAVEN_PASSWORD - Maven repository password (optional for validation)"
    echo
    exit 1
fi

# Run main function with arguments
main "$@"
```

### `.github/workflows/abi-report.yml`

```yaml
name: hdf5 Check Application Binary Interface (ABI)

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      use_tag:
        description: 'Release version tag'
        type: string
        required: false
        default: snapshot
      use_environ:
        description: 'Environment to locate files'
        type: string
        required: true
        default: snapshots
      file_base:
        description: "The common base name of the binary"
        required: true
        type: string
      file_ref:
        description: "The reference name for the release binary"
        required: true
        type: string

permissions:
  contents: read

jobs:
  check:
    runs-on: ubuntu-latest
    continue-on-error: true

    steps:
      - name: Install System dependencies
        run: |
          sudo apt update
          sudo apt install -q -y abi-compliance-checker abi-dumper
          sudo apt install -q -y japi-compliance-checker

      - name: Convert hdf5 reference name (Linux)
        id: convert-hdf5lib-refname
        run:  |
          FILE_DOTS=$(echo "${{ inputs.file_ref }}" | sed -r "s/([0-9]+)\.([0-9]+)\.([0-9]+)\.([0-9]+).*/\1\.\2\.\3-\4/")
          echo "HDF5R_DOTS=$FILE_DOTS" >> $GITHUB_OUTPUT
          FILE_DOTSMAIN=$(echo "${{ inputs.file_ref }}" | sed -r "s/([0-9]+)\.([0-9]+)\.([0-9]+).*/\1\.\2\.\3/")
          echo "HDF5R_DOTSMAIN=$FILE_DOTSMAIN" >> $GITHUB_OUTPUT

      - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Get published binary (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
            name: tgz-ubuntu-2404_gcc-binary
            path: ${{ github.workspace }}

      - name: List files for the space (Linux)
        run: |
          ls -l ${{ github.workspace }}

      - name: Uncompress gh binary (Linux)
        run: tar -zxvf ${{ github.workspace }}/${{ inputs.file_base }}-ubuntu-2404_gcc.tar.gz

      - name: Uncompress hdf5 binary (Linux)
        run:  |
          cd "${{ github.workspace }}/hdf5"
          tar -zxvf ${{ github.workspace }}/hdf5/HDF5-*-Linux.tar.gz --strip-components 1

      - name: List files for the HDF space (Linux)
        run: |
          ls -l ${{ github.workspace }}/hdf5
          ls -l ${{ github.workspace }}/hdf5/HDF_Group/HDF5

      - name: set hdf5lib name
        id: set-hdf5lib-name
        run: |
          HDF5DIR=${{ github.workspace }}/hdf5/HDF_Group/HDF5/
          FILE_NAME_HDF5=$(ls ${{ github.workspace }}/hdf5/HDF_Group/HDF5)
          FILE_VERS=$(echo "$FILE_NAME_HDF5" | sed -r "s/([0-9]+\.[0-9]+\.[0-9]+).*/\1/")
          echo "HDF5_ROOT=$HDF5DIR$FILE_NAME_HDF5" >> $GITHUB_OUTPUT
          echo "HDF5_VERS=$FILE_VERS" >> $GITHUB_OUTPUT

      - name: Download reference version
        run: |
          mkdir "${{ github.workspace }}/hdf5R"
          cd "${{ github.workspace }}/hdf5R"
          wget -q https://github.com/HDFGroup/hdf5/releases/download/hdf5_${{ inputs.file_ref }}/hdf5-${{ steps.convert-hdf5lib-refname.outputs.HDF5R_DOTS }}-ubuntu-2404_gcc.tar.gz
          tar zxf hdf5-${{ steps.convert-hdf5lib-refname.outputs.HDF5R_DOTS }}-ubuntu-2404_gcc.tar.gz

      - name: List files for the space (Linux)
        run: |
          ls -l ${{ github.workspace }}/hdf5R

      - name: Uncompress hdf5 reference binary (Linux)
        run:  |
          cd "${{ github.workspace }}/hdf5R"
          tar -zxvf ${{ github.workspace }}/hdf5R/hdf5/HDF5-${{ inputs.file_ref }}-Linux.tar.gz --strip-components 1

      - name: List files for the HDFR space (Linux)
        run: |
          ls -l ${{ github.workspace }}/hdf5R
          ls -l ${{ github.workspace }}/hdf5R/HDF_Group/HDF5

      - name: set hdf5lib reference name
        id: set-hdf5lib-refname
        run: |
          HDF5RDIR=${{ github.workspace }}/hdf5R/HDF_Group/HDF5/
          FILE_NAME_HDF5R=$(ls ${{ github.workspace }}/hdf5R/HDF_Group/HDF5)
          echo "HDF5R_ROOT=$HDF5RDIR$FILE_NAME_HDF5R" >> $GITHUB_OUTPUT
          echo "HDF5R_VERS=$FILE_NAME_HDF5R" >> $GITHUB_OUTPUT

      - name: List files for the lib spaces (Linux)
        run: |
          ls -l ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/lib
          ls -l ${{ steps.set-hdf5lib-refname.outputs.HDF5R_ROOT }}/lib

      - name: Run Java API report
        run: |
          japi-compliance-checker ${{ steps.set-hdf5lib-refname.outputs.HDF5R_ROOT }}/lib/jarhdf5-${{ steps.convert-hdf5lib-refname.outputs.HDF5R_DOTSMAIN }}.jar ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/lib/jarhdf5-${{ steps.set-hdf5lib-name.outputs.HDF5_VERS }}.jar
        continue-on-error: true

      - name: Run ABI report
        run: |
          abi-dumper ${{ steps.set-hdf5lib-refname.outputs.HDF5R_ROOT }}/lib/libhdf5.so -o ABI-0.dump -public-headers ${{ steps.set-hdf5lib-refname.outputs.HDF5R_ROOT }}/include
          abi-dumper ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/lib/libhdf5.so -o ABI-1.dump -public-headers ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/include
          abi-compliance-checker -l ${{ inputs.file_base }} -old ABI-0.dump -new ABI-1.dump
        continue-on-error: true

      - name: Run hl ABI report
        run: |
          abi-dumper ${{ steps.set-hdf5lib-refname.outputs.HDF5R_ROOT }}/lib/libhdf5_hl.so -o ABI-2.dump -public-headers ${{ steps.set-hdf5lib-refname.outputs.HDF5R_ROOT }}/include
          abi-dumper ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/lib/libhdf5_hl.so -o ABI-3.dump -public-headers ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/include
          abi-compliance-checker -l ${{ inputs.file_base }}_hl -old ABI-2.dump -new ABI-3.dump
        continue-on-error: true

      - name: Run cpp ABI report
        run: |
          abi-dumper ${{ steps.set-hdf5lib-refname.outputs.HDF5R_ROOT }}/lib/libhdf5_cpp.so -o ABI-4.dump -public-headers ${{ steps.set-hdf5lib-refname.outputs.HDF5R_ROOT }}/include
          abi-dumper ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/lib/libhdf5_cpp.so -o ABI-5.dump -public-headers ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/include
          abi-compliance-checker -l ${{ inputs.file_base }}_cpp -old ABI-4.dump -new ABI-5.dump
        continue-on-error: true

      - name: Run hl_cpp ABI report
        run: |
          abi-dumper ${{ steps.set-hdf5lib-refname.outputs.HDF5R_ROOT }}/lib/libhdf5_hl_cpp.so -o ABI-6.dump -public-headers ${{ steps.set-hdf5lib-refname.outputs.HDF5R_ROOT }}/include
          abi-dumper ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/lib/libhdf5_hl_cpp.so -o ABI-7.dump -public-headers ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/include
          abi-compliance-checker -l ${{ inputs.file_base }}_hl_cpp -old ABI-6.dump -new ABI-7.dump
        continue-on-error: true

      - name: Copy ABI reports
        run: |
          cp compat_reports/jarhdf5-/${{ steps.convert-hdf5lib-refname.outputs.HDF5R_DOTSMAIN }}_to_${{ steps.set-hdf5lib-name.outputs.HDF5_VERS }}/compat_report.html ${{ inputs.file_base }}-java_compat_report.html
          ls -l compat_reports/${{ inputs.file_base }}/X_to_Y
          cp compat_reports/${{ inputs.file_base }}/X_to_Y/compat_report.html ${{ inputs.file_base }}-hdf5_compat_report.html
          ls -l compat_reports/${{ inputs.file_base }}_hl/X_to_Y
          cp compat_reports/${{ inputs.file_base }}_hl/X_to_Y/compat_report.html ${{ inputs.file_base }}-hdf5_hl_compat_report.html
          ls -l compat_reports/${{ inputs.file_base }}_cpp/X_to_Y
          cp compat_reports/${{ inputs.file_base }}_cpp/X_to_Y/compat_report.html ${{ inputs.file_base }}-hdf5_cpp_compat_report.html
          ls -l compat_reports/${{ inputs.file_base }}_hl_cpp/X_to_Y
          cp compat_reports/${{ inputs.file_base }}_hl_cpp/X_to_Y/compat_report.html ${{ inputs.file_base }}-hdf5_hl_cpp_compat_report.html
        continue-on-error: true

      - name: List files for the report spaces (Linux)
        run: |
          ls -l compat_reports
          ls -l *.html

      - name: Publish ABI reports
        id: publish-abi-reports
        run: |
          mkdir "${{ runner.workspace }}/buildabi"
          mkdir "${{ runner.workspace }}/buildabi/hdf5"
          cp ${{ inputs.file_base }}-hdf5_compat_report.html ${{ runner.workspace }}/buildabi/hdf5
          cp ${{ inputs.file_base }}-hdf5_hl_compat_report.html ${{ runner.workspace }}/buildabi/hdf5
          cp ${{ inputs.file_base }}-hdf5_cpp_compat_report.html ${{ runner.workspace }}/buildabi/hdf5
          cp ${{ inputs.file_base }}-java_compat_report.html ${{ runner.workspace }}/buildabi/hdf5
          cd "${{ runner.workspace }}/buildabi"
          tar -zcvf ${{ inputs.file_base }}.html.abi.reports.tar.gz hdf5
        shell: bash

      - name: Save output as artifact
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: abi-reports
          path: |
            ${{ runner.workspace }}/buildabi/${{ inputs.file_base }}.html.abi.reports.tar.gz
```

### `.github/workflows/analysis.yml`

```yaml
name: hdf5 dev CTest analysis runs

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      snap_name:
        description: 'The name in the source tarballs'
        type: string
        required: false
        default: hdfsrc
      file_base:
        description: "The common base name of the source tarballs"
        required: true
        type: string
      use_environ:
        description: 'Environment to locate files'
        type: string
        required: true
        default: snapshots

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  coverage_test_linux_GCC:
  # Linux (Ubuntu) w/ gcc + coverage
  #
    name: "Ubuntu GCC Coverage"
    runs-on: ubuntu-22.04
    steps:
    - name: Install Dependencies (Linux_coverage)
      run: |
        sudo apt update
        sudo apt-get install ninja-build doxygen graphviz curl build-essential
        sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
        sudo apt-get install lcov -q -y

    - name: Set file base name (Linux_coverage)
      id: set-file-base
      run: |
        FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
        echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
        if [[ '${{ inputs.use_environ }}' == 'release' ]]
        then
          SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
        else
          SOURCE_NAME_BASE=$(echo "hdfsrc")
        fi
        echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

    # Get files created by release script
    - name: Get tgz-tarball (Linux_coverage)
      uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
      with:
        name: tgz-tarball
        path: ${{ github.workspace }}

    - name: List files for the space (Linux_coverage)
      run: |
        ls -l ${{ github.workspace }}
        ls ${{ runner.workspace }}

    - name: Uncompress source (Linux_coverage)
      run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

    - name: Copy script files for the space (Linux_coverage)
      run: |
        cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
        cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

    - name: List files for the hdf5 (Linux_coverage)
      run: |
        ls ${{ runner.workspace }}/hdf5

    - name: Create options file (Linux_coverage)
      uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
      with:
        path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
        write-mode: overwrite
        contents: |
            set (CTEST_DROP_SITE_INIT "my.cdash.org")
            # Change following line to submit to your CDash dashboard to a different CDash project
            set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
            #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
            set (LOCAL_COVERAGE_TEST "TRUE")
            set (LOCAL_USE_GCOV "TRUE")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DBUILD_SHARED_LIBS:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DBUILD_STATIC_LIBS:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_COVERAGE:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DCODE_COVERAGE:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_PACK_EXAMPLES:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_PACKAGE_EXTLIBS:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_NO_PACKAGES:BOOL=ON")

    - name: Run CTest (Linux_coverage)
      run: |
        cd "${{ runner.workspace }}/hdf5"
        ctest -S HDF5config.cmake,CTEST_SITE_EXT=${{ github.event.repository.full_name }}_COV,LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C Debug -O hdf5.log
      shell: bash
      continue-on-error: true

    # Save log files created by CTest script
    - name: Save log (Linux_coverage)
      uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
      with:
        name: clang-coverage-log
        path: ${{ runner.workspace }}/hdf5/hdf5.log
        if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

  build_and_test_linux_LeakSanitizer:
  # Linux (Ubuntu) w/ clang + LeakSanitizer
  #
    name: "Ubuntu Clang LeakSanitizer"
    runs-on: ubuntu-22.04
    steps:
      - name: Install Dependencies (Linux_Leak)
        run: |
          sudo apt update
          sudo apt-get install ninja-build doxygen graphviz curl libtinfo5

      - name: add clang to env
        uses: KyleMayes/install-llvm-action@98e68e10c96dffcb7bfed8b2144541a66b49aa02 # v2.0.8
        id: setup-clang
        with:
          env: true
          version: '18.1'

      - name: Set file base name (Linux_Leak)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (Linux_Leak)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: tgz-tarball
          path: ${{ github.workspace }}

      - name: List files for the space (Linux_Leak)
        run: |
          ls -l ${{ github.workspace }}
          ls ${{ runner.workspace }}

      - name: Uncompress source (Linux_Leak)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Copy script files for the space (Linux_Leak)
        run: |
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

      - name: List files for the hdf5 (Linux_Leak)
        run: |
          ls ${{ runner.workspace }}/hdf5

      - name: Create options file (Linux_Leak)
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
              set (CTEST_DROP_SITE_INIT "my.cdash.org")
              # Change following line to submit to your CDash dashboard to a different CDash project
              set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
              #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
              set (MODEL "Sanitize")
              set (GROUP "Sanitize")
              set (LOCAL_MEMCHECK_TEST "TRUE")
              set (CTEST_MEMORYCHECK_TYPE "LeakSanitizer")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DBUILD_SHARED_LIBS:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DBUILD_STATIC_LIBS:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SANITIZERS:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_USE_SANITIZER:STRING=Leak")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest (Linux_Leak)
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=${{ github.event.repository.full_name }}-LEAK,LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C Debug -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (Linux_Leak)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: leak-ubuntu-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

  build_and_test_linux_AddressSanitizer:
  # Linux (Ubuntu) w/ clang + AddressSanitizer
  #
    name: "Ubuntu Clang AddressSanitizer"
    runs-on: ubuntu-22.04
    steps:
      - name: Install Dependencies (Linux_Address)
        run: |
          sudo apt update
          sudo apt-get install ninja-build doxygen graphviz curl libtinfo5

      - name: add clang to env
        uses: KyleMayes/install-llvm-action@98e68e10c96dffcb7bfed8b2144541a66b49aa02 # v2.0.8
        id: setup-clang
        with:
          env: true
          version: '18.1'

      - name: Set file base name (Linux_Address)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (Linux_Address)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: tgz-tarball
          path: ${{ github.workspace }}

      - name: List files for the space (Linux_Address)
        run: |
          ls -l ${{ github.workspace }}
          ls ${{ runner.workspace }}

      - name: Uncompress source (Linux_Address)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Copy script files for the space (Linux_Address)
        run: |
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

      - name: List files for the hdf5 (Linux_Address)
        run: |
          ls ${{ runner.workspace }}/hdf5

      - name: Create options file (Linux_Address)
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
              set (CTEST_DROP_SITE_INIT "my.cdash.org")
              # Change following line to submit to your CDash dashboard to a different CDash project
              set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
              #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
              set (MODEL "Sanitize")
              set (GROUP "Sanitize")
              set (LOCAL_MEMCHECK_TEST "TRUE")
              set (CTEST_MEMORYCHECK_TYPE "AddressSanitizer")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DBUILD_SHARED_LIBS:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DBUILD_STATIC_LIBS:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SANITIZERS:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_USE_SANITIZER:STRING=Address")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest (Linux_Address)
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=${{ github.event.repository.full_name }}-ADDR,LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C Debug -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (Linux_Address)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: address-ubuntu-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

  build_and_test_linux_UndefinedBehaviorSanitizer:
  # Linux (Ubuntu) w/ clang + UndefinedBehaviorSanitizer
  #
    name: "Ubuntu Clang UndefinedBehaviorSanitizer"
    runs-on: ubuntu-22.04
    steps:
      - name: Install Dependencies (Linux_UndefinedBehavior)
        run: |
          sudo apt update
          sudo apt-get install ninja-build doxygen graphviz curl libtinfo5

      - name: add clang to env
        uses: KyleMayes/install-llvm-action@98e68e10c96dffcb7bfed8b2144541a66b49aa02 # v2.0.8
        id: setup-clang
        with:
          env: true
          version: '18.1'

      - name: Set file base name (Linux_UndefinedBehavior)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (Linux_UndefinedBehavior)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: tgz-tarball
          path: ${{ github.workspace }}

      - name: List files for the space (Linux_UndefinedBehavior)
        run: |
          ls -l ${{ github.workspace }}
          ls ${{ runner.workspace }}

      - name: Uncompress source (Linux_UndefinedBehavior)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Copy script files for the space (Linux_UndefinedBehavior)
        run: |
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

      - name: List files for the hdf5 (Linux_UndefinedBehavior)
        run: |
          ls ${{ runner.workspace }}/hdf5

      - name: Create options file (Linux_UndefinedBehavior)
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
              set (CTEST_DROP_SITE_INIT "my.cdash.org")
              # Change following line to submit to your CDash dashboard to a different CDash project
              set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
              #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
              set (MODEL "Sanitize")
              set (GROUP "Sanitize")
              set (LOCAL_MEMCHECK_TEST "TRUE")
              set (CTEST_MEMORYCHECK_TYPE "UndefinedBehaviorSanitizer")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DBUILD_SHARED_LIBS:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DBUILD_STATIC_LIBS:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SANITIZERS:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_USE_SANITIZER:STRING=Undefined")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest (Linux_UndefinedBehavior)
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=${{ github.event.repository.full_name }}-UNDEF,LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C Debug -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (Linux_UndefinedBehavior)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: undefined-ubuntu-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
```

### `.github/workflows/aocc.yml`

```yaml
name: hdf5 dev PAR aocc ompi

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  aocc_build_and_test:
    name: "aocc ${{ inputs.build_mode }}"
    runs-on: ubuntu-latest
    env:
      AOCCVERDOT: "5.0.0"
      AOCCVERDASH: "5-0"
      MPIVERDOT: "5.0.7"
      #MPIVERUND: Change is needed in the lines below.

    steps:
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install Dependencies
        shell: bash
        run: |
          sudo apt update
          sudo apt install -y libaec-dev ninja-build
          sudo apt install -y doxygen libncurses-dev libquadmath0 libstdc++6 libxml2
          sudo apt install -y zlib1g-dev libcurl4-openssl-dev libjpeg-dev wget curl bzip2
          sudo apt install -y m4 flex bison cmake libzip-dev openssl build-essential

      - name: Install ${{ env.AOCCVERDOT }} AOCC
        shell: bash
        run: |
          wget https://download.amd.com/developer/eula/aocc/aocc-${{ env.AOCCVERDASH }}/aocc-compiler-${{ env.AOCCVERDOT }}.tar
          tar -xvf aocc-compiler-${{ env.AOCCVERDOT }}.tar
          cd aocc-compiler-${{ env.AOCCVERDOT }}
          bash install.sh
          source /home/runner/work/hdf5/hdf5/setenv_AOCC.sh
          which clang
          which flang
          clang -v

      - name: Cache OpenMPI ${{ env.MPIVERDOT }} installation
        id: cache-openmpi-5_0_7
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: /home/runner/work/hdf5/hdf5/openmpi-${{ env.MPIVERDOT }}-install
          key: ${{ runner.os }}-${{ runner.arch }}-openmpi-5_0_7-cache

      - name: Install OpenMPI ${{ env.MPIVERDOT }}
        if: ${{ steps.cache-openmpi-5_0_7.outputs.cache-hit != 'true' }}
        run: |
          export LD_LIBRARY_PATH=/home/runner/work/hdf5/hdf5/aocc-compiler-${{ env.AOCCVERDOT }}/lib:/usr/local/lib
          wget https://download.open-mpi.org/release/open-mpi/v5.0/openmpi-${{ env.MPIVERDOT }}.tar.gz
          tar zxvf openmpi-${{ env.MPIVERDOT }}.tar.gz
          cd openmpi-${{ env.MPIVERDOT }}
          ./configure CC=/home/runner/work/hdf5/hdf5/aocc-compiler-${{ env.AOCCVERDOT }}/bin/clang FC=/home/runner/work/hdf5/hdf5/aocc-compiler-${{ env.AOCCVERDOT }}/bin/flang --prefix=/home/runner/work/hdf5/hdf5/openmpi-${{ env.MPIVERDOT }}-install
          make
          make install

      - name: Configure
        shell: bash
        run: |
          export LD_LIBRARY_PATH=/home/runner/work/hdf5/hdf5/aocc-compiler-${{ env.AOCCVERDOT }}/lib:/home/runner/work/hdf5/hdf5/openmpi-${{ env.MPIVERDOT }}-install/lib:/usr/local/lib
          export LD_RUN_PATH=/home/runner/work/hdf5/hdf5/aocc-compiler-${{ env.AOCCVERDOT }}/lib:/home/runner/work/hdf5/hdf5/openmpi-${{ env.MPIVERDOT }}-install/lib:/usr/local/lib
          export PATH=/home/runner/work/hdf5/hdf5/openmpi-${{ env.MPIVERDOT }}-install/bin:/usr/local/bin:$PATH
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          CC=mpicc cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake -G Ninja \
          -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
          -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF \
          -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF \
          -DHDF5_ENABLE_PARALLEL:BOOL=ON \
          -DHDF5_ENABLE_SUBFILING_VFD:BOOL=ON \
          -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
          -DLIBAEC_USE_LOCALCONTENT=OFF \
          -DZLIB_USE_LOCALCONTENT=OFF \
          -DHDF5_BUILD_FORTRAN:BOOL=OFF \
          -DHDF5_BUILD_JAVA:BOOL=OFF \
          -DMPIEXEC_MAX_NUMPROCS:STRING="2" \
          $GITHUB_WORKSPACE
          #cat src/libhdf5.settings

      - name: Build
        shell: bash
        run: |
          cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      - name: Run Tests
        shell: bash
        run: |
          ctest . -E MPI_TEST --parallel 2 -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      - name: Run Parallel Tests
        shell: bash
        run: |
          ctest . -R MPI_TEST -E "_by_chunk|_by_pattern" -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build
```

### `.github/workflows/arm-main.yml`

```yaml
name: hdf5 dev CI

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      cmake_version:
        description: "3.26.0 or later, latest"
        required: true
        type: string
      thread_safety:
        description: "TS or empty"
        required: true
        type: string
      concurrent:
        description: "CC or empty"
        required: true
        type: string
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string
      save_binary:
        description: "binary-ext-name or missing"
        required: false
        default: "skip"
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  build_and_test_win:
    name: "Windows MSVC-${{ inputs.build_mode }}-${{ inputs.thread_safety }}-${{ inputs.concurrent }}"
    runs-on: windows-11-arm
    if: ${{ inputs.build_mode != 'Debug' }}
    steps:
      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
          cmakeVersion: ${{ inputs.cmake_version }}
          ninjaVersion: latest

      - name: Check CMake Version
        shell: bash
        run: |
          which cmake
          cmake --version

      - name: Set up JDK 21
        uses: actions/setup-java@v5
        with:
          java-version: '21'
          distribution: 'temurin'

      - name: Set environment for MSVC (Windows)
        run: |
          # Set these environment variables so CMake picks the correct compiler
          echo "CXX=cl.exe" >> $GITHUB_ENV
          echo "CC=cl.exe" >> $GITHUB_ENV

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G "Visual Studio 17 2022" -A ARM64 \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DHDF5_ENABLE_DOXY_WARNINGS:BOOL=ON \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_JAVA=ON \
            -DHDF5_BUILD_DOC:BOOL=ON \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF \
            -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF \
            -DZLIB_USE_LOCALCONTENT:BOOL=OFF \
            -DHDF5_PACK_EXAMPLES:BOOL=ON \
            -DHDF5_PACKAGE_EXTLIBS:BOOL=ON \
            $GITHUB_WORKSPACE
        shell: bash
        if: ${{ inputs.thread_safety != 'TS' && inputs.concurrent != 'CC'}}

      - name: Configure (Thread-Safe)
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G "Visual Studio 17 2022" -A ARM64 \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DHDF5_ENABLE_THREADSAFE:BOOL=ON \
            -DHDF5_ENABLE_CONCURRENCY:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_JAVA:BOOL=OFF \
            -DHDF5_BUILD_HL_LIB:BOOL=OFF \
            -DHDF5_BUILD_DOC:BOOL=OFF \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF \
            -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF \
            -DZLIB_USE_LOCALCONTENT:BOOL=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=OFF \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=OFF \
            $GITHUB_WORKSPACE
        shell: bash
        if: ${{ inputs.thread_safety == 'TS' && inputs.concurrent != 'CC'}}

      - name: Configure (Concurrency)
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G "Visual Studio 17 2022" -A ARM64 \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_THREADSAFE:BOOL=OFF \
            -DHDF5_ENABLE_CONCURRENCY:BOOL=ON \
            -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_JAVA:BOOL=OFF \
            -DHDF5_BUILD_HL_LIB:BOOL=OFF \
            -DHDF5_BUILD_DOC=OFF \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=OFF \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=OFF \
            $GITHUB_WORKSPACE
        shell: bash
        if: ${{ inputs.thread_safety != 'TS' && inputs.concurrent == 'CC'}}

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
      - name: Run Tests
        run: ctest . --parallel 2 -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      - name: Run Package
        run: cpack -C ${{ inputs.build_mode }} -V
        working-directory: ${{ runner.workspace }}/build

      - name: List files in the space
        run: |
              ls -l ${{ runner.workspace }}/build

      # Save files created by CTest script
      - name: Save published binary (Windows)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: zip-vs2022_cl-${{ inputs.build_mode }}-${{ inputs.save_binary }}-binary
              path: ${{ runner.workspace }}/build/HDF5-*-win64.zip
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
        if:  ${{ (inputs.thread_safety != 'TS') && (inputs.concurrent != 'CC') && ( inputs.save_binary != 'skip') }}

  build_and_test_linux:
    name: "Ubuntu gcc-${{ inputs.build_mode }}-${{ inputs.thread_safety }}-${{ inputs.concurrent }}"
    runs-on: ubuntu-24.04-arm
    steps:
      - name: Install Dependencies (Linux)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build graphviz
          sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
           mkdir "${{ runner.workspace }}/build"
           cd "${{ runner.workspace }}/build"
           cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
             -G Ninja \
             --log-level=VERBOSE \
             -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
             -DHDF5_ENABLE_DOXY_WARNINGS:BOOL=ON \
             -DHDF5_BUILD_DOC:BOOL=ON \
             -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
             -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON \
             -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF \
             -DZLIB_USE_LOCALCONTENT:BOOL=OFF \
             -DHDF5_PACK_EXAMPLES:BOOL=ON \
             -DHDF5_PACKAGE_EXTLIBS:BOOL=ON \
             $GITHUB_WORKSPACE
        shell: bash
        if: ${{ inputs.thread_safety != 'TS' && inputs.concurrent != 'CC'}}

      - name: Configure (Thread-Safe)
        run: |
           mkdir "${{ runner.workspace }}/build"
           cd "${{ runner.workspace }}/build"
           cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
             -G Ninja \
             -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
             -DHDF5_ENABLE_THREADSAFE:BOOL=ON \
             -DHDF5_ENABLE_CONCURRENCY:BOOL=OFF \
             -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
             -DHDF5_BUILD_FORTRAN:BOOL=OFF \
             -DHDF5_BUILD_JAVA:BOOL=OFF \
             -DHDF5_BUILD_HL_LIB:BOOL=OFF \
             -DHDF5_BUILD_DOC:BOOL=OFF \
             -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
             -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON \
             -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF \
             -DZLIB_USE_LOCALCONTENT:BOOL=OFF \
             -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
             -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
             $GITHUB_WORKSPACE
        shell: bash
        if: ${{ inputs.thread_safety == 'TS' && inputs.concurrent != 'CC'}}

      - name: Configure (Concurrency)
        run: |
           mkdir "${{ runner.workspace }}/build"
           cd "${{ runner.workspace }}/build"
           cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
             -G Ninja \
             -DHDF5_ENABLE_THREADSAFE:BOOL=OFF \
             -DHDF5_ENABLE_CONCURRENCY:BOOL=ON \
             -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
             -DHDF5_BUILD_FORTRAN:BOOL=OFF \
             -DHDF5_BUILD_JAVA:BOOL=OFF \
             -DHDF5_BUILD_HL_LIB:BOOL=OFF \
             -DHDF5_BUILD_DOC=OFF \
             -DLIBAEC_USE_LOCALCONTENT=OFF \
             -DZLIB_USE_LOCALCONTENT=OFF \
             -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
             -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
             $GITHUB_WORKSPACE
        shell: bash
        if: ${{ inputs.thread_safety != 'TS' && inputs.concurrent == 'CC'}}

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
      - name: Run Tests
        run: ctest . --parallel 2 -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      - name: Run Package
        run: cpack -C ${{ inputs.build_mode }} -V
        working-directory: ${{ runner.workspace }}/build

      - name: List files in the space
        run: |
               ls -l ${{ runner.workspace }}/build

      - name: Save published binary (linux)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: tgz-ubuntu-2404_gcc-${{ inputs.build_mode }}-${{ inputs.save_binary }}-binary
          path: ${{ runner.workspace }}/build/HDF5-*-Linux.tar.gz
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
        if:  ${{ (inputs.thread_safety != 'TS') && (inputs.concurrent != 'CC') && ( inputs.save_binary != 'skip') }}

  build_and_test_mac_latest:
    name: "MacOS Clang-${{ inputs.build_mode }}-${{ inputs.thread_safety }}-${{ inputs.concurrent }}"
    runs-on: macos-15
    steps:
      - name: Install Dependencies (macOS)
        run: brew install ninja curl

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Check CMake Version
        shell: bash
        run: |
          which cmake
          cmake --version

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DHDF5_ENABLE_DOXY_WARNINGS:BOOL=ON \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_DOC:BOOL=ON \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON \
            -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF \
            -DZLIB_USE_LOCALCONTENT:BOOL=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=OFF \
            -DHDF5_PACK_EXAMPLES:BOOL=ON \
            -DHDF5_PACKAGE_EXTLIBS:BOOL=ON \
            -DHDF5_PACK_MACOSX_DMG:BOOL=OFF \
            $GITHUB_WORKSPACE
        shell: bash
        if: ${{ inputs.thread_safety != 'TS' && inputs.concurrent != 'CC'}}

      - name: Configure (Thread-Safe)
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DHDF5_ENABLE_THREADSAFE:BOOL=ON \
            -DHDF5_ENABLE_CONCURRENCY:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_JAVA:BOOL=OFF \
            -DHDF5_BUILD_HL_LIB:BOOL=OFF \
            -DHDF5_BUILD_DOC:BOOL=OFF \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON \
            -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF \
            -DZLIB_USE_LOCALCONTENT:BOOL=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=OFF \
            -DHDF5_PACK_MACOSX_DMG:BOOL=OFF \
            $GITHUB_WORKSPACE
        shell: bash
        if: ${{ inputs.thread_safety == 'TS' && inputs.concurrent != 'CC'}}

      - name: Configure (Concurrency)
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DHDF5_ENABLE_THREADSAFE:BOOL=OFF \
            -DHDF5_ENABLE_CONCURRENCY:BOOL=ON \
            -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_JAVA:BOOL=OFF \
            -DHDF5_BUILD_HL_LIB:BOOL=OFF \
            -DHDF5_BUILD_DOC=OFF \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=OFF \
            $GITHUB_WORKSPACE
        shell: bash
        if: ${{ inputs.thread_safety != 'TS' && inputs.concurrent == 'CC'}}

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
      - name: Run Tests
        run: ctest . --parallel 2 -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      - name: Run Package
        run: |
          # Disable indexing to prevent file locking during CPack
          sudo mdutil -i off / || true

          # Run cpack with retry
          cpack -C ${{ inputs.build_mode }} -V || (sleep 5 && cpack -C ${{ inputs.build_mode }} -V)
        working-directory: ${{ runner.workspace }}/build
        shell: bash

      - name: List files in the space
        run: |
              ls -l ${{ runner.workspace }}/build

      - name: Save published binary (Mac_latest)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: tgz-macos14_clang-${{ inputs.build_mode }}-${{ inputs.save_binary }}-binary
              path: ${{ runner.workspace }}/build/HDF5-*-Darwin.tar.gz
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
        if: ${{ (inputs.thread_safety != 'TS') && (inputs.concurrent != 'CC') && ( inputs.save_binary != 'skip') }}
```

### `.github/workflows/bintest.yml`

```yaml
name: hdf5 examples bintest runs

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string
      save_binary:
        description: "binary-ext-name or missing"
        required: true
        default: "skip"
        type: string
      java_version:
        description: "Java version to use for testing (19, 21, 24, 25, latest)"
        required: false
        default: "19"
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  test_binary_win:
  # Windows w/ MSVC
  #
    name: "Windows MSVC Binary Test"
    runs-on: windows-latest
    steps:
      - name: Install Dependencies (Windows)
        run: choco install ninja

      - name: Set up JDK ${{ inputs.java_version }}
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: |
            ${{
              inputs.save_binary == 'ffm' && '25' ||
              inputs.java_version == 'latest' && '24' ||
              inputs.java_version
            }}
          distribution: ${{ inputs.save_binary == 'ffm' && 'oracle' || 'temurin' }}

      - name: Enable Developer Command Prompt
        uses: ilammy/msvc-dev-cmd@0b201ec74fa43914dc39ae48a89fd1d8cb592756 # v1.13.0

      # Get files created by cmake-ctest script
      - name: Get published binary (Windows)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: zip-vs2022_cl-${{ inputs.build_mode }}-${{ inputs.save_binary }}-binary
              path: ${{ github.workspace }}/hdf5

      - name: Uncompress hdf5 binary (Win)
        working-directory: ${{ github.workspace }}/hdf5
        run: 7z x HDF5-*-win64.zip
        shell: bash

      - name: List files for the space (Win)
        run: |
              ls -l ${{ github.workspace }}
              ls -l ${{ github.workspace }}/hdf5

      - name: create hdf5 location (Win)
        working-directory: ${{ github.workspace }}/hdf5
        run: |
          New-Item -Path "${{ github.workspace }}/HDF_Group/HDF5" -ItemType Directory
          Copy-Item -Path "${{ github.workspace }}/hdf5/HDF*/*" -Destination "${{ github.workspace }}/HDF_Group/HDF5" -Recurse
        shell: pwsh

      - name: List files for the space (Win)
        run: ls -l ${{ github.workspace }}/HDF_Group/HDF5

      - name: set hdf5lib name
        id: set-hdf5lib-name
        run: |
          HDF5DIR="${{ github.workspace }}/HDF_Group/HDF5"
          echo "HDF5_ROOT=$HDF5DIR$FILE_NAME_HDF5" >> $GITHUB_OUTPUT
          echo "HDF5_PLUGIN_PATH=$HDF5_ROOT/lib/plugin" >> $GITHUB_OUTPUT
        shell: bash

      - name: List files for the binaries (Win)
        run: |
            ls -l ${{ github.workspace }}/HDF_Group/HDF5

      - name: using powershell
        shell: pwsh
        run: Get-Location

      - name: List files for the space (Windows)
        run: |
              Get-ChildItem -Path ${{ github.workspace }}
              Get-ChildItem -Path ${{ runner.workspace }}
        shell: pwsh

      - name: Run CTest (Windows)
        env:
          HDF5_ROOT: ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}
          HDF5_PLUGIN_PATH: ${{ steps.set-hdf5lib-name.outputs.HDF5_PLUGIN_PATH }}
        run: |
          cd "${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/HDF5Examples"
          cmake --workflow --preset=ci-StdShar-MSVC --fresh
        shell: bash

  test_binary_linux:
  # Linux (Ubuntu) w/ gcc
  #
    name: "Ubuntu gcc Binary Test"
    runs-on: ubuntu-latest
    steps:
      - name: Install Dependencies (Linux)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz

      - name: Set up JDK ${{ inputs.java_version }}
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: |
            ${{
              inputs.save_binary == 'ffm' && '25' ||
              inputs.java_version == 'latest' && '24' ||
              inputs.java_version
            }}
          distribution: ${{ inputs.save_binary == 'ffm' && 'oracle' || 'temurin' }}

      - name: Get published binary (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-ubuntu-2404_gcc-${{ inputs.build_mode }}-${{ inputs.save_binary }}-binary
              path: ${{ github.workspace }}

      - name: Uncompress hdf5 binary (Linux)
        run:  |
            cd "${{ github.workspace }}"
            tar -zxvf ${{ github.workspace }}/HDF5-*-Linux.tar.gz --strip-components 1

      - name: set hdf5lib name
        id: set-hdf5lib-name
        run: |
          HDF5DIR=${{ github.workspace }}/HDF_Group/HDF5/
          FILE_NAME_HDF5=$(ls ${{ github.workspace }}/HDF_Group/HDF5)
          echo "HDF5_ROOT=$HDF5DIR$FILE_NAME_HDF5" >> $GITHUB_OUTPUT
          echo "HDF5_PLUGIN_PATH=$HDF5_ROOT/lib/plugin" >> $GITHUB_OUTPUT

      - name: List files for the binaries (Linux)
        run: |
            ls -l ${{ github.workspace }}/HDF_Group/HDF5

      - name: List files for the space (Linux)
        run: |
              ls -l ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: Run CTest (Linux)
        env:
          HDF5_ROOT: ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}
          HDF5_PLUGIN_PATH: ${{ steps.set-hdf5lib-name.outputs.HDF5_PLUGIN_PATH }}
        run: |
          cd "${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/share/HDF5Examples"
          cmake --workflow --preset=ci-StdShar-GNUC --fresh
        shell: bash

  test_binary_mac_latest:
  # MacOS w/ Clang
  #
    name: "MacOS Clang Binary Test"
    runs-on: macos-latest
    steps:
      - name: Install Dependencies (MacOS_latest)
        run: brew install ninja doxygen

      - name: Set up JDK ${{ inputs.java_version }}
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: |
            ${{
              inputs.save_binary == 'ffm' && '25' ||
              inputs.java_version == 'latest' && '24' ||
              inputs.java_version
            }}
          distribution: ${{ inputs.save_binary == 'ffm' && 'oracle' || 'temurin' }}

      - name: Get published binary (MacOS_latest)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: tgz-macos14_clang-${{ inputs.build_mode }}-${{ inputs.save_binary }}-binary
          path: ${{ github.workspace }}

      - name: Uncompress hdf5 binary (MacOS_latest)
        run:  |
          cd "${{ github.workspace }}"
          tar -zxvf ${{ github.workspace }}/HDF5-*-Darwin.tar.gz --strip-components 1

      - name: set hdf5lib name
        id: set-hdf5lib-name
        run: |
          HDF5DIR=${{ github.workspace }}/HDF_Group/HDF5/
          FILE_NAME_HDF5=$(ls ${{ github.workspace }}/HDF_Group/HDF5)
          echo "HDF5_ROOT=$HDF5DIR$FILE_NAME_HDF5" >> $GITHUB_OUTPUT
          echo "HDF5_PLUGIN_PATH=$HDF5_ROOT/lib/plugin" >> $GITHUB_OUTPUT

      - name: List files for the binaries (MacOS_latest)
        run: |
          ls -l ${{ github.workspace }}/HDF_Group/HDF5

      - name: List files for the space (MacOS_latest)
        run: |
          ls ${{ github.workspace }}
          ls ${{ runner.workspace }}

      # symlinks the compiler executables to a common location 
      - name: Setup GNU Fortran
        uses: fortran-lang/setup-fortran@47809fdb6e637da656ce9ada436527b240c1287f # v1
        id: setup-fortran
        with:
          compiler: gcc
          version: 14

      - name: Run CTest (MacOS_latest)
        id: run-ctest
        env:
          HDF5_ROOT: ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}
          HDF5_PLUGIN_PATH: ${{ steps.set-hdf5lib-name.outputs.HDF5_PLUGIN_PATH }}
        run: |
          cd "${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/share/HDF5Examples"
          cmake --workflow --preset=ci-StdShar-macos-Clang --fresh
        shell: bash

  test_h5cc_linux:
  # Linux (Ubuntu) w/ gcc
  #
    name: "Ubuntu h5cc Binary Test"
    runs-on: ubuntu-latest
    steps:
      - name: Install Dependencies (Linux)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz

      - name: Set up JDK ${{ inputs.java_version }}
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: |
            ${{
              inputs.save_binary == 'ffm' && '25' ||
              inputs.java_version == 'latest' && '24' ||
              inputs.java_version
            }}
          distribution: ${{ inputs.save_binary == 'ffm' && 'oracle' || 'temurin' }}

      - name: Get published binary (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-ubuntu-2404_gcc-${{ inputs.build_mode }}-${{ inputs.save_binary }}-binary
              path: ${{ github.workspace }}

      - name: Uncompress hdf5 binary (Linux)
        run:  |
            cd "${{ github.workspace }}"
            tar -zxvf ${{ github.workspace }}/HDF5-*-Linux.tar.gz --strip-components 1

      - name: set hdf5lib name
        id: set-hdf5lib-name
        run: |
            HDF5DIR=${{ github.workspace }}/HDF_Group/HDF5/
            FILE_NAME_HDF5=$(ls ${{ github.workspace }}/HDF_Group/HDF5)
            echo "HDF5_ROOT=$HDF5DIR$FILE_NAME_HDF5" >> $GITHUB_OUTPUT
            echo "HDF5_PLUGIN_PATH=$HDF5_ROOT/lib/plugin" >> $GITHUB_OUTPUT

      - name: List files for the binaries (Linux)
        run: |
            ls -l ${{ github.workspace }}/HDF_Group/HDF5

      - name: List files for the space (Linux)
        run: |
              ls -l ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: Run testpc (Linux)
        env:
          HDF5_HOME: ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}
          HDF5_PLUGIN_PATH: ${{ steps.set-hdf5lib-name.outputs.HDF5_PLUGIN_PATH }}
        run: |
          mkdir "${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/share/build"
          cd "${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/share/HDF5Examples"
          sh ./test-pc.sh ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/share/HDF5Examples ${{ steps.set-hdf5lib-name.outputs.HDF5_ROOT }}/share/build .
        shell: bash
```

### `.github/workflows/build-aws-c-s3.yml`

```yaml
name: Build aws-c-s3 library

# Reusable workflow to build aws-c-s3 library from source for Ubuntu
# This workflow is called by vfd-ros3.yml and other workflows that need aws-c-s3

on:
  workflow_call:
    inputs:
      build_mode:
        description: "Build type (CMAKE_BUILD_TYPE)"
        required: true
        type: string
      aws_c_s3_tag:
        description: "Tag of aws-c-s3 to use when building from source"
        required: false
        type: string
        default: ""

permissions:
  contents: read

jobs:
  check_artifact:
    name: "Check for existing aws-c-s3 artifact"
    runs-on: ubuntu-latest
    outputs:
      has_artifact: ${{ steps.check_artifact.outputs.exists }}
    steps:
      - name: Check if 'libaws-c-s3-${{ inputs.build_mode }}' exists
        id: check_artifact
        uses: softwareforgood/check-artifact-v4-existence@9772fb919376f476e208587e99db557f956a2276 # v0
        with:
          name: libaws-c-s3-${{ inputs.build_mode }}

      - name: Status of artifact check
        if: steps.check_artifact.outputs.exists == 'true'
        run: echo "Artifact 'libaws-c-s3-${{ inputs.build_mode }}' exists.."

  # Build the aws-c-s3 library from source using the specified tag
  # and cache the results, currently only on Ubuntu. The result is
  # compressed into a 'libaws-c-s3.tar' archive to preserve permissions
  # and then is uploaded as the artifact 'libaws-c-s3' which can later
  # be downloaded with 'actions/download-artifact' and then uncompressed
  # with 'tar xvf libaws-c-s3.tar -C <directory>'. The uncompressed build
  # directory will be called 'aws-c-s3-build'.
  build_aws_c_s3:
    # Ubuntu doesn't have a package for aws-c-s3 yet
    name: "Build aws-c-s3 library from source"
    runs-on: ubuntu-latest
    needs: check_artifact
    if: ${{ needs.check_artifact.outputs.has_artifact == 'false' }}
    steps:
      - name: Get aws-c-s3 sources (main)
        if: inputs.aws_c_s3_tag == ''
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: awslabs/aws-c-s3
          path: aws-c-s3

      - name: Get aws-c-s3 sources (tag)
        if: inputs.aws_c_s3_tag != ''
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: awslabs/aws-c-s3
          path: aws-c-s3
          ref: ${{ inputs.aws_c_s3_tag }}

      - name: Get aws-c-s3 commit hash
        shell: bash
        id: get-sha
        run: |
          cd $GITHUB_WORKSPACE/aws-c-s3
          export AWSCS3_SHA=$(git rev-parse HEAD)
          echo "AWSCS3_SHA=$AWSCS3_SHA" >> $GITHUB_ENV
          echo "sha=$AWSCS3_SHA" >> $GITHUB_OUTPUT
          # Output SHA for debugging
          echo "AWSCS3_SHA=$AWSCS3_SHA"

      - name: Cache/Restore aws-c-s3 (GCC) installation
        id: cache-aws-c-s3-ubuntu-gcc
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: ${{ runner.workspace }}/aws-c-s3-build
          key: ${{ runner.os }}-${{ runner.arch }}-gcc-aws-c-s3-${{ steps.get-sha.outputs.sha }}-${{ inputs.build_mode }}

      - name: Get aws-lc sources
        if: ${{ steps.cache-aws-c-s3-ubuntu-gcc.outputs.cache-hit != 'true' }}
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: aws/aws-lc
          path: aws-lc

      - name: Get s2n-tls sources
        if: ${{ steps.cache-aws-c-s3-ubuntu-gcc.outputs.cache-hit != 'true' }}
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: aws/s2n-tls
          path: s2n-tls

      - name: Get aws-c-common sources
        if: ${{ steps.cache-aws-c-s3-ubuntu-gcc.outputs.cache-hit != 'true' }}
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: awslabs/aws-c-common
          path: aws-c-common

      - name: Get aws-checksums sources
        if: ${{ steps.cache-aws-c-s3-ubuntu-gcc.outputs.cache-hit != 'true' }}
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: awslabs/aws-checksums
          path: aws-checksums

      - name: Get aws-c-cal sources
        if: ${{ steps.cache-aws-c-s3-ubuntu-gcc.outputs.cache-hit != 'true' }}
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: awslabs/aws-c-cal
          path: aws-c-cal

      - name: Get aws-c-io sources
        if: ${{ steps.cache-aws-c-s3-ubuntu-gcc.outputs.cache-hit != 'true' }}
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: awslabs/aws-c-io
          path: aws-c-io

      - name: Get aws-c-compression sources
        if: ${{ steps.cache-aws-c-s3-ubuntu-gcc.outputs.cache-hit != 'true' }}
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: awslabs/aws-c-compression
          path: aws-c-compression

      - name: Get aws-c-http sources
        if: ${{ steps.cache-aws-c-s3-ubuntu-gcc.outputs.cache-hit != 'true' }}
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: awslabs/aws-c-http
          path: aws-c-http

      - name: Get aws-c-sdkutils sources
        if: ${{ steps.cache-aws-c-s3-ubuntu-gcc.outputs.cache-hit != 'true' }}
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: awslabs/aws-c-sdkutils
          path: aws-c-sdkutils

      - name: Get aws-c-auth sources
        if: ${{ steps.cache-aws-c-s3-ubuntu-gcc.outputs.cache-hit != 'true' }}
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: awslabs/aws-c-auth
          path: aws-c-auth

      - name: Build aws-c-s3 from source
        if: ${{ (steps.cache-aws-c-s3-ubuntu-gcc.outputs.cache-hit != 'true') }}
        run: |
          # Build aws-lc
          echo "Building aws-lc"
          cmake -S aws-lc -B aws-lc/build \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/aws-c-s3-build \
            -DBUILD_SHARED_LIBS=1
          cmake --build aws-lc/build --parallel 3 --config ${{ inputs.build_mode }} --target install
          # Build s2n-tls
          echo "Building s2n-tls"
          cmake -S s2n-tls -B s2n-tls/build \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/aws-c-s3-build \
            -DCMAKE_PREFIX_PATH=${{ runner.workspace }}/aws-c-s3-build \
            -DBUILD_SHARED_LIBS=1
          cmake --build s2n-tls/build --parallel 3 --config ${{ inputs.build_mode }} --target install
          # Build aws-c-common
          echo "Building aws-c-common"
          cmake -S aws-c-common -B aws-c-common/build \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/aws-c-s3-build \
            -DBUILD_SHARED_LIBS=1
          cmake --build aws-c-common/build --parallel 3 --config ${{ inputs.build_mode }} --target install
          # Build aws-checksums
          echo "Building aws-checksums"
          cmake -S aws-checksums -B aws-checksums/build \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/aws-c-s3-build \
            -DCMAKE_PREFIX_PATH=${{ runner.workspace }}/aws-c-s3-build \
            -DBUILD_SHARED_LIBS=1
          cmake --build aws-checksums/build --parallel 3 --config ${{ inputs.build_mode }} --target install
          # Build aws-c-cal
          echo "Building aws-c-cal"
          cmake -S aws-c-cal -B aws-c-cal/build \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/aws-c-s3-build \
            -DCMAKE_PREFIX_PATH=${{ runner.workspace }}/aws-c-s3-build \
            -DBUILD_SHARED_LIBS=1
          cmake --build aws-c-cal/build --parallel 3 --config ${{ inputs.build_mode }} --target install
          # Build aws-c-io
          echo "Building aws-c-io"
          cmake -S aws-c-io -B aws-c-io/build \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/aws-c-s3-build \
            -DCMAKE_PREFIX_PATH=${{ runner.workspace }}/aws-c-s3-build \
            -DBUILD_SHARED_LIBS=1
          cmake --build aws-c-io/build --parallel 3 --config ${{ inputs.build_mode }} --target install
          # Build aws-c-compression
          echo "Building aws-c-compression"
          cmake -S aws-c-compression -B aws-c-compression/build \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/aws-c-s3-build \
            -DCMAKE_PREFIX_PATH=${{ runner.workspace }}/aws-c-s3-build \
            -DBUILD_SHARED_LIBS=1
          cmake --build aws-c-compression/build --parallel 3 --config ${{ inputs.build_mode }} --target install
          # Build aws-c-http
          echo "Building aws-c-http"
          cmake -S aws-c-http -B aws-c-http/build \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/aws-c-s3-build \
            -DCMAKE_PREFIX_PATH=${{ runner.workspace }}/aws-c-s3-build \
            -DBUILD_SHARED_LIBS=1
          cmake --build aws-c-http/build --parallel 3 --config ${{ inputs.build_mode }} --target install
          # Build aws-c-sdkutils
          echo "Building aws-c-sdkutils"
          cmake -S aws-c-sdkutils -B aws-c-sdkutils/build \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/aws-c-s3-build \
            -DCMAKE_PREFIX_PATH=${{ runner.workspace }}/aws-c-s3-build \
            -DBUILD_SHARED_LIBS=1
          cmake --build aws-c-sdkutils/build --parallel 3 --config ${{ inputs.build_mode }} --target install
          # Build aws-c-auth
          echo "Building aws-c-auth"
          cmake -S aws-c-auth -B aws-c-auth/build \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/aws-c-s3-build \
            -DCMAKE_PREFIX_PATH=${{ runner.workspace }}/aws-c-s3-build \
            -DBUILD_SHARED_LIBS=1
          cmake --build aws-c-auth/build --parallel 3 --config ${{ inputs.build_mode }} --target install
          # Build aws-c-s3
          echo "Building aws-c-s3"
          cmake -S aws-c-s3 -B aws-c-s3/build \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/aws-c-s3-build \
            -DCMAKE_PREFIX_PATH=${{ runner.workspace }}/aws-c-s3-build \
            -DBUILD_SHARED_LIBS=1
          cmake --build aws-c-s3/build --parallel 3 --config ${{ inputs.build_mode }} --target install

      - name: Tar aws-c-s3 installation to preserve permissions for artifact
        run: tar -cvf libaws-c-s3.tar -C ${{ runner.workspace }} aws-c-s3-build

      - name: Save aws-c-s3 installation artifact
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: libaws-c-s3-${{ inputs.build_mode }}
          path: libaws-c-s3.tar
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
```

### `.github/workflows/build_mpich_source.yml`

```yaml
# Build MPICH from source using the latest commit on the
# 'main' branch and cache the results. The result is compressed
# into a 'mpich.tar' archive to preserve permissions and
# then is uploaded as the artifact 'mpich' which can later
# be downloaded with 'actions/download-artifact' and then
# uncompressed with 'tar xvf mpich.tar -C <directory>'

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      build_mode:
        description: "Release vs. Debug build"
        required: true
        type: string

permissions:
  contents: read

jobs:
  ubuntu_gcc_build_and_test:
    name: "Build MPICH ${{ inputs.build_mode }} (GCC)"

    runs-on: ubuntu-latest

    steps:
      - name: Install Linux dependencies
        run: |
          sudo apt-get update
          sudo apt-get install build-essential libtool libtool-bin

      - name: Get MPICH source
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: 'pmodels/mpich'
          path: 'mpich'
          submodules: recursive

      - name: Get MPICH commit hash
        shell: bash
        id: get-sha
        run: |
          cd $GITHUB_WORKSPACE/mpich
          export MPICH_SHA=$(git rev-parse HEAD)
          echo "MPICH_SHA=$MPICH_SHA" >> $GITHUB_ENV
          echo "sha=$MPICH_SHA" >> $GITHUB_OUTPUT
          # Output SHA for debugging
          echo "MPICH_SHA=$MPICH_SHA"

      - name: Cache MPICH (GCC) installation
        id: cache-mpich-ubuntu-gcc
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: ${{ runner.workspace }}/mpich
          key: ${{ runner.os }}-${{ runner.arch }}-gcc-mpich-${{ steps.get-sha.outputs.sha }}-${{ inputs.build_mode }}

      # Enable threads=multiple for testing with Subfiling and
      # VOL connectors that require MPI_THREAD_MULTIPLE
      - name: Install MPICH (GCC) (Release)
        if: ${{ steps.cache-mpich-ubuntu-gcc.outputs.cache-hit != 'true' && (inputs.build_mode == 'Release') }}
        run: |
          cd $GITHUB_WORKSPACE/mpich
          ./autogen.sh
          ./configure \
            CC=gcc \
            --prefix=${{ runner.workspace }}/mpich \
            --enable-threads=multiple
          make -j2
          make install

      # Enable threads=multiple for testing with Subfiling and
      # VOL connectors that require MPI_THREAD_MULTIPLE
      - name: Install MPICH (GCC) (Debug)
        if: ${{ steps.cache-mpich-ubuntu-gcc.outputs.cache-hit != 'true' && (inputs.build_mode == 'Debug') }}
        run: |
          cd $GITHUB_WORKSPACE/mpich
          ./autogen.sh
          ./configure \
            CC=gcc \
            --prefix=${{ runner.workspace }}/mpich \
            --enable-g=most \
            --enable-debuginfo \
            --enable-threads=multiple
          make -j2
          make install

      - name: Tar MPICH installation to preserve permissions for artifact
        run: tar -cvf mpich.tar -C ${{ runner.workspace }} mpich

      - name: Save MPICH installation artifact
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: mpich
          path: mpich.tar
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
```

### `.github/workflows/build_openmpi_source.yml`

```yaml
# Build OpenMPI from source using the latest commit on the
# 'main' branch and cache the results. The result is compressed
# into a 'openmpi.tar' archive to preserve permissions and
# then is uploaded as the artifact 'openmpi' which can later
# be downloaded with 'actions/download-artifact' and then
# uncompressed with 'tar xvf openmpi.tar -C <directory>'

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      build_mode:
        description: "Release vs. Debug build"
        required: true
        type: string

permissions:
  contents: read

jobs:
  ubuntu_gcc_build_and_test:
    name: "Build OpenMPI ${{ inputs.build_mode }} (GCC)"

    runs-on: ubuntu-latest

    steps:
      - name: Install Linux dependencies
        run: |
          sudo apt-get update
          sudo apt-get install build-essential libtool libtool-bin

      - name: Get OpenMPI source
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: 'open-mpi/ompi'
          path: 'ompi'
          submodules: recursive

      - name: Get OpenMPI commit hash
        shell: bash
        id: get-sha
        run: |
          cd $GITHUB_WORKSPACE/ompi
          export OPENMPI_SHA=$(git rev-parse HEAD)
          echo "OPENMPI_SHA=$OPENMPI_SHA" >> $GITHUB_ENV
          echo "sha=$OPENMPI_SHA" >> $GITHUB_OUTPUT
          # Output SHA for debugging
          echo "OPENMPI_SHA=$OPENMPI_SHA"

      - name: Cache OpenMPI (GCC) installation
        id: cache-openmpi-ubuntu-gcc
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: ${{ runner.workspace }}/openmpi
          key: ${{ runner.os }}-${{ runner.arch }}-gcc-openmpi-${{ steps.get-sha.outputs.sha }}-${{ inputs.build_mode }}

      - name: Install OpenMPI (GCC) (Release)
        if: ${{ steps.cache-openmpi-ubuntu-gcc.outputs.cache-hit != 'true' && (inputs.build_mode == 'Release') }}
        run: |
          cd $GITHUB_WORKSPACE/ompi
          ./autogen.pl
          ./configure \
            CC=gcc \
            --prefix=${{ runner.workspace }}/openmpi
          make -j2
          make install

      - name: Install OpenMPI (GCC) (Debug)
        if: ${{ steps.cache-openmpi-ubuntu-gcc.outputs.cache-hit != 'true' && (inputs.build_mode == 'Debug') }}
        run: |
          cd $GITHUB_WORKSPACE/ompi
          ./autogen.pl
          ./configure \
            CC=gcc \
            --prefix=${{ runner.workspace }}/openmpi \
            --enable-debug
          make -j2
          make install

      - name: Tar OpenMPI installation to preserve permissions for artifact
        run: tar -cvf openmpi.tar -C ${{ runner.workspace }} openmpi

      - name: Save OpenMPI installation artifact
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: openmpi
          path: openmpi.tar
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
```

### `.github/workflows/call-workflows.yml`

```yaml
name: hdf5 dev cmake CI

# Triggers the workflow on push or pull request or on demand
on:
  workflow_dispatch:
  push:
  pull_request:
    branches: [ develop ]
    paths-ignore:
      - '.github/CODEOWNERS'
      - '.github/FUNDING.yml'
      - 'doc/**'
      - 'release_docs/**'
      - 'ACKNOWLEDGEMENTS'
      - 'LICENSE**'
      - '**.md'

# Using concurrency to cancel any in-progress job or run
concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref && github.ref || github.run_id }}
  cancel-in-progress: true

permissions:
  contents: read
  packages: write
  pull-requests: write

jobs:
    call-workflow-special-cmake:
      name: "Special Workflows"
      uses: ./.github/workflows/main-spc.yml

    # Build aws-c-s3 library for ROS3 workflows
    build-aws-c-s3-release:
      name: "Build aws-c-s3 (Release)"
      uses: ./.github/workflows/build-aws-c-s3.yml
      with:
        build_mode: "Release"
        aws_c_s3_tag: "v0.8.0"

    call-workflow-ros3-cmake:
      name: "ROS3 VFD Workflows"
      needs: build-aws-c-s3-release
      uses: ./.github/workflows/vfd-ros3.yml
      with:
        build_mode: "Release"
        aws_c_s3_build_type: "package"

    call-workflow-ros3-ffm-cmake:
      name: "ROS3 VFD FFM Workflows"
      needs: build-aws-c-s3-release
      uses: ./.github/workflows/vfd-ros3.yml
      with:
        build_mode: "Release"
        aws_c_s3_build_type: "package"
        save_binary: "ffm"
        java_version: "latest"
        force_java_implementation: "ffm"

    call-debug-concurrent-cmake:
        name: "Debug Concurrency Workflows"
        uses: ./.github/workflows/main.yml
        with:
            cmake_version: "latest"
            concurrent: "CC"
            thread_safety: ""
            build_mode: "Debug"

    call-static-debug-concurrent-cmake:
        name: "Debug Static Concurrency Workflows"
        uses: ./.github/workflows/main-static.yml
        with:
            cmake_version: "latest"
            concurrent: "CC"
            thread_safety: ""
            build_mode: "Debug"

    call-release-concurrent-cmake:
        name: "Release Concurrency Workflows"
        uses: ./.github/workflows/main.yml
        with:
            cmake_version: "latest"
            concurrent: "CC"
            thread_safety: ""
            build_mode: "Release"

    call-debug-thread-cmake:
        name: "Debug Thread-Safety Workflows"
        uses: ./.github/workflows/main.yml
        with:
            cmake_version: "latest"
            concurrent: ""
            thread_safety: "TS"
            build_mode: "Debug"

    call-static-debug-thread-cmake:
        name: "Debug Static Thread-Safety Workflows"
        uses: ./.github/workflows/main-static.yml
        with:
            cmake_version: "latest"
            concurrent: ""
            thread_safety: "TS"
            build_mode: "Debug"

    call-release-thread-cmake:
        name: "Release Thread-Safety Workflows"
        uses: ./.github/workflows/main.yml
        with:
            cmake_version: "latest"
            concurrent: ""
            thread_safety: "TS"
            build_mode: "Release"

    call-debug-cmake:
        name: "Debug Workflows"
        uses: ./.github/workflows/main.yml
        with:
            cmake_version: "latest"
            concurrent: ""
            thread_safety: ""
            build_mode: "Debug"
            force_java_implementation: "jni"

    call-debug-static-cmake:
        name: "Debug Static Workflows"
        uses: ./.github/workflows/main-static.yml
        with:
            cmake_version: "latest"
            concurrent: ""
            thread_safety: ""
            build_mode: "Debug"

    call-release-cross:
        name: "Release Cross Compile Workflows"
        uses: ./.github/workflows/cross-compile.yml
        with:
            build_mode: "Release"

    call-release-cmake:
        name: "Release Workflows"
        uses: ./.github/workflows/main.yml
        with:
            cmake_version: "latest"
            concurrent: ""
            thread_safety: ""
            build_mode: "Release"
            save_binary: "std"
            force_java_implementation: "jni"

    call-release-static:
        name: "Release Static Workflows"
        uses: ./.github/workflows/main-static.yml
        with:
            cmake_version: "latest"
            concurrent: ""
            thread_safety: ""
            build_mode: "Release"

    call-release-cmakeMin:
        name: "Minimum Release Workflows"
        uses: ./.github/workflows/main.yml
        with:
            cmake_version: "3.26.0"
            concurrent: ""
            thread_safety: ""
            build_mode: "Release"
            force_java_implementation: "jni"

    call-jni-latest-java:
        name: "JNI Latest Java Testing"
        uses: ./.github/workflows/main.yml
        with:
            cmake_version: "latest"
            concurrent: ""
            thread_safety: ""
            build_mode: "Release"
            java_version: "latest"
            force_java_implementation: "jni"

    call-ffm-latest-java:
        name: "FFM Latest Java Testing"
        uses: ./.github/workflows/main.yml
        with:
            cmake_version: "latest"
            concurrent: ""
            thread_safety: ""
            build_mode: "Release"
            save_binary: "ffm"
            java_version: "latest"
            force_java_implementation: "ffm"

    call-arm64-cmake:
        name: "arm64 Workflows"
        uses: ./.github/workflows/arm-main.yml
        with:
            cmake_version: "latest"
            concurrent: ""
            thread_safety: ""
            build_mode: "Release"
            save_binary: "arm64"

    call-debug-arm64-cmake:
        name: "arm64 Workflows"
        uses: ./.github/workflows/arm-main.yml
        with:
            cmake_version: "latest"
            concurrent: ""
            thread_safety: ""
            build_mode: "Debug"

    call-maven-staging:
        name: "Maven Staging Tests"
        needs: call-release-cmake
        uses: ./.github/workflows/maven-staging.yml
        with:
            test_maven_deployment: true
            use_snapshot_version: true
            java_implementation: "jni"
            platforms: "all-platforms"

    call-maven-ffm-staging:
        name: "Maven Staging Tests"
        needs: call-ffm-latest-java
        uses: ./.github/workflows/maven-staging.yml
        with:
            test_maven_deployment: true
            use_snapshot_version: true
            java_implementation: "ffm"
            platforms: "all-platforms"

    call-release-bintest:
        name: "Test Release Binaries"
        needs: [call-release-cmake, call-maven-staging]
        uses: ./.github/workflows/bintest.yml
        with:
            build_mode: "Release"
            save_binary: "std"
            java_version: "21"

    call-release-ffm-bintest:
        name: "Test FFM Release Binaries"
        needs: [call-ffm-latest-java, call-maven-ffm-staging]
        uses: ./.github/workflows/bintest.yml
        with:
            build_mode: "Release"
            save_binary: "ffm"
            java_version: "latest"

    call-release-par:
        name: "Parallel Release Workflows"
        uses: ./.github/workflows/main-par.yml
        with:
            build_mode: "Release"

    call-debug-par:
        name: "Parallel Debug Workflows"
        uses: ./.github/workflows/main-par.yml
        with:
            build_mode: "Debug"

    call-release-special-par:
        name: "Parallel Release Special Workflows"
        uses: ./.github/workflows/main-par-spc.yml
        with:
            build_mode: "Release"

    call-debug-special-par:
        name: "Parallel Debug Special Workflows"
        uses: ./.github/workflows/main-par-spc.yml
        with:
            build_mode: "Debug"

    call-release-cmake-intel:
        name: "Intel Workflows"
        uses: ./.github/workflows/intel.yml
        with:
            build_mode: "Release"

    call-release-cmake-nvhpc:
        name: "nvhpc Workflows"
        uses: ./.github/workflows/nvhpc.yml
        with:
            build_mode: "Release"

    call-release-cmake-aocc:
        name: "aocc Workflows"
        uses: ./.github/workflows/aocc.yml
        with:
            build_mode: "Release"

    call-release-cmake-xpr:
        name: "TestExpress Workflows"
        uses: ./.github/workflows/testxpr.yml

#    call-release-cmake-julia:
#        name: "Julia Workflows"
#        uses: ./.github/workflows/julia.yml
#        with:
#         build_mode: "Release"

    call-release-cmake-msys2:
        name: "Msys2 Workflows"
        uses: ./.github/workflows/msys2.yml
        with:
          build_mode: "Release"

    call-release-cmake-i386:
        name: "i386 Workflows"
        uses: ./.github/workflows/i386.yml
        with:
            build_mode: "Release"
```

### `.github/workflows/clang-format-check.yml`

```yaml
name: clang-format Check
on:
  pull_request:

permissions:
  contents: read

jobs:
  formatting-check:
    name: Formatting Check
    runs-on: ubuntu-latest
    if: "!contains(github.event.head_commit.message, 'skip-ci')"
    steps:
    - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

    - name: Run clang-format style check for C and Java code
      uses: DoozyX/clang-format-lint-action@bcb4eb2cb0d707ee4f3e5cc3b456eb075f12cf73 # v0.20
      with:
        source: '.'
        extensions: 'c,h,cpp,hpp,java'
        clangFormatVersion: 17
        style: file
        exclude: './config ./hl/src/H5LTanalyze.c ./hl/src/H5LTparse.c ./hl/src/H5LTparse.h ./src/H5Epubgen.h ./src/H5Einit.h ./src/H5Eterm.h ./src/H5Edefin.h ./src/H5version.h ./src/H5overflow.h'
```

### `.github/workflows/clang-format-fix.yml`

```yaml
# NOTE: This action requires write permissions to be set in your GitHub
#       repo/fork for it to be able to commit changes.
#
# This is currently enabled via:
#
#   settings > Actions > General > Workflow permissions
#
# which you will need to set to "Read and write permissions"
#
name: clang-format Commit Changes
on: 
  workflow_dispatch:
  push:

permissions:
  contents: read

jobs:
  formatting-check:
    name: Commit Format Changes
    runs-on: ubuntu-latest
    if: "!contains(github.event.head_commit.message, 'skip-ci')"
    permissions:
        contents: write # In order to allow EndBug/add-and-commit to commit changes
    steps:
    - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

    - name: Fix C and Java formatting issues detected by clang-format
      uses: DoozyX/clang-format-lint-action@bcb4eb2cb0d707ee4f3e5cc3b456eb075f12cf73 # v0.20
      with:
        source: '.'
        extensions: 'c,h,cpp,hpp,java'
        clangFormatVersion: 17
        inplace: True
        style: file
        exclude: './config ./hl/src/H5LTanalyze.c ./hl/src/H5LTparse.c ./hl/src/H5LTparse.h ./src/H5Epubgen.h ./src/H5Einit.h ./src/H5Eterm.h ./src/H5Edefin.h ./src/H5version.h ./src/H5overflow.h'

    - uses: EndBug/add-and-commit@a94899bca583c204427a224a7af87c02f9b325d5 # v9.1.4
      with:
          author_name: github-actions
          author_email: 41898282+github-actions[bot]@users.noreply.github.com
          message: 'Committing clang-format changes'
```

### `.github/workflows/codeql.yml`

```yaml
name: "CodeQL"

on:
  push:
    branches: [ "develop" ]
  pull_request:
    branches: [ "develop" ]
  schedule:
    - cron: "16 7 * * 0"
  workflow_dispatch:  # Allow manual triggering

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write

    strategy:
      fail-fast: false

    steps:
      - name: Install Dependencies (Linux)
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            openmpi-bin openmpi-common mpi-default-dev \
            zlib1g-dev libaec-dev

          # Set env vars
          echo "CC=mpicc" >> $GITHUB_ENV
          echo "FC=mpif90" >> $GITHUB_ENV
          echo "F77=mpif90" >> $GITHUB_ENV

      - name: Checkout
        uses: actions/checkout@v3

      - name: Configure HDF5
        run: |
          mkdir build; cd build
          cmake -G "Unix Makefiles" \
            -DCMAKE_INSTALL_PREFIX=$PWD/hdf5 \
            -DCMAKE_BUILD_TYPE=Debug \
            -DHDF5_ENABLE_PARALLEL:BOOL=ON \
            -DHDF5_ENABLE_SUBFILING_VFD:BOOL=ON \
            -DHDF5_BUILD_TOOLS:BOOL=ON \
            -DBUILD_SHARED_LIBS:BOOL=ON \
            -DBUILD_STATIC_LIBS:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_JAVA:BOOL=OFF \
            -DBUILD_TESTING:BOOL=OFF \
            -DHDF5_BUILD_EXAMPLES:BOOL=OFF \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
            -DZLIB_INCLUDE_DIR=/usr/include \
            -DZLIB_LIBRARY=/usr/lib/x86_64-linux-gnu/libz.so \
            ..

      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: c-cpp
          build-mode: manual
          queries: +security-and-quality
          
          config: |
             query-filters:
               - exclude:
                   id: cpp/toctou-race-condition
               - exclude:
                   id: cpp/short-global-name

      - name: Build
        run: |
          cd build
          cmake --build . --config Debug
        shell: bash

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          category: "/language:c-cpp"
          output: sarif-results
          upload: failure-only

      - name: filter-sarif
        uses: advanced-security/filter-sarif@v1
        with:
          patterns: |
            -**/test/**
            -**/testpar/**
            -**/tools/test/**
          input: sarif-results/cpp.sarif
          output: sarif-results/cpp.sarif

      - name: Upload SARIF
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: sarif-results/cpp.sarif
```

### `.github/workflows/codespell.yml`

```yaml
# GitHub Action to automate the identification of common misspellings in text files
# https://github.com/codespell-project/codespell
# https://github.com/codespell-project/actions-codespell
name: codespell
on: [push, pull_request]
permissions:
  contents: read
jobs:
  codespell:
    name: Check for spelling errors
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
      - uses: codespell-project/actions-codespell@bca0a5887de255a8903221c67b6478c7501c5edc # master
```

### `.github/workflows/cross-compile.yml`

```yaml
name: hdf5 dev CMake CI

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  build_and_test_linux:
    name: "Ubuntu mingw64-${{ inputs.build_mode }}"
    runs-on: ubuntu-24.04
    steps:
      - name: Install CMake Dependencies (Linux)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build graphviz
          sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev

      - name: Install Wine64 and dependencies
        run: |
          sudo apt update
          sudo apt install -y wine64 winbind

#      - name: Verify Wine64 installation
#        run: wine64 --version

      - name: Install MinGW64 Cross-Compiler
        run: |
          sudo apt update
          sudo apt-get install -y mingw-w64
          sudo apt-get install gfortran-mingw-w64-x86-64-win32

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CMAKE CONFIGURE
      - name: CMake Configure
        run: |
           mkdir "${{ runner.workspace }}/build"
           cd "${{ runner.workspace }}/build"
           cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
             -G Ninja \
             --log-level=VERBOSE \
             -DCMAKE_TOOLCHAIN_FILE:STRING=config/toolchain/mingw64.cmake \
             -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
             -DHDF5_ENABLE_DOXY_WARNINGS:BOOL=ON \
             -DHDF5_BUILD_DOC:BOOL=ON \
             -DHDF5_BUILD_JAVA:BOOL=OFF \
             -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=OFF \
             -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
             -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON \
             -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF \
             -DZLIB_USE_LOCALCONTENT:BOOL=OFF \
             -DHDF5_PACK_EXAMPLES:BOOL=ON \
             -DHDF5_PACKAGE_EXTLIBS:BOOL=ON \
             -DHDF5_TEST_API:BOOL=OFF \
             -DHDF5_USE_PREGEN:BOOL=ON \
             $GITHUB_WORKSPACE
        shell: bash

      # BUILD
      - name: CMake Build
        run: cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
#      - name: CMake Run Tests
#        run: ctest . --parallel 2 -C ${{ inputs.build_mode }} -E "err_compat|filenotclosed|swmr|cache_api"
#        working-directory: ${{ runner.workspace }}/build

      # RUN Failing TESTS
#      - name: CMake Run Tests
#        run: ctest . --parallel 2 -C ${{ inputs.build_mode }} -R "err_compat|filenotclosed|swmr|cache_api"
#        working-directory: ${{ runner.workspace }}/build
#        continue-on-error: true

      - name: CMake Run Package
        run: cpack -C ${{ inputs.build_mode }} -V
        working-directory: ${{ runner.workspace }}/build

      - name: List files in the space
        run: |
               ls -l ${{ runner.workspace }}/build

      - name: Save published binary (linux)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: tgz-ubuntu-2404_mingw64-${{ inputs.build_mode }}-binary
          path: ${{ runner.workspace }}/build/HDF5-*-win64.zip
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
        if:  ${{ inputs.build_mode != 'Debug' }}
```

### `.github/workflows/ctest.yml`

```yaml
name: hdf5 dev CTest runs

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      snap_name:
        description: 'The name in the source tarballs'
        type: string
        required: false
        default: hdfsrc
      file_base:
        description: "The common base name of the source tarballs"
        required: true
        type: string
      preset_name:
        description: "The common base name of the preset configuration name to control the build"
        required: true
        type: string
      use_environ:
        description: 'Environment to locate files'
        type: string
        required: true
        default: snapshots
      cmake_version:
        description: "3.26.0 or later, latest"
        required: true
        type: string
      maven_enabled:
        description: 'Enable Maven artifact generation and upload'
        type: boolean
        required: false
        default: false
    secrets:
        APPLE_CERTS_BASE64:
            required: true
        APPLE_CERTS_BASE64_PASSWD:
            required: true
        KEYCHAIN_PASSWD:
            required: true
        AZURE_TENANT_ID:
            required: true
        AZURE_CLIENT_ID:
            required: true
        AZURE_CLIENT_SECRET:
            required: true
        AZURE_ENDPOINT:
            required: true
        AZURE_CODE_SIGNING_NAME:
            required: true
        AZURE_CERT_PROFILE_NAME:
            required: true

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  check-secret:
    name: Check Secrets exists
    runs-on: ubuntu-latest
    outputs:
      sign-state: ${{ steps.set-signing-state.outputs.BINSIGN }}
    steps:
      - name: Identify Signing Status
        id: set-signing-state
        env: 
            signing_secret: ${{ secrets.AZURE_ENDPOINT }}
        run: |
          if [[ '${{ env.signing_secret }}' == '' ]]
          then
            SIGN_VAL=$(echo 'notexists')
          else
            SIGN_VAL=$(echo 'exists')
          fi
          echo "BINSIGN=$SIGN_VAL" >> $GITHUB_OUTPUT
        shell: bash

      - run: echo "signing is ${{ steps.set-signing-state.outputs.BINSIGN }}."

  build_and_test_win:
  # Windows w/ MSVC
  #
    name: "Windows MSVC CTest"
    runs-on: windows-latest
    needs: [check-secret]
    steps:
      - name: Install Dependencies (Windows)
        run: choco install ninja

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Enable Developer Command Prompt
        uses: ilammy/msvc-dev-cmd@0b201ec74fa43914dc39ae48a89fd1d8cb592756 # v1.13.0

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Check CMake Version
        shell: bash
        run: |
          which cmake
          cmake --version

      - name: Set up JDK 21
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '21'
          distribution: 'temurin'

      - name: Set file base name (Windows)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT
        shell: bash

      # Get files created by release script
      - name: Get zip-tarball (Windows)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
              name: zip-tarball
              path: ${{ github.workspace }}

      - name: using powershell
        shell: pwsh
        run: Get-Location

      - name: List files for the space (Windows)
        run: |
              Get-ChildItem -Path ${{ github.workspace }}
              Get-ChildItem -Path ${{ runner.workspace }}
        shell: pwsh

      - name: Uncompress source (Windows)
        working-directory: ${{ github.workspace }}
        run: 7z x ${{ steps.set-file-base.outputs.FILE_BASE }}.zip
        shell: bash

      - name: Install TrustedSigning (Windows)
        run: |
            Invoke-WebRequest -Uri https://dist.nuget.org/win-x86-commandline/latest/nuget.exe -OutFile .\nuget.exe
            .\nuget.exe install Microsoft.Windows.SDK.BuildTools -Version 10.0.22621.3233 -x
            .\nuget.exe install Microsoft.Trusted.Signing.Client -Version 1.0.86 -x
        shell: pwsh
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}

      - name: create-json
        id: create-json
        uses: jsdaniell/create-json@b8e77fa01397ca39cc4a6198cc29a3be5481afef # v1.2.3
        with:
            name: "credentials.json"
            dir: '${{ steps.set-file-base.outputs.SOURCE_BASE }}'
            json: '{"Endpoint": "${{ secrets.AZURE_ENDPOINT }}","CodeSigningAccountName": "${{ secrets.AZURE_CODE_SIGNING_NAME }}","CertificateProfileName": "${{ secrets.AZURE_CERT_PROFILE_NAME }}"}'
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}

      - name: Run CTest (Windows)
        env:
          BINSIGN: ${{ needs.check-secret.outputs.sign-state }}
          SIGNTOOLDIR: ${{ github.workspace }}/Microsoft.Windows.SDK.BuildTools/bin/10.0.22621.0/x64
        run: |
          cd "${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}"
          cmake --workflow --preset=${{ inputs.preset_name }}-MSVC --fresh
        shell: bash

      - name: Sign files with Trusted Signing
        uses: azure/trusted-signing-action@fc390cf8ed0f14e248a542af1d838388a47c7a7c # v0.5.10
        with:
          azure-tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          azure-client-id: ${{ secrets.AZURE_CLIENT_ID }}
          azure-client-secret: ${{ secrets.AZURE_CLIENT_SECRET }}
          endpoint: ${{ secrets.AZURE_ENDPOINT }}
          trusted-signing-account-name: ${{ secrets.AZURE_CODE_SIGNING_NAME }}
          certificate-profile-name: ${{ secrets.AZURE_CERT_PROFILE_NAME }}
          files-folder: ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-MSVC
          files-folder-filter: msi
          file-digest: SHA256
          timestamp-rfc3161: http://timestamp.acs.microsoft.com
          timestamp-digest: SHA256
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}

      - name: Publish binary (Windows)
        id: publish-ctest-binary
        run: |
          mkdir "${{ runner.workspace }}/build"
          mkdir "${{ runner.workspace }}/build/hdf5"
          Copy-Item -Path ${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}/LICENSE -Destination ${{ runner.workspace }}/build/hdf5/
          Copy-Item -Path ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-MSVC/README.md -Destination ${{ runner.workspace }}/build/hdf5/
          Copy-Item -Path ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-MSVC/* -Destination ${{ runner.workspace }}/build/hdf5/ -Include *.zip
          cd "${{ runner.workspace }}/build"
          7z a -tzip ${{ steps.set-file-base.outputs.FILE_BASE }}-win-vs2022_cl.zip hdf5
        shell: pwsh

      - name: Publish msi binary (Windows)
        id: publish-ctest-msi-binary
        run: |
          mkdir "${{ runner.workspace }}/buildmsi"
          Copy-Item -Path ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-MSVC/* -Destination ${{ runner.workspace }}/buildmsi/${{ steps.set-file-base.outputs.FILE_BASE }}-win-vs2022_cl.msi -Include *.msi
        shell: pwsh

      - name: List files in the space (Windows)
        run: |
              Get-ChildItem -Path ${{ github.workspace }}
              Get-ChildItem -Path ${{ runner.workspace }}
        shell: pwsh

      # Save files created by CTest script
      - name: Save published binary (Windows)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: zip-vs2022_cl-binary
              path: ${{ runner.workspace }}/build/${{ steps.set-file-base.outputs.FILE_BASE }}-win-vs2022_cl.zip
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

      - name: Save published msi binary (Windows)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: msi-vs2022_cl-binary
              path: ${{ runner.workspace }}/buildmsi/${{ steps.set-file-base.outputs.FILE_BASE }}-win-vs2022_cl.msi
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

  build_and_test_linux:
  # Linux (Ubuntu) w/ gcc
  #
    name: "Ubuntu gcc"
    runs-on: ubuntu-latest
    needs: [check-secret]
    steps:
      - name: Install Dependencies (Linux)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build graphviz

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Check CMake Version
        shell: bash
        run: |
          which cmake
          cmake --version

      - name: Set up JDK 21
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '21'
          distribution: 'temurin'

      - name: Set file base name (Linux)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (Linux)
        run: |
              ls -l ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: Uncompress source (Linux)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Run CTest (Linux)
        id: run-ctest
        run: |
          cd "${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}"
          if [ "${{ inputs.maven_enabled }}" == "true" ]; then
            if [ "${{ inputs.use_environ }}" == "release" ]; then
              echo "Building with Maven release preset"
              ACTIVE_PRESET_BASE=$(echo "${{ inputs.preset_name }}-GNUC-Maven")
            else
              echo "Building with Maven snapshot preset"
              ACTIVE_PRESET_BASE=$(echo "${{ inputs.preset_name }}-GNUC-Maven-Snapshot")
            fi
          else
            echo "Building with standard preset"
            ACTIVE_PRESET_BASE=$(echo "${{ inputs.preset_name }}-GNUC")
          fi
          echo "ACTIVE_PRESET=$ACTIVE_PRESET_BASE" >> $GITHUB_OUTPUT
          cmake --workflow --preset=$ACTIVE_PRESET_BASE --fresh
        shell: bash

      - name: Publish binary (Linux)
        id: publish-ctest-binary
        run: |
          mkdir "${{ runner.workspace }}/build"
          mkdir "${{ runner.workspace }}/build/hdf5"
          cp ${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}/LICENSE ${{ runner.workspace }}/build/hdf5
          cp ${{ runner.workspace }}/hdf5/build/${{ steps.run-ctest.outputs.ACTIVE_PRESET }}/README.md ${{ runner.workspace }}/build/hdf5
          cp ${{ runner.workspace }}/hdf5/build/${{ steps.run-ctest.outputs.ACTIVE_PRESET }}/*.tar.gz ${{ runner.workspace }}/build/hdf5
          cd "${{ runner.workspace }}/build"
          tar -zcvf ${{ steps.set-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.tar.gz hdf5
        shell: bash

      - name: Publish deb binary (Linux)
        id: publish-ctest-deb-binary
        run: |
          mkdir "${{ runner.workspace }}/builddeb"
          cp ${{ runner.workspace }}/hdf5/build/${{ steps.run-ctest.outputs.ACTIVE_PRESET }}/*.deb ${{ runner.workspace }}/builddeb/${{ steps.set-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.deb
        shell: bash

      - name: Publish rpm binary (Linux)
        id: publish-ctest-rpm-binary
        run: |
          mkdir "${{ runner.workspace }}/buildrpm"
          cp ${{ runner.workspace }}/hdf5/build/${{ steps.run-ctest.outputs.ACTIVE_PRESET }}/*.rpm ${{ runner.workspace }}/buildrpm/${{ steps.set-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.rpm
        shell: bash

      - name: List files in the space (Linux)
        run: |
              ls ${{ github.workspace }}
              ls -l ${{ runner.workspace }}

      # Save files created by CTest script
      - name: Save published binary (Linux)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: tgz-ubuntu-2404_gcc-binary
              path: ${{ runner.workspace }}/build/${{ steps.set-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.tar.gz
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

      - name: Save published binary deb (Linux)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: deb-ubuntu-2404_gcc-binary
              path: ${{ runner.workspace }}/builddeb/${{ steps.set-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.deb
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

      - name: Save published binary rpm (Linux)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: rpm-ubuntu-2404_gcc-binary
              path: ${{ runner.workspace }}/buildrpm/${{ steps.set-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.rpm
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

      # Save doxygen files created by CTest script
      - name: Save published doxygen (Linux)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: docs-doxygen
              path: ${{ runner.workspace }}/hdf5/build/${{ steps.run-ctest.outputs.ACTIVE_PRESET }}/hdf5lib_docs/html
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

      # Upload Maven artifacts when Maven deployment is enabled
      - name: Collect Maven artifacts (Linux)
        if: ${{ inputs.maven_enabled == true }}
        run: |
          echo "Collecting Maven artifacts for deployment..."
          mkdir -p ${{ runner.workspace }}/maven-artifacts

          # Determine the build directory based on Maven preset used
          BUILD_DIR="${{ runner.workspace }}/hdf5/build/${{ steps.run-ctest.outputs.ACTIVE_PRESET }}"

          echo "Looking for artifacts in: ${BUILD_DIR}"

          # Copy JAR files
          find "${BUILD_DIR}" -name "*.jar" -not -name "*test*" -not -name "*H5Ex_*" -exec cp {} ${{ runner.workspace }}/maven-artifacts/ \;

          # Copy POM files
          find "${BUILD_DIR}" -name "pom.xml" -exec cp {} ${{ runner.workspace }}/maven-artifacts/ \;

          # List collected artifacts
          echo "Collected Maven artifacts:"
          ls -la ${{ runner.workspace }}/maven-artifacts/
        shell: bash

      - name: Upload Maven artifacts (Linux)
        if: ${{ inputs.maven_enabled == true }}
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: Linux-${{ inputs.preset_name }}-artifacts
              path: ${{ runner.workspace }}/maven-artifacts
              if-no-files-found: warn

  build_and_test_mac_latest:
  # MacOS w/ Clang
  #
    name: "MacOS Clang"
    runs-on: macos-latest
    needs: [check-secret]
    steps:
      - name: Install Dependencies (MacOS_latest)
        run: brew install ninja

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: check clang version
        shell: bash
        run: |
          which clang
          clang -v

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Check CMake Version
        shell: bash
        run: |
          which cmake
          cmake --version

      - name: Install the Apple certificate and provisioning profile
        shell: bash
        env:
          BUILD_CERTIFICATE_BASE64: ${{ secrets.APPLE_CERTS_BASE64 }}
          P12_PASSWORD: ${{ secrets.APPLE_CERTS_BASE64_PASSWD }}
          KEYCHAIN_PASSWD: ${{ secrets.KEYCHAIN_PASSWD }}
        run: |
            # create variables
            CERTIFICATE_PATH=$RUNNER_TEMP/build_certificate.p12
            KEYCHAIN_FILE=${{ vars.KEYCHAIN_NAME }}.keychain
            # import certificate from secrets
            echo $BUILD_CERTIFICATE_BASE64 | base64 --decode > $CERTIFICATE_PATH
            security -v create-keychain -p $KEYCHAIN_PASSWD $KEYCHAIN_FILE
            security -v list-keychain -d user -s $KEYCHAIN_FILE
            security -v list-keychains
            security -v set-keychain-settings -lut 21600 $KEYCHAIN_FILE
            security -v unlock-keychain -p $KEYCHAIN_PASSWD $KEYCHAIN_FILE
            # import certificate to keychain
            security -v import $CERTIFICATE_PATH -P $P12_PASSWORD -A -t cert -f pkcs12 -k $KEYCHAIN_FILE
            security -v set-key-partition-list -S apple-tool:,codesign:,apple: -k $KEYCHAIN_PASSWD $KEYCHAIN_FILE
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}

      - name: Set up JDK 19
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '21'
          distribution: 'temurin'

      - name: Set file base name (MacOS_latest)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (MacOS_latest)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (MacOS_latest)
        run: |
              ls ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: Uncompress source (MacOS_latest)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      # symlinks the compiler executables to a common location 
      - name: Setup GNU Fortran
        uses: fortran-lang/setup-fortran@47809fdb6e637da656ce9ada436527b240c1287f # v1
        id: setup-fortran
        with:
          compiler: gcc
          version: 14

      - name: Run CTest (MacOS_latest)
        id: run-ctest
        env:
          BINSIGN: ${{ needs.check-secret.outputs.sign-state }}
          SIGNER: ${{ vars.SIGNER }}
        run: |
          cd "${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}"
          cmake --workflow --preset=${{ inputs.preset_name }}-macos-Clang --fresh
        shell: bash

      - name: Sign dmg (MacOS_latest)
        id: sign-dmg
        env:
          KEYCHAIN_PASSWD: ${{ secrets.KEYCHAIN_PASSWD }}
          KEYCHAIN_NAME: ${{ vars.KEYCHAIN_NAME }}
          SIGNER: ${{ vars.SIGNER }}
          NOTARY_USER: ${{ vars.NOTARY_USER }}
          NOTARY_KEY: ${{ vars.NOTARY_KEY }}
        run: |
          /usr/bin/codesign --force --timestamp --options runtime --entitlements ${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/install/distribution.entitlements --verbose=4 --strict --sign ${{ env.SIGNER }} --deep ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-macos-Clang/*.dmg
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}
        shell: bash

      - name: Check dmg timestamp (MacOS_latest)
        run: |
          /usr/bin/codesign -dvv ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-macos-Clang/*.dmg
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}
        shell: bash

      - name: Verify dmg (MacOS_latest)
        run: |
          /usr/bin/hdiutil verify ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-macos-Clang/*.dmg
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}
        shell: bash

      - name: Notarize dmg (MacOS_latest)
        id: notarize-dmg
        env:
          KEYCHAIN_PASSWD: ${{ secrets.KEYCHAIN_PASSWD }}
          KEYCHAIN_NAME: ${{ vars.KEYCHAIN_NAME }}
          SIGNER: ${{ vars.SIGNER }}
          NOTARY_USER: ${{ vars.NOTARY_USER }}
          NOTARY_KEY: ${{ vars.NOTARY_KEY }}
        run: |
          jsonout=$(/usr/bin/xcrun notarytool submit --wait --output-format json --apple-id ${{ env.NOTARY_USER }} --password ${{ env.NOTARY_KEY }} --team-id ${{ env.SIGNER }} ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-macos-Clang/*.dmg)
          echo "JSONOUT=$jsonout" >> $GITHUB_OUTPUT
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}
        shell: bash

      - name: Get ID token (MacOS_latest)
        id: get-id-token
        run: |
          echo "notary result is ${{ fromJson(steps.notarize-dmg.outputs.JSONOUT) }}"
          token=${{ fromJson(steps.notarize-dmg.outputs.JSONOUT).id }}
          echo "ID_TOKEN=$token" >> "$GITHUB_OUTPUT"
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}
        shell: bash

      - name: post notary check (MacOS_latest)
        id: post-notary
        env:
          KEYCHAIN_PASSWD: ${{ secrets.KEYCHAIN_PASSWD }}
          KEYCHAIN_NAME: ${{ vars.KEYCHAIN_NAME }}
          SIGNER: ${{ vars.SIGNER }}
          NOTARY_USER: ${{ vars.NOTARY_USER }}
          NOTARY_KEY: ${{ vars.NOTARY_KEY }}
        run: |
          {
            echo 'NOTARYOUT<<EOF'
            /usr/bin/xcrun notarytool info --apple-id ${{ env.NOTARY_USER }} --password ${{ env.NOTARY_KEY }} --team-id ${{ env.SIGNER }} ${{ steps.get-id-token.outputs.ID_TOKEN }}
            echo EOF
          } >> $GITHUB_OUTPUT
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}
        shell: bash

      - name: Get notary info (MacOS_latest)
        id: get-notary-info
        run: |
          echo "notary info is ${{ steps.post-notary.outputs.NOTARYOUT }}."
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}
        shell: bash

      - name: Staple dmg (MacOS_latest)
        id: staple-dmg
        env:
          KEYCHAIN_PASSWD: ${{ secrets.KEYCHAIN_PASSWD }}
          KEYCHAIN_NAME: ${{ vars.KEYCHAIN_NAME }}
          SIGNER: ${{ vars.SIGNER }}
          NOTARY_USER: ${{ vars.NOTARY_USER }}
          NOTARY_KEY: ${{ vars.NOTARY_KEY }}
        run: |
          /usr/bin/xcrun stapler staple ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-macos-Clang/*.dmg
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}
        shell: bash
        continue-on-error: true

      - name: Publish binary (MacOS_latest)
        id: publish-ctest-binary
        run: |
          mkdir "${{ runner.workspace }}/build"
          mkdir "${{ runner.workspace }}/build/hdf5"
          cp ${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}/LICENSE ${{ runner.workspace }}/build/hdf5
          cp ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-macos-Clang/README.md ${{ runner.workspace }}/build/hdf5
          cp ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-macos-Clang/*.tar.gz ${{ runner.workspace }}/build/hdf5
          cd "${{ runner.workspace }}/build"
          tar -zcvf ${{ steps.set-file-base.outputs.FILE_BASE }}-macos14_clang.tar.gz hdf5
        shell: bash

      - name: Publish dmg binary (MacOS_latest)
        id: publish-ctest-dmg-binary
        run: |
          mkdir "${{ runner.workspace }}/builddmg"
          cp ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-macos-Clang/*.dmg ${{ runner.workspace }}/builddmg/${{ steps.set-file-base.outputs.FILE_BASE }}-macos14_clang.dmg
        shell: bash

      - name: List files in the space (MacOS_latest)
        run: |
              ls ${{ github.workspace }}
              ls -l ${{ runner.workspace }}

      # Save files created by CTest script
      - name: Save published binary (MacOS_latest)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: tgz-macos14_clang-binary
              path: ${{ runner.workspace }}/build/${{ steps.set-file-base.outputs.FILE_BASE }}-macos14_clang.tar.gz
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

      - name: Save published dmg binary (MacOS_latest)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: tgz-macos14_clang-dmg-binary
              path: ${{ runner.workspace }}/builddmg/${{ steps.set-file-base.outputs.FILE_BASE }}-macos14_clang.dmg
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

  build_and_test_S3_linux:
  # Linux S3 (Ubuntu) w/ gcc
  #
    name: "Ubuntu gcc S3"
    runs-on: ubuntu-latest
    steps:
      - name: Install Dependencies (Linux S3)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Install aws-c-s3 (Cached installation)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
          name: libaws-c-s3-Release

      - name: Untar libaws-c-s3 installation
        run: |
          tar xvf libaws-c-s3.tar -C ${{ runner.workspace }}

      - name: List contents of libaws-c-s3 installation
        run: |
          ls -lR ${{ runner.workspace }}/aws-c-s3-build

      - name: Setup environment
        shell: bash
        run: |
          echo "LD_LIBRARY_PATH=${{ runner.workspace }}/aws-c-s3-build/lib:$LD_LIBRARY_PATH" >> $GITHUB_ENV
          echo "CMAKE_PREFIX_PATH=${{ runner.workspace }}/aws-c-s3-build" >> $GITHUB_ENV

      - name: Set file base name (Linux S3)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (Linux S3)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (Linux S3)
        run: |
              ls -l ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: Uncompress source (Linux S3)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Run CTest (Linux S3)
        run: |
          cd "${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}"
          cmake --workflow --preset=${{ inputs.preset_name }}-GNUC-S3 --fresh
        shell: bash

      - name: Publish binary (Linux S3)
        id: publish-ctest-binary
        run: |
          mkdir "${{ runner.workspace }}/build"
          mkdir "${{ runner.workspace }}/build/hdf5"
          cp ${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}/LICENSE ${{ runner.workspace }}/build/hdf5
          cp ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-GNUC-S3/README.md ${{ runner.workspace }}/build/hdf5
          cp ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-GNUC-S3/*.tar.gz ${{ runner.workspace }}/build/hdf5
          cd "${{ runner.workspace }}/build"
          tar -zcvf ${{ steps.set-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc_s3.tar.gz hdf5
        shell: bash

      - name: List files in the space (Linux S3)
        run: |
              ls ${{ github.workspace }}
              ls -l ${{ runner.workspace }}

      # Save files created by CTest script
      - name: Save published binary (Linux S3)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: tgz-ubuntu-2404_gcc_s3-binary
              path: ${{ runner.workspace }}/build/${{ steps.set-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc_s3.tar.gz
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

####### intel builds
  build_and_test_win_intel:
  # Windows w/ OneAPI
  #
    name: "Windows Intel CTest"
    runs-on: windows-latest
    needs: [check-secret]
    steps:
      - name: Install Dependencies (Windows_intel)
        run: choco install ninja

      - name: add oneAPI to env
        uses: fortran-lang/setup-fortran@47809fdb6e637da656ce9ada436527b240c1287f # v1
        id: setup-fortran
        with:
          compiler: intel
          version: '2025.0'

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Set file base name (Windows_intel)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT
        shell: bash

      # Get files created by release script
      - name: Get zip-tarball (Windows_intel)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
              name: zip-tarball
              path: ${{ github.workspace }}

      - name: using powershell
        shell: pwsh
        run: Get-Location

      - name: List files for the space (Windows_intel)
        run: |
              Get-ChildItem -Path ${{ github.workspace }}
              Get-ChildItem -Path ${{ runner.workspace }}
        shell: pwsh

      - name: Uncompress source (Windows_intel)
        working-directory: ${{ github.workspace }}
        run: 7z x ${{ steps.set-file-base.outputs.FILE_BASE }}.zip
        shell: bash

      - name: Install TrustedSigning (Windows)
        run: |
            Invoke-WebRequest -Uri https://dist.nuget.org/win-x86-commandline/latest/nuget.exe -OutFile .\nuget.exe
            .\nuget.exe install Microsoft.Windows.SDK.BuildTools -Version 10.0.22621.3233 -x
            .\nuget.exe install Microsoft.Trusted.Signing.Client -Version 1.0.86 -x
        shell: pwsh
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}

      - name: create-json
        id: create-json
        uses: jsdaniell/create-json@b8e77fa01397ca39cc4a6198cc29a3be5481afef # v1.2.3
        with:
            name: "credentials.json"
            dir: '${{ steps.set-file-base.outputs.SOURCE_BASE }}'
            json: '{"Endpoint": "${{ secrets.AZURE_ENDPOINT }}","CodeSigningAccountName": "${{ secrets.AZURE_CODE_SIGNING_NAME }}","CertificateProfileName": "${{ secrets.AZURE_CERT_PROFILE_NAME }}"}'
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}

      - name: Run CTest (Windows_intel) with oneapi
        env:
          FC: ${{ steps.setup-fortran.outputs.fc }}
          CC: ${{ steps.setup-fortran.outputs.cc }}
          
          BINSIGN: ${{ needs.check-secret.outputs.sign-state }}
          SIGNTOOLDIR: ${{ github.workspace }}/Microsoft.Windows.SDK.BuildTools/bin/10.0.22621.0/x64
        run: |
          cd "${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}"
          cmake --workflow --preset=${{ inputs.preset_name }}-win-Intel --fresh
        shell: pwsh

      - name: Sign files with Trusted Signing (Windows_intel)
        uses: azure/trusted-signing-action@fc390cf8ed0f14e248a542af1d838388a47c7a7c # v0.5.10
        with:
          azure-tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          azure-client-id: ${{ secrets.AZURE_CLIENT_ID }}
          azure-client-secret: ${{ secrets.AZURE_CLIENT_SECRET }}
          endpoint: ${{ secrets.AZURE_ENDPOINT }}
          trusted-signing-account-name: ${{ secrets.AZURE_CODE_SIGNING_NAME }}
          certificate-profile-name: ${{ secrets.AZURE_CERT_PROFILE_NAME }}
          files-folder: ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-Intel
          files-folder-filter: msi
          file-digest: SHA256
          timestamp-rfc3161: http://timestamp.acs.microsoft.com
          timestamp-digest: SHA256
        if: ${{ needs.check-secret.outputs.sign-state == 'exists' }}

      - name: Publish binary (Windows_intel)
        id: publish-ctest-binary
        run: |
          mkdir "${{ runner.workspace }}/build"
          mkdir "${{ runner.workspace }}/build/hdf5"
          Copy-Item -Path ${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}/LICENSE -Destination ${{ runner.workspace }}/build/hdf5/
          Copy-Item -Path ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-Intel/README.md -Destination ${{ runner.workspace }}/build/hdf5/
          Copy-Item -Path ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-Intel/* -Destination ${{ runner.workspace }}/build/hdf5/ -Include *.zip
          cd "${{ runner.workspace }}/build"
          7z a -tzip ${{ steps.set-file-base.outputs.FILE_BASE }}-win-vs2022_intel.zip hdf5
        shell: pwsh

      - name: Publish msi binary (Windows_intel)
        id: publish-ctest-msi-binary
        run: |
          mkdir "${{ runner.workspace }}/buildmsi"
          Copy-Item -Path ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-Intel/* -Destination ${{ runner.workspace }}/buildmsi/${{ steps.set-file-base.outputs.FILE_BASE }}-win-vs2022_intel.msi -Include *.msi
        shell: pwsh

      - name: List files in the space (Windows_intel)
        run: |
              Get-ChildItem -Path ${{ github.workspace }}
              Get-ChildItem -Path ${{ runner.workspace }}
        shell: pwsh

      # Save files created by CTest script
      - name: Save published binary (Windows_intel)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: zip-vs2022_intel-binary
              path: ${{ runner.workspace }}/build/${{ steps.set-file-base.outputs.FILE_BASE }}-win-vs2022_intel.zip
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

      - name: Save published msi binary (Windows_intel)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: msi-vs2022_intel-binary
              path: ${{ runner.workspace }}/buildmsi/${{ steps.set-file-base.outputs.FILE_BASE }}-win-vs2022_intel.msi
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

  build_and_test_linux_intel:
  # Linux (Ubuntu) w/ OneAPI
  #
    name: "Ubuntu Intel"
    runs-on: ubuntu-latest
    needs: [check-secret]
    steps:
      - name: Install Dependencies (Linux_intel)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz

      - name: add oneAPI to env
        uses: fortran-lang/setup-fortran@47809fdb6e637da656ce9ada436527b240c1287f # v1
        id: setup-fortran
        with:
          compiler: intel
          version: '2025.0'

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Set file base name (Linux_intel)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (Linux_intel)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (Linux_intel)
        run: |
              ls -l ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: Uncompress source (Linux_intel)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Run CTest (Linux_intel)
        env:
          FC: ${{ steps.setup-fortran.outputs.fc }}
          CC: ${{ steps.setup-fortran.outputs.cc }}
        run: |
          cd "${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}"
          cmake --workflow --preset=${{ inputs.preset_name }}-Intel --fresh
        shell: bash

      - name: Publish binary (Linux_intel)
        id: publish-ctest-binary
        run: |
          mkdir "${{ runner.workspace }}/build"
          mkdir "${{ runner.workspace }}/build/hdf5"
          cp ${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}/LICENSE ${{ runner.workspace }}/build/hdf5
          cp ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-Intel/README.md ${{ runner.workspace }}/build/hdf5
          cp ${{ runner.workspace }}/hdf5/build/${{ inputs.preset_name }}-Intel/*.tar.gz ${{ runner.workspace }}/build/hdf5
          cd "${{ runner.workspace }}/build"
          tar -zcvf ${{ steps.set-file-base.outputs.FILE_BASE }}-ubuntu-2404_intel.tar.gz hdf5
        shell: bash

      - name: List files in the space (Linux_intel)
        run: |
              ls ${{ github.workspace }}
              ls -l ${{ runner.workspace }}

      # Save files created by CTest script
      - name: Save published binary (Linux_intel)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: tgz-ubuntu-2404_intel-binary
              path: ${{ runner.workspace }}/build/${{ steps.set-file-base.outputs.FILE_BASE }}-ubuntu-2404_intel.tar.gz
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
```

### `.github/workflows/cve.yml`

```yaml
name: cve dev

# Triggers the workflow on push or pull request or on demand
on:
  workflow_dispatch:
  push:
  pull_request:
    branches: [ develop ]
    paths-ignore:
      - '.github/CODEOWNERS'
      - '.github/FUNDING.yml'
      - 'doc/**'
      - 'release_docs/**'
      - 'ACKNOWLEDGEMENTS'
      - 'LICENSE**'
      - '**.md'

# Using concurrency to cancel any in-progress job or run
concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref && github.ref || github.run_id }}
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  build:
    name: CVE regression
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install HDF5
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -DCMAKE_INSTALL_PREFIX=/usr/local -DBUILD_TESTING:BOOL=OFF $GITHUB_WORKSPACE
          make
          sudo make install
      - name: Checkout CVE test repository
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: HDFGroup/cve_hdf5
          path: cve_hdf5
      - name: Run regression tests
        run: |
          cd cve_hdf5
          export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/lib"
          ./test_hdf5_cve.sh /usr/local/bin ./cve_out
```

### `.github/workflows/cygwin.yml`

```yaml
name: hdf5 dev cygwin

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      snap_name:
        description: 'The name in the source tarballs'
        type: string
        required: false
        default: hdfsrc
      file_base:
        description: "The common base name of the source tarballs"
        required: true
        type: string
      use_environ:
        description: 'Environment to locate files'
        type: string
        required: true
        default: snapshots

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  cygwin_build_and_test:
    name: "cygwin-${{ inputs.build_mode }}"
    runs-on: windows-latest
    timeout-minutes: 40
    steps:
      - name: Set git to use LF
        run: |
          git config --global core.autocrlf input

      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install Cygwin
        uses: cygwin/cygwin-install-action@b9bf9147075ee9811ac11beee9351eeb93e2f2fb # master
        with:
          packages: cmake gcc-fortran make ninja zlib-devel flex bison perl

      - name: Set file base name (Cygwin)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT
        shell: C:\cygwin\bin\bash.exe -eo pipefail -o igncr '{0}'

      # Get files created by release script
      - name: Get zip-tarball (Cygwin)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: zip-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (Cygwin)
        run: |
            ls -l ${{ github.workspace }}
            ls ${{ runner.workspace }}

      - name: Uncompress source (Cygwin)
        working-directory: ${{ github.workspace }}
        run: 7z x ${{ steps.set-file-base.outputs.FILE_BASE }}.zip
        shell: C:\cygwin\bin\bash.exe -eo pipefail -o igncr '{0}'

      - name: Copy script files for the space (Cygwin)
        run: |
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

      - name: List files for the hdf5 (Cygwin)
        run: |
            ls ${{ runner.workspace }}/hdf5

      - name: Create options file (Cygwin)
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
            set (CTEST_DROP_SITE_INIT "my.cdash.org")
            # Change following line to submit to your CDash dashboard to a different CDash project
            #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
            set (MODEL "GHDaily")
            set (GROUP "GHDaily")
            set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=NOTICE")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_GENERATE_HEADERS:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DENABLE_ZFP:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DENABLE_ZSTD:BOOL=OFF")

      - name: Run CTest (Cygwin)
        shell: C:\cygwin\bin\bash.exe -eo pipefail -o igncr '{0}'
        env:
            CTEST_OUTPUT_ON_FAILURE: true
        run: |
          export PATH=/usr/bin:$PATH
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=GH-${{ github.event.repository.full_name }}-CYG,LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C Release -O hdf5.log
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (Cygwin)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
            name: gcc-cygwin-log
            path: ${{ runner.workspace }}/hdf5/hdf5.log
            if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
```

### `.github/workflows/daily-build.yml`

```yaml
name: hdf5 dev daily build

# Triggers the workflow on demand or on a call from another workflow
# NOTE: inputs must be duplicated between triggers
on:
  workflow_dispatch:
    inputs:
      use_ignore:
        description: 'Ignore has_changes check'
        type: string
        required: false
        default: check
  workflow_call:
    inputs:
      use_ignore:
        description: 'Ignore has_changes check'
        type: string
        required: false
        default: check
    outputs:
      has_changes:
        description: "Whether there were changes the previous day"
        value: ${{ jobs.call-workflow-tarball.outputs.has_changes }}
      file_base:
        description: "The common base name of the source tarballs"
        value: ${{ jobs.call-workflow-tarball.outputs.file_base }}

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  get-old-names:
    runs-on: ubuntu-latest
    outputs:
      hdf5-name: ${{ steps.gethdf5base.outputs.HDF5_NAME_BASE }}
      run-ignore: ${{ steps.getinputs.outputs.INPUTS_IGNORE }}

    steps:
    - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

    - name: Get hdf5 release base name
      uses: dsaltares/fetch-gh-release-asset@aa2ab1243d6e0d5b405b973c89fa4d06a2d0fff7 # master
      with:
        version: 'tags/snapshot'
        file: 'last-file.txt'
      continue-on-error: true

    - name: Read base-name file
      id: gethdf5base
      run: echo "HDF5_NAME_BASE=$(cat last-file.txt)" >> $GITHUB_OUTPUT

    - run: echo "hdf5 base name is ${{ steps.gethdf5base.outputs.HDF5_NAME_BASE }}."

    - name: Read inputs
      id: getinputs
      run: |
        echo "INPUTS_IGNORE=${{ inputs.use_ignore }}" >> $GITHUB_OUTPUT

    - run: echo "use_ignore is ${{ steps.getinputs.outputs.INPUTS_IGNORE }}."

  call-workflow-tarball:
    needs: [get-old-names]
    uses: ./.github/workflows/tarball.yml
    with:
      use_ignore: ${{ needs.get-old-names.outputs.run-ignore }}
      use_tag: snapshot
      use_environ: snapshots

  call-aws-c-s3-build:
    needs: call-workflow-tarball
    name: "Build aws-c-s3 library"
    uses: ./.github/workflows/build-aws-c-s3.yml
    with:
      build_mode: "Release"
      # Use latest release for building from source on Ubuntu
      # until a package is available to install
      aws_c_s3_tag: "v0.8.0"
    if: ${{ ((needs.call-workflow-tarball.outputs.has_changes == 'true') || (needs.get-old-names.outputs.run-ignore == 'ignore')) }}

  call-release-cmake-cygwin:
    needs: [get-old-names, call-workflow-tarball]
    name: "Cygwin Workflows"
    uses: ./.github/workflows/cygwin.yml
    with:
      file_base: ${{ needs.call-workflow-tarball.outputs.file_base }}
      use_environ: snapshots
    if: ${{ ((needs.call-workflow-tarball.outputs.has_changes == 'true') || (needs.get-old-names.outputs.run-ignore == 'ignore')) }}

  call-workflow-c-script:
    needs: [get-old-names, call-workflow-tarball, call-aws-c-s3-build]
    uses: ./.github/workflows/script.yml
    with:
      cmake_version: "latest"
      file_base: ${{ needs.call-workflow-tarball.outputs.file_base }}
      use_environ: snapshots
    if: ${{ ((needs.call-workflow-tarball.outputs.has_changes == 'true') || (needs.get-old-names.outputs.run-ignore == 'ignore')) }}

  call-workflow-par-script:
    needs: [get-old-names, call-workflow-tarball]
    uses: ./.github/workflows/par-script.yml
    with:
      cmake_version: "latest"
      file_base: ${{ needs.call-workflow-tarball.outputs.file_base }}
      use_environ: snapshots
      build_mode: "Release"
    if: ${{ ((needs.call-workflow-tarball.outputs.has_changes == 'true') || (needs.get-old-names.outputs.run-ignore == 'ignore')) }}

  call-workflow-par-source:
    needs: [get-old-names, call-workflow-tarball]
    uses: ./.github/workflows/par-source.yml
    with:
      cmake_version: "latest"
      file_base: ${{ needs.call-workflow-tarball.outputs.file_base }}
      use_environ: snapshots
      build_mode: "Release"
    if: ${{ ((needs.call-workflow-tarball.outputs.has_changes == 'true') || (needs.get-old-names.outputs.run-ignore == 'ignore')) }}

  call-developer-cmake:
    needs: [get-old-names, call-workflow-tarball]
    name: "Developer Workflows"
    uses: ./.github/workflows/dev.yml
    with:
      file_base: ${{ needs.call-workflow-tarball.outputs.file_base }}
    if: ${{ ((needs.call-workflow-tarball.outputs.has_changes == 'true') || (needs.get-old-names.outputs.run-ignore == 'ignore')) }}

  call-workflow-sanitizers:
    needs: [get-old-names, call-workflow-tarball]
    uses: ./.github/workflows/analysis.yml
    with:
      file_base: ${{ needs.call-workflow-tarball.outputs.file_base }}
      use_environ: snapshots
    if: ${{ ((needs.call-workflow-tarball.outputs.has_changes == 'true') || (needs.get-old-names.outputs.run-ignore == 'ignore')) }}

  call-workflow-ctest:
    needs: [get-old-names, call-workflow-tarball, call-aws-c-s3-build]
    uses: ./.github/workflows/ctest.yml
    with:
      cmake_version: 'latest'
      preset_name: ci-StdShar
      file_base: ${{ needs.call-workflow-tarball.outputs.file_base }}
#      use_tag: snapshot
      use_environ: snapshots
    secrets:
        APPLE_CERTS_BASE64: ${{ secrets.APPLE_CERTS_BASE64 }}
        APPLE_CERTS_BASE64_PASSWD: ${{ secrets.APPLE_CERTS_BASE64_PASSWD }}
        KEYCHAIN_PASSWD: ${{ secrets.KEYCHAIN_PASSWD }}
        AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
        AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
        AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
        AZURE_ENDPOINT: ${{ secrets.AZURE_ENDPOINT }}
        AZURE_CODE_SIGNING_NAME: ${{ secrets.AZURE_CODE_SIGNING_NAME }}
        AZURE_CERT_PROFILE_NAME: ${{ secrets.AZURE_CERT_PROFILE_NAME }}
    if: ${{ ((needs.call-workflow-tarball.outputs.has_changes == 'true') || (needs.get-old-names.outputs.run-ignore == 'ignore')) }}

  call-workflow-abi:
    needs: [get-old-names, call-workflow-tarball, call-workflow-ctest]
    uses: ./.github/workflows/abi-report.yml
    with:
      file_ref: '1.14.5'
      file_base: ${{ needs.call-workflow-tarball.outputs.file_base }}
      use_tag: snapshot
      use_environ: snapshots
    if: ${{ ((needs.call-workflow-tarball.outputs.has_changes == 'true') || (needs.get-old-names.outputs.run-ignore == 'ignore')) }}

  call-workflow-release:
    needs: [get-old-names, call-workflow-tarball, call-workflow-ctest, call-workflow-abi]
    permissions:
      contents: write # In order to allow tag creation
    uses: ./.github/workflows/release-files.yml
    with:
      file_base: ${{ needs.call-workflow-tarball.outputs.file_base }}
      file_branch: ${{ needs.call-workflow-tarball.outputs.file_branch }}
      file_sha: ${{ needs.call-workflow-tarball.outputs.file_sha }}
      use_tag: snapshot
      use_environ: snapshots
    if: ${{ ((needs.call-workflow-tarball.outputs.has_changes == 'true') || (needs.get-old-names.outputs.run-ignore == 'ignore')) }}

  call-workflow-remove:
    needs: [get-old-names, call-workflow-tarball, call-workflow-ctest, call-workflow-abi, call-workflow-release]
    permissions:
      contents: write # In order to allow file deletion
    uses: ./.github/workflows/remove-files.yml
    with:
      file_base: ${{ needs.get-old-names.outputs.hdf5-name }}
      use_tag: snapshot
      use_environ: snapshots
    if: ${{ ((needs.call-workflow-tarball.outputs.has_changes == 'true') || (needs.get-old-names.outputs.run-ignore == 'ignore')) && (needs.get-old-names.outputs.hdf5-name != needs.call-workflow-tarball.outputs.file_base) }}
```

### `.github/workflows/daily-schedule.yml`

```yaml
name: hdf5 dev daily build on schedule

# Triggers the workflow on a schedule or on demand
on:
  workflow_dispatch:
  schedule:
    - cron: "6 0 * * *"

permissions:
    contents: write # In order to allow tag creation and file deletion

jobs:
  daily-build:
    uses: ./.github/workflows/daily-build.yml
    with:
      use_ignore: check
    if: "github.repository_owner == 'HDFGroup'"

  call-workflow-publish:
    needs: [daily-build]
    if: ${{ ((needs.daily-build.outputs.has_changes == 'true') && (github.repository_owner == 'HDFGroup')) }}
    runs-on: ubuntu-latest
    steps:
      - name: Get file base name
        id: get-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ needs.daily-build.outputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          fetch-depth: 0
          ref: '${{ github.head_ref || github.ref_name }}'

      # Get files created by tarball script
      - name: Get doxygen (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: docs-doxygen
          path: ${{ github.workspace }}/${{ steps.get-file-base.outputs.FILE_BASE }}.doxygen

      - name: Setup AWS CLI
        uses: aws-actions/configure-aws-credentials@61815dcd50bd041e203e49132bacad1fd04d2708 # v5
        with:
               aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
               aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
               aws-region: ${{ secrets.AWS_REGION }}

      - name: Sync userguide to latest S3 bucket
        run: |
          aws s3 sync ./${{ steps.get-file-base.outputs.FILE_BASE }}.doxygen s3://${{ secrets.AWS_S3_BUCKET }}/documentation/hdf5/latest --delete
```

### `.github/workflows/dev.yml`

```yaml
name: hdf5 Developer Mode CI

on:
  workflow_call:
    inputs:
      file_base:
        description: "The common base name of the source tarballs"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  linux_build_and_test:
    name: "Linux Developer Mode Express Test Workflows"

    # Don't run the action if the commit message says to skip CI
    if: "!contains(github.event.head_commit.message, 'skip-ci')"

    runs-on: ubuntu-latest
    steps:
      - name: Set file base name (Windows)
        id: set-file-base
        run: |
           FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
           echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT

      - name: Install Linux Dependencies
        run: |
           sudo apt-get update
           sudo apt-get install ninja-build doxygen graphviz
           sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
           sudo apt install libaec0 libaec-dev
           sudo apt install gcc-12 g++-12 gfortran-12
           echo "CC=gcc-12" >> $GITHUB_ENV
           echo "CXX=g++-12" >> $GITHUB_ENV
           echo "FC=gfortran-12" >> $GITHUB_ENV

      # Get files created by release script
      - name: Get tgz-tarball (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (Linux)
        run: |
              ls -l ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: Uncompress source (Linux)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: List files for the space (Linux)
        run: |
              ls -l ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: Configure
        shell: bash
        run: |
            mkdir "${{ runner.workspace }}/build"
            cd "${{ runner.workspace }}/build"
            cmake -C ${{ github.workspace }}/hdfsrc/config/cmake/cacheinit.cmake \
                 -G Ninja \
                 -DCMAKE_BUILD_TYPE=Developer \
                 -DBUILD_SHARED_LIBS=ON \
                 -DHDF5_ENABLE_ALL_WARNINGS=ON \
                 -DHDF5_ENABLE_DEBUG_H5B2=ON \
                 -DHDF5_ENABLE_DEBUG_H5FS_ASSERT=ON \
                 -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
                 -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
                 -DHDF5_BUILD_FORTRAN=OFF \
                 -DHDF5_BUILD_JAVA=OFF \
                 -DHDF5_BUILD_DOC=OFF \
                 -DLIBAEC_USE_LOCALCONTENT=OFF \
                 -DZLIB_USE_LOCALCONTENT=OFF \
                 -DHDF_TEST_EXPRESS=0 \
                 ${{ github.workspace }}/hdfsrc

      - name: Build
        run: cmake --build . --parallel 3 --config Developer
        working-directory: ${{ runner.workspace }}/build

      - name: Run Tests
        env:
          HDF5TestExpress: 0
        run: ctest . --parallel 2 -C Developer -R H5TESTXPR -E fheap
        working-directory: ${{ runner.workspace }}/build

  win_build_and_test:
    name: "Windows Developer Mode Express Test Workflows"

    # Don't run the action if the commit message says to skip CI
    if: "!contains(github.event.head_commit.message, 'skip-ci')"

    runs-on: windows-latest
    steps:
      - name: Install Dependencies (Windows)
        run: choco install ninja

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Enable Developer Command Prompt
        uses: ilammy/msvc-dev-cmd@0b201ec74fa43914dc39ae48a89fd1d8cb592756 # v1.13.0

      - name: Check CMake Version
        shell: bash
        run: |
          which cmake
          cmake --version

      - name: Set file base name (Windows)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT
        shell: bash

      # Get files created by release script
      - name: Get zip-tarball (Windows)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: zip-tarball
              path: ${{ github.workspace }}

      - name: using powershell
        shell: pwsh
        run: Get-Location

      - name: List files for the space (Windows)
        run: |
              Get-ChildItem -Path ${{ github.workspace }}
              Get-ChildItem -Path ${{ runner.workspace }}
        shell: pwsh

      - name: Uncompress source (Windows)
        working-directory: ${{ github.workspace }}
        run: 7z x ${{ steps.set-file-base.outputs.FILE_BASE }}.zip
        shell: bash

      - name: Configure (Windows)
        shell: pwsh
        run: |
          mkdir "${{ runner.workspace }}/build"
          Set-Location -Path "${{ runner.workspace }}\\build"
          cmake -C ${{ github.workspace }}/hdfsrc/config/cmake/cacheinit.cmake -G Ninja -DCMAKE_BUILD_TYPE=Developer -DBUILD_SHARED_LIBS=ON -DHDF5_ENABLE_ALL_WARNINGS=ON -DHDF5_ENABLE_DEBUG_H5B2=ON -DHDF5_ENABLE_DEBUG_H5FS_ASSERT=ON -DHDF5_ENABLE_PARALLEL:BOOL=OFF -DHDF5_BUILD_CPP_LIB=OFF -DHDF5_BUILD_FORTRAN=OFF -DHDF5_BUILD_JAVA=OFF -DHDF5_BUILD_DOC=OFF -DLIBAEC_USE_LOCALCONTENT=OFF -DZLIB_USE_LOCALCONTENT=OFF -DHDF_TEST_EXPRESS=0 ${{ github.workspace }}/hdfsrc

      - name: Build
        run: cmake --build . --parallel 3 --config Developer
        working-directory: ${{ runner.workspace }}/build

      - name: Run Tests
        env:
          HDF5TestExpress: 0
        run: ctest . --parallel 2 -C Developer -R H5TESTXPR -E fheap
        working-directory: ${{ runner.workspace }}/build

  build_and_test_mac_latest:
  # MacOS w/ Clang
  #
    name: "MacOS Clang"
    runs-on: macos-latest
    steps:
      - name: Install Dependencies (MacOS_latest)
        run: brew install ninja

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Check Clang Version
        shell: bash
        run: |
          which clang
          clang -v

      - name: Check CMake Version
        shell: bash
        run: |
          which cmake
          cmake --version

      - name: Set file base name (MacOS_latest)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (MacOS_latest)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (MacOS_latest)
        run: |
              ls ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: Uncompress source (MacOS_latest)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Configure
        shell: bash
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C ${{ github.workspace }}/hdfsrc/config/cmake/cacheinit.cmake \
               -G Ninja \
               -DCMAKE_BUILD_TYPE=Developer \
               -DBUILD_SHARED_LIBS=ON \
               -DHDF5_ENABLE_ALL_WARNINGS=ON \
               -DHDF5_ENABLE_DEBUG_H5B2=ON \
               -DHDF5_ENABLE_DEBUG_H5FS_ASSERT=ON \
               -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
               -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
               -DHDF5_BUILD_FORTRAN=OFF \
               -DHDF5_BUILD_JAVA=OFF \
               -DHDF5_BUILD_DOC=OFF \
               -DLIBAEC_USE_LOCALCONTENT=OFF \
               -DZLIB_USE_LOCALCONTENT=OFF \
               -DHDF_TEST_EXPRESS=0 \
               ${{ github.workspace }}/hdfsrc

      - name: Build
        run: cmake --build . --parallel 3 --config Developer
        working-directory: ${{ runner.workspace }}/build

      - name: Run Tests
        env:
          HDF5TestExpress: 0
        run: ctest . --parallel 2 -C Developer -R H5TESTXPR -E fheap
        working-directory: ${{ runner.workspace }}/build

  ####### clang builds
  build_and_test_linux_clang:
    # Linux (Ubuntu) w/ clang
    #
    name: "Ubuntu Clang"
    runs-on: ubuntu-22.04
    steps:
      - name: Install Dependencies (Linux_clang)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz curl libtinfo5

      - name: add clang to env
        uses: KyleMayes/install-llvm-action@98e68e10c96dffcb7bfed8b2144541a66b49aa02 # v2.0.8
        id: setup-clang
        with:
          env: true
          version: '18.1'

      - name: Check Clang Version
        shell: bash
        run: |
          which clang
          clang -v

      - name: Check CMake Version
        shell: bash
        run: |
          which cmake
          cmake --version

      - name: Set file base name (Linux_clang)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (Linux_clang)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: tgz-tarball
          path: ${{ github.workspace }}

      - name: List files for the space (Linux_clang)
        run: |
          ls -l ${{ github.workspace }}
          ls ${{ runner.workspace }}

      - name: Uncompress source (Linux_clang)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Configure
        shell: bash
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C ${{ github.workspace }}/hdfsrc/config/cmake/cacheinit.cmake \
               -G Ninja \
               -DCMAKE_BUILD_TYPE=Developer \
               -DBUILD_SHARED_LIBS=ON \
               -DHDF5_ENABLE_ALL_WARNINGS=ON \
               -DHDF5_ENABLE_DEBUG_H5B2=ON \
               -DHDF5_ENABLE_DEBUG_H5FS_ASSERT=ON \
               -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
               -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
               -DHDF5_BUILD_FORTRAN=OFF \
               -DHDF5_BUILD_JAVA=OFF \
               -DHDF5_BUILD_DOC=OFF \
               -DLIBAEC_USE_LOCALCONTENT=OFF \
               -DZLIB_USE_LOCALCONTENT=OFF \
               -DHDF_TEST_EXPRESS=0 \
               ${{ github.workspace }}/hdfsrc

      - name: Build
        run: cmake --build . --parallel 3 --config Developer
        working-directory: ${{ runner.workspace }}/build

      - name: Run Tests
        env:
          HDF5TestExpress: 0
        run: ctest . --parallel 2 -C Developer -R H5TESTXPR -E fheap
        working-directory: ${{ runner.workspace }}/build
```

### `.github/workflows/freebsd.yml`

```yaml
name: FreeBSD CI

# Triggers the workflow on push or pull request or on demand
on:
  workflow_dispatch:
  push:
  pull_request:
    branches: [ develop ]
    paths-ignore:
      - '.github/CODEOWNERS'
      - '.github/FUNDING.yml'
      - 'doc/**'
      - 'release_docs/**'
      - 'ACKNOWLEDGEMENTS'
      - 'LICENSE**'
      - '**.md'

# Using concurrency to cancel any in-progress job or run
concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref && github.ref || github.run_id }}
  cancel-in-progress: true

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  freebsd-build-and-test:
    runs-on: ubuntu-latest
    name: FreeBSD ${{ matrix.freebsd-version }} Build and Test

    # Don't run the action if the commit message says to skip CI
    if: "!contains(github.event.head_commit.message, 'skip-ci')"

    strategy:
      fail-fast: false
      matrix:
        freebsd-version: ['13.5', '14.3', '15.0']

    steps:
      - name: Checkout repository
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Build and test on FreeBSD
        uses: vmactions/freebsd-vm@670398e4236735b8b65805c3da44b7a511fb8b27 # v1
        with:
          release: ${{ matrix.freebsd-version }}
          usesh: true
          prepare: |
            pkg install -y cmake ninja pkgconf bash curl
          run: |
            set -e

            # Configure the build
            mkdir build
            cd build
            cmake -C ../config/cmake/cacheinit.cmake \
              -G Ninja \
              --log-level=VERBOSE \
              -DCMAKE_BUILD_TYPE=Release \
              -DBUILD_SHARED_LIBS:BOOL=ON \
              -DHDF5_ENABLE_ALL_WARNINGS:BOOL=ON \
              -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
              -DHDF5_BUILD_CPP_LIB:BOOL=ON \
              -DHDF5_BUILD_FORTRAN:BOOL=OFF \
              -DHDF5_BUILD_JAVA:BOOL=OFF \
              -DHDF5_BUILD_DOC:BOOL=OFF \
              -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
              -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON \
              -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF \
              -DZLIB_USE_LOCALCONTENT:BOOL=OFF \
              -DHDF5_TEST_API:BOOL=ON \
              -DHDF5_TEST_SHELL_SCRIPTS:BOOL=OFF \
              -DENABLE_EXTENDED_TESTS:BOOL=OFF \
              ..
            echo ""

            # Build
            cmake --build . --parallel 3 --config Release
            echo ""

            # Run tests
            ctest . --parallel 2 -C Release
            echo ""
```

### `.github/workflows/h5py.yml`

```yaml
name: h5py

# Triggers the workflow on a schedule or on demand
on:
  workflow_dispatch:
  schedule:
    - cron: "6 0 * * *"

permissions:
  contents: read

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Spack
      uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
      with:
        repository: spack/spack
        path: ./spack

    - name: Run a multi-line script
      run: |
        . ./spack/share/spack/setup-env.sh
        ./spack/bin/spack spec py-h5py@master+mpi
         sed -i 's/hdf5@1.10.6:1.14/hdf5@1.10.6:/g' \
        /home/runner/.spack/package_repos/*/repos/spack_repo/builtin/packages/py_h5py/package.py
        ./spack/bin/spack spec py-h5py@master+mpi ^hdf5@develop-2.0
        ./spack/bin/spack install py-h5py@master+mpi ^hdf5@develop-2.0
        ./spack/bin/spack install py-pytest
        ./spack/bin/spack install py-pytest-mpi
        spack load py-h5py
        spack load py-pytest
        spack load py-pytest-mpi
        python -c "import h5py; h5py.run_tests(); print(h5py.version.info);"
```

### `.github/workflows/hdfeos5.yml`

```yaml
name: hdfeos5 dev

# Triggers the workflow on push or pull request or on demand
on:
  workflow_dispatch:
  push:
  pull_request:
    branches: [ develop ]
    paths-ignore:
      - '.github/CODEOWNERS'
      - '.github/FUNDING.yml'
      - 'doc/**'
      - 'release_docs/**'
      - 'ACKNOWLEDGEMENTS'
      - 'LICENSE**'
      - '**.md'

permissions:
  contents: read

# Using concurrency to cancel any in-progress job or run
concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref && github.ref || github.run_id }}
  cancel-in-progress: true

jobs:
  build:
    name: Build hdfeos5
    runs-on: ubuntu-latest
    steps:
      - name: Install System dependencies
        run: |
            sudo apt-get update
            sudo apt-get install ninja-build
            sudo apt install libssl3 libssl-dev libcurl4
            sudo apt install -y libaec-dev zlib1g-dev automake autoconf libcurl4-openssl-dev libjpeg-dev wget curl bzip2 m4 flex bison cmake libzip-dev doxygen openssl libtool libtool-bin

      - name: Checkout HDF5
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Configure
        run: |
            mkdir "${{ runner.workspace }}/build"
            cd "${{ runner.workspace }}/build"
            cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
              -G Ninja \
              -DCMAKE_BUILD_TYPE=Release \
              -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
              -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
              -DHDF5_BUILD_FORTRAN=OFF \
              -DHDF5_BUILD_JAVA=OFF \
              -DHDF5_BUILD_DOC=OFF \
              -DLIBAEC_USE_LOCALCONTENT=OFF \
              -DZLIB_USE_LOCALCONTENT=OFF \
              -DBUILD_TESTING:BOOL=OFF \
              -DDEFAULT_API_VERSION:STRING=v16 \
              -DCMAKE_INSTALL_PREFIX:PATH=/usr/local \
              $GITHUB_WORKSPACE
        shell: bash

      - name: Build
        run: cmake --build . --parallel 3 --config Release
        working-directory: ${{ runner.workspace }}/build

      - name: Install HDF5
        run: |
          sudo cmake --install . --config Release --prefix="/usr/local"
        working-directory: ${{ runner.workspace }}/build

      - name: Install HDF-EOS5
        run: |
          wget -O HDF-EOS5.2.0.tar.gz "https://git.earthdata.nasa.gov/projects/DAS/repos/hdfeos5/raw/hdf-eos5-2.0-src.tar.gz?at=refs%2Fheads%2FHDFEOS5_2.0"
          tar zxvf HDF-EOS5.2.0.tar.gz
          cd hdf-eos5-2.0
          ./configure CC=/usr/local/bin/h5cc --prefix=/usr/local/ --enable-install-include
          make
          LD_LIBRARY_PATH="/usr/local/lib:${LD_LIBRARY_PATH}"
          LD_LIBRARY_PATH=${LD_LIBRARY_PATH} make check
          sudo make install
```

### `.github/workflows/i386.yml`

```yaml
name: hdf5 dev i386

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  i386_build_and_test:
    name: "i386 ${{ inputs.build_mode }}"
    runs-on: ubuntu-latest
    steps:
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: setup alpine
        uses: jirutka/setup-alpine@ae3b3ddba35054804fc4a3507b519fa7e8152050 # v1
        with:
          arch: x86
          packages: >
            build-base
            libaec-dev
            libgit2-dev
            cmake

      - name: Configure
        shell: alpine.sh --root {0}
        run: |
          mkdir build
          cd build
          cmake -C ../config/cmake/cacheinit.cmake -G "Unix Makefiles" \
          --log-level=VERBOSE \
          -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
          -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF \
          -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF \
          -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=OFF \
          -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
          -DHDF5_BUILD_FORTRAN:BOOL=OFF \
          -DHDF5_BUILD_JAVA:BOOL=OFF \
          ..

      - name: Build
        shell: alpine.sh --root {0}
        run: |
          cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: build

      - name: Run Tests
        shell: alpine.sh --root {0}
        run: |
          ctest . --parallel 2 -C ${{ inputs.build_mode }}
        working-directory: build
```

### `.github/workflows/intel.yml`

```yaml
name: hdf5 dev icx CI

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  intel_oneapi_linux:
    name: "linux-oneapi ${{ inputs.build_mode }}"
    runs-on: ubuntu-latest
    steps:
      - name: Get Sources (Linux)
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install Dependencies (Linux)
        shell: bash
        run: |
           sudo apt-get update
           sudo apt-get install ninja-build doxygen graphviz
           sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev

      - name: Install oneAPI (Linux)
        uses: fortran-lang/setup-fortran@47809fdb6e637da656ce9ada436527b240c1287f # v1
        id: setup-fortran
        with:
          compiler: intel
          version: '2025.2'

      - name: Configure (Linux)
        shell: bash
        env:
          FC: ${{ steps.setup-fortran.outputs.fc }}
          CC: ${{ steps.setup-fortran.outputs.cc }}
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake -G Ninja --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DHDF5_BUILD_FORTRAN:BOOL=ON \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            ${{ github.workspace }}

      - name: Build (Linux)
        shell: bash
        env:
          FC: ${{ steps.setup-fortran.outputs.fc }}
          CC: ${{ steps.setup-fortran.outputs.cc }}
        run: |
          cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      - name: Run Tests (Linux)
        shell: bash
        env:
          FC: ${{ steps.setup-fortran.outputs.fc }}
          CC: ${{ steps.setup-fortran.outputs.cc }}
        run: |
          ctest . --parallel 2 -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

  Intel_oneapi_windows:
    name: "windows-oneapi ${{ inputs.build_mode }}"
    runs-on: windows-latest
    steps:
      - name: Get Sources (Windows)
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install Dependencies (Windows)
        run: choco install ninja

      - name: install oneAPI (Windows)
        uses: fortran-lang/setup-fortran@47809fdb6e637da656ce9ada436527b240c1287f # v1
        id: setup-fortran
        with:
          compiler: intel
          version: '2025.2'

      - name: Configure (Windows)
        shell: pwsh
        env:
          FC: ${{ steps.setup-fortran.outputs.fc }}
          CC: ${{ steps.setup-fortran.outputs.cc }}
        run: |
          mkdir "${{ runner.workspace }}/build"
          Set-Location -Path "${{ runner.workspace }}\\build"
          cmake -C ${{ github.workspace }}/config/cmake/cacheinit.cmake -G Ninja -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} -DHDF5_BUILD_FORTRAN=ON -DHDF5_BUILD_CPP_LIB=ON -DLIBAEC_USE_LOCALCONTENT=OFF -DZLIB_USE_LOCALCONTENT=OFF ${{ github.workspace }}

      - name: Build (Windows)
        shell: pwsh
        env:
          FC: ${{ steps.setup-fortran.outputs.fc }}
          CC: ${{ steps.setup-fortran.outputs.cc }}
        run: |
          cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      - name: Run Tests (Windows)
        shell: pwsh
        env:
          FC: ${{ steps.setup-fortran.outputs.fc }}
          CC: ${{ steps.setup-fortran.outputs.cc }}
        run: |
          ctest . --parallel 2 -C ${{ inputs.build_mode }} -E tfloatsattrs
        working-directory: ${{ runner.workspace }}/build
```

### `.github/workflows/java-examples-maven-test.yml`

```yaml
name: Java Examples Maven Testing

on:
  workflow_call:
    inputs:
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string
      maven_artifacts_version:
        description: "HDF5 Maven artifacts version to test against"
        required: true
        type: string

permissions:
  contents: read

jobs:
  # Parallel testing by platform and example category
  test-examples-linux:
    name: "Test Examples Linux (${{ matrix.category }})"
    runs-on: ubuntu-latest
    continue-on-error: true  # Non-blocking failures
    strategy:
      fail-fast: false
      matrix:
        category: [H5D, H5T, H5G, TUTR]
    steps:
      - name: Checkout code
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Download Maven artifacts (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: maven-staging-artifacts-linux-x86_64
          path: ./maven-artifacts
        continue-on-error: true

      - name: Set up JDK 21
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '21'
          distribution: 'temurin'

      - name: Cache Maven dependencies
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-examples-${{ hashFiles('**/pom-examples.xml*') }}
          restore-keys: |
            ${{ runner.os }}-maven-examples-
            ${{ runner.os }}-maven-

      - name: Identify HDF5 JAR files
        run: |
          echo "=== Identifying HDF5 JAR files ==="

          # Find HDF5 JAR files (not dependencies)
          HDF5_JARS=$(find ./maven-artifacts -name "*hdf5*.jar" -o -name "jarhdf5*.jar")
          DEP_JARS=$(find ./maven-artifacts -name "*.jar" ! -name "*hdf5*" ! -name "jarhdf5*")

          echo "HDF5 JARs found:"
          echo "$HDF5_JARS"
          echo ""
          echo "Dependency JARs found:"
          echo "$DEP_JARS"

          if [ -z "$HDF5_JARS" ]; then
            echo "âŒ No HDF5 JAR files found!"
            echo "All available JARs:"
            find ./maven-artifacts -name "*.jar"
            exit 1
          fi

      - name: Test Examples Category ${{ matrix.category }}
        id: test-examples
        run: |
          cd HDF5Examples/JAVA/${{ matrix.category }}

          echo "=== Testing ${{ matrix.category }} Examples ==="

          # Create test results directory
          mkdir -p ../../../test-results/${{ matrix.category }}

          FAILED_EXAMPLES=""
          TOTAL_EXAMPLES=0
          PASSED_EXAMPLES=0

          # Test each Java example in the category
          for java_file in *.java; do
            if [ -f "$java_file" ]; then
              TOTAL_EXAMPLES=$((TOTAL_EXAMPLES + 1))
              example_name=$(basename "$java_file" .java)
              echo "--- Testing $example_name ---"

              # Build classpath (use platform-specific artifacts)
              MAVEN_ARTIFACTS_DIR="../../../maven-artifacts"
              HDF5_JAR=$(find "$MAVEN_ARTIFACTS_DIR" -name "*hdf5*.jar" -o -name "jarhdf5*.jar" | head -1)
              DEP_JARS=$(find "$MAVEN_ARTIFACTS_DIR" -name "slf4j-api*.jar" -o -name "slf4j-simple*.jar")

              if [ -z "$HDF5_JAR" ]; then
                echo "âŒ No HDF5 JAR found"
                find "$MAVEN_ARTIFACTS_DIR" -name "*.jar" | head -10
                continue
              fi

              CLASSPATH="$HDF5_JAR"
              for dep_jar in $DEP_JARS; do
                CLASSPATH="$CLASSPATH:$dep_jar"
              done

              # Compile the example
              if javac -cp "$CLASSPATH" "$java_file"; then
                echo "âœ“ Compilation successful for $example_name"

                # Run the example and capture output
                if timeout 30s java -cp ".:$CLASSPATH" "$example_name" > "../../../test-results/${{ matrix.category }}/${example_name}.output" 2>&1; then
                  # Validate output using pattern matching
                  output_file="../../../test-results/${{ matrix.category }}/${example_name}.output"

                  # Check for common success patterns
                  if grep -q -i -E "(dataset|datatype|group|success|created|written|read)" "$output_file" && \
                     ! grep -q -i -E "(error|exception|failed|cannot)" "$output_file"; then
                    echo "âœ“ Execution and validation successful for $example_name"
                    PASSED_EXAMPLES=$((PASSED_EXAMPLES + 1))
                  else
                    echo "âœ— Output validation failed for $example_name"
                    FAILED_EXAMPLES="$FAILED_EXAMPLES $example_name"
                    echo "Output:"
                    cat "$output_file"
                  fi
                else
                  # Check if failure is due to expected native library issue (acceptable for Maven-only testing)
                  output_file="../../../test-results/${{ matrix.category }}/${example_name}.output"
                  if grep -q "UnsatisfiedLinkError.*hdf5_java.*java.library.path" "$output_file"; then
                    echo "âœ“ Expected native library error for Maven-only testing: $example_name"
                    echo "  (This confirms JAR structure is correct)"
                    PASSED_EXAMPLES=$((PASSED_EXAMPLES + 1))
                  else
                    echo "âœ— Unexpected execution failure for $example_name"
                    FAILED_EXAMPLES="$FAILED_EXAMPLES $example_name"
                    echo "Output:"
                    cat "$output_file"
                  fi
                fi
              else
                echo "âœ— Compilation failed for $example_name"
                FAILED_EXAMPLES="$FAILED_EXAMPLES $example_name"
              fi
              echo ""
            fi
          done

          # Summary
          echo "=== ${{ matrix.category }} Summary ==="
          echo "Total examples: $TOTAL_EXAMPLES"
          echo "Passed: $PASSED_EXAMPLES"
          echo "Failed: $((TOTAL_EXAMPLES - PASSED_EXAMPLES))"

          if [ -n "$FAILED_EXAMPLES" ]; then
            echo "Failed examples:$FAILED_EXAMPLES"
            echo "failed-examples=$FAILED_EXAMPLES" >> $GITHUB_OUTPUT
            echo "test-status=FAILED" >> $GITHUB_OUTPUT
          else
            echo "All examples passed!"
            echo "test-status=PASSED" >> $GITHUB_OUTPUT
          fi

          echo "total-examples=$TOTAL_EXAMPLES" >> $GITHUB_OUTPUT
          echo "passed-examples=$PASSED_EXAMPLES" >> $GITHUB_OUTPUT

      - name: Upload failure artifacts (Linux ${{ matrix.category }})
        if: steps.test-examples.outputs.test-status == 'FAILED'
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: java-examples-failure-linux-${{ matrix.category }}-${{ github.run_id }}
          path: |
            test-results/${{ matrix.category }}/
            HDF5Examples/JAVA/${{ matrix.category }}/*.class
            HDF5Examples/JAVA/${{ matrix.category }}/*.h5
          retention-days: 7

  test-examples-windows:
    name: "Test Examples Windows (${{ matrix.category }})"
    runs-on: windows-latest
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        category: [H5D, H5T, H5G, TUTR]
    steps:
      - name: Checkout code
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Download Maven artifacts (Windows)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: maven-staging-artifacts-windows-x86_64
          path: ./maven-artifacts
        continue-on-error: true

      - name: Set up JDK 21
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '21'
          distribution: 'temurin'

      - name: Cache Maven dependencies
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-examples-${{ hashFiles('**/pom-examples.xml*') }}

      - name: Test Examples Category ${{ matrix.category }}
        id: test-examples
        shell: pwsh
        run: |
          cd HDF5Examples/JAVA/${{ matrix.category }}

          Write-Host "=== Testing ${{ matrix.category }} Examples ==="

          # Create test results directory
          New-Item -ItemType Directory -Force -Path "../../../test-results/${{ matrix.category }}"

          $FAILED_EXAMPLES = @()
          $TOTAL_EXAMPLES = 0
          $PASSED_EXAMPLES = 0

          # Test each Java example in the category
          Get-ChildItem -Filter "*.java" | ForEach-Object {
            $TOTAL_EXAMPLES++
            $example_name = $_.BaseName
            Write-Host "--- Testing $example_name ---"

            # Compile the example
            $compile_result = javac -cp "../../../maven-artifacts/*.jar" $_.Name
            if ($LASTEXITCODE -eq 0) {
              Write-Host "âœ“ Compilation successful for $example_name"

              # Run the example and capture output
              $timeout_cmd = "timeout 30s java -cp '.;../../../maven-artifacts/*' $example_name"
              $output_file = "../../../test-results/${{ matrix.category }}/${example_name}.output"

              try {
                & cmd /c "$timeout_cmd > `"$output_file`" 2>&1"
                if ($LASTEXITCODE -eq 0) {
                  # Validate output using pattern matching
                  $content = Get-Content $output_file -Raw
                  if (($content -match "(?i)(dataset|datatype|group|success|created|written|read)") -and
                      ($content -notmatch "(?i)(error|exception|failed|cannot)")) {
                    Write-Host "âœ“ Execution and validation successful for $example_name"
                    $PASSED_EXAMPLES++
                  } else {
                    Write-Host "âœ— Output validation failed for $example_name"
                    $FAILED_EXAMPLES += $example_name
                    Write-Host "Output:"
                    Get-Content $output_file
                  }
                } else {
                  Write-Host "âœ— Execution failed for $example_name"
                  $FAILED_EXAMPLES += $example_name
                }
              } catch {
                Write-Host "âœ— Execution failed for $example_name"
                $FAILED_EXAMPLES += $example_name
              }
            } else {
              Write-Host "âœ— Compilation failed for $example_name"
              $FAILED_EXAMPLES += $example_name
            }
            Write-Host ""
          }

          # Summary
          Write-Host "=== ${{ matrix.category }} Summary ==="
          Write-Host "Total examples: $TOTAL_EXAMPLES"
          Write-Host "Passed: $PASSED_EXAMPLES"
          Write-Host "Failed: $($TOTAL_EXAMPLES - $PASSED_EXAMPLES)"

          if ($FAILED_EXAMPLES.Count -gt 0) {
            Write-Host "Failed examples: $($FAILED_EXAMPLES -join ' ')"
            echo "failed-examples=$($FAILED_EXAMPLES -join ' ')" >> $env:GITHUB_OUTPUT
            echo "test-status=FAILED" >> $env:GITHUB_OUTPUT
          } else {
            Write-Host "All examples passed!"
            echo "test-status=PASSED" >> $env:GITHUB_OUTPUT
          }

          echo "total-examples=$TOTAL_EXAMPLES" >> $env:GITHUB_OUTPUT
          echo "passed-examples=$PASSED_EXAMPLES" >> $env:GITHUB_OUTPUT

      - name: Upload failure artifacts (Windows ${{ matrix.category }})
        if: steps.test-examples.outputs.test-status == 'FAILED'
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: java-examples-failure-windows-${{ matrix.category }}-${{ github.run_id }}
          path: |
            test-results/${{ matrix.category }}/
            HDF5Examples/JAVA/${{ matrix.category }}/*.class
            HDF5Examples/JAVA/${{ matrix.category }}/*.h5

  test-examples-macos:
    name: "Test Examples macOS (${{ matrix.category }})"
    runs-on: macos-latest
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        category: [H5D, H5T, H5G, TUTR]
    steps:
      - name: Checkout code
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Download Maven artifacts (macOS aarch64)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: maven-staging-artifacts-macos-aarch64
          path: ./maven-artifacts
        continue-on-error: true

      - name: Set up JDK 21
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '21'
          distribution: 'temurin'

      - name: Cache Maven dependencies
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-examples-${{ hashFiles('**/pom-examples.xml*') }}

      - name: Test Examples Category ${{ matrix.category }}
        id: test-examples
        run: |
          cd HDF5Examples/JAVA/${{ matrix.category }}

          echo "=== Testing ${{ matrix.category }} Examples ==="

          # Create test results directory
          mkdir -p ../../../test-results/${{ matrix.category }}

          FAILED_EXAMPLES=""
          TOTAL_EXAMPLES=0
          PASSED_EXAMPLES=0

          # Test each Java example in the category
          for java_file in *.java; do
            if [ -f "$java_file" ]; then
              TOTAL_EXAMPLES=$((TOTAL_EXAMPLES + 1))
              example_name=$(basename "$java_file" .java)
              echo "--- Testing $example_name ---"

              # Build classpath (use platform-specific artifacts)
              MAVEN_ARTIFACTS_DIR="../../../maven-artifacts"
              HDF5_JAR=$(find "$MAVEN_ARTIFACTS_DIR" -name "*hdf5*.jar" -o -name "jarhdf5*.jar" | head -1)
              DEP_JARS=$(find "$MAVEN_ARTIFACTS_DIR" -name "slf4j-api*.jar" -o -name "slf4j-simple*.jar")

              if [ -z "$HDF5_JAR" ]; then
                echo "âŒ No HDF5 JAR found"
                find "$MAVEN_ARTIFACTS_DIR" -name "*.jar" | head -10
                continue
              fi

              CLASSPATH="$HDF5_JAR"
              for dep_jar in $DEP_JARS; do
                CLASSPATH="$CLASSPATH:$dep_jar"
              done

              # Compile the example
              if javac -cp "$CLASSPATH" "$java_file"; then
                echo "âœ“ Compilation successful for $example_name"

                # Run the example and capture output
                if timeout 30s java -cp ".:$CLASSPATH" "$example_name" > "../../../test-results/${{ matrix.category }}/${example_name}.output" 2>&1; then
                  # Validate output using pattern matching
                  output_file="../../../test-results/${{ matrix.category }}/${example_name}.output"

                  # Check for common success patterns
                  if grep -q -i -E "(dataset|datatype|group|success|created|written|read)" "$output_file" && \
                     ! grep -q -i -E "(error|exception|failed|cannot)" "$output_file"; then
                    echo "âœ“ Execution and validation successful for $example_name"
                    PASSED_EXAMPLES=$((PASSED_EXAMPLES + 1))
                  else
                    echo "âœ— Output validation failed for $example_name"
                    FAILED_EXAMPLES="$FAILED_EXAMPLES $example_name"
                  fi
                else
                  echo "âœ— Execution failed for $example_name"
                  FAILED_EXAMPLES="$FAILED_EXAMPLES $example_name"
                fi
              else
                echo "âœ— Compilation failed for $example_name"
                FAILED_EXAMPLES="$FAILED_EXAMPLES $example_name"
              fi
              echo ""
            fi
          done

          # Summary
          echo "=== ${{ matrix.category }} Summary ==="
          echo "Total examples: $TOTAL_EXAMPLES"
          echo "Passed: $PASSED_EXAMPLES"
          echo "Failed: $((TOTAL_EXAMPLES - PASSED_EXAMPLES))"

          if [ -n "$FAILED_EXAMPLES" ]; then
            echo "Failed examples:$FAILED_EXAMPLES"
            echo "failed-examples=$FAILED_EXAMPLES" >> $GITHUB_OUTPUT
            echo "test-status=FAILED" >> $GITHUB_OUTPUT
          else
            echo "All examples passed!"
            echo "test-status=PASSED" >> $GITHUB_OUTPUT
          fi

          echo "total-examples=$TOTAL_EXAMPLES" >> $GITHUB_OUTPUT
          echo "passed-examples=$PASSED_EXAMPLES" >> $GITHUB_OUTPUT

      - name: Upload failure artifacts (macOS ${{ matrix.category }})
        if: steps.test-examples.outputs.test-status == 'FAILED'
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: java-examples-failure-macos-${{ matrix.category }}-${{ github.run_id }}
          path: |
            test-results/${{ matrix.category }}/
            HDF5Examples/JAVA/${{ matrix.category }}/*.class
            HDF5Examples/JAVA/${{ matrix.category }}/*.h5

  analyze-cross-platform-failures:
    name: "Analyze Cross-Platform Failures"
    runs-on: ubuntu-latest
    needs: [test-examples-linux, test-examples-windows, test-examples-macos]
    if: always()
    steps:
      - name: Collect Test Results
        run: |
          echo "=== Java Examples Testing Summary ==="

          # Collect results from matrix outputs (this would need to be enhanced
          # to actually collect the outputs from the matrix jobs)

          echo "Cross-platform failure analysis:"
          echo "- Examples failing on multiple platforms require attention"
          echo "- Single-platform failures may be platform-specific issues"

          # This step would analyze patterns in failures across platforms
          # and generate appropriate alerts/issues for multi-platform failures

      - name: Generate Test Summary Report
        run: |
          cat > java-examples-test-summary.md << 'EOF'
          # Java Examples Maven Testing Summary

          **Build Mode**: ${{ inputs.build_mode }}
          **Maven Version**: ${{ inputs.maven_artifacts_version }}
          **Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")

          ## Platform Results
          - **Linux**: See individual category results
          - **Windows**: See individual category results
          - **macOS**: See individual category results

          ## Cross-Platform Analysis
          Examples failing on multiple platforms require investigation.

          EOF

          echo "Test summary report generated"

      - name: Upload Test Summary
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: java-examples-test-summary-${{ github.run_id }}
          path: java-examples-test-summary.md
          retention-days: 30
```

### `.github/workflows/java-implementation-test.yml`

```yaml
name: Java Implementation Testing (FFM vs JNI)

# Test both FFM and JNI implementations across multiple Java versions
on:
  pull_request:
    branches: [ develop, main ]
    paths:
      - 'java/**'
      - 'CMakeBuildOptions.cmake'
      - 'CMakePresets.json'
      - 'config/cmake-presets/hidden-presets.json'
      - '.github/workflows/java-implementation-test.yml'
      - '.github/scripts/test-java-implementations.sh'
  workflow_call:
    inputs:
      java_versions:
        description: 'Java versions to test (comma-separated)'
        type: string
        required: false
        default: '11,17,21,24'
      test_mode:
        description: 'Test mode (build, maven, full)'
        type: string
        required: false
        default: 'build'
      platforms:
        description: 'Platforms to test'
        type: string
        required: false
        default: 'ubuntu-latest'
  workflow_dispatch:
    inputs:
      java_versions:
        description: 'Java versions to test'
        type: string
        required: false
        default: '11,17,21,24'
      test_mode:
        description: 'Test mode'
        type: choice
        required: false
        default: 'build'
        options:
          - 'build'
          - 'maven'
          - 'full'
      platforms:
        description: 'Platforms to test'
        type: choice
        required: false
        default: 'ubuntu-latest'
        options:
          - 'ubuntu-latest'
          - 'windows-latest'
          - 'macos-latest'
          - 'all-platforms'

env:
  CMAKE_GENERATOR: Ninja

jobs:
  setup-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.generate-matrix.outputs.matrix }}
    steps:
      - name: Generate test matrix
        id: generate-matrix
        run: |
          # Parse inputs
          JAVA_VERSIONS="${{ inputs.java_versions || '11,17,21,25' }}"
          PLATFORMS="${{ inputs.platforms || 'ubuntu-latest' }}"

          # Expand platforms if needed
          if [[ "$PLATFORMS" == "all-platforms" ]]; then
            PLATFORMS="ubuntu-latest,windows-latest,macos-latest"
          fi

          # Generate matrix
          matrix_json="["
          first_entry=true

          for platform in $(echo "$PLATFORMS" | tr ',' ' '); do
            for java_version in $(echo "$JAVA_VERSIONS" | tr ',' ' '); do
              # Determine available implementations
              if [[ $java_version -ge 25 ]]; then
                implementations="ffm jni"
              else
                implementations="jni"
              fi

              for impl in $implementations; do
                if [ "$first_entry" = true ]; then
                  first_entry=false
                else
                  matrix_json="$matrix_json,"
                fi

                matrix_json="$matrix_json{\"os\":\"$platform\",\"java-version\":\"$java_version\",\"implementation\":\"$impl\"}"
              done
            done
          done

          matrix_json="$matrix_json]"

          echo "Generated matrix:"
          echo "$matrix_json" | jq '.'

          echo "matrix=$matrix_json" >> $GITHUB_OUTPUT

  test-implementations:
    needs: setup-matrix
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.setup-matrix.outputs.matrix) }}

    name: Test Java ${{ matrix.java-version }} ${{ matrix.implementation }} on ${{ matrix.os }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          submodules: recursive

      - name: Set up Java ${{ matrix.java-version }} (${{ matrix.implementation }})
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          distribution: ${{ matrix.implementation == 'ffm' && 'oracle' || 'temurin' }}
          java-version: ${{ matrix.implementation == 'ffm' && '25' || matrix.java-version }}

      - name: Setup jextract (FFM builds only)
        if: ${{ matrix.implementation == 'ffm' }}
        uses: ./.github/actions/setup-jextract
        with:
          java-version: '25'

      - name: Setup Build Environment (Linux)
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y ninja-build libaec-dev zlib1g-dev

      - name: Setup Build Environment (macOS)
        if: runner.os == 'macOS'
        run: |
          brew install ninja libaec

      - name: Setup Build Environment (Windows)
        if: runner.os == 'Windows'
        run: |
          choco install ninja

      - name: Cache CMake build
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: |
            build-test-*
          key: ${{ runner.os }}-java${{ matrix.java-version }}-${{ matrix.implementation }}-${{ hashFiles('**/CMakeLists.txt', 'CMakePresets.json') }}
          restore-keys: |
            ${{ runner.os }}-java${{ matrix.java-version }}-${{ matrix.implementation }}-
            ${{ runner.os }}-java${{ matrix.java-version }}-

      - name: Verify Java version and implementation compatibility
        run: |
          java -version
          echo "Testing Java ${{ matrix.java-version }} with ${{ matrix.implementation }} implementation"

          # Additional validation for FFM
          if [[ "${{ matrix.implementation }}" == "ffm" ]]; then
            if [[ ${{ matrix.java-version }} -lt 25 ]]; then
              echo "::error::FFM implementation requires Java 25, got Java ${{ matrix.java-version }}"
              exit 1
            fi
            echo "FFM implementation validated for Java ${{ matrix.java-version }}"
          fi

      - name: Run implementation tests
        shell: bash
        run: |
          chmod +x .github/scripts/test-java-implementations.sh
          .github/scripts/test-java-implementations.sh \
            ${{ matrix.java-version }} \
            ${{ matrix.implementation }} \
            ${{ inputs.test_mode || 'build' }}

      - name: Upload build artifacts on failure
        if: failure()
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: build-logs-${{ matrix.os }}-java${{ matrix.java-version }}-${{ matrix.implementation }}
          path: |
            build-test-*/CMakeCache.txt
            build-test-*/CMakeFiles/CMakeError.log
            build-test-*/CMakeFiles/CMakeOutput.log
          retention-days: 7

      - name: Upload Maven artifacts (if generated)
        if: inputs.test_mode == 'maven' || inputs.test_mode == 'full'
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: maven-artifacts-${{ matrix.os }}-java${{ matrix.java-version }}-${{ matrix.implementation }}
          path: |
            build-test-*/java/**/target/*.jar
            build-test-*/java/**/pom.xml
          retention-days: 3

  validate-artifacts:
    needs: [setup-matrix, test-implementations]
    runs-on: ubuntu-latest
    if: inputs.test_mode == 'maven' || inputs.test_mode == 'full'

    steps:
      - name: Checkout repository
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Download all Maven artifacts
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          pattern: maven-artifacts-*
          path: artifacts/

      - name: Validate artifact differentiation
        run: |
          echo "Validating Maven artifact differentiation..."

          # Check for FFM artifacts
          ffm_artifacts=$(find artifacts/ -name "*hdf5-java-ffm*" | wc -l)
          jni_artifacts=$(find artifacts/ -name "*hdf5-java-jni*" | wc -l)

          echo "Found $ffm_artifacts FFM artifacts and $jni_artifacts JNI artifacts"

          if [[ $ffm_artifacts -eq 0 && $jni_artifacts -eq 0 ]]; then
            echo "::error::No Maven artifacts found!"
            exit 1
          fi

          # Verify no mixed artifacts
          mixed_artifacts=$(find artifacts/ -name "*hdf5-java-*" | grep -v -E "(ffm|jni)" | wc -l)
          if [[ $mixed_artifacts -gt 0 ]]; then
            echo "::error::Found artifacts without proper FFM/JNI differentiation"
            find artifacts/ -name "*hdf5-java-*" | grep -v -E "(ffm|jni)"
            exit 1
          fi

          echo "âœ… Artifact differentiation validation passed"

      - name: Validate POM files
        run: |
          echo "Validating POM file contents..."

          for pom in $(find artifacts/ -name "pom.xml"); do
            echo "Checking POM: $pom"

            # Extract artifact ID
            artifact_id=$(grep -o '<artifactId>hdf5-java-[^<]*</artifactId>' "$pom" | sed 's/<[^>]*>//g')
            echo "Artifact ID: $artifact_id"

            # Validate implementation metadata
            if grep -q "hdf5-java-ffm" "$pom"; then
              if ! grep -q "FFM" "$pom"; then
                echo "::error::FFM POM missing implementation metadata"
                exit 1
              fi
              echo "âœ… FFM POM validation passed"
            elif grep -q "hdf5-java-jni" "$pom"; then
              if ! grep -q "JNI" "$pom"; then
                echo "::error::JNI POM missing implementation metadata"
                exit 1
              fi
              echo "âœ… JNI POM validation passed"
            else
              echo "::error::POM has unrecognized artifact ID: $artifact_id"
              exit 1
            fi
          done

  report-results:
    needs: [setup-matrix, test-implementations, validate-artifacts]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Generate test report
        run: |
          echo "## Java Implementation Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Test Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **Java Versions**: ${{ inputs.java_versions || '11,17,21,25' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Test Mode**: ${{ inputs.test_mode || 'build' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Platforms**: ${{ inputs.platforms || 'ubuntu-latest' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Report job statuses
          if [[ "${{ needs.test-implementations.result }}" == "success" ]]; then
            echo "âœ… **Implementation Tests**: PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Implementation Tests**: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          if [[ "${{ needs.validate-artifacts.result }}" == "success" ]]; then
            echo "âœ… **Artifact Validation**: PASSED" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ needs.validate-artifacts.result }}" == "skipped" ]]; then
            echo "â­ï¸ **Artifact Validation**: SKIPPED" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Artifact Validation**: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Implementation Matrix" >> $GITHUB_STEP_SUMMARY
          echo "| Java Version | FFM Support | JNI Support |" >> $GITHUB_STEP_SUMMARY
          echo "|--------------|-------------|-------------|" >> $GITHUB_STEP_SUMMARY
          echo "| 11 | âŒ | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| 17 | âŒ | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| 21 | âŒ | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| 24+ | âœ… (optional) | âœ… (default) |" >> $GITHUB_STEP_SUMMARY
```

### `.github/workflows/julia.yml`

```yaml
name: hdf5 dev julia

on:
  workflow_call:
    inputs:
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string

permissions:
  contents: read

jobs:
  julia_build_and_test:
    name: "julia ${{ inputs.build_mode }}"
    runs-on: ubuntu-latest
    steps:
      - name: Get HDF5 source
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install Dependencies
        shell: bash
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz
          sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
          sudo apt install -y libaec-dev zlib1g-dev wget curl bzip2 flex bison cmake libzip-dev openssl build-essential

      - name: Configure
        shell: bash
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake -G Ninja \
          -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
          -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF \
          -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF \
          -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
          -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
          -DLIBAEC_USE_LOCALCONTENT=OFF \
          -DZLIB_USE_LOCALCONTENT=OFF \
          -DHDF5_BUILD_FORTRAN:BOOL=OFF \
          -DHDF5_BUILD_JAVA:BOOL=OFF \
          -DCMAKE_INSTALL_PREFIX=/tmp \
          $GITHUB_WORKSPACE

      - name: Build
        shell: bash
        run: |
          cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      - name: Install HDF5
        shell: bash
        run: |
          cmake --install .
        working-directory: ${{ runner.workspace }}/build

      - name: Install julia
        uses: julia-actions/setup-julia@5c9647d97b78a5debe5164e9eec09d653d29bd71 # latest
        with:
          version: '1.6'
          arch: 'x64'

      - name: Get julia hdf5 source
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: JuliaIO/HDF5.jl
          path: .

      - name: Generate LocalPreferences
        run: |
          echo '[HDF5]' >> LocalPreferences.toml
          echo 'libhdf5 = "/tmp/lib/libhdf5.so"' >> LocalPreferences.toml
          echo 'libhdf5_hl = "/tmp/lib/libhdf5_hl.so"' >> LocalPreferences.toml

      - uses: julia-actions/julia-buildpkg@e3eb439fad4f9aba7da2667e7510e4a46ebc46e1 # latest

      - name: Julia Run Tests
        uses: julia-actions/julia-runtest@d60b785c6f2bdf4ebfb18b2b6f7d93b7dfb0efe3 # latest
        env:
          JULIA_DEBUG: Main
```

### `.github/workflows/linkchecker.yml`

```yaml
name: Link Checker

on:
  workflow_dispatch:
  push:
    branches: [develop]

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref && github.ref || github.run_id }}
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  check-links:
    name: Check Documentation Links
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            libunwind-dev \
            graphviz \
            doxygen \
            cmake

      - name: Build Documentation
        id: build_docs
        run: |
          mkdir build
          cd build
          cmake -DHDF5_BUILD_DOC:BOOL=ON ..
          make doxygen
          echo "html_path=$(pwd)/hdf5lib_docs/html" >> $GITHUB_OUTPUT

      - name: Run Comprehensive Link Check
        id: lychee
        uses: lycheeverse/lychee-action@a8c4c7cb88f0c7386610c35eb25108e448569cb0 # v2.7.0
        with:
          args: >
            --base ${{ steps.build_docs.outputs.html_path }}
            --max-retries 4
            --max-concurrency 4
            --retry-wait-time 10
            --timeout 30
            --accept 200..=299,429
            --user-agent "Mozilla/5.0 (compatible; Lychee/v0.1)"
            --skip-missing
            --exclude ".*%23.*"
            --exclude "eigen.tuxfamily.org"
            --exclude "gnu.org"
            --exclude "en.wikipedia.org"
            --exclude "help.hdfgroup.org"
            --exclude "libpng.org"
            --exclude "my.cdash.org"
            --exclude "www.oreilly.com"
            --exclude "preshing.com"
            --exclude "semver.org"
            --exclude "sourceforge.net"
            --exclude "www.youtube.com"
            --exclude "youtu.be"
            --exclude "hdfeos.org"
            --exclude "github.com"
            --exclude "stackoverflow.com"
            --exclude "reddit.com"
            --exclude "twitter.com"
            --exclude "linkedin.com"
            "${{ steps.build_docs.outputs.html_path }}/**/*.html"
          output: lychee-report.md
          format: markdown
          jobSummary: false
          # Let the action fail naturally - we'll handle it in the summary step
          fail: true

      - name: Publish Report to Job Summary
        if: always()
        run: |
          echo "## ðŸ”— Link Checker Report" >> "$GITHUB_STEP_SUMMARY"

          # Check if lychee step succeeded or failed
          if [ "${{ steps.lychee.outcome }}" = "success" ]; then
            echo "âœ… **No broken links found!**" >> "$GITHUB_STEP_SUMMARY"
          elif [ "${{ steps.lychee.outcome }}" = "failure" ]; then
            # Count actual broken links marked with the âŒ emoji.
            # Default to 0 if grep finds nothing or fails.
            FAILED_COUNT=$(grep -c "âŒ" lychee-report.md 2>/dev/null || echo 0)

            if [ "$FAILED_COUNT" -gt 0 ]; then
              echo "âŒ **Found $FAILED_COUNT broken link(s)!**" >> "$GITHUB_STEP_SUMMARY"
            else
              # If the step failed but no broken links found, it's likely a different error
              echo "âš ï¸ **Link checker encountered an error.**" >> "$GITHUB_STEP_SUMMARY"
              echo "This may indicate a configuration, network, or file access issue." >> "$GITHUB_STEP_SUMMARY"
            fi
          else
            echo "âš ï¸ **Link checker step was skipped or cancelled.**" >> "$GITHUB_STEP_SUMMARY"
          fi

          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "### Full Report:" >> "$GITHUB_STEP_SUMMARY"
          if [ -f "lychee-report.md" ]; then
            echo '```' >> "$GITHUB_STEP_SUMMARY"
            cat lychee-report.md >> "$GITHUB_STEP_SUMMARY"
            echo '```' >> "$GITHUB_STEP_SUMMARY"
          else
            echo "No detailed report available." >> "$GITHUB_STEP_SUMMARY"
          fi
```

### `.github/workflows/macos-26-matrix.yml`

```yaml
name: macOS-26 Matrix Testing

on:
   workflow_dispatch:
   schedule:
     - cron: "0 9 * * *"

# Using concurrency to cancel any in-progress job or run
concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref && github.ref || github.run_id }}
  cancel-in-progress: true

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  build-test:
    strategy:
      fail-fast: false
      matrix:
        # Test combinations of key options
        include:
          # Shared libraries with OpenMPI
          - name: "macOS-26 Shared OpenMPI Fortran Java"
            os: macos-26
            mpi: openmpi
            shared_libs: ON
            static_libs: OFF
            cpp: OFF
            fortran: OFF
            java: ON
            hl: ON
            threadsafe: OFF
            parallel: ON
            tools: ON
            tests: ON
            examples: ON
            zlib: ON
            szip: ON

          # Shared libraries with MPICH
          - name: "macOS-26 Shared MPICH Fortran"
            os: macos-26
            mpi: mpich
            shared_libs: ON
            static_libs: OFF
            cpp: OFF
            fortran: ON
            java: OFF
            hl: ON
            threadsafe: OFF
            parallel: ON
            tools: ON
            tests: ON
            examples: ON
            zlib: ON
            szip: ON

          # Static libraries with OpenMPI
          - name: "macOS-26 Static OpenMPI"
            os: macos-26
            mpi: openmpi
            shared_libs: OFF
            static_libs: ON
            cpp: OFF
            fortran: OFF
            java: OFF
            hl: ON
            threadsafe: OFF
            parallel: ON
            tools: ON
            tests: ON
            examples: ON
            zlib: ON
            szip: OFF

          # Both static and shared with MPICH
          - name: "macOS-26 Both MPICH All Features"
            os: macos-26
            mpi: mpich
            shared_libs: ON
            static_libs: ON
            cpp: ON
            fortran: ON
            java: OFF
            hl: ON
            threadsafe: OFF
            parallel: ON
            tools: ON
            tests: ON
            examples: ON
            zlib: ON
            szip: ON

          # Serial build with thread-safety
          - name: "macOS-26 Serial Threadsafe"
            os: macos-26
            mpi: none
            shared_libs: ON
            static_libs: ON
            cpp: ON
            fortran: OFF
            java: OFF
            hl: OFF
            threadsafe: ON
            parallel: OFF
            tools: ON
            tests: ON
            examples: ON
            zlib: ON
            szip: ON

          # Minimal configuration
          - name: "macOS-26 Minimal Config"
            os: macos-26
            mpi: none
            shared_libs: ON
            static_libs: OFF
            cpp: OFF
            fortran: OFF
            java: OFF
            hl: OFF
            threadsafe: OFF
            parallel: OFF
            tools: OFF
            tests: ON
            examples: OFF
            zlib: OFF
            szip: OFF

          # Plugin support with OpenMPI
          - name: "macOS-26 Plugins OpenMPI"
            os: macos-26
            mpi: openmpi
            shared_libs: ON
            static_libs: OFF
            cpp: OFF
            fortran: OFF
            java: OFF
            hl: ON
            threadsafe: OFF
            parallel: ON
            tools: ON
            tests: ON
            examples: ON
            zlib: ON
            szip: ON
            plugins: ON

          # No High Level API with MPICH
          - name: "macOS-26 No HL MPICH"
            os: macos-26
            mpi: mpich
            shared_libs: ON
            static_libs: ON
            cpp: ON
            fortran: OFF
            java: OFF
            hl: OFF
            threadsafe: OFF
            parallel: ON
            tools: ON
            tests: ON
            examples: ON
            zlib: ON
            szip: ON

    name: ${{ matrix.name }}
    runs-on: ${{ matrix.os }}
    if: "!contains(github.event.head_commit.message, 'skip-ci')"

    steps:
    - name: Install Dependencies
      run: |
        # Install base dependencies
        brew install autoconf automake libtool

        # Install compression libraries
        brew install zlib
        if [[ "${{ matrix.szip }}" == "ON" ]]; then
          brew install libaec
        fi

        # Install MPI libraries
        if [[ "${{ matrix.mpi }}" == "openmpi" ]]; then
          brew install openmpi
        elif [[ "${{ matrix.mpi }}" == "mpich" ]]; then
          brew install mpich
        fi


    - name: Get Sources
      uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

    - name: Configure
      run: |
        mkdir "${{ runner.workspace }}/build"
        cd "${{ runner.workspace }}/build"

        # Base CMake options
        CMAKE_OPTS="-G Ninja"
        CMAKE_OPTS="$CMAKE_OPTS -DCMAKE_BUILD_TYPE=Release"
        CMAKE_OPTS="$CMAKE_OPTS -DBUILD_SHARED_LIBS=${{ matrix.shared_libs }}"
        CMAKE_OPTS="$CMAKE_OPTS -DBUILD_STATIC_LIBS=${{ matrix.static_libs }}"
        CMAKE_OPTS="$CMAKE_OPTS -DHDF5_BUILD_CPP_LIB=${{ matrix.cpp }}"
        CMAKE_OPTS="$CMAKE_OPTS -DHDF5_BUILD_FORTRAN=${{ matrix.fortran }}"
        CMAKE_OPTS="$CMAKE_OPTS -DHDF5_BUILD_JAVA=${{ matrix.java }}"
        CMAKE_OPTS="$CMAKE_OPTS -DHDF5_BUILD_HL_LIB=${{ matrix.hl }}"
        CMAKE_OPTS="$CMAKE_OPTS -DHDF5_ENABLE_THREADSAFE=${{ matrix.threadsafe }}"
        CMAKE_OPTS="$CMAKE_OPTS -DHDF5_ENABLE_PARALLEL=${{ matrix.parallel }}"
        CMAKE_OPTS="$CMAKE_OPTS -DHDF5_BUILD_TOOLS=${{ matrix.tools }}"
        CMAKE_OPTS="$CMAKE_OPTS -DBUILD_TESTING=${{ matrix.tests }}"
        CMAKE_OPTS="$CMAKE_OPTS -DHDF5_BUILD_EXAMPLES=${{ matrix.examples }}"
        CMAKE_OPTS="$CMAKE_OPTS -DHDF5_ENABLE_ZLIB_SUPPORT=${{ matrix.zlib }}"
        CMAKE_OPTS="$CMAKE_OPTS -DHDF5_ENABLE_SZIP_SUPPORT=${{ matrix.szip }}"
        CMAKE_OPTS="$CMAKE_OPTS -DHDF5_ALLOW_UNSUPPORTED=ON"

        # Plugin support if specified
        if [[ "${{ matrix.plugins }}" == "ON" ]]; then
          CMAKE_OPTS="$CMAKE_OPTS -DHDF5_ENABLE_PLUGIN_SUPPORT=ON"
          CMAKE_OPTS="$CMAKE_OPTS -DHDF5_ALLOW_EXTERNAL_SUPPORT=TGZ"
          CMAKE_OPTS="$CMAKE_OPTS -DPLUGIN_USE_LOCALCONTENT=OFF"
        fi

        # Set Java environment if needed
        if [[ "${{ matrix.java }}" == "ON" ]]; then
          export JAVA_HOME=$(/usr/libexec/java_home)
          CMAKE_OPTS="$CMAKE_OPTS -DJAVA_HOME=$JAVA_HOME"
        fi

        # Set Fortran compiler if needed
        if [[ "${{ matrix.fortran }}" == "ON" ]]; then
          CMAKE_OPTS="$CMAKE_OPTS -DCMAKE_Fortran_COMPILER=gfortran-15"
        fi

        # MPI configuration
        if [[ "${{ matrix.parallel }}" == "ON" ]]; then
          CMAKE_OPTS="$CMAKE_OPTS -DMPIEXEC_MAX_NUMPROCS=2"
          if [[ "${{ matrix.mpi }}" == "openmpi" ]]; then
            CMAKE_OPTS="$CMAKE_OPTS -DMPI_C_COMPILER=mpicc"
            CMAKE_OPTS="$CMAKE_OPTS -DMPIEXEC_PREFLAGS=--oversubscribe"
          elif [[ "${{ matrix.mpi }}" == "mpich" ]]; then
            CMAKE_OPTS="$CMAKE_OPTS -DMPI_C_COMPILER=mpicc"
            CMAKE_OPTS="$CMAKE_OPTS -DCMAKE_Fortran_COMPILER=mpifort"
            sudo ln -s /opt/homebrew/bin/gfortran-15 /opt/homebrew/bin/gfortran
            mpifort --version
          fi
        fi

        echo "Configuration options: $CMAKE_OPTS"
        cmake $CMAKE_OPTS "$GITHUB_WORKSPACE"
      shell: bash

    - name: Build
      run: cmake --build . --config Release --parallel 2
      working-directory: ${{ runner.workspace }}/build

    - name: Test
      run: |
        # Run tests with appropriate settings for each configuration
        if [[ "${{ matrix.parallel }}" == "ON" ]]; then
          # For parallel builds, limit concurrent tests
          ctest . -C Release --parallel 1
        else
          # For serial builds, can run more tests concurrently
          ctest . -C Release --parallel 2
        fi
      working-directory: ${{ runner.workspace }}/build
      env:
        # Set test timeout
        CTEST_TIMEOUT: 3600

    - name: Upload Test Results
      uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
      if: always()
      with:
        name: test-results-${{ matrix.name }}
        path: |
          ${{ runner.workspace }}/build/Testing/Temporary/CTestCostData.txt
          ${{ runner.workspace }}/build/Testing/Temporary/LastTest.log
        retention-days: 7

    - name: Show Test Summary
      if: always()
      run: |
        echo "=== Test Summary for ${{ matrix.name }} ==="
        if [[ -f Testing/Temporary/LastTest.log ]]; then
          tail -50 Testing/Temporary/LastTest.log
        fi
      working-directory: ${{ runner.workspace }}/build
```

### `.github/workflows/main-par-spc.yml`

```yaml
name: hdf5 dev parallel special CI

on:
  workflow_call:
    inputs:
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  #
  # The GitHub runners are inadequate for running parallel HDF5 tests,
  # so we catch most issues in daily testing. What we have here is just
  # a compile check to make sure nothing obvious is broken.
  # A workflow that builds the library
  # Parallel Linux (Ubuntu) w/ gcc
  #
  Build_parallel_werror:
    name: "Parallel GCC-${{ inputs.build_mode }}"
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install Linux Dependencies
        run: |
           sudo apt-get update
           sudo apt-get install ninja-build doxygen graphviz
           sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
           sudo apt install gcc-12 g++-12 gfortran-12
           sudo apt install libaec0 libaec-dev
           sudo apt install openmpi-bin openmpi-common mpi-default-dev
           echo "CC=mpicc" >> $GITHUB_ENV
           echo "FC=mpif90" >> $GITHUB_ENV

      - name: Configure
        shell: bash
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          CC=mpicc cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_WARNINGS_AS_ERRORS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=ON \
            -DMPIEXEC_NUMPROC_FLAG:STRING=-n \
            -DMPIEXEC_MAX_NUMPROCS:STRING=2 \
            -DHDF5_ENABLE_SUBFILING_VFD:BOOL=ON \
            -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN=OFF \
            -DHDF5_BUILD_JAVA=OFF \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=OFF \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
            -DHDF5_PACK_EXAMPLES:BOOL=OFF \
            $GITHUB_WORKSPACE

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
      - name: Run Tests
        run: ctest . -E MPI_TEST --parallel 2 -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build
        if: ${{ inputs.thread_safety != 'TS' }}

      - name: Run Parallel Tests
        run: ctest . -R MPI_TEST -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build
        if: ${{ matrix.run_tests && (inputs.thread_safety != 'TS') }}
```

### `.github/workflows/main-par.yml`

```yaml
name: hdf5 dev parallel CI

on:
  workflow_call:
    inputs:
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  #
  # The GitHub runners are inadequate for running parallel HDF5 tests,
  # so we catch most issues in daily testing. What we have here is just
  # a compile check to make sure nothing obvious is broken.
  # A workflow that builds the library
  # Parallel Linux (Ubuntu) w/ gcc
  #
  Build_parallel:
    name: "Parallel GCC-${{ inputs.build_mode }}"
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install Linux Dependencies
        run: |
           sudo apt-get update
           sudo apt-get install ninja-build doxygen graphviz
           sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
           sudo apt install gcc-12 g++-12 gfortran-12
           sudo apt install libaec0 libaec-dev
           sudo apt install openmpi-bin openmpi-common mpi-default-dev
           echo "CC=mpicc" >> $GITHUB_ENV
           echo "FC=mpif90" >> $GITHUB_ENV

      - name: Configure
        shell: bash
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          CC=mpicc cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=ON \
            -DMPIEXEC_NUMPROC_FLAG:STRING=-n \
            -DMPIEXEC_MAX_NUMPROCS:STRING=2 \
            -DHDF5_ENABLE_SUBFILING_VFD:BOOL=ON \
            -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN=ON \
            -DHDF5_BUILD_JAVA=OFF \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=OFF \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=OFF \
            -DHDF5_PACK_EXAMPLES:BOOL=ON \
            $GITHUB_WORKSPACE

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      #
      # RUN TESTS
      #
      - name: Run Tests
        run: ctest . -E MPI_TEST --parallel 2 -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build
        if: ${{ inputs.thread_safety != 'TS' }}

      - name: Run Parallel Tests
        run: ctest . -R MPI_TEST -E "_by_chunk|_by_pattern" -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build
        if: ${{ inputs.thread_safety != 'TS' }}
```

### `.github/workflows/main-spc.yml`

```yaml
name: hdf5 dev special CI

# Controls when the action will run. Triggers the workflow on a call
on:
  workflow_call:

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

# A workflow run is made up of one or more jobs that can run sequentially or
# in parallel. We just have one job, but the matrix items defined below will
# run in parallel.
jobs:
  #
  # SPECIAL BUILDS
  #
  # These are not built into the matrix and instead
  # become NEW configs as their name would clobber one of the matrix
  # names (so make sure the names are UNIQUE).
  #

  build_v1_6:
    name: "gcc DBG v1.6 default API"
    runs-on: ubuntu-latest
    steps:
      # SETUP
      - name: Install Linux Dependencies
        run: |
           sudo apt-get update
           sudo apt-get install ninja-build doxygen graphviz
           sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
           sudo apt install gcc-12 g++-12 gfortran-12
           echo "CC=gcc-12" >> $GITHUB_ENV
           echo "CXX=g++-12" >> $GITHUB_ENV
           echo "FC=gfortran-12" >> $GITHUB_ENV
 
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            -DCMAKE_BUILD_TYPE=Debug \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_BUILD_FORTRAN=ON \
            -DHDF5_BUILD_JAVA=ON \
            -DHDF5_BUILD_DOC=OFF \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
            -DHDF5_DEFAULT_API_VERSION:STRING=v16 \
            $GITHUB_WORKSPACE
        shell: bash

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config Debug
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
      - name: Run Tests
        run: |
          ctest . --parallel 2 -C Debug -E "testhdf5-base|cache_api|dt_arith|H5TEST-dtypes|err_compat"
        working-directory: ${{ runner.workspace }}/build

      - name: Run Expected to Fail Tests
        run: |
          ctest . --parallel 2 -C Debug -R "testhdf5-base|cache_api|dt_arith|H5TEST-dtypes|err_compat"
        working-directory: ${{ runner.workspace }}/build
        continue-on-error: true

  build_v1_8:
    name: "gcc DBG v1.8 default API"
    runs-on: ubuntu-latest
    steps:
      # SETUP
      - name: Install Linux Dependencies
        run: |
           sudo apt-get update
           sudo apt-get install ninja-build doxygen graphviz
           sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
           sudo apt install gcc-12 g++-12 gfortran-12
           echo "CC=gcc-12" >> $GITHUB_ENV
           echo "CXX=g++-12" >> $GITHUB_ENV
           echo "FC=gfortran-12" >> $GITHUB_ENV
 
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            -DCMAKE_BUILD_TYPE=Debug \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_BUILD_FORTRAN=ON \
            -DHDF5_BUILD_JAVA=ON \
            -DHDF5_BUILD_DOC=OFF \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
            -DHDF5_DEFAULT_API_VERSION:STRING=v18 \
            $GITHUB_WORKSPACE
        shell: bash

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config Debug
        working-directory: ${{ runner.workspace }}/build

      #
      # RUN TESTS
      #
      - name: Run Tests
        run: ctest . --parallel 2 -C Debug
        working-directory: ${{ runner.workspace }}/build

  build_v1_10:
    name: "gcc DBG v1.10 default API"
    runs-on: ubuntu-latest
    steps:
      # SETUP
      - name: Install Linux Dependencies
        run: |
           sudo apt-get update
           sudo apt-get install ninja-build doxygen graphviz
           sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
           sudo apt install gcc-12 g++-12 gfortran-12
           echo "CC=gcc-12" >> $GITHUB_ENV
           echo "CXX=g++-12" >> $GITHUB_ENV
           echo "FC=gfortran-12" >> $GITHUB_ENV
 
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            -DCMAKE_BUILD_TYPE=Debug \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_BUILD_FORTRAN=ON \
            -DHDF5_BUILD_JAVA=ON \
            -DHDF5_BUILD_DOC=OFF \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
            -DHDF5_DEFAULT_API_VERSION:STRING=v110 \
            $GITHUB_WORKSPACE
        shell: bash

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config Debug
        working-directory: ${{ runner.workspace }}/build

      #
      # RUN TESTS
      #
      - name: Run Tests
        run: ctest . --parallel 2 -C Debug
        working-directory: ${{ runner.workspace }}/build

  build_v1_12:
    name: "gcc DBG v1.12 default API"
    runs-on: ubuntu-latest
    steps:
      # SETUP
      - name: Install Linux Dependencies
        run: |
           sudo apt-get update
           sudo apt-get install ninja-build doxygen graphviz
           sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
           sudo apt install gcc-12 g++-12 gfortran-12
           echo "CC=gcc-12" >> $GITHUB_ENV
           echo "CXX=g++-12" >> $GITHUB_ENV
           echo "FC=gfortran-12" >> $GITHUB_ENV
 
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            -DCMAKE_BUILD_TYPE=Debug \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_BUILD_FORTRAN=ON \
            -DHDF5_BUILD_JAVA=ON \
            -DHDF5_BUILD_DOC=OFF \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
            -DHDF5_DEFAULT_API_VERSION:STRING=v112 \
            $GITHUB_WORKSPACE
        shell: bash

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config Debug
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
      - name: Run Tests
        run: ctest . --parallel 2 -C Debug
        working-directory: ${{ runner.workspace }}/build

  build_v1_14:
    name: "gcc DBG v1.14 default API"
    runs-on: ubuntu-latest
    steps:
      # SETUP
      - name: Install Linux Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz
          sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
          sudo apt install gcc-12 g++-12 gfortran-12
          echo "CC=gcc-12" >> $GITHUB_ENV
          echo "CXX=g++-12" >> $GITHUB_ENV
          echo "FC=gfortran-12" >> $GITHUB_ENV

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            -DCMAKE_BUILD_TYPE=Debug \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_BUILD_FORTRAN=ON \
            -DHDF5_BUILD_JAVA=ON \
            -DHDF5_BUILD_DOC=OFF \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
            -DHDF5_DEFAULT_API_VERSION:STRING=v114 \
            $GITHUB_WORKSPACE
        shell: bash

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config Debug
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
      - name: Run Tests
        run: ctest . --parallel 2 -C Debug
        working-directory: ${{ runner.workspace }}/build

  build_v2_0_linux:
    name: "gcc DBG v2.0.0 default API no deprecated"
    runs-on: ubuntu-latest
    steps:
      # SETUP
      - name: Install Linux Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz
          sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
          sudo apt install gcc-12 g++-12 gfortran-12
          echo "CC=gcc-12" >> $GITHUB_ENV
          echo "CXX=g++-12" >> $GITHUB_ENV
          echo "FC=gfortran-12" >> $GITHUB_ENV

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            -DCMAKE_BUILD_TYPE=Debug \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_BUILD_FORTRAN=ON \
            -DHDF5_BUILD_JAVA=ON \
            -DHDF5_BUILD_DOC=OFF \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
            -DHDF5_ENABLE_DEPRECATED_SYMBOLS:BOOL=OFF \
            -DHDF5_DEFAULT_API_VERSION:STRING=v200 \
            $GITHUB_WORKSPACE
        shell: bash

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config Debug
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
      - name: Run Tests
        run: ctest . --parallel 2 -C Debug
        working-directory: ${{ runner.workspace }}/build

  build_v2_0_win:
    name: "Intel DBG v2.0.0 default API no deprecated"
    runs-on: windows-latest
    steps:
      # SETUP
      - name: Install Dependencies (Windows)
        run: choco install ninja

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: install oneAPI (Windows)
        uses: fortran-lang/setup-fortran@47809fdb6e637da656ce9ada436527b240c1287f # v1
        id: setup-fortran
        with:
          compiler: intel
          version: '2025.0'

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Configure (Windows)
        shell: pwsh
        env:
          FC: ${{ steps.setup-fortran.outputs.fc }}
          CC: ${{ steps.setup-fortran.outputs.cc }}
        run: |
          mkdir "${{ runner.workspace }}/build"
          Set-Location -Path "${{ runner.workspace }}\\build"
          cmake -C ${{ github.workspace }}/config/cmake/cacheinit.cmake -G Ninja -DCMAKE_BUILD_TYPE=Debug -DBUILD_SHARED_LIBS=ON -DHDF5_ENABLE_ALL_WARNINGS=ON -DHDF5_ENABLE_PARALLEL:BOOL=OFF -DHDF5_BUILD_CPP_LIB:BOOL=ON -DHDF5_BUILD_FORTRAN=ON -DHDF5_BUILD_JAVA=ON -DHDF5_BUILD_DOC=OFF -DLIBAEC_USE_LOCALCONTENT=OFF -DZLIB_USE_LOCALCONTENT=OFF -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON -DHDF5_ENABLE_DIRECT_VFD:BOOL=OFF -DHDF5_ENABLE_DEPRECATED_SYMBOLS:BOOL=OFF -DHDF5_DEFAULT_API_VERSION:STRING=v200 ${{ github.workspace }}

      - name: Build (Windows)
        shell: pwsh
        env:
          FC: ${{ steps.setup-fortran.outputs.fc }}
          CC: ${{ steps.setup-fortran.outputs.cc }}
        run: |
          cmake --build . --parallel 3 --config Debug
        working-directory: ${{ runner.workspace }}/build

      - name: Run Tests (Windows)
        shell: pwsh
        env:
          FC: ${{ steps.setup-fortran.outputs.fc }}
          CC: ${{ steps.setup-fortran.outputs.cc }}
        run: |
          ctest . --parallel 2 -C Debug
        working-directory: ${{ runner.workspace }}/build

  build_system_zlib:
    name: "gcc system zlib"
    runs-on: ubuntu-latest
    steps:
      # SETUP
      - name: Install Linux Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz
          sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
          sudo apt install gcc-12 g++-12 gfortran-12 zlib1g-dev 
          echo "CC=gcc-12" >> $GITHUB_ENV
          echo "CXX=g++-12" >> $GITHUB_ENV
          echo "FC=gfortran-12" >> $GITHUB_ENV

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=Release \
            -DBUILD_SHARED_LIBS:BOOL=ON \
            -DHDF5_ENABLE_ALL_WARNINGS:BOOL=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_BUILD_FORTRAN:BOOL=ON \
            -DHDF5_BUILD_JAVA:BOOL=ON \
            -DHDF5_BUILD_DOC:BOOL=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
            -DHDF5_ALLOW_EXTERNAL_SUPPORT:STRING="NO" \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
            -DZLIB_USE_LOCALCONTENT:BOOL=OFF \
            -DZLIB_USE_EXTERNAL:BOOL=OFF \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF \
            -DHDF5_ENABLE_SZIP_ENCODING:BOOL=OFF \
            -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF \
            $GITHUB_WORKSPACE
        shell: bash

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config Debug
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
      - name: Run Tests
        run: ctest . --parallel 2 -C Debug
        working-directory: ${{ runner.workspace }}/build

  build_zlibng:
    name: "gcc use zlib-ng filter"
    runs-on: ubuntu-latest
    steps:
      # SETUP
      - name: Install Linux Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz
          sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
          sudo apt install gcc-12 g++-12 gfortran-12
          echo "CC=gcc-12" >> $GITHUB_ENV
          echo "CXX=g++-12" >> $GITHUB_ENV
          echo "FC=gfortran-12" >> $GITHUB_ENV

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=Release \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_BUILD_FORTRAN=ON \
            -DHDF5_BUILD_JAVA=ON \
            -DHDF5_BUILD_DOC=OFF \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
            -DHDF5_USE_ZLIB_NG:BOOL=ON \
            $GITHUB_WORKSPACE
        shell: bash

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config Release
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
      - name: Run Tests
        run: ctest . --parallel 2 -C Release
        working-directory: ${{ runner.workspace }}/build

  build_nofilter:
    name: "gcc no filters"
    runs-on: ubuntu-latest
    steps:
      # SETUP
      - name: Install Linux Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz
          sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
          sudo apt install gcc-12 g++-12 gfortran-12
          echo "CC=gcc-12" >> $GITHUB_ENV
          echo "CXX=g++-12" >> $GITHUB_ENV
          echo "FC=gfortran-12" >> $GITHUB_ENV

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=Release \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_BUILD_FORTRAN=ON \
            -DHDF5_BUILD_JAVA=ON \
            -DHDF5_BUILD_DOC=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF \
            -DHDF5_ENABLE_SZIP_SUPPORT:=BOOL=OFF \
            -DHDF5_ENABLE_SZIP_ENCODING:=BOOL=OFF \
            $GITHUB_WORKSPACE
        shell: bash

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config Release
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
      - name: Run Tests
        run: ctest . --parallel 2 -C Release
        working-directory: ${{ runner.workspace }}/build

  build_debug_werror:
    name: "gcc DBG -Werror (build only)"
    runs-on: ubuntu-latest
    steps:
      # SETUP
      - name: Install Linux Dependencies
        run: |
           sudo apt-get update
           sudo apt-get install ninja-build doxygen graphviz
           sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
           sudo apt install gcc-12 g++-12 gfortran-12
           echo "CC=gcc-12" >> $GITHUB_ENV
           echo "CXX=g++-12" >> $GITHUB_ENV
           echo "FC=gfortran-12" >> $GITHUB_ENV

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=Debug \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_WARNINGS_AS_ERRORS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_BUILD_FORTRAN=OFF \
            -DHDF5_BUILD_JAVA=ON \
            -DHDF5_BUILD_DOC=OFF \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
            $GITHUB_WORKSPACE
        shell: bash

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config Release
        working-directory: ${{ runner.workspace }}/build

  build_release_werror:
    name: "gcc REL -Werror (build only)"
    runs-on: ubuntu-latest
    steps:
      # SETUP
      - name: Install Linux Dependencies
        run: |
           sudo apt-get update
           sudo apt-get install ninja-build doxygen graphviz
           sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
           sudo apt install gcc-12 g++-12 gfortran-12
           echo "CC=gcc-12" >> $GITHUB_ENV
           echo "CXX=g++-12" >> $GITHUB_ENV
           echo "FC=gfortran-12" >> $GITHUB_ENV

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=Release \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_WARNINGS_AS_ERRORS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_BUILD_FORTRAN=ON \
            -DHDF5_BUILD_JAVA=ON \
            -DHDF5_BUILD_DOC=OFF \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=ON \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=ON \
            $GITHUB_WORKSPACE
        shell: bash

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config Release
        working-directory: ${{ runner.workspace }}/build
```

### `.github/workflows/main-static.yml`

```yaml
name: hdf5 dev CI

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      cmake_version:
        description: "3.26.0 or later, latest"
        required: true
        type: string
      thread_safety:
        description: "TS or empty"
        required: true
        type: string
      concurrent:
        description: "CC or empty"
        required: true
        type: string
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string
      save_binary:
        description: "binary-ext-name or missing"
        required: false
        default: "skip"
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  # A workflow that builds the library and runs all the tests
  Static_build_and_test:
    strategy:
      # The current matrix has one dimensions:
      #
      # * config name
      #
      # Most configuration information is added via the 'include' mechanism,
      # which will append the key-value pairs in the configuration where the
      # names match.
      matrix:
        name:
          - "Windows Static MSVC"
          - "Ubuntu Static gcc"
          - "MacOS Static Clang"

        # This is where we list the bulk of the options for each configuration.
        # The key-value pair values are usually appropriate for being CMake
        # configure values, so be aware of that.

        include:

          - name: "Windows Static MSVC"
            ostype: windows
            os: windows-latest
            shared: OFF
            cpp: ON
            fortran: OFF
            java: OFF
            docs: OFF
            libaecfc: ON
            localaec: OFF
            zlibfc: ON
            localzlib: OFF
            parallel: OFF
            mirror_vfd: OFF
            direct_vfd: OFF
            ros3_vfd: OFF
            generator: "-G \"Visual Studio 17 2022\" -A x64"
            run_tests: true

          - name: "Ubuntu Static gcc"
            ostype: ubuntu
            os: ubuntu-latest
            shared: OFF
            cpp: ON
            fortran: ON
            java: OFF
            docs: OFF
            libaecfc: ON
            localaec: OFF
            zlibfc: ON
            localzlib: OFF
            parallel: OFF
            mirror_vfd: ON
            direct_vfd: ON
            ros3_vfd: OFF
            generator: "-G Ninja"
            run_tests: true

          - name: "MacOS Static Clang"
            ostype: macos
            os: macos-latest
            shared: OFF
            cpp: ON
            fortran: OFF
            java: OFF
            docs: OFF
            libaecfc: ON
            localaec: OFF
            zlibfc: ON
            localzlib: OFF
            parallel: OFF
            mirror_vfd: ON
            direct_vfd: OFF
            ros3_vfd: OFF
            generator: "-G Ninja"
            run_tests: true

    if: ${{ inputs.thread_safety != 'TS' && inputs.concurrent != 'CC'}}
    # Sets the job's name from the properties
    name: "${{ matrix.name }}-${{ inputs.build_mode }}-${{ inputs.thread_safety }}-${{ inputs.concurrent }}"

    # The type of runner that the job will run on
    runs-on: ${{ matrix.os }}

  # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      #Useful for debugging
      - name: Dump matrix context
        run: echo '${{ toJSON(matrix) }}'

      - name: Install Dependencies (Linux)
        run: |
           sudo apt-get update
           sudo apt-get install ninja-build graphviz
           sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
        if: matrix.ostype == 'ubuntu'

      # CMake gets libaec from fetchcontent

      - name: Install Dependencies (macOS)
        run: brew install ninja curl
        if: ${{ matrix.ostype == 'macos' }}

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Check CMake Version
        shell: bash
        run: |
          which cmake
          cmake --version

      - name: Set environment for MSVC (Windows)
        run: |
          # Set these environment variables so CMake picks the correct compiler
          echo "CXX=cl.exe" >> $GITHUB_ENV
          echo "CC=cl.exe" >> $GITHUB_ENV
        if:  matrix.ostype == 'windows'

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            ${{ matrix.generator }} \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DBUILD_SHARED_LIBS:BOOL=OFF \
            -DBUILD_STATIC_LIBS:BOOL=ON \
            -DHDF5_ENABLE_ALL_WARNINGS:BOOL=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=${{ matrix.parallel }} \
            -DHDF5_BUILD_CPP_LIB:BOOL=${{ matrix.cpp }} \
            -DHDF5_BUILD_FORTRAN:BOOL=${{ matrix.fortran }} \
            -DHDF5_BUILD_JAVA:BOOL=OFF \
            -DHDF5_BUILD_DOC:BOOL=OFF \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=${{ matrix.zlibfc }} \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=${{ matrix.libaecfc }} \
            -DLIBAEC_USE_LOCALCONTENT:BOOL=${{ matrix.localaec }} \
            -DZLIB_USE_LOCALCONTENT:BOOL=${{ matrix.localzlib }} \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=${{ matrix.mirror_vfd }} \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=${{ matrix.direct_vfd }} \
            -DHDF5_ENABLE_ROS3_VFD:BOOL=${{ matrix.ros3_vfd }} \
            -DHDF5_PACK_EXAMPLES:BOOL=ON \
            -DHDF5_PACKAGE_EXTLIBS:BOOL=ON \
            -DHDF5_PACK_MACOSX_DMG:BOOL=OFF \
            $GITHUB_WORKSPACE
        shell: bash

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
      - name: Run Tests
        run: ctest . --parallel 2 -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build
        if: ${{ matrix.run_tests }}
```

### `.github/workflows/main.yml`

```yaml
name: hdf5 dev CI

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      cmake_version:
        description: "3.26.0 or later, latest"
        required: true
        type: string
      thread_safety:
        description: "TS or empty"
        required: true
        type: string
      concurrent:
        description: "CC or empty"
        required: true
        type: string
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string
      save_binary:
        description: "binary-ext-name or missing"
        required: false
        default: "skip"
        type: string
      java_version:
        description: "Java version for testing (11, 17, 21, 24, latest, auto)"
        required: false
        default: "auto"
        type: string
      force_java_implementation:
        description: "Force specific Java implementation (auto, ffm, jni)"
        required: false
        default: "jni"
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:

  # A workflow that builds the library and runs all the tests
  Build_and_test:
    strategy:
      # The current matrix has one dimensions:
      #
      # * config name
      #
      # Most configuration information is added via the 'include' mechanism,
      # which will append the key-value pairs in the configuration where the
      # names match.
      matrix:
        name:
          - "Windows MSVC"
          - "Ubuntu gcc"
          - "MacOS Clang"

        # This is where we list the bulk of the options for each configuration.
        # The key-value pair values are usually appropriate for being CMake
        # configure values, so be aware of that.

        include:

          # Windows w/ MSVC
          #
          # No Fortran, parallel, or VFDs that rely on POSIX things
          - name: "Windows MSVC"
            ostype: windows
            os: windows-latest
            shared: ON
            cpp: ON
            fortran: OFF
            java: ON
            docs: ON
            libaecfc: ON
            localaec: OFF
            zlibfc: ON
            localzlib: OFF
            parallel: OFF
            mirror_vfd: OFF
            direct_vfd: OFF
            generator: "-G \"Visual Studio 17 2022\" -A x64"
            run_tests: true

          # Linux (Ubuntu) w/ gcc
          #
          - name: "Ubuntu gcc"
            ostype: ubuntu
            os: ubuntu-latest
            shared: ON
            cpp: ON
            fortran: ON
            java: ON
            docs: ON
            libaecfc: ON
            localaec: OFF
            zlibfc: ON
            localzlib: OFF
            parallel: OFF
            mirror_vfd: ON
            direct_vfd: ON
            generator: "-G Ninja"
            run_tests: true

          # MacOS w/ Clang
          #
          - name: "MacOS Clang"
            ostype: macos
            os: macos-latest
            shared: ON
            cpp: ON
            fortran: OFF
            java: ON
            docs: ON
            libaecfc: ON
            localaec: OFF
            zlibfc: ON
            localzlib: OFF
            parallel: OFF
            mirror_vfd: ON
            direct_vfd: OFF
            generator: "-G Ninja"
            run_tests: true

    # Sets the job's name from the properties
    name: "${{ matrix.name }}-${{ inputs.build_mode }}-${{ inputs.thread_safety }}-${{ inputs.concurrent }}"

    # Don't run the action if the commit message says to skip CI
    if: "!contains(github.event.head_commit.message, 'skip-ci')"

    # The type of runner that the job will run on
    runs-on: ${{ matrix.os }}

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      #Useful for debugging
      - name: Dump matrix context
        run: echo '${{ toJSON(matrix) }}'

      - name: Install Dependencies (Linux)
        run: |
           sudo apt-get update
           sudo apt-get install ninja-build graphviz
           sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
        if: matrix.ostype == 'ubuntu'

      # CMake gets libaec from fetchcontent

      - name: Install Dependencies (macOS)
        run: brew install ninja curl
        if: ${{ matrix.ostype == 'macos' }}

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Check CMake Version
        shell: bash
        run: |
          which cmake
          cmake --version

      - name: Set up Java (if specified or FFM required)
        if: inputs.java_version != 'auto' || inputs.force_java_implementation == 'ffm'
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          distribution: ${{ inputs.force_java_implementation == 'ffm' && 'oracle' || 'temurin' }}
          java-version: |
            ${{
              inputs.force_java_implementation == 'ffm' && '25' ||
              inputs.java_version == 'latest' && '24' ||
              inputs.java_version
            }}

      - name: Verify Java Setup
        if: inputs.java_version != 'auto' || inputs.force_java_implementation == 'ffm'
        run: |
          java -version
          echo "JAVA_HOME=$JAVA_HOME"
          echo "Selected Java implementation: ${{ inputs.force_java_implementation }}"

      - name: Set environment for MSVC (Windows)
        run: |
          # Set these environment variables so CMake picks the correct compiler
          echo "CXX=cl.exe" >> $GITHUB_ENV
          echo "CC=cl.exe" >> $GITHUB_ENV
        if:  matrix.ostype == 'windows'

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Setup jextract (FFM builds only)
        if: ${{ inputs.force_java_implementation == 'ffm' }}
        uses: ./.github/actions/setup-jextract
        with:
          java-version: '25'

      # CONFIGURE
      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            ${{ matrix.generator }} \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DBUILD_SHARED_LIBS:BOOL=${{ matrix.shared }} \
            -DHDF5_ENABLE_ALL_WARNINGS:BOOL=ON \
            -DHDF5_ENABLE_DOXY_WARNINGS:BOOL=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=${{ matrix.parallel }} \
            -DHDF5_BUILD_CPP_LIB:BOOL=${{ matrix.cpp }} \
            -DHDF5_BUILD_FORTRAN:BOOL=${{ matrix.fortran }} \
            -DHDF5_BUILD_JAVA:BOOL=${{ matrix.java }} \
            -DHDF5_BUILD_DOC:BOOL=${{ matrix.docs }} \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=${{ matrix.zlibfc }} \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=${{ matrix.libaecfc }} \
            -DLIBAEC_USE_LOCALCONTENT:BOOL=${{ matrix.localaec }} \
            -DZLIB_USE_LOCALCONTENT:BOOL=${{ matrix.localzlib }} \
            -DHDF5_TEST_API:BOOL=ON \
            -DHDF5_TEST_SHELL_SCRIPTS:BOOL=ON \
            -DENABLE_EXTENDED_TESTS:BOOL=OFF \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=${{ matrix.mirror_vfd }} \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=${{ matrix.direct_vfd }} \
            -DHDF5_PACK_EXAMPLES:BOOL=ON \
            -DHDF5_PACKAGE_EXTLIBS:BOOL=ON \
            -DHDF5_PACK_MACOSX_DMG:BOOL=OFF \
            -DHDF5_ENABLE_JNI:BOOL=${{ inputs.force_java_implementation == 'jni' }} \
            $GITHUB_WORKSPACE
        shell: bash
        if: ${{ inputs.thread_safety != 'TS' && inputs.concurrent != 'CC'}}

      - name: Configure (Thread-Safe)
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            ${{ matrix.generator }} \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DBUILD_SHARED_LIBS:BOOL=ON \
            -DBUILD_STATIC_LIBS:BOOL=${{ (matrix.ostype != 'windows') }} \
            -DHDF5_ENABLE_ALL_WARNINGS:BOOL=ON \
            -DHDF5_ENABLE_THREADSAFE:BOOL=ON \
            -DHDF5_ENABLE_CONCURRENCY:BOOL=OFF \
            -DHDF5_ENABLE_PARALLEL:BOOL=${{ matrix.parallel }} \
            -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_JAVA:BOOL=OFF \
            -DHDF5_BUILD_HL_LIB:BOOL=OFF \
            -DHDF5_BUILD_DOC:BOOL=OFF \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=${{ matrix.zlibfc }} \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=${{ matrix.libaecfc }} \
            -DLIBAEC_USE_LOCALCONTENT:BOOL=${{ matrix.localaec }} \
            -DZLIB_USE_LOCALCONTENT:BOOL=${{ matrix.localzlib }} \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=${{ matrix.mirror_vfd }} \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=${{ matrix.direct_vfd }} \
            -DHDF5_PACK_EXAMPLES:BOOL=ON \
            -DHDF5_PACK_MACOSX_DMG:BOOL=OFF \
            $GITHUB_WORKSPACE
        shell: bash
        if: ${{ inputs.thread_safety == 'TS' && inputs.concurrent != 'CC'}}

      - name: Configure (Concurrency)
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            ${{ matrix.generator }} \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DBUILD_SHARED_LIBS=ON \
            -DBUILD_STATIC_LIBS=${{ (matrix.ostype != 'windows') }} \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_THREADSAFE:BOOL=OFF \
            -DHDF5_ENABLE_CONCURRENCY:BOOL=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=${{ matrix.parallel }} \
            -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_JAVA:BOOL=OFF \
            -DHDF5_BUILD_HL_LIB:BOOL=OFF \
            -DHDF5_BUILD_DOC=OFF \
            -DLIBAEC_USE_LOCALCONTENT=${{ matrix.localaec }} \
            -DZLIB_USE_LOCALCONTENT=${{ matrix.localzlib }} \
            -DHDF5_ENABLE_MIRROR_VFD:BOOL=${{ matrix.mirror_vfd }} \
            -DHDF5_ENABLE_DIRECT_VFD:BOOL=${{ matrix.direct_vfd }} \
            -DHDF5_PACK_EXAMPLES:BOOL=ON \
            $GITHUB_WORKSPACE
        shell: bash
        if: ${{ inputs.thread_safety != 'TS' && inputs.concurrent == 'CC'}}

      # BUILD
      - name: Build
        run: cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      # RUN TESTS
      - name: Run Tests
        run: ctest . --parallel 2 -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build
        if: ${{ matrix.run_tests }}

      - name: Run Package
        run: |
          if [[ "${{ matrix.ostype }}" == "macos" ]]; then
            # Disable indexing to prevent file locking during CPack
            sudo mdutil -i off / || true

            # Run cpack with retry
            cpack -C ${{ inputs.build_mode }} -V || (sleep 5 && cpack -C ${{ inputs.build_mode }} -V)
          else
            cpack -C ${{ inputs.build_mode }} -V
          fi
        working-directory: ${{ runner.workspace }}/build
        shell: bash
#        if: ${{ (matrix.ostype != 'macos') && (inputs.build_mode != 'Debug') }}

#      - name: Run Package (Mac_latest)
#        run: cpack -C ${{ inputs.build_mode }} -G STGZ -V
#        if: ${{ (matrix.ostype == 'macos') }}

      - name: List files in the space
        run: |
              ls -l ${{ runner.workspace }}/build

      # Save files created by CTest script
      - name: Save published binary (Windows)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: zip-vs2022_cl-${{ inputs.build_mode }}-${{ inputs.save_binary }}-binary
              path: ${{ runner.workspace }}/build/HDF5-*-win64.zip
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
        if:  ${{ (matrix.ostype == 'windows') && ( inputs.save_binary != 'skip') }}

      - name: Save published binary (linux)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: tgz-ubuntu-2404_gcc-${{ inputs.build_mode }}-${{ inputs.save_binary }}-binary
              path: ${{ runner.workspace }}/build/HDF5-*-Linux.tar.gz
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
        if:  ${{ (matrix.ostype == 'ubuntu') && ( inputs.save_binary != 'skip') }}

      - name: Save published binary (Mac_latest)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: tgz-macos14_clang-${{ inputs.build_mode }}-${{ inputs.save_binary }}-binary
              path: ${{ runner.workspace }}/build/HDF5-*-Darwin.tar.gz
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
        if: ${{ (matrix.ostype == 'macos') && ( inputs.save_binary != 'skip') }}
```

### `.github/workflows/markdown-link-check.yml`

```yaml
name: Check Markdown links

# Triggers the workflow on push or pull request or on demand
on:
  workflow_dispatch:
  schedule:
      - cron: "0 10 * * 6"

# The config file handles things like http 500 errors from sites like GitLab
# and http 200 responses
jobs:
  markdown-link-check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@61b9e3751b92087fd0b06925ba6dd6314e06f089 # master
    - uses: gaurav-nelson/github-action-markdown-link-check@5c5dfc0ac2e225883c0e5f03a85311ec2830d368 # v1
      with:
        config-file: '.github/workflows/markdown_config.json'
```

### `.github/workflows/markdown_config.json`

```json
{
    "aliveStatusCodes": [200, 500]
}
```

### `.github/workflows/maven-build-test.yml`

```yaml
name: Maven Build and Test (All Platforms)

# Standalone workflow for building and testing Maven packages across all platforms
# This workflow is useful for:
# - Testing Maven package creation in forks before submitting PRs
# - Validating Maven artifacts and deployment process
# - Testing both FFM and JNI implementations independently

on:
  workflow_dispatch:
    inputs:
      platforms:
        description: 'Platforms to build'
        type: choice
        required: false
        default: 'all-platforms'
        options:
          - 'linux-only'
          - 'linux-windows'
          - 'linux-macos'
          - 'all-platforms'
      java_implementation:
        description: 'Java implementation to test'
        type: choice
        required: false
        default: 'both'
        options:
          - 'both'
          - 'ffm'
          - 'jni'
          - 'auto'
      test_deployment:
        description: 'Deploy to GitHub Packages for testing'
        type: boolean
        required: false
        default: false
      test_examples:
        description: 'Run Java examples tests'
        type: boolean
        required: false
        default: true

permissions:
  contents: read
  packages: write

jobs:
  build-maven-packages:
    name: Build Maven Packages
    uses: ./.github/workflows/maven-staging.yml
    permissions:
      contents: read
      packages: write
      pull-requests: write
    with:
      test_maven_deployment: true
      use_snapshot_version: true
      platforms: ${{ inputs.platforms }}
      java_implementation: ${{ inputs.java_implementation }}

  deploy-to-packages:
    name: Deploy to GitHub Packages (Test)
    runs-on: ubuntu-latest
    needs: build-maven-packages
    if: ${{ inputs.test_deployment && needs.build-maven-packages.result == 'success' }}
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Download all Maven staging artifacts
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
          pattern: maven-staging-artifacts-*
          path: ./artifacts

      - name: List downloaded artifacts
        run: |
          echo "=== Downloaded artifacts ==="
          ls -R ./artifacts/

      - name: Set up Java
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Deploy to GitHub Packages
        run: |
          # Find all JAR files
          for artifact_dir in ./artifacts/maven-staging-artifacts-*/; do
            if [ -d "$artifact_dir" ]; then
              echo "Processing: $artifact_dir"

              # Find POM file
              POM_FILE=$(find "$artifact_dir" -name "pom.xml" | head -1)

              # Find JAR files (main artifacts, not sources/javadoc)
              for jar_file in $(find "$artifact_dir" -name "jarhdf5-*.jar" ! -name "*sources*" ! -name "*javadoc*"); do
                echo "Deploying: $(basename "$jar_file")"

                # Extract classifier from filename if present
                jar_basename=$(basename "$jar_file")
                if [[ "$jar_basename" =~ jarhdf5-[0-9.]+-SNAPSHOT-(.+)\.jar ]]; then
                  CLASSIFIER="${BASH_REMATCH[1]}"
                  echo "  Classifier: $CLASSIFIER"

                  # Determine artifact ID based on classifier
                  if [[ "$CLASSIFIER" == *"ffm"* ]] || [[ "$jar_file" == *"ffm"* ]]; then
                    ARTIFACT_ID="hdf5-java-ffm"
                  else
                    ARTIFACT_ID="hdf5-java-jni"
                  fi

                  mvn deploy:deploy-file \
                    -DgroupId=org.hdfgroup \
                    -DartifactId="$ARTIFACT_ID" \
                    -Dversion="${{ needs.build-maven-packages.outputs.version }}" \
                    -Dpackaging=jar \
                    -Dfile="$jar_file" \
                    -Dclassifier="$CLASSIFIER" \
                    -DpomFile="$POM_FILE" \
                    -DrepositoryId=github \
                    -Durl=${{ format('https://maven.pkg.github.com/{0}', github.repository) }} \
                    -Dusername=${{ github.actor }} \
                    -Dpassword=${{ secrets.GITHUB_TOKEN }}
                fi
              done
            fi
          done
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  test-jni-package:
    name: Test JNI Maven Package
    needs: [build-maven-packages, deploy-to-packages]
    runs-on: ubuntu-latest
    if: |
      inputs.test_deployment &&
      needs.deploy-to-packages.result == 'success' &&
      (inputs.java_implementation == 'both' || inputs.java_implementation == 'jni')
    steps:
      - name: Checkout HDF5 repository (contains HDF5Examples)
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Set up Java 21 for JNI
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '21'
          distribution: 'temurin'

      - name: Download HDF5 installation from workflow
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
          name: hdf5-install-linux-x86_64-jni
          path: ${{ runner.workspace }}/hdf5-install

      - name: Verify HDF5 installation
        run: |
          echo "HDF5 installation contents:"
          ls -la "${{ runner.workspace }}/hdf5-install"
          if [ -d "${{ runner.workspace }}/hdf5-install/lib" ]; then
            echo "Libraries:"
            ls -la "${{ runner.workspace }}/hdf5-install/lib" | grep -E '\.so' || echo "No shared libraries found"
          fi

      - name: Test JNI examples
        run: |
          cd HDF5Examples/JAVA
          ./test-maven-jni.sh "${{ needs.build-maven-packages.outputs.version }}" "${{ format('https://maven.pkg.github.com/{0}', github.repository) }}"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_ACTOR: ${{ github.actor }}
          HDF5_HOME: ${{ runner.workspace }}/hdf5-install

      - name: Upload test artifacts (JNI)
        if: always()
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: jni-test-results-${{ needs.build-maven-packages.outputs.version }}
          path: |
            HDF5Examples/JAVA/build/maven-test-jni/

  test-ffm-package:
    name: Test FFM Maven Package
    needs: [build-maven-packages, deploy-to-packages]
    runs-on: ubuntu-latest
    if: |
      inputs.test_deployment &&
      needs.deploy-to-packages.result == 'success' &&
      (inputs.java_implementation == 'both' || inputs.java_implementation == 'ffm')
    steps:
      - name: Checkout HDF5 repository (contains HDF5Examples)
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Set up Java 25 for FFM
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '25'
          distribution: 'oracle'

      - name: Download HDF5 installation from workflow
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
          name: hdf5-install-linux-x86_64-ffm
          path: ${{ runner.workspace }}/hdf5-install

      - name: Verify HDF5 installation
        run: |
          echo "HDF5 installation contents:"
          ls -la "${{ runner.workspace }}/hdf5-install"
          if [ -d "${{ runner.workspace }}/hdf5-install/lib" ]; then
            echo "Libraries:"
            ls -la "${{ runner.workspace }}/hdf5-install/lib" | grep -E '\.so' || echo "No shared libraries found"
          fi

      - name: Test FFM examples
        run: |
          cd HDF5Examples/JAVA
          ./test-maven-ffm.sh "${{ needs.build-maven-packages.outputs.version }}" "${{ format('https://maven.pkg.github.com/{0}', github.repository) }}"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_ACTOR: ${{ github.actor }}
          HDF5_HOME: ${{ runner.workspace }}/hdf5-install

      - name: Upload test artifacts (FFM)
        if: always()
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: ffm-test-results-${{ needs.build-maven-packages.outputs.version }}
          path: |
            HDF5Examples/JAVA/build/maven-test-ffm/

  summarize-results:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [build-maven-packages, deploy-to-packages, test-jni-package, test-ffm-package]
    if: always()
    steps:
      - name: Generate test summary
        run: |
          echo "# Maven Build and Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Configuration:**" >> $GITHUB_STEP_SUMMARY
          echo "- Platforms: ${{ inputs.platforms }}" >> $GITHUB_STEP_SUMMARY
          echo "- Java Implementation: ${{ inputs.java_implementation }}" >> $GITHUB_STEP_SUMMARY
          echo "- Deployment Test: ${{ inputs.test_deployment }}" >> $GITHUB_STEP_SUMMARY
          echo "- Examples Test: ${{ inputs.test_examples }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Build results
          if [ "${{ needs.build-maven-packages.result }}" == "success" ]; then
            echo "âœ… **Maven Package Build:** PASSED" >> $GITHUB_STEP_SUMMARY
            echo "   - Version: ${{ needs.build-maven-packages.outputs.version }}" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Maven Package Build:** FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          # Deployment results
          if [ "${{ inputs.test_deployment }}" == "true" ]; then
            if [ "${{ needs.deploy-to-packages.result }}" == "success" ]; then
              echo "âœ… **Package Deployment:** PASSED" >> $GITHUB_STEP_SUMMARY
              echo "   - Repository: https://github.com/${{ github.repository }}/packages" >> $GITHUB_STEP_SUMMARY
            elif [ "${{ needs.deploy-to-packages.result }}" == "skipped" ]; then
              echo "â­ï¸  **Package Deployment:** SKIPPED (build failed)" >> $GITHUB_STEP_SUMMARY
            else
              echo "âŒ **Package Deployment:** FAILED" >> $GITHUB_STEP_SUMMARY
            fi

            # Test results
            JNI_RESULT="${{ needs.test-jni-package.result }}"
            FFM_RESULT="${{ needs.test-ffm-package.result }}"

            if [ "$JNI_RESULT" == "success" ] || [ "$JNI_RESULT" == "skipped" ]; then
              if [ "$FFM_RESULT" == "success" ] || [ "$FFM_RESULT" == "skipped" ]; then
                echo "âœ… **Package Testing:** PASSED" >> $GITHUB_STEP_SUMMARY
              else
                echo "âš ï¸ **Package Testing:** PARTIAL (FFM failed)" >> $GITHUB_STEP_SUMMARY
              fi
            elif [ "$FFM_RESULT" == "success" ] || [ "$FFM_RESULT" == "skipped" ]; then
              echo "âš ï¸ **Package Testing:** PARTIAL (JNI failed)" >> $GITHUB_STEP_SUMMARY
            else
              echo "âŒ **Package Testing:** FAILED" >> $GITHUB_STEP_SUMMARY
            fi
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          # Final status
          JNI_OK=$([[ "${{ needs.test-jni-package.result }}" =~ ^(success|skipped)$ ]] && echo "true" || echo "false")
          FFM_OK=$([[ "${{ needs.test-ffm-package.result }}" =~ ^(success|skipped)$ ]] && echo "true" || echo "false")

          if [ "${{ needs.build-maven-packages.result }}" == "success" ] && \
             ([ "${{ inputs.test_deployment }}" == "false" ] || \
              ([ "${{ needs.deploy-to-packages.result }}" == "success" ] && \
               [ "$JNI_OK" == "true" ] && [ "$FFM_OK" == "true" ])); then
            echo "## ðŸŽ‰ All Tests Passed!" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Maven packages are ready for release." >> $GITHUB_STEP_SUMMARY
          else
            echo "## âš ï¸ Some Tests Failed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Please review the logs above for details." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "**Next Steps:**" >> $GITHUB_STEP_SUMMARY
          if [ "${{ github.repository }}" != "HDFGroup/hdf5" ]; then
            echo "- Review artifacts in [GitHub Packages](https://github.com/${{ github.repository }}/packages)" >> $GITHUB_STEP_SUMMARY
            echo "- Test manually by adding your repository to Maven settings" >> $GITHUB_STEP_SUMMARY
            echo "- Submit PR to HDFGroup/hdf5 when ready" >> $GITHUB_STEP_SUMMARY
          else
            echo "- Artifacts are ready for official release" >> $GITHUB_STEP_SUMMARY
            echo "- Run the full release workflow to deploy to Maven Central" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Check overall status
        run: |
          BUILD_RESULT="${{ needs.build-maven-packages.result }}"
          DEPLOY_RESULT="${{ needs.deploy-to-packages.result }}"
          JNI_RESULT="${{ needs.test-jni-package.result }}"
          FFM_RESULT="${{ needs.test-ffm-package.result }}"

          if [ "$BUILD_RESULT" != "success" ]; then
            echo "âŒ Build failed"
            exit 1
          fi

          if [ "${{ inputs.test_deployment }}" == "true" ]; then
            if [ "$DEPLOY_RESULT" != "success" ] && [ "$DEPLOY_RESULT" != "skipped" ]; then
              echo "âŒ Deployment failed"
              exit 1
            fi

            if [ "$JNI_RESULT" != "success" ] && [ "$JNI_RESULT" != "skipped" ]; then
              echo "âŒ JNI testing failed"
              exit 1
            fi

            if [ "$FFM_RESULT" != "success" ] && [ "$FFM_RESULT" != "skipped" ]; then
              echo "âŒ FFM testing failed"
              exit 1
            fi
          fi

          echo "âœ… All tests passed successfully"
```

### `.github/workflows/maven-deploy.yml`

```yaml
name: Maven Repository Deployment

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      file_base:
        description: "The common base name of the source tarballs (legacy, not used)"
        required: false
        type: string
        default: ""
      preset_name:
        description: "The preset configuration name used for build (legacy, not used)"
        required: false
        type: string
        default: ""
      repository_url:
        description: 'Maven repository URL (GitHub Packages or Maven Central)'
        required: false
        type: string
        default: 'https://maven.pkg.github.com/HDFGroup/hdf5'
      repository_id:
        description: 'Maven repository ID for server configuration'
        required: false
        type: string
        default: 'github'
      deploy_snapshots:
        description: 'Deploy snapshot versions (-SNAPSHOT suffix)'
        required: false
        type: boolean
        default: false
      dry_run:
        description: 'Perform validation without actual deployment'
        required: false
        type: boolean
        default: false
    secrets:
      MAVEN_USERNAME:
        description: 'Maven repository username'
        required: true
      MAVEN_PASSWORD:
        description: 'Maven repository password/token'
        required: true
      GPG_PRIVATE_KEY:
        description: 'GPG private key for signing (Maven Central)'
        required: false
      GPG_PASSPHRASE:
        description: 'GPG passphrase for signing'
        required: false

permissions:
  contents: read
  packages: write

jobs:
  check-secret:
    name: Check Secrets exists
    runs-on: ubuntu-latest
    outputs:
      gpg-state: ${{ steps.set-gpg-state.outputs.HAVEGPG }}
    steps:
      - name: Identify GPG Status
        id: set-gpg-state
        env: 
            gpg_secret: ${{ secrets.GPG_PRIVATE_KEY }}
        run: |
          if [[ '${{ env.gpg_secret }}' == '' ]]
          then
            GPG_VAL=$(echo 'notexists')
          else
            GPG_VAL=$(echo 'exists')
          fi
          echo "HAVEGPG=$GPG_VAL" >> $GITHUB_OUTPUT
        shell: bash

      - run: echo "gpg key is ${{ steps.set-gpg-state.outputs.HAVEGPG }}."

  validate-artifacts:
    name: Validate Build Artifacts
    runs-on: ubuntu-latest
    outputs:
      hdf5-version: ${{ steps.extract-version.outputs.hdf5-version }}
      jar-files: ${{ steps.find-jars.outputs.jar-files }}
      pom-file: ${{ steps.find-pom.outputs.pom-file }}
      platform-classifier: ${{ steps.platform-info.outputs.classifier }}
    steps:
      - name: Download all Maven artifacts
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
          pattern: maven-staging-artifacts-*
          path: ./artifacts
          merge-multiple: false
        continue-on-error: true

      - name: List downloaded artifacts
        run: |
          echo "=== Downloaded Artifacts Structure ==="
          find ./artifacts -type d -maxdepth 2
          echo ""
          echo "=== JAR files found ==="
          find ./artifacts -name "*.jar" -type f

      - name: Find JAR files
        id: find-jars
        run: |
          # Find only main HDF5 JAR files across all artifact directories
          # Handles both single implementation and multiple implementation builds
          JAR_FILES=""
          echo "=== Scanning for main HDF5 JAR files ==="

          # Artifacts are in subdirectories like:
          # ./artifacts/maven-staging-artifacts-linux-x86_64-ffm/
          # ./artifacts/maven-staging-artifacts-linux-x86_64-jni/
          # ./artifacts/maven-staging-artifacts-linux-x86_64/  (if single implementation)

          for artifact_dir in ./artifacts/maven-staging-artifacts-*/; do
            if [ -d "$artifact_dir" ]; then
              artifact_name=$(basename "$artifact_dir")
              echo "Scanning artifact directory: $artifact_name"

              # Extract platform and implementation from directory name
              # Format: maven-staging-artifacts-{platform}-{implementation}
              # or:     maven-staging-artifacts-{platform}
              PLATFORM=""
              IMPLEMENTATION=""

              if [[ "$artifact_name" =~ maven-staging-artifacts-(.+)-(ffm|jni)$ ]]; then
                # Has implementation suffix
                PLATFORM="${BASH_REMATCH[1]}"
                IMPLEMENTATION="${BASH_REMATCH[2]}"
                echo "  Platform: $PLATFORM, Implementation: $IMPLEMENTATION"
              elif [[ "$artifact_name" =~ maven-staging-artifacts-(.+)$ ]]; then
                # No implementation suffix (single implementation build)
                PLATFORM="${BASH_REMATCH[1]}"
                IMPLEMENTATION="auto"
                echo "  Platform: $PLATFORM, Implementation: auto"
              fi

              # Find main HDF5 JAR files (jarhdf5-*.jar) and examples JAR files (hdf5-java-examples-*.jar)
              artifact_jars=$(find "$artifact_dir" \( -name "jarhdf5-*.jar" -o -name "hdf5-java-examples-*.jar" \) ! -name "*sources*" ! -name "*javadoc*" 2>/dev/null || true)

              if [ -n "$artifact_jars" ]; then
                echo "  Found HDF5 JARs:"
                echo "$artifact_jars" | while read jar; do echo "    - $(basename "$jar")"; done

                # Store JAR files with metadata (format: jar_path|platform|implementation)
                for jar in $artifact_jars; do
                  if [ -z "$JAR_FILES" ]; then
                    JAR_FILES="${jar}|${PLATFORM}|${IMPLEMENTATION}"
                  else
                    JAR_FILES="${JAR_FILES},${jar}|${PLATFORM}|${IMPLEMENTATION}"
                  fi
                done
              else
                echo "  No HDF5 JARs found"
              fi
            fi
          done

          echo "jar-files=${JAR_FILES}" >> $GITHUB_OUTPUT
          echo ""
          echo "=== Final JAR list for deployment ==="
          echo "$JAR_FILES" | tr ',' '\n' | while IFS='|' read jar_path platform impl; do
            if [ -n "$jar_path" ]; then
              echo "  - $(basename "$jar_path") [platform: $platform, implementation: $impl]"
            fi
          done

          if [ -z "${JAR_FILES}" ]; then
            echo "ERROR: No main HDF5 JAR files found in artifacts"
            echo "Available files:"
            find ./artifacts -name "*.jar" -o -name "*.xml" 2>/dev/null | head -20
            exit 1
          fi

      - name: Find POM file
        id: find-pom
        run: |
          POM_FILE=$(find ./artifacts -name "pom.xml" | head -1)
          echo "pom-file=${POM_FILE}" >> $GITHUB_OUTPUT
          echo "Found POM file: ${POM_FILE}"

          if [ -z "${POM_FILE}" ]; then
            echo "ERROR: No POM file found in artifacts"
            exit 1
          fi

      - name: Extract version information
        id: extract-version
        run: |
          # Extract version from POM file
          VERSION=$(grep -o '<version>[^<]*</version>' "${{ steps.find-pom.outputs.pom-file }}" | head -1 | sed 's/<[^>]*>//g')
          echo "hdf5-version=${VERSION}" >> $GITHUB_OUTPUT
          echo "Extracted HDF5 version: ${VERSION}"

      - name: Determine platform classifier
        id: platform-info
        run: |
          # Extract platform classifier from JAR filename
          JAR_FILE=$(echo "${{ steps.find-jars.outputs.jar-files }}" | cut -d',' -f1)
          CLASSIFIER=""

          if [[ "${JAR_FILE}" == *"linux"* ]]; then
            CLASSIFIER="linux-x86_64"
          elif [[ "${JAR_FILE}" == *"windows"* ]]; then
            CLASSIFIER="windows-x86_64"
          elif [[ "${JAR_FILE}" == *"macos"* ]]; then
            if [[ "${JAR_FILE}" == *"aarch64"* ]]; then
              CLASSIFIER="macos-aarch64"
            else
              CLASSIFIER="macos-x86_64"
            fi
          fi

          echo "classifier=${CLASSIFIER}" >> $GITHUB_OUTPUT
          echo "Platform classifier: ${CLASSIFIER}"

      - name: Quick artifact validation
        run: |
          echo "=== Quick Artifact Validation ==="

          # Count artifacts
          jar_count=$(echo "${{ steps.find-jars.outputs.jar-files }}" | tr ',' '\n' | wc -l)
          echo "Found ${jar_count} JAR file(s) for deployment"

          # Basic validation (artifacts should already be validated by staging workflow)
          # Parse format: jar_path|platform|implementation,jar_path|platform|implementation,...
          echo "${{ steps.find-jars.outputs.jar-files }}" | tr ',' '\n' | while IFS='|' read -r jar_file platform impl; do
            # Skip empty entries
            [ -z "$jar_file" ] && continue

            if [ ! -f "${jar_file}" ]; then
              echo "ERROR: JAR file not found: ${jar_file}"
              exit 1
            fi

            jar_name=$(basename "${jar_file}")
            echo "âœ“ [${platform}/${impl}] ${jar_name}"
          done

          # Quick POM validation
          if [ -f "${{ steps.find-pom.outputs.pom-file }}" ]; then
            echo "âœ“ POM file found: $(basename "${{ steps.find-pom.outputs.pom-file }}")"
          else
            echo "ERROR: POM file not found"
            exit 1
          fi

          echo "âœ“ Quick validation passed - artifacts ready for deployment"

  deploy-maven:
    name: Deploy to Maven Repository
    runs-on: ubuntu-latest
    needs: [check-secret, validate-artifacts]
    if: ${{ !inputs.dry_run }}
    steps:
      - name: Download all Maven artifacts
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
          pattern: maven-staging-artifacts-*
          path: ./artifacts
          merge-multiple: false

      - name: List downloaded artifacts for deployment
        run: |
          echo "=== Artifacts ready for deployment ==="
          find ./artifacts -name "*.jar" -type f | while read jar; do
            echo "  - $jar ($(du -h "$jar" | cut -f1))"
          done

      - name: Set up Java
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Create Maven settings.xml
        run: |
          mkdir -p ~/.m2
          cat > ~/.m2/settings.xml << 'EOF'
          <?xml version="1.0" encoding="UTF-8"?>
          <settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"
                    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                    xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0
                    https://maven.apache.org/xsd/settings-1.0.0.xsd">
            <servers>
              <server>
                <id>${{ inputs.repository_id }}</id>
                <username>${{ secrets.MAVEN_USERNAME }}</username>
                <password>${{ secrets.MAVEN_PASSWORD }}</password>
              </server>
            </servers>
          </settings>
          EOF

      - name: Import GPG key (if provided)
        if: ${{ needs.check-secret.outputs.gpg-state == 'exists' }}
        run: |
          echo "${{ secrets.GPG_PRIVATE_KEY }}" | gpg --batch --import
          echo "GPG key imported for artifact signing"

      - name: Deploy JAR artifacts
        env:
          HDF5_VERSION: ${{ needs.validate-artifacts.outputs.hdf5-version }}
          PLATFORM_CLASSIFIER: ${{ needs.validate-artifacts.outputs.platform-classifier }}
        run: |
          echo "=== Maven Deployment Debug Info ==="
          echo "Version: ${HDF5_VERSION}"
          echo "Repository: ${{ inputs.repository_url }}"
          echo "Repository ID: ${{ inputs.repository_id }}"
          echo "Platform Classifier: ${PLATFORM_CLASSIFIER}"
          echo "Dry Run: ${{ inputs.dry_run }}"
          echo "Deploy Snapshots: ${{ inputs.deploy_snapshots }}"
          echo "Username: ${{ secrets.MAVEN_USERNAME }}"
          echo "Password length: ${#MAVEN_PASSWORD} chars"
          echo "GPG Private Key available: ${{ secrets.GPG_PRIVATE_KEY != '' }}"

          # Check Maven configuration
          echo "=== Maven Configuration ==="
          mvn --version
          cat ~/.m2/settings.xml

          # Set GPG options if available
          GPG_OPTS=""
          if [ -n "${{ secrets.GPG_PRIVATE_KEY }}" ]; then
            GPG_OPTS="-Dgpg.passphrase=${{ secrets.GPG_PASSPHRASE }}"
            echo "GPG signing enabled"
          else
            echo "GPG signing disabled (no private key)"
          fi

          # Deploy each JAR file with proper artifact ID and platform classifier
          success_count=0
          total_count=0

          echo "=== Starting JAR Deployment ==="
          # Parse jar-files output: format is "jar_path|platform|implementation,jar_path|platform|implementation,..."
          echo "${{ needs.validate-artifacts.outputs.jar-files }}" | tr ',' '\n' | while IFS='|' read -r jar_file platform impl; do
            # Skip empty entries
            [ -z "$jar_file" ] && continue

            total_count=$((total_count + 1))
            jar_basename=$(basename "${jar_file}")

            echo "--- Processing JAR $total_count: $jar_basename ---"
            echo "Full path: $jar_file"
            echo "Platform: $platform"
            echo "Implementation: $impl"

            # Verify file exists
            if [ ! -f "$jar_file" ]; then
              echo "âŒ ERROR: JAR file does not exist: $jar_file"
              continue
            fi

            # Show file details
            echo "File size: $(du -h "$jar_file" | cut -f1)"
            echo "File permissions: $(ls -l "$jar_file")"

            # Determine artifact type and settings
            if [[ "${jar_basename}" == *"hdf5-java-examples"* ]]; then
              # Java Examples artifact (platform-independent)
              ARTIFACT_ID="hdf5-java-examples"
              CURRENT_CLASSIFIER=""  # Examples JAR has no platform classifier
              classifier_opts=""
              echo "Artifact type: Java Examples (platform-independent)"
            else
              # Main HDF5 Java library - determine implementation
              # Check JAR contents to detect FFM vs JNI
              if jar tf "$jar_file" | grep -q "org/hdfgroup/javahdf5/hdf5_h.class"; then
                DETECTED_IMPL="ffm"
              elif jar tf "$jar_file" | grep -q "hdf/hdf5lib/H5.class"; then
                DETECTED_IMPL="jni"
              else
                echo "âš ï¸  WARNING: Could not detect implementation from JAR contents, using metadata: $impl"
                DETECTED_IMPL="$impl"
              fi

              # Set artifact ID based on detected/provided implementation
              if [ "$DETECTED_IMPL" = "ffm" ]; then
                ARTIFACT_ID="hdf5-java-ffm"
                echo "Artifact type: HDF5 Java FFM Library"
              elif [ "$DETECTED_IMPL" = "jni" ]; then
                ARTIFACT_ID="hdf5-java-jni"
                echo "Artifact type: HDF5 Java JNI Library"
              else
                # Fallback for 'auto' or unknown - use generic name
                ARTIFACT_ID="hdf5-java"
                echo "Artifact type: HDF5 Java Library (auto)"
              fi

              # Set platform classifier (always present for main library)
              CURRENT_CLASSIFIER="$platform"

              # Determine classifier options for main library
              classifier_opts=""
              if [ -n "${CURRENT_CLASSIFIER}" ] && [[ "${jar_basename}" != *"sources"* ]] && [[ "${jar_basename}" != *"javadoc"* ]]; then
                classifier_opts="-Dclassifier=${CURRENT_CLASSIFIER}"
                echo "Platform classifier: ${CURRENT_CLASSIFIER}"
              fi
            fi

            # Check if this is a dry run
            if [ "${{ inputs.dry_run }}" == "true" ]; then
              echo "ðŸ§ª DRY RUN: Would deploy ${jar_basename} as ${ARTIFACT_ID} with classifier ${CURRENT_CLASSIFIER:-none}"
              echo "Command would be: mvn deploy:deploy-file -DgroupId=org.hdfgroup -DartifactId=${ARTIFACT_ID} -Dversion=${HDF5_VERSION} -Dfile=${jar_file} ${classifier_opts}"
              success_count=$((success_count + 1))
            else
              echo "ðŸš€ Deploying ${jar_basename}..."

              # Deploy with Maven (with verbose output for debugging)
              deploy_cmd="mvn deploy:deploy-file \
                -DgroupId=org.hdfgroup \
                -DartifactId=\"${ARTIFACT_ID}\" \
                -Dversion=\"${HDF5_VERSION}\" \
                -Dfile=\"${jar_file}\" \
                -DpomFile=\"${{ needs.validate-artifacts.outputs.pom-file }}\" \
                -DrepositoryId=\"${{ inputs.repository_id }}\" \
                -Durl=\"${{ inputs.repository_url }}\" \
                ${classifier_opts} \
                ${GPG_OPTS} \
                -B -X"

              echo "Command: $deploy_cmd"

              if eval $deploy_cmd; then
                echo "âœ“ Successfully deployed: ${jar_basename}"
                success_count=$((success_count + 1))
              else
                deploy_exit_code=$?
                echo "âœ— Failed to deploy: ${jar_basename} (exit code: $deploy_exit_code)"

                # Try to get more specific error information
                echo "=== Debugging deployment failure ==="
                echo "Testing repository connectivity..."
                curl -I "${{ inputs.repository_url }}" || echo "Repository not accessible via curl"

                echo "Testing authentication..."
                curl -u "${{ secrets.MAVEN_USERNAME }}:${{ secrets.MAVEN_PASSWORD }}" \
                     -I "${{ inputs.repository_url }}" || echo "Authentication test failed"
              fi
            fi
          done

          echo "=== Deployment Summary ==="
          echo "Successful deployments: ${success_count}/${total_count}"

          if [ ${success_count} -eq ${total_count} ]; then
            echo "ðŸŽ‰ All artifacts deployed successfully!"
          else
            echo "âŒ Some deployments failed"
            exit 1
          fi

      - name: Verify deployment
        env:
          HDF5_VERSION: ${{ needs.validate-artifacts.outputs.hdf5-version }}
        run: |
          echo "=== Deployment Verification ==="

          # Wait for repository to process
          sleep 10

          # For GitHub Packages, we can verify using the API
          if [[ "${{ inputs.repository_url }}" == *"maven.pkg.github.com"* ]]; then
            echo "Verifying GitHub Packages deployment..."

            # Extract owner/repo from URL
            REPO_PATH=$(echo "${{ inputs.repository_url }}" | sed 's|.*maven.pkg.github.com/||')

            curl -s -H "Authorization: token ${{ secrets.MAVEN_PASSWORD }}" \
              "https://api.github.com/users/HDFGroup/packages?package_type=maven" | \
              grep -q "hdf5-java" && echo "âœ“ Package verified in GitHub Packages" || echo "âš  Package verification pending"
          fi

          echo "Deployment verification completed"

  create-release-notes:
    name: Create Maven Release Notes
    runs-on: ubuntu-latest
    needs: [validate-artifacts, deploy-maven]
    if: ${{ always() && needs.validate-artifacts.result == 'success' }}
    steps:
      - name: Generate deployment summary
        run: |
          cat > maven-deployment-summary.md << 'EOF'
          # Maven Deployment Summary

          **Version**: ${{ needs.validate-artifacts.outputs.hdf5-version }}
          **Repository**: ${{ inputs.repository_url }}
          **Deployment Status**: ${{ needs.deploy-maven.result || 'skipped (dry-run)' }}

          ## HDF5 Java Library

          ```xml
          <dependency>
              <groupId>org.hdfgroup</groupId>
              <artifactId>hdf5-java</artifactId>
              <version>${{ needs.validate-artifacts.outputs.hdf5-version }}</version>
              <classifier>linux-x86_64</classifier> <!-- or windows-x86_64, macos-x86_64, macos-aarch64 -->
          </dependency>
          ```

          ## HDF5 Java Examples

          ```xml
          <dependency>
              <groupId>org.hdfgroup</groupId>
              <artifactId>hdf5-java-examples</artifactId>
              <version>${{ needs.validate-artifacts.outputs.hdf5-version }}</version>
          </dependency>
          ```

          ## Gradle Dependencies

          ```kotlin
          // HDF5 Java Library
          implementation("org.hdfgroup:hdf5-java:${{ needs.validate-artifacts.outputs.hdf5-version }}:linux-x86_64")

          // HDF5 Java Examples (62 examples)
          implementation("org.hdfgroup:hdf5-java-examples:${{ needs.validate-artifacts.outputs.hdf5-version }}")
          ```

          ## Available Packages
          - **hdf5-java**: Platform-specific HDF5 Java bindings (4 platform variants)
          - **hdf5-java-examples**: 62 Java examples (platform-independent)
          EOF

          echo "Maven deployment summary created"

      - name: Upload deployment summary
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: maven-deployment-summary
          path: maven-deployment-summary.md
          retention-days: 30
```

### `.github/workflows/maven-staging.yml`

```yaml
name: Maven Staging Repository Test

# Triggers on pull requests that modify Maven-related files
on:
  pull_request:
    branches: [ develop, main ]
    paths:
      - 'java/src/hdf/hdf5lib/**'
      - 'HDF5Examples/JAVA/**'
      - '.github/workflows/maven-*.yml'
      - '.github/workflows/java-examples-*.yml'
      - 'CMakePresets.json'
      - '**/CMakeLists.txt'
      - 'java/src/hdf/hdf5lib/pom.xml.in'
      - 'HDF5Examples/JAVA/pom-examples.xml.in'
  workflow_call:
    inputs:
      test_maven_deployment:
        description: 'Test Maven deployment to staging'
        type: boolean
        required: false
        default: true
      use_snapshot_version:
        description: 'Use snapshot version (-SNAPSHOT suffix)'
        type: boolean
        required: false
        default: true
      platforms:
        description: 'Build platforms for Maven artifacts'
        type: string
        required: false
        default: 'all-platforms'
      java_implementation:
        description: 'Java implementation to test'
        type: string
        required: false
        default: 'both'
    outputs:
      version:
        description: 'Maven artifact version that was built'
        value: ${{ jobs.test-maven-deployment.outputs.version }}
  workflow_dispatch:
    inputs:
      test_maven_deployment:
        description: 'Test Maven deployment to staging'
        type: boolean
        required: false
        default: true
      use_snapshot_version:
        description: 'Use snapshot version (-SNAPSHOT suffix)'
        type: boolean
        required: false
        default: true
      platforms:
        description: 'Build platforms for Maven artifacts'
        type: choice
        required: false
        default: 'all-platforms'
        options:
          - 'linux-only'
          - 'linux-windows'
          - 'linux-macos'
          - 'all-platforms'
      java_implementation:
        description: 'Java implementation to test'
        type: choice
        required: false
        default: 'both'
        options:
          - 'both'
          - 'ffm'
          - 'jni'
          - 'auto'

permissions:
  contents: read
  packages: write
  pull-requests: write

jobs:
  detect-changes:
    name: Detect Maven-related Changes
    runs-on: ubuntu-latest
    outputs:
      maven-changes: ${{ steps.changes.outputs.maven }}
      should-test: ${{ steps.should-test.outputs.result }}
    steps:
      - name: Checkout code
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          fetch-depth: 0

      - name: Detect changes in Maven-related files
        id: changes
        run: |
          # Check if this is a manual trigger
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "maven=true" >> $GITHUB_OUTPUT
            echo "Manual workflow dispatch - Maven testing enabled"
            exit 0
          fi

          # For push events, check the last commit
          if [ "${{ github.event_name }}" == "push" ]; then
            echo "maven=true" >> $GITHUB_OUTPUT
            echo "Push event - Maven testing enabled"
            exit 0
          fi

          # For pull requests, check changed files
          if [ -n "${{ github.base_ref }}" ]; then
            git fetch origin ${{ github.base_ref }}
            MAVEN_FILES=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | grep -E "(java/src/hdf/hdf5lib/|HDF5Examples/JAVA/|maven|pom\.xml|CMakePresets\.json)" || true)
          else
            # Fallback: assume changes if base_ref not available
            MAVEN_FILES="true"
          fi

          if [ -n "$MAVEN_FILES" ]; then
            echo "maven=true" >> $GITHUB_OUTPUT
            echo "Maven-related changes detected"
          else
            echo "maven=false" >> $GITHUB_OUTPUT
            echo "No Maven-related changes detected"
          fi

      - name: Determine if Maven testing should run
        id: should-test
        run: |
          if [ "${{ steps.changes.outputs.maven }}" == "true" ] || [ "${{ inputs.test_maven_deployment }}" == "true" ]; then
            echo "result=true" >> $GITHUB_OUTPUT
            echo "Maven testing will be performed"
          else
            echo "result=false" >> $GITHUB_OUTPUT
            echo "Maven testing will be skipped"
          fi

  determine-matrix:
    name: Determine Build Matrix
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.should-test == 'true' }}
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Determine build matrix
        id: set-matrix
        run: |
          JAVA_IMPL="${{ inputs.java_implementation || 'auto' }}"
          PLATFORMS="${{ inputs.platforms || 'all-platforms' }}"

          # Determine which implementations to build
          case "$JAVA_IMPL" in
            "ffm")
              IMPLEMENTATIONS='["ffm"]'
              ;;
            "jni")
              IMPLEMENTATIONS='["jni"]'
              ;;
            "both")
              IMPLEMENTATIONS='["ffm", "jni"]'
              ;;
            "auto"|*)
              # Auto means build based on Java version (defaults handled by presets)
              # We'll build once with auto setting
              IMPLEMENTATIONS='["auto"]'
              ;;
          esac

          # Determine which platforms to build
          case "$PLATFORMS" in
            "linux-only")
              PLATFORM_LIST='[{"name": "Linux", "os": "ubuntu-latest", "compiler": "GNUC", "arch": "x86_64"}]'
              ;;
            "linux-windows")
              PLATFORM_LIST='[{"name": "Linux", "os": "ubuntu-latest", "compiler": "GNUC", "arch": "x86_64"}, {"name": "Windows", "os": "windows-latest", "compiler": "MSVC", "arch": "x86_64"}]'
              ;;
            "linux-macos")
              PLATFORM_LIST='[{"name": "Linux", "os": "ubuntu-latest", "compiler": "GNUC", "arch": "x86_64"}, {"name": "macOS-x86_64", "os": "macos-14", "compiler": "Clang", "arch": "x86_64"}, {"name": "macOS-aarch64", "os": "macos-latest", "compiler": "Clang", "arch": "aarch64"}]'
              ;;
            "all-platforms"|*)
              PLATFORM_LIST='[{"name": "Linux", "os": "ubuntu-latest", "compiler": "GNUC", "arch": "x86_64"}, {"name": "Windows", "os": "windows-latest", "compiler": "MSVC", "arch": "x86_64"}, {"name": "macOS-x86_64", "os": "macos-14", "compiler": "Clang", "arch": "x86_64"}, {"name": "macOS-aarch64", "os": "macos-latest", "compiler": "Clang", "arch": "aarch64"}]'
              ;;
          esac

          # Create the full matrix combining platforms and implementations
          MATRIX="{\"include\":["
          FIRST=true
          for platform in $(echo "$PLATFORM_LIST" | jq -c '.[]'); do
            for impl in $(echo "$IMPLEMENTATIONS" | jq -r '.[]'); do
              if [ "$FIRST" = true ]; then
                FIRST=false
              else
                MATRIX+=","
              fi

              # Extract platform details
              PLATFORM_NAME=$(echo "$platform" | jq -r '.name')
              PLATFORM_OS=$(echo "$platform" | jq -r '.os')
              PLATFORM_COMPILER=$(echo "$platform" | jq -r '.compiler')
              PLATFORM_ARCH=$(echo "$platform" | jq -r '.arch')

              # Create artifact name suffix
              # Platform name already includes arch for macOS (e.g., macOS-x86_64)
              PLATFORM_LOWER="${PLATFORM_NAME,,}"
              if [[ "$PLATFORM_LOWER" == *"-"* ]]; then
                # Platform name already has arch suffix (macOS case)
                BASE_NAME="$PLATFORM_LOWER"
              else
                # Add arch suffix (Linux, Windows case)
                BASE_NAME="${PLATFORM_LOWER}-${PLATFORM_ARCH}"
              fi

              if [ "$impl" = "auto" ]; then
                ARTIFACT_SUFFIX="$BASE_NAME"
              else
                ARTIFACT_SUFFIX="${BASE_NAME}-${impl}"
              fi

              MATRIX+="{\"platform\":\"$PLATFORM_NAME\",\"os\":\"$PLATFORM_OS\",\"compiler\":\"$PLATFORM_COMPILER\",\"arch\":\"$PLATFORM_ARCH\",\"implementation\":\"$impl\",\"artifact-suffix\":\"$ARTIFACT_SUFFIX\"}"
            done
          done
          MATRIX+="]}"

          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
          echo "Generated matrix:"
          echo "$MATRIX" | jq '.'

  build-maven-artifacts:
    name: "Build Maven Artifacts (${{ matrix.platform }} - ${{ matrix.implementation }})"
    needs: [detect-changes, determine-matrix]
    if: ${{ needs.detect-changes.outputs.should-test == 'true' }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.determine-matrix.outputs.matrix) }}
    steps:
      - name: Install Dependencies (Linux)
        if: ${{ runner.os == 'Linux' }}
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz

      - name: Install Dependencies (macOS)
        if: ${{ runner.os == 'macOS' }}
        run: |
          brew install ninja doxygen graphviz

      - name: Install Dependencies (Windows)
        if: ${{ runner.os == 'Windows' }}
        run: |
          choco install ninja
          choco install doxygen.install
          choco install graphviz

      - name: Enable Developer Command Prompt (Windows)
        if: ${{ runner.os == 'Windows' }}
        uses: ilammy/msvc-dev-cmd@0b201ec74fa43914dc39ae48a89fd1d8cb592756 # v1.13.0

      - name: Set up JDK (Java 25 for FFM builds)
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: ${{ matrix.implementation == 'ffm' && '25' || '21' }}
          distribution: ${{ matrix.implementation == 'ffm' && 'oracle' || 'temurin' }}

      - name: Checkout code
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Setup jextract (FFM builds only)
        if: ${{ matrix.implementation == 'ffm' }}
        uses: ./.github/actions/setup-jextract
        with:
          java-version: '25'

      - name: Set preset name
        id: set-preset
        shell: bash
        run: |
          # Determine implementation suffix for preset based on matrix
          JAVA_IMPL="${{ matrix.implementation }}"
          case "$JAVA_IMPL" in
            "ffm")
              IMPL_SUFFIX="-FFM"
              ;;
            "jni"|"auto"|*)
              # JNI and auto use default Maven presets (JNI is default as of HDF5 2.0)
              IMPL_SUFFIX=""
              ;;
          esac

          # Determine snapshot setting
          SNAPSHOT_SUFFIX=""
          if [ "${{ inputs.use_snapshot_version }}" == "true" ] || [ "${{ github.event_name }}" == "pull_request" ]; then
            SNAPSHOT_SUFFIX="-Snapshot"
          fi

          # Build preset name: ci-MinShar-{COMPILER}-Maven{-FFM}{-Snapshot}
          # Note: Generic Maven presets default to JNI (no suffix needed)
          PRESET_NAME="ci-MinShar-${{ matrix.compiler }}-Maven${IMPL_SUFFIX}${SNAPSHOT_SUFFIX}"

          echo "preset=$PRESET_NAME" >> $GITHUB_OUTPUT
          echo "Using preset: $PRESET_NAME"
          echo "Platform: ${{ matrix.platform }}, Compiler: ${{ matrix.compiler }}, Implementation: $JAVA_IMPL"

      - name: Build HDF5 with Maven support
        id: buildhdf5
        shell: bash
        run: |
          # macOS: Disable Spotlight indexing to prevent file locking during CPack
          if [[ "$RUNNER_OS" == "macOS" ]]; then
            echo "Disabling Spotlight indexing temporarily"
            sudo mdutil -i off / || true
          fi

          cd "${{ github.workspace }}"

          # Run workflow with retry logic for macOS CPack failures
          if [[ "$RUNNER_OS" == "macOS" ]]; then
            echo "Running CMake workflow with macOS retry logic..."
            if ! cmake --workflow --preset="${{ steps.set-preset.outputs.preset }}" --fresh; then
              echo "âš ï¸ First attempt failed, retrying in 5 seconds..."
              sleep 5
              cmake --workflow --preset="${{ steps.set-preset.outputs.preset }}" --fresh
            fi
          else
            # Non-macOS: run normally without retry
            cmake --workflow --preset="${{ steps.set-preset.outputs.preset }}" --fresh
          fi

      - name: Extract version information
        id: version-info
        shell: bash
        run: |
          # Find the generated POM file across all possible locations
          BUILD_ROOT="${{ runner.workspace }}/build/${{ steps.set-preset.outputs.preset }}"
          echo "Looking for POM file in: $BUILD_ROOT"

          # Try multiple search patterns for cross-platform compatibility
          POM_FILE=""

          # Search patterns in order of preference
          if [ -z "$POM_FILE" ]; then
            POM_FILE=$(find "$BUILD_ROOT" -name "pom.xml" -path "*/java/*" 2>/dev/null | head -1)
          fi

          if [ -z "$POM_FILE" ]; then
            POM_FILE=$(find "$BUILD_ROOT" -name "pom.xml" 2>/dev/null | head -1)
          fi

          # Try Maven artifacts directory if build structure differs
          if [ -z "$POM_FILE" ] && [ -d "${{ runner.workspace }}/maven-artifacts" ]; then
            POM_FILE=$(find "${{ runner.workspace }}/maven-artifacts" -name "pom.xml" 2>/dev/null | head -1)
          fi

          if [ -n "$POM_FILE" ] && [ -f "$POM_FILE" ]; then
            # Extract version using robust pattern that works across platforms
            VERSION=$(grep -o '<version>[^<]*</version>' "$POM_FILE" | head -1 | sed 's/<[^>]*>//g' | tr -d '\r\n')

            # Validate version format
            if [[ "$VERSION" =~ ^[0-9]+\.[0-9]+\.[0-9]+.*$ ]]; then
              echo "version=$VERSION" >> $GITHUB_OUTPUT
              echo "âœ“ Detected version: $VERSION"
              echo "âœ“ POM file: $POM_FILE"
            else
              echo "version=unknown" >> $GITHUB_OUTPUT
              echo "âŒ Invalid version format detected: $VERSION"
              exit 1
            fi
          else
            echo "version=unknown" >> $GITHUB_OUTPUT
            echo "âŒ Could not find POM file"
            echo "Available files in build root:"
            find "$BUILD_ROOT" -name "*.xml" -o -name "pom*" 2>/dev/null | head -10 || echo "No XML files found"
            exit 1
          fi

      - name: Collect Maven artifacts
        shell: bash
        run: |
          echo "Collecting Maven artifacts for testing..."
          mkdir -p "${{ runner.workspace }}/maven-artifacts"

          BUILD_ROOT="${{ runner.workspace }}/build/${{ steps.set-preset.outputs.preset }}"
          echo "Looking for Maven artifacts in build root: $BUILD_ROOT"

          if [ ! -d "$BUILD_ROOT" ]; then
            echo "ERROR: Build root directory does not exist: $BUILD_ROOT"
            exit 1
          fi

          # Debug: Show what's in the build directory
          echo "Build directory contents:"
          find "$BUILD_ROOT" -maxdepth 3 -type d 2>/dev/null | head -20 || echo "Directory listing completed"

          # Find build directory (try multiple patterns)
          BUILD_DIR=$(find "$BUILD_ROOT" -name "*Maven*" -type d 2>/dev/null | head -1)

          if [ -z "$BUILD_DIR" ]; then
            # Try looking for java directory as fallback (more specific patterns)
            BUILD_DIR=$(find "$BUILD_ROOT" -path "*/java/*" -type d 2>/dev/null | head -1)
            if [ -z "$BUILD_DIR" ]; then
              BUILD_DIR=$(find "$BUILD_ROOT" -name "java" -type d 2>/dev/null | head -1)
            fi
          fi

          if [ -z "$BUILD_DIR" ]; then
            # Try looking for any directory with JAR files
            JAR_FILE=$(find "$BUILD_ROOT" -name "*.jar" -type f 2>/dev/null | head -1)
            if [ -n "$JAR_FILE" ]; then
              BUILD_DIR=$(dirname "$JAR_FILE")
              echo "Found JAR file in: $BUILD_DIR"
            fi
          fi

          if [ -z "$BUILD_DIR" ]; then
            # Last resort: look for the build directory itself if it contains artifacts
            if find "$BUILD_ROOT" -name "*.jar" -o -name "pom.xml" | grep -q .; then
              BUILD_DIR="$BUILD_ROOT"
              echo "Using build root as artifacts directory"
            fi
          fi

          if [ -z "$BUILD_DIR" ]; then
            echo "ERROR: Could not find Maven build directory with any of the patterns"
            echo "Available directories:"
            find "$BUILD_ROOT" -maxdepth 2 -type d
            exit 1
          fi

          echo "Looking for artifacts in: $BUILD_DIR"

          # Debug: Show all JAR files in build directory
          echo "All JAR files in build directory:"
          find "$BUILD_DIR" -name "*.jar" -type f 2>/dev/null | head -20 || true

          # Debug: Show specifically what we're looking for
          echo "SLF4J JAR files in build directory:"
          find "$BUILD_DIR" -name "*slf4j*.jar" -type f

          # Copy JAR files (excluding test and H5Ex_ example JARs)
          find "$BUILD_DIR" -name "*.jar" -not -name "*test*" -not -name "*H5Ex_*" -exec cp {} "${{ runner.workspace }}/maven-artifacts/" \;

          # Also look for Maven dependencies in common locations
          echo "Looking for Maven dependencies in additional locations..."

          # Check if there's a Maven repository in the build area
          if [ -d "$BUILD_ROOT" ]; then
            find "$BUILD_ROOT" -name "*slf4j*.jar" -type f -exec cp {} "${{ runner.workspace }}/maven-artifacts/" \;
          fi

          # Check common Maven local repository locations
          MAVEN_REPO_PATHS=(
            "$HOME/.m2/repository"
            "$BUILD_ROOT/.m2/repository"
            "${{ runner.workspace }}/.m2/repository"
          )

          for repo_path in "${MAVEN_REPO_PATHS[@]}"; do
            if [ -d "$repo_path" ]; then
              echo "Checking Maven repository: $repo_path"
              find "$repo_path" -name "slf4j-api*.jar" -o -name "slf4j-simple*.jar" 2>/dev/null | head -2 | while read jar_file; do
                if [ -f "$jar_file" ]; then
                  echo "Found dependency: $jar_file"
                  cp "$jar_file" "${{ runner.workspace }}/maven-artifacts/"
                fi
              done
            fi
          done

          # Copy POM files
          find "$BUILD_DIR" -name "pom.xml" -exec cp {} "${{ runner.workspace }}/maven-artifacts/" \;

          # List collected artifacts
          echo "Collected Maven artifacts:"
          ls -la "${{ runner.workspace }}/maven-artifacts/"

      - name: Validate artifacts
        id: artifacts-check
        shell: bash
        run: |
          ARTIFACT_COUNT=$(find "${{ runner.workspace }}/maven-artifacts" -name "*.jar" | wc -l)
          POM_COUNT=$(find "${{ runner.workspace }}/maven-artifacts" -name "pom.xml" | wc -l)

          echo "Found $ARTIFACT_COUNT JAR files and $POM_COUNT POM files"

          if [ $ARTIFACT_COUNT -gt 0 ] && [ $POM_COUNT -gt 0 ]; then
            echo "created=true" >> $GITHUB_OUTPUT
            echo "âœ… Artifacts successfully created"
          else
            echo "created=false" >> $GITHUB_OUTPUT
            echo "âŒ Artifact creation failed"
            exit 1
          fi

      - name: Verify JAR contents (CRITICAL - Prevents Incomplete Packages)
        shell: bash
        run: |
          echo "::group::Verify JAR contents"
          echo "============================================"
          echo "Verifying Maven artifacts contain HDF5 classes"
          echo "============================================"

          ARTIFACT_DIR="${{ runner.workspace }}/maven-artifacts"
          VERIFICATION_FAILED=false
          IMPL="${{ matrix.implementation }}"

          # Find the main HDF5 JAR (not slf4j dependencies)
          HDF5_JAR=$(find "$ARTIFACT_DIR" -name "*hdf5*.jar" ! -name "*sources*" ! -name "*javadoc*" ! -name "*slf4j*" | head -1)

          if [ -z "$HDF5_JAR" ]; then
            # Try alternative naming (jarhdf5)
            HDF5_JAR=$(find "$ARTIFACT_DIR" -name "jarhdf5*.jar" ! -name "*sources*" ! -name "*javadoc*" | head -1)
          fi

          if [ -z "$HDF5_JAR" ]; then
            echo "::error::Could not find HDF5 JAR file in artifacts"
            echo "Available JAR files:"
            find "$ARTIFACT_DIR" -name "*.jar" -ls
            exit 1
          fi

          echo "Found HDF5 JAR: $HDF5_JAR"
          echo "JAR size: $(du -h "$HDF5_JAR" | cut -f1)"
          echo ""

          # Determine implementation type (FFM or JNI)
          if [ "$IMPL" = "ffm" ]; then
            echo "Verifying FFM implementation..."
            echo "Expected package: org.hdfgroup.javahdf5.*"

            # Check for key FFM classes
            if jar tf "$HDF5_JAR" | grep -q "org/hdfgroup/javahdf5/hdf5_h.class"; then
              CLASS_COUNT=$(jar tf "$HDF5_JAR" | grep "org/hdfgroup/javahdf5.*\.class" | wc -l)
              echo "âœ… JAR contains HDF5 FFM classes"
              echo "   Found $CLASS_COUNT FFM classes"

              # List some sample classes to verify content
              echo "   Sample classes:"
              jar tf "$HDF5_JAR" | grep "org/hdfgroup/javahdf5.*\.class" | head -5 2>/dev/null | sed 's/^/   - /' || true
            else
              echo "::error::JAR does not contain HDF5 FFM classes!"
              echo "JAR only contains:"
              jar tf "$HDF5_JAR" | head -30 2>/dev/null || true
              VERIFICATION_FAILED=true
            fi

          else
            # JNI or auto (defaults to JNI)
            echo "Verifying JNI implementation..."
            echo "Expected package: hdf.hdf5lib.*"

            # Check for key JNI classes
            if jar tf "$HDF5_JAR" | grep -q "hdf/hdf5lib/H5.class"; then
              CLASS_COUNT=$(jar tf "$HDF5_JAR" | grep "hdf/hdf5lib.*\.class" | wc -l)
              echo "âœ… JAR contains HDF5 JNI classes"
              echo "   Found $CLASS_COUNT JNI classes"

              # Verify key classes are present
              MISSING_CLASSES=""
              for key_class in "hdf/hdf5lib/H5.class" "hdf/hdf5lib/HDF5Constants.class" "hdf/hdf5lib/HDFNativeData.class"; do
                if ! jar tf "$HDF5_JAR" | grep -q "$key_class"; then
                  MISSING_CLASSES="$MISSING_CLASSES $key_class"
                fi
              done

              if [ -n "$MISSING_CLASSES" ]; then
                echo "::warning::Some expected classes are missing:$MISSING_CLASSES"
              fi

              # List some sample classes to verify content
              echo "   Sample classes:"
              jar tf "$HDF5_JAR" | grep "hdf/hdf5lib.*\.class" | head -5 2>/dev/null | sed 's/^/   - /' || true
            else
              echo "::error::JAR does not contain HDF5 JNI classes!"
              echo "JAR only contains:"
              jar tf "$HDF5_JAR" | head -30 2>/dev/null || true
              VERIFICATION_FAILED=true
            fi
          fi

          # Additional checks
          echo ""
          echo "Additional JAR checks:"

          # Check JAR size (should be > 50KB for real HDF5 classes)
          JAR_SIZE=$(stat -f%z "$HDF5_JAR" 2>/dev/null || stat -c%s "$HDF5_JAR" 2>/dev/null || echo "0")
          if [ "$JAR_SIZE" -lt 51200 ]; then
            echo "::warning::JAR size is suspiciously small: $JAR_SIZE bytes (expected > 50KB)"
            echo "   This may indicate the JAR only contains dependencies."
          else
            echo "âœ… JAR size is reasonable: $(($JAR_SIZE / 1024)) KB"
          fi

          # Count total files in JAR
          TOTAL_FILES=$(jar tf "$HDF5_JAR" | wc -l)
          echo "âœ… Total files in JAR: $TOTAL_FILES"

          echo "::endgroup::"

          if [ "$VERIFICATION_FAILED" = true ]; then
            echo ""
            echo "::error::âŒ JAR verification failed!"
            echo "::error::The Maven artifact does not contain HDF5 classes."
            echo "::error::This would result in an incomplete/broken package being deployed."
            echo "::error::"
            echo "::error::Possible causes:"
            echo "::error::  1. CMake add_jar not executing correctly"
            echo "::error::  2. Wrong JAR file being collected"
            echo "::error::  3. Build failure that went undetected"
            echo "::error::"
            echo "::error::Please review the build logs and CMake configuration."
            exit 1
          fi

          echo ""
          echo "âœ… JAR verification passed - artifact is complete!"

      - name: Run validation script
        shell: bash
        run: |
          if [ -f .github/scripts/validate-maven-artifacts.sh ]; then
            echo "Running Maven artifact validation..."
            .github/scripts/validate-maven-artifacts.sh "${{ runner.workspace }}/maven-artifacts"
          else
            echo "Validation script not found - skipping validation"
          fi

      - name: Install HDF5 binaries for testing
        shell: bash
        run: |
          BUILD_ROOT="${{ runner.workspace }}/build/${{ steps.set-preset.outputs.preset }}"
          INSTALL_DIR="${{ runner.workspace }}/hdf5-install"

          echo "Installing HDF5 binaries to: $INSTALL_DIR"
          cd "$BUILD_ROOT"
          cmake --install . --prefix "$INSTALL_DIR"

          echo "Installation contents:"
          ls -la "$INSTALL_DIR"
          if [ -d "$INSTALL_DIR/lib" ]; then
            echo "Libraries:"
            ls -la "$INSTALL_DIR/lib" | grep -E '\.so|\.dylib|\.dll' || echo "No shared libraries found"
          fi

      - name: Upload Maven artifacts
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: maven-staging-artifacts-${{ matrix.artifact-suffix }}
          path: ${{ runner.workspace }}/maven-artifacts
          retention-days: 7

      - name: Upload HDF5 installation for testing
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: hdf5-install-${{ matrix.artifact-suffix }}
          path: ${{ runner.workspace }}/hdf5-install
          retention-days: 7

  test-maven-deployment:
    name: Test Maven Deployment
    runs-on: ubuntu-latest
    needs: [detect-changes, determine-matrix, build-maven-artifacts]
    if: ${{ always() && needs.detect-changes.outputs.should-test == 'true' }}
    outputs:
      version: ${{ steps.extract-version.outputs.version }}
    steps:
      - name: Set up JDK 21
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '21'
          distribution: 'temurin'

      - name: Download all Maven artifacts
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
          pattern: maven-staging-artifacts-*
          path: ./artifacts
          merge-multiple: false

      - name: Checkout code for version extraction
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          path: source

      - name: Extract version for testing
        id: extract-version
        run: |
          # Extract version from first available POM file
          POM_FILE=$(find ./artifacts -name "pom.xml" 2>/dev/null | head -1)
          if [ -n "$POM_FILE" ]; then
            VERSION=$(grep -o '<version>[^<]*</version>' "$POM_FILE" | head -1 | sed 's/<[^>]*>//g')
            echo "version=$VERSION" >> $GITHUB_OUTPUT
            echo "âœ… Extracted version from POM: $VERSION"
          else
            # Fallback: Extract version from H5public.h
            echo "âš ï¸ No POM file found, extracting version from source..."
            if [ -f "source/src/H5public.h" ]; then
              H5_MAJOR=$(grep '#define H5_VERS_MAJOR' source/src/H5public.h | awk '{print $3}')
              H5_MINOR=$(grep '#define H5_VERS_MINOR' source/src/H5public.h | awk '{print $3}')
              H5_RELEASE=$(grep '#define H5_VERS_RELEASE' source/src/H5public.h | awk '{print $3}')

              # Determine if this should be a SNAPSHOT version
              if [ "${{ inputs.use_snapshot_version }}" == "true" ] || [ "${{ github.event_name }}" == "pull_request" ]; then
                VERSION="${H5_MAJOR}.${H5_MINOR}.${H5_RELEASE}-SNAPSHOT"
              else
                VERSION="${H5_MAJOR}.${H5_MINOR}.${H5_RELEASE}"
              fi

              echo "version=$VERSION" >> $GITHUB_OUTPUT
              echo "âœ… Extracted version from H5public.h: $VERSION"
            else
              echo "::error::Could not extract version from POM or source files"
              exit 1
            fi
          fi

      - name: Test Maven deployment (dry run)
        run: |
          echo "=== Maven Deployment Test (Dry Run) ==="
          echo "Version: ${{ steps.extract-version.outputs.version }}"

          echo "Repository: GitHub Packages (staging)"

          # List artifacts to be deployed by platform
          echo "Artifacts ready for deployment:"
          for platform_dir in ./artifacts/*/; do
            if [ -d "$platform_dir" ]; then
              platform_name=$(basename "$platform_dir")
              echo "Platform: $platform_name"
              find "$platform_dir" -name "*.jar" -exec basename {} \; | sed 's/^/  - /'
            fi
          done

          # Create test Maven settings
          mkdir -p ~/.m2
          cat > ~/.m2/settings.xml << 'EOF'
          <?xml version="1.0" encoding="UTF-8"?>
          <settings xmlns="http://maven.apache.org/SETTINGS/1.0.0">
            <servers>
              <server>
                <id>github</id>
                <username>${env.GITHUB_ACTOR}</username>
                <password>${env.GITHUB_TOKEN}</password>
              </server>
            </servers>
          </settings>
          EOF

          # Simulate deployment validation for each platform
          total_jars=0
          valid_jars=0

          for jar_file in $(find ./artifacts -name "*.jar"); do
            jar_name=$(basename "$jar_file")
            platform=$(dirname "$jar_file" | sed 's|./artifacts/||')
            total_jars=$((total_jars + 1))

            echo "[$platform] Testing: $jar_name"

            # Test JAR integrity
            if jar tf "$jar_file" > /dev/null 2>&1; then
              echo "  âœ“ JAR integrity verified"
              valid_jars=$((valid_jars + 1))
            else
              echo "  âŒ JAR integrity check failed"
            fi
          done

          echo "Validation summary: $valid_jars/$total_jars JARs passed integrity check"

          if [ $valid_jars -ne $total_jars ]; then
            echo "âŒ Some artifacts failed validation"
            exit 1
          fi

          echo "ðŸŽ‰ Dry run deployment test passed!"

  test-java-examples-maven:
    name: "Test Java Examples with Maven Artifacts (${{ matrix.platform }} - ${{ matrix.implementation }})"
    runs-on: ${{ matrix.os }}
    needs: [detect-changes, determine-matrix, build-maven-artifacts]
    if: ${{ always() && needs.detect-changes.outputs.should-test == 'true' && needs.build-maven-artifacts.result == 'success' }}
    continue-on-error: true  # Non-blocking failures
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.determine-matrix.outputs.matrix) }}
    steps:
      - name: Checkout code
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Set up JDK (Java 25 for FFM builds)
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: ${{ matrix.implementation == 'ffm' && '25' || '21' }}
          distribution: ${{ matrix.implementation == 'ffm' && 'oracle' || 'temurin' }}

      - name: Install timeout command on macOS
        if: ${{ startsWith(matrix.platform, 'macOS') }}
        run: |
          # Install GNU coreutils which includes gtimeout
          brew install coreutils
          echo "Installed gtimeout: $(which gtimeout)"

      - name: Download Maven artifacts (${{ matrix.platform }} - ${{ matrix.implementation }})
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
          name: maven-staging-artifacts-${{ matrix.artifact-suffix }}
          path: ./maven-artifacts/${{ matrix.platform }}
        continue-on-error: true

      - name: Cache Maven dependencies
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-examples-${{ hashFiles('**/pom-examples.xml*') }}
          restore-keys: |
            ${{ runner.os }}-maven-examples-
            ${{ runner.os }}-maven-

      - name: Test Java Examples (Unix)
        if: ${{ matrix.platform != 'Windows' }}
        id: test-examples-unix
        shell: bash
        run: |
          echo "=== Java Examples Maven Integration Test (${{ matrix.platform }}) ==="

          # Test one representative example from each category
          cd HDF5Examples/JAVA

          # Get absolute path to maven artifacts for this platform
          MAVEN_ARTIFACTS_DIR="$(realpath ../../maven-artifacts/${{ matrix.platform }})"
          echo "Maven artifacts directory (${{ matrix.platform }}): $MAVEN_ARTIFACTS_DIR"

          # Find HDF5 JAR files (not dependencies like slf4j) - use platform-specific artifacts
          HDF5_JAR=$(find "$MAVEN_ARTIFACTS_DIR" -name "*hdf5*.jar" -o -name "jarhdf5*.jar" 2>/dev/null | head -1)
          if [ -z "$HDF5_JAR" ]; then
            echo "âŒ No HDF5 JAR files found for testing"
            echo "Available JAR files:"
            find "$MAVEN_ARTIFACTS_DIR" -name "*.jar"
            exit 1
          fi

          echo "Using HDF5 JAR file: $HDF5_JAR"

          # Detect JAR type by checking for FFM-specific classes
          # FFM JARs contain org.hdfgroup.javahdf5 package, JNI JARs contain hdf.hdf5lib
          if jar tf "$HDF5_JAR" | grep -q "org/hdfgroup/javahdf5/hdf5_h.class"; then
            JAR_TYPE="ffm"
            echo "Detected FFM JAR (contains org.hdfgroup.javahdf5 package)"
          else
            JAR_TYPE="jni"
            echo "Detected JNI JAR (contains hdf.hdf5lib package)"
          fi

          # Determine which examples directory to use based on detected JAR type
          # This ensures we test with examples that match the JAR's implementation
          if [ "$JAR_TYPE" = "jni" ]; then
            EXAMPLES_DIR="compat"
            echo "Using JNI-compatible examples from compat/ directory"
          else
            EXAMPLES_DIR="."
            echo "Using FFM examples from root directory"
          fi

          # Also find any dependency JARs - only include slf4j-api and slf4j-simple, exclude slf4j-nop to avoid conflicts
          DEP_JARS=$(find "$MAVEN_ARTIFACTS_DIR" -name "slf4j-api*.jar" -o -name "slf4j-simple*.jar")
          if [ -n "$DEP_JARS" ]; then
            echo "Found dependency JARs:"
            echo "$DEP_JARS"
          else
            echo "No SLF4J dependency JARs found in artifacts, downloading them directly..."

            # Download SLF4J dependencies directly using Maven
            SLF4J_VERSION="2.0.16"
            TEMP_POM="$MAVEN_ARTIFACTS_DIR/temp-pom.xml"

            # Create a minimal POM to download dependencies using echo
            echo '<?xml version="1.0" encoding="UTF-8"?>' > "$TEMP_POM"
            echo '<project xmlns="http://maven.apache.org/POM/4.0.0"' >> "$TEMP_POM"
            echo '         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"' >> "$TEMP_POM"
            echo '         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">' >> "$TEMP_POM"
            echo '    <modelVersion>4.0.0</modelVersion>' >> "$TEMP_POM"
            echo '    <groupId>temp</groupId>' >> "$TEMP_POM"
            echo '    <artifactId>temp</artifactId>' >> "$TEMP_POM"
            echo '    <version>1.0</version>' >> "$TEMP_POM"
            echo '    <dependencies>' >> "$TEMP_POM"
            echo '        <dependency>' >> "$TEMP_POM"
            echo '            <groupId>org.slf4j</groupId>' >> "$TEMP_POM"
            echo '            <artifactId>slf4j-api</artifactId>' >> "$TEMP_POM"
            echo '            <version>2.0.16</version>' >> "$TEMP_POM"
            echo '        </dependency>' >> "$TEMP_POM"
            echo '        <dependency>' >> "$TEMP_POM"
            echo '            <groupId>org.slf4j</groupId>' >> "$TEMP_POM"
            echo '            <artifactId>slf4j-simple</artifactId>' >> "$TEMP_POM"
            echo '            <version>2.0.16</version>' >> "$TEMP_POM"
            echo '        </dependency>' >> "$TEMP_POM"
            echo '    </dependencies>' >> "$TEMP_POM"
            echo '</project>' >> "$TEMP_POM"

            # Download dependencies
            if (cd "$MAVEN_ARTIFACTS_DIR" && mvn -f temp-pom.xml dependency:copy-dependencies -DoutputDirectory=. -DincludeScope=runtime -q); then
              # Clean up temporary POM
              rm -f "$TEMP_POM"

              # Re-scan for dependency JARs
              DEP_JARS=$(find "$MAVEN_ARTIFACTS_DIR" -name "slf4j-api*.jar" -o -name "slf4j-simple*.jar")
              if [ -n "$DEP_JARS" ]; then
                echo "Successfully downloaded SLF4J dependencies:"
                echo "$DEP_JARS"
              else
                echo "Failed to download SLF4J dependencies"
              fi
            else
              echo "Error downloading dependencies with Maven"
              rm -f "$TEMP_POM"
            fi
          fi

          FAILED_EXAMPLES=""
          TOTAL_EXAMPLES=0
          PASSED_EXAMPLES=0

          # Save current directory (HDF5Examples/JAVA)
          JAVA_DIR="$(pwd)"

          # Test representative examples from each category
          for category in H5D H5T H5G TUTR; do
            if [ -d "$EXAMPLES_DIR/$category" ]; then
              cd "$JAVA_DIR/$EXAMPLES_DIR/$category"

              # Find first .java file in category
              EXAMPLE_FILE=$(ls *.java | head -1)
              if [ -f "$EXAMPLE_FILE" ]; then
                TOTAL_EXAMPLES=$((TOTAL_EXAMPLES + 1))
                example_name=$(basename "$EXAMPLE_FILE" .java)
                echo "--- Testing $category/$example_name ---"

                # Build classpath with HDF5 JAR and dependencies (already absolute paths)
                CLASSPATH="$HDF5_JAR"
                if [ -n "$DEP_JARS" ]; then
                  for dep_jar in $DEP_JARS; do
                    CLASSPATH="$CLASSPATH:$dep_jar"
                  done
                fi

                echo "Using classpath: $CLASSPATH"

                # Set compilation flags based on detected JAR type
                # This ensures we use correct flags for the actual JAR implementation
                if [ "$JAR_TYPE" = "ffm" ]; then
                  # FFM requires preview features (Java 25)
                  JAVAC_FLAGS="--enable-preview --release 25"
                  JAVA_FLAGS="--enable-preview"
                else
                  # JNI uses standard Java
                  JAVAC_FLAGS=""
                  JAVA_FLAGS=""
                fi

                # Test compilation
                COMPILE_OUTPUT=$(javac $JAVAC_FLAGS -cp "$CLASSPATH" "$EXAMPLE_FILE" 2>&1)
                if [ $? -eq 0 ]; then
                  echo "âœ“ Compilation successful for $category/$example_name"

                  # Test execution (with cross-platform timeout)
                  if command -v gtimeout >/dev/null 2>&1; then
                    # macOS with GNU coreutils timeout
                    EXEC_RESULT=$(gtimeout 10s java $JAVA_FLAGS -cp ".:$CLASSPATH" "$example_name" >/tmp/${example_name}.out 2>&1; echo $?)
                  elif command -v timeout >/dev/null 2>&1; then
                    # Linux/Windows with timeout command
                    EXEC_RESULT=$(timeout 10s java $JAVA_FLAGS -cp ".:$CLASSPATH" "$example_name" >/tmp/${example_name}.out 2>&1; echo $?)
                  else
                    # Fallback - run without timeout (Java examples should complete quickly)
                    echo "Note: Running without timeout"
                    java $JAVA_FLAGS -cp ".:$CLASSPATH" "$example_name" >/tmp/${example_name}.out 2>&1
                    EXEC_RESULT=$?
                  fi

                  if [ "$EXEC_RESULT" -eq 0 ]; then
                    # Basic output validation
                    if grep -q -i -E "(dataset|datatype|group|success|created|written|read)" /tmp/${example_name}.out && \
                       ! grep -q -i -E "(error|exception|failed|cannot)" /tmp/${example_name}.out; then
                      echo "âœ“ Execution and validation successful for $category/$example_name"
                      PASSED_EXAMPLES=$((PASSED_EXAMPLES + 1))
                    else
                      echo "âœ— Output validation failed for $category/$example_name"
                      echo "Output:"
                      cat /tmp/${example_name}.out
                      FAILED_EXAMPLES="$FAILED_EXAMPLES $category/$example_name"
                    fi
                  else
                    # Check if failure is due to expected native library issue (acceptable for Maven-only testing)
                    if grep -q "UnsatisfiedLinkError.*hdf5_java.*java.library.path" /tmp/${example_name}.out; then
                      echo "âœ“ Expected native library error for Maven-only testing: $category/$example_name"
                      echo "  (This confirms JAR structure is correct)"
                      PASSED_EXAMPLES=$((PASSED_EXAMPLES + 1))
                    else
                      echo "âœ— Unexpected execution failure for $category/$example_name"
                      echo "Output:"
                      cat /tmp/${example_name}.out
                      FAILED_EXAMPLES="$FAILED_EXAMPLES $category/$example_name"
                    fi
                  fi
                else
                  echo "âœ— Compilation failed for $category/$example_name"
                  echo "Compilation error output:"
                  echo "$COMPILE_OUTPUT"
                  FAILED_EXAMPLES="$FAILED_EXAMPLES $category/$example_name"
                fi
              fi
            fi
          done

          echo "=== Java Examples Test Summary (${{ matrix.platform }}) ==="
          echo "Total representative examples tested: $TOTAL_EXAMPLES"
          echo "Passed: $PASSED_EXAMPLES"
          echo "Failed: $((TOTAL_EXAMPLES - PASSED_EXAMPLES))"

          if [ -n "$FAILED_EXAMPLES" ]; then
            echo "Failed examples:$FAILED_EXAMPLES"
            echo "âŒ Some Java examples failed - but continuing (non-blocking)"
            echo "test-status=FAILED" >> $GITHUB_OUTPUT
          else
            echo "âœ… All representative Java examples passed!"
            echo "test-status=PASSED" >> $GITHUB_OUTPUT
          fi

          echo "total-examples=$TOTAL_EXAMPLES" >> $GITHUB_OUTPUT
          echo "passed-examples=$PASSED_EXAMPLES" >> $GITHUB_OUTPUT

      - name: Test Java Examples (Windows)
        if: ${{ matrix.platform == 'Windows' }}
        id: test-examples-windows
        shell: pwsh
        run: |
          Write-Host "=== Java Examples Maven Integration Test (Windows) ==="

          # Test one representative example from each category
          Set-Location HDF5Examples/JAVA

          # Get absolute path to maven artifacts for this platform
          $MAVEN_ARTIFACTS_DIR = Resolve-Path "../../maven-artifacts/${{ matrix.platform }}"
          Write-Host "Maven artifacts directory (Windows): $MAVEN_ARTIFACTS_DIR"

          # Find HDF5 JAR files (not dependencies like slf4j)
          $HDF5_JAR = Get-ChildItem -Path $MAVEN_ARTIFACTS_DIR -Filter "*hdf5*.jar" -Recurse | Select-Object -First 1
          if (-not $HDF5_JAR) {
            $HDF5_JAR = Get-ChildItem -Path $MAVEN_ARTIFACTS_DIR -Filter "jarhdf5*.jar" -Recurse | Select-Object -First 1
          }

          if (-not $HDF5_JAR) {
            Write-Host "âŒ No HDF5 JAR files found for testing"
            Write-Host "Available JAR files:"
            Get-ChildItem -Path $MAVEN_ARTIFACTS_DIR -Filter "*.jar" -Recurse
            exit 1
          }

          Write-Host "Using HDF5 JAR file: $($HDF5_JAR.FullName)"

          # Detect JAR type by checking for FFM-specific classes
          # FFM JARs contain org.hdfgroup.javahdf5 package, JNI JARs contain hdf.hdf5lib
          $jar_contents = & jar tf $HDF5_JAR.FullName
          if ($jar_contents -match "org/hdfgroup/javahdf5/hdf5_h.class") {
            $JAR_TYPE = "ffm"
            Write-Host "Detected FFM JAR (contains org.hdfgroup.javahdf5 package)"
          } else {
            $JAR_TYPE = "jni"
            Write-Host "Detected JNI JAR (contains hdf.hdf5lib package)"
          }

          # Determine which examples directory to use based on detected JAR type
          # This ensures we test with examples that match the JAR's implementation
          if ($JAR_TYPE -eq "jni") {
            $EXAMPLES_DIR = "compat"
            Write-Host "Using JNI-compatible examples from compat/ directory"
          } else {
            $EXAMPLES_DIR = "."
            Write-Host "Using FFM examples from root directory"
          }

          # Find dependency JARs - only include slf4j-api and slf4j-simple
          $DEP_JARS = @()
          $DEP_JARS += Get-ChildItem -Path $MAVEN_ARTIFACTS_DIR -Filter "slf4j-api*.jar" -Recurse
          $DEP_JARS += Get-ChildItem -Path $MAVEN_ARTIFACTS_DIR -Filter "slf4j-simple*.jar" -Recurse

          if ($DEP_JARS.Count -gt 0) {
            Write-Host "Found dependency JARs:"
            $DEP_JARS | ForEach-Object { Write-Host "  $($_.FullName)" }
          } else {
            Write-Host "No SLF4J dependency JARs found in artifacts, downloading them directly..."

            # Download SLF4J dependencies directly using Maven
            $SLF4J_VERSION = "2.0.16"
            $TEMP_POM = "$MAVEN_ARTIFACTS_DIR\temp-pom.xml"

            # Create a minimal POM to download dependencies using PowerShell strings
            Write-Host "Creating temporary POM at: $TEMP_POM"
            '<?xml version="1.0" encoding="UTF-8"?>' | Out-File $TEMP_POM -Encoding UTF8
            '<project xmlns="http://maven.apache.org/POM/4.0.0"' | Out-File $TEMP_POM -Append -Encoding UTF8
            '         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"' | Out-File $TEMP_POM -Append -Encoding UTF8
            '         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">' | Out-File $TEMP_POM -Append -Encoding UTF8
            '    <modelVersion>4.0.0</modelVersion>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '    <groupId>temp</groupId>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '    <artifactId>temp</artifactId>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '    <version>1.0</version>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '    <dependencies>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '        <dependency>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '            <groupId>org.slf4j</groupId>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '            <artifactId>slf4j-api</artifactId>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '            <version>2.0.16</version>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '        </dependency>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '        <dependency>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '            <groupId>org.slf4j</groupId>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '            <artifactId>slf4j-simple</artifactId>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '            <version>2.0.16</version>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '        </dependency>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '    </dependencies>' | Out-File $TEMP_POM -Append -Encoding UTF8
            '</project>' | Out-File $TEMP_POM -Append -Encoding UTF8

            # Download dependencies
            try {
              Write-Host "Running Maven command: mvn -f $TEMP_POM dependency:copy-dependencies -DoutputDirectory=$MAVEN_ARTIFACTS_DIR -DincludeScope=runtime -q"
              $mvn_result = & mvn -f $TEMP_POM dependency:copy-dependencies "-DoutputDirectory=$MAVEN_ARTIFACTS_DIR" -DincludeScope=runtime -q
              $mvn_exit_code = $LASTEXITCODE

              # Clean up temporary POM
              Remove-Item $TEMP_POM -Force -ErrorAction SilentlyContinue

              if ($mvn_exit_code -eq 0) {
                # Re-scan for dependency JARs
                $DEP_JARS = @()
                $DEP_JARS += Get-ChildItem -Path $MAVEN_ARTIFACTS_DIR -Filter "slf4j-api*.jar" -Recurse
                $DEP_JARS += Get-ChildItem -Path $MAVEN_ARTIFACTS_DIR -Filter "slf4j-simple*.jar" -Recurse

                if ($DEP_JARS.Count -gt 0) {
                  Write-Host "Successfully downloaded SLF4J dependencies:"
                  $DEP_JARS | ForEach-Object { Write-Host "  $($_.FullName)" }
                } else {
                  Write-Host "Maven succeeded but no SLF4J JARs found"
                }
              } else {
                Write-Host "Maven command failed with exit code: $mvn_exit_code"
                Write-Host "Maven output: $mvn_result"
              }
            } catch {
              Write-Host "Error downloading dependencies: $($_.Exception.Message)"
            }
          }

          $FAILED_EXAMPLES = @()
          $TOTAL_EXAMPLES = 0
          $PASSED_EXAMPLES = 0

          # Save current directory (HDF5Examples/JAVA)
          $JAVA_DIR = Get-Location

          # Test representative examples from each category
          foreach ($category in @("H5D", "H5T", "H5G", "TUTR")) {
            $category_path = Join-Path $EXAMPLES_DIR $category
            $full_category_path = Join-Path $JAVA_DIR $category_path
            if (Test-Path $full_category_path) {
              Set-Location $full_category_path

              # Find first .java file in category
              $EXAMPLE_FILE = Get-ChildItem -Filter "*.java" | Select-Object -First 1
              if ($EXAMPLE_FILE) {
                $TOTAL_EXAMPLES++
                $example_name = $EXAMPLE_FILE.BaseName
                Write-Host "--- Testing $category/$example_name ---"

                # Build classpath with HDF5 JAR and dependencies
                $CLASSPATH = $HDF5_JAR.FullName
                foreach ($dep_jar in $DEP_JARS) {
                  $CLASSPATH += ";$($dep_jar.FullName)"
                }

                Write-Host "Using classpath: $CLASSPATH"

                # Set compilation flags based on detected JAR type
                # This ensures we use correct flags for the actual JAR implementation
                if ($JAR_TYPE -eq "ffm") {
                  # FFM requires preview features (Java 25)
                  $JAVAC_FLAGS = @("--enable-preview", "--release", "25")
                  $JAVA_FLAGS = @("--enable-preview")
                } else {
                  # JNI uses standard Java
                  $JAVAC_FLAGS = @()
                  $JAVA_FLAGS = @()
                }

                # Test compilation
                $javac_args = $JAVAC_FLAGS + @("-cp", $CLASSPATH, $EXAMPLE_FILE.Name)
                $compileResult = & javac $javac_args 2>&1
                if ($LASTEXITCODE -eq 0) {
                  Write-Host "âœ“ Compilation successful for $category/$example_name"

                  # Test execution (with PowerShell timeout)
                  $output_file = "../../../$example_name.out"

                  # Use PowerShell's Start-Process with timeout for Windows
                  try {
                    # Create temporary files for stdout and stderr
                    $stdout_file = "$output_file.stdout"
                    $stderr_file = "$output_file.stderr"

                    # Build java arguments with optional FFM flags
                    $java_args = $JAVA_FLAGS + @("-cp", ".;$CLASSPATH", $example_name)
                    $process = Start-Process -FilePath "java" -ArgumentList $java_args -RedirectStandardOutput $stdout_file -RedirectStandardError $stderr_file -NoNewWindow -PassThru

                    # Wait for the process with timeout
                    if ($process.WaitForExit(10000)) {  # 10 seconds in milliseconds
                      $execResult = $process.ExitCode

                      # Combine stdout and stderr into single output file
                      $stdout_content = if (Test-Path $stdout_file) {
                        try { Get-Content $stdout_file -Raw -ErrorAction SilentlyContinue } catch { "" }
                      } else { "" }
                      $stderr_content = if (Test-Path $stderr_file) {
                        try { Get-Content $stderr_file -Raw -ErrorAction SilentlyContinue } catch { "" }
                      } else { "" }

                      $combined_content = "$stdout_content$stderr_content"
                      if ([string]::IsNullOrWhiteSpace($combined_content)) {
                        $combined_content = "No output generated"
                      }
                      $combined_content | Out-File $output_file -Encoding UTF8

                      # Clean up temporary files
                      if (Test-Path $stdout_file) { Remove-Item $stdout_file -Force }
                      if (Test-Path $stderr_file) { Remove-Item $stderr_file -Force }
                    } else {
                      # Process timed out
                      try { $process.Kill() } catch { }
                      $execResult = 1
                      "Process timed out after 10 seconds" | Out-File $output_file -Encoding UTF8

                      # Clean up temporary files
                      if (Test-Path $stdout_file) { Remove-Item $stdout_file -Force }
                      if (Test-Path $stderr_file) { Remove-Item $stderr_file -Force }
                    }
                  } catch {
                    $execResult = 1
                    "Execution error: $($_.Exception.Message)" | Out-File $output_file -Encoding UTF8
                  }

                  if ($execResult -eq 0) {
                    # Basic output validation
                    $content = Get-Content $output_file -Raw
                    if (($content -match "(?i)(dataset|datatype|group|success|created|written|read)") -and
                        ($content -notmatch "(?i)(error|exception|failed|cannot)")) {
                      Write-Host "âœ“ Execution and validation successful for $category/$example_name"
                      $PASSED_EXAMPLES++
                    } else {
                      Write-Host "âœ— Output validation failed for $category/$example_name"
                      Write-Host "Output:"
                      Get-Content $output_file
                      $FAILED_EXAMPLES += "$category/$example_name"
                    }
                  } else {
                    # Check if failure is due to expected native library issue
                    $content = Get-Content $output_file -Raw
                    if ($content -match "UnsatisfiedLinkError.*hdf5_java.*java.library.path") {
                      Write-Host "âœ“ Expected native library error for Maven-only testing: $category/$example_name"
                      Write-Host "  (This confirms JAR structure is correct)"
                      $PASSED_EXAMPLES++
                    } else {
                      Write-Host "âœ— Unexpected execution failure for $category/$example_name"
                      Write-Host "Output:"
                      Get-Content $output_file
                      $FAILED_EXAMPLES += "$category/$example_name"
                    }
                  }
                } else {
                  Write-Host "âœ— Compilation failed for $category/$example_name"
                  Write-Host "Compilation error output:"
                  Write-Host $compileResult
                  $FAILED_EXAMPLES += "$category/$example_name"
                }
              }
            }
          }

          Write-Host "=== Java Examples Test Summary (Windows) ==="
          Write-Host "Total representative examples tested: $TOTAL_EXAMPLES"
          Write-Host "Passed: $PASSED_EXAMPLES"
          Write-Host "Failed: $($TOTAL_EXAMPLES - $PASSED_EXAMPLES)"

          if ($FAILED_EXAMPLES.Count -gt 0) {
            Write-Host "Failed examples: $($FAILED_EXAMPLES -join ' ')"
            Write-Host "âŒ Some Java examples failed - but continuing (non-blocking)"
            echo "test-status=FAILED" >> $env:GITHUB_OUTPUT
          } else {
            Write-Host "âœ… All representative Java examples passed!"
            echo "test-status=PASSED" >> $env:GITHUB_OUTPUT
          }

          echo "total-examples=$TOTAL_EXAMPLES" >> $env:GITHUB_OUTPUT
          echo "passed-examples=$PASSED_EXAMPLES" >> $env:GITHUB_OUTPUT

      - name: Upload failure artifacts (Java Examples)
        if: steps.test-examples-unix.outputs.test-status == 'FAILED' || steps.test-examples-windows.outputs.test-status == 'FAILED'
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: java-examples-staging-failure-${{ matrix.platform }}-${{ github.run_id }}
          path: |
            /tmp/*.out
            *.out
            HDF5Examples/JAVA/*/*.class
            HDF5Examples/JAVA/*/*.h5
          retention-days: 7

  comment-pr:
    name: Comment on Pull Request
    runs-on: ubuntu-latest
    needs: [detect-changes, determine-matrix, build-maven-artifacts, test-maven-deployment]
    if: ${{ github.event_name == 'pull_request' && needs.detect-changes.outputs.should-test == 'true' && needs.build-maven-artifacts.result == 'success' && needs.test-maven-deployment.result == 'success' }}
    steps:
      - name: Generate comment body
        id: comment
        run: |
          COMMENT="## âœ… Maven Staging Tests Passed

          Maven artifacts successfully generated and validated.

          Ready for Maven deployment to GitHub Packages."

          echo "comment<<EOF" >> $GITHUB_OUTPUT
          echo "$COMMENT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Comment on PR
        # Skip commenting if running from a fork due to permission restrictions
        if: ${{ github.event.pull_request.head.repo.full_name == github.repository }}
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            try {
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: process.env.COMMENT_BODY
              });
              console.log('âœ… Comment posted successfully');
            } catch (error) {
              console.log('âŒ Failed to post comment:', error.message);
              console.log('This may be due to insufficient permissions or fork restrictions');
              // Don't fail the workflow if commenting fails
            }
        env:
          COMMENT_BODY: ${{ steps.comment.outputs.comment }}

      - name: Output test results (fallback)
        if: ${{ github.event.pull_request.head.repo.full_name != github.repository }}
        run: |
          echo "==========================================="
          echo "âœ… Maven Staging Tests Passed"
          echo "Maven artifacts successfully generated and validated"
          echo "Ready for Maven deployment to GitHub Packages"
          echo "==========================================="
          echo "Note: Running from fork - PR comment skipped due to permission restrictions"

  cleanup:
    name: Cleanup Staging Artifacts
    runs-on: ubuntu-latest
    needs: [detect-changes, determine-matrix, build-maven-artifacts, test-maven-deployment]
    if: ${{ always() && needs.detect-changes.outputs.should-test == 'true' }}
    steps:
      - name: Cleanup summary
        run: |
          echo "=== Maven Staging Cleanup ==="
          echo "Artifacts will be automatically cleaned up after 7 days"
          echo "Build artifacts are stored in GitHub Actions artifacts"
          echo "No persistent staging repository cleanup needed"
```

### `.github/workflows/msys2.yml`

```yaml
name: hdf5 dev MSys2 CI

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:

  Build_and_test:

    # The type of runner that the job will run on
    runs-on: windows-latest
    strategy:
      matrix:
        include:
          - { icon: 'â¬›', sys: mingw32 }
          - { icon: 'ðŸŸ¦', sys: mingw64 }
          - { icon: 'ðŸŸ¨', sys: ucrt64  }
          - { icon: 'ðŸŸ§', sys: clang64 }
    name: ${{ matrix.icon }} MSYS2-${{ matrix.sys }}-${{ inputs.build_mode }}
    defaults:
      run:
        shell: msys2 {0}

    steps:
      - name: '${{ matrix.icon }} Setup MSYS2'
        uses: msys2/setup-msys2@fb197b72ce45fb24f17bf3f807a388985654d1f2 # v2
        with:
          msystem: ${{matrix.sys}}
          update: true
          install: >-
           base-devel
           git
           gcc
           make
           development
          pacboy: >-
           toolchain:p
           cmake:p
           ninja:p

      - name: Set git to use LF
        run: |
          git config --global core.autocrlf input

      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Configure
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_TOOLCHAIN_FILE="" \
            -DHDF5_GENERATE_HEADERS:BOOL=OFF \
            -DBUILD_SHARED_LIBS:BOOL=ON \
            -DHDF5_ENABLE_ALL_WARNINGS:BOOL=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_JAVA:BOOL=OFF \
            -DHDF5_BUILD_DOC:BOOL=OFF \
            $GITHUB_WORKSPACE

      - name: Build
        run: |
          cmake --build . --parallel 2 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      - name: Run Tests
        run: |
          ctest . --parallel 2 -C ${{ inputs.build_mode }} -E "cache_api|dt_arith|err_compat"
        working-directory: ${{ runner.workspace }}/build

      - name: Run Expected to Fail Tests
        run: |
          ctest . --parallel 2 -C ${{ inputs.build_mode }} -R "cache_api|dt_arith|err_compat"
        working-directory: ${{ runner.workspace }}/build
        continue-on-error: true
```

### `.github/workflows/netcdf.yml`

```yaml
name: netCDF dev

# Triggers the workflow on push or pull request or on demand
on:
  workflow_dispatch:
  push:
  pull_request:
    branches: [ develop ]
    paths-ignore:
      - '.github/CODEOWNERS'
      - '.github/FUNDING.yml'
      - 'doc/**'
      - 'release_docs/**'
      - 'ACKNOWLEDGEMENTS'
      - 'LICENSE**'
      - '**.md'

permissions:
  contents: read

# Using concurrency to cancel any in-progress job or run
concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref && github.ref || github.run_id }}
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
    - name: Install System dependencies
      run: |
        sudo apt-get update
        sudo apt-get install ninja-build
        sudo apt install libssl3 libssl-dev libcurl4
        sudo apt install -y libaec-dev zlib1g-dev automake autoconf libcurl4-openssl-dev libjpeg-dev wget curl bzip2 m4 flex bison cmake libzip-dev doxygen openssl libtool libtool-bin

    - name: Checkout HDF5
      uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

    - name: Configure
      run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            -G Ninja \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=Release \
            -DHDF5_ONLY_SHARED_LIBS:BOOL=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_JAVA:BOOL=OFF \
            -DHDF5_BUILD_DOC:BOOL=OFF \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
            -DZLIB_USE_LOCALCONTENT:BOOL=OFF \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
            -DBUILD_TESTING:BOOL=OFF \
            -DCMAKE_INSTALL_PREFIX:PATH=/usr/local \
            $GITHUB_WORKSPACE
      shell: bash

    - name: Build
      run: cmake --build . --parallel 3 --config Release
      working-directory: ${{ runner.workspace }}/build

    - name: Install HDF5
      run: |
        sudo cmake --install . --config Release --prefix="/usr/local"
      working-directory: ${{ runner.workspace }}/build

    - name: Checkout netCDF
      uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
      with:
        repository: unidata/netcdf-c
        path: netcdf-c


    - name: Test netCDF
      run: |
        cd netcdf-c
        autoreconf -if
        # NOTE: --disable-byterange should be removed when the HTTP VFD has been updated
        CFLAGS=${CFLAGS} LDFLAGS=${LDFLAGS} LD_LIBRARY_PATH=${LD_LIBRARY_PATH} ./configure --enable-hdf5 --enable-dap --disable-dap-remote-tests --disable-byterange --enable-external-server-tests
        cat config.log
        cat libnetcdf.settings
        CFLAGS=${CFLAGS} LDFLAGS=${LDFLAGS} LD_LIBRARY_PATH=${LD_LIBRARY_PATH} make -j
        CFLAGS=${CFLAGS} LDFLAGS=${LDFLAGS} LD_LIBRARY_PATH=${LD_LIBRARY_PATH} make check TESTS="" -j
        LD_LIBRARY_PATH="/home/runner/work/hdf5/hdf5/netcdf-c/liblib/.libs:/usr/local/lib:${LD_LIBRARY_PATH}"
        CFLAGS=${CFLAGS} LDFLAGS=${LDFLAGS} LD_LIBRARY_PATH=${LD_LIBRARY_PATH} make check -j
```

### `.github/workflows/nvhpc.yml`

```yaml
name: hdf5 dev nvhpc

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  nvhpc_build_and_test:
    name: "nvhpc ${{ inputs.build_mode }}"
    runs-on: ubuntu-latest
    env:
      CUDAVERDOT: "13.0"
      NVERDOT: "25.9"
      NVERDASH: "25-9"

    steps:
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Free Disk Space (Ubuntu)
        uses: jlumbroso/free-disk-space@54081f138730dfa15788a46383842cd2f914a1be # v1.3.1
        with:
          android: true
          dotnet: true
          haskell: true
          large-packages: true
          tool-cache: false
          swap-storage: true 

      - name: Install Dependencies
        shell: bash
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz
          sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
          sudo apt install -y libaec-dev zlib1g-dev wget curl bzip2 flex bison cmake libzip-dev openssl build-essential

      - name: Install NVHPC
        shell: bash
        run: |
          curl https://developer.download.nvidia.com/hpc-sdk/ubuntu/DEB-GPG-KEY-NVIDIA-HPC-SDK | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-hpcsdk-archive-keyring.gpg
          echo 'deb [signed-by=/usr/share/keyrings/nvidia-hpcsdk-archive-keyring.gpg] https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64 /' | sudo tee /etc/apt/sources.list.d/nvhpc.list
          sudo apt-get update -y
          sudo apt-get install -y nvhpc-${{ env.NVERDASH }}
          echo "NVHPCSDK=/opt/nvidia/hpc_sdk" >> $GITHUB_ENV
          echo "OMPI_CXX=/opt/nvidia/hpc_sdk/Linux_x86_64/${{ env.NVERDOT }}/compilers/bin/nvc++" >> $GITHUB_ENV
          echo "OMPI_CC=/opt/nvidia/hpc_sdk/Linux_x86_64/${{ env.NVERDOT }}/compilers/bin/nvc" >> $GITHUB_ENV
          echo "OMPI_FC=/opt/nvidia/hpc_sdk/Linux_x86_64/${{ env.NVERDOT }}/compilers/bin/nvfortran" >> $GITHUB_ENV
          echo "CC=/opt/nvidia/hpc_sdk/Linux_x86_64/${{ env.NVERDOT }}/comm_libs/hpcx/bin/mpicc" >> $GITHUB_ENV
          echo "FC=/opt/nvidia/hpc_sdk/Linux_x86_64/${{ env.NVERDOT }}/comm_libs/hpcx/bin/mpifort" >> $GITHUB_ENV
          echo "LD_LIBRARY_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/${{ env.NVERDOT }}/cuda/${{ env.CUDAVERDOT }}/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/${{ env.NVERDOT }}/compilers/lib" >> $GITHUB_ENV
          echo "DESTDIR=/tmp" >> $GITHUB_ENV

      - name: Configure
        shell: bash
        run: |
          export PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/${{ env.NVERDOT }}/comm_libs/hpcx/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/${{ env.NVERDOT }}/compilers/bin:$PATH
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake -G Ninja \
          -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
          -DHDF5_ENABLE_PARALLEL:BOOL=ON \
          -DMPIEXEC_NUMPROC_FLAG:STRING=-np \
          -DMPIEXEC_MAX_NUMPROCS:STRING=2 \
          -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
          -DHDF5_BUILD_FORTRAN:BOOL=ON \
          -DHDF5_BUILD_JAVA:BOOL=OFF \
          -DMPIEXEC_MAX_NUMPROCS:STRING="2" \
          -DMPIEXEC_PREFLAGS:STRING="--mca;opal_warn_on_missing_libcuda;0" \
          $GITHUB_WORKSPACE

      - name: Build
        shell: bash
        run: |
          cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      # Skipping dt_arith and dtransform while we investigate long double failures
      - name: Run Tests
        shell: bash
        run: |
          ctest . -E "MPI_TEST|H5TEST-dt_arith" --parallel 2 -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      - name: Run Expected To Fail Tests
        shell: bash
        run: |
          ctest . -R "H5TEST-dt_arith" --parallel 2 -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build
        continue-on-error: true

      - name: Run Parallel Tests
        shell: bash
        run: |
          export UCX_WARN_UNUSED_ENV_VARS=n
          ctest . -R MPI_TEST -E "_by_chunk|_by_pattern" -C ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build
```

### `.github/workflows/openbsd.yml`

```yaml
name: OpenBSD CI

# Triggers the workflow on push or pull request or on demand
on:
  workflow_dispatch:
  push:
  pull_request:
    branches: [ develop ]
    paths-ignore:
      - '.github/CODEOWNERS'
      - '.github/FUNDING.yml'
      - 'doc/**'
      - 'release_docs/**'
      - 'ACKNOWLEDGEMENTS'
      - 'LICENSE**'
      - '**.md'

# Using concurrency to cancel any in-progress job or run
concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref && github.ref || github.run_id }}
  cancel-in-progress: true

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  openbsd-build-and-test:
    runs-on: ubuntu-latest
    name: OpenBSD ${{ matrix.openbsd-version }} Build and Test

    # Don't run the action if the commit message says to skip CI
    if: "!contains(github.event.head_commit.message, 'skip-ci')"

    strategy:
      fail-fast: false
      matrix:
        openbsd-version: ['7.5']

    steps:
      - name: Checkout repository
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Build and test on OpenBSD
        uses: vmactions/openbsd-vm@2e29de1eb150dfe1c9c97b84ff2b7896f14ca690 # v1
        with:
          release: ${{ matrix.openbsd-version }}
          usesh: true
          prepare: |
            echo "https://ftp.openbsd.org/pub/OpenBSD" > /etc/installurl
            pkg_add cmake gmake pkgconf curl gcc-11.2.0p11
          run: |
            set -e

            # Set up library path for gcc
            export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH

            # Get number of processors (OpenBSD uses sysctl in /sbin)
            NPROC=$(/sbin/sysctl -n hw.ncpu)
            echo "Number of processors: $NPROC"

            # Configure the build
            mkdir build
            cd build
            cmake -C ../config/cmake/cacheinit.cmake \
              --log-level=VERBOSE \
              -DCMAKE_BUILD_TYPE=Release \
              -DCMAKE_C_COMPILER=egcc \
              -DCMAKE_CXX_COMPILER=eg++ \
              -DCMAKE_MAKE_PROGRAM=gmake \
              -DBUILD_SHARED_LIBS:BOOL=ON \
              -DHDF5_ENABLE_ALL_WARNINGS:BOOL=ON \
              -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
              -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
              -DHDF5_BUILD_FORTRAN:BOOL=OFF \
              -DHDF5_BUILD_JAVA:BOOL=OFF \
              -DHDF5_BUILD_DOC:BOOL=OFF \
              -DHDF5_BUILD_HL_LIB:BOOL=OFF \
              -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
              -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON \
              -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF \
              -DZLIB_USE_LOCALCONTENT:BOOL=OFF \
              -DHDF5_TEST_API:BOOL=ON \
              -DHDF5_TEST_SHELL_SCRIPTS:BOOL=OFF \
              -DENABLE_EXTENDED_TESTS:BOOL=OFF \
              ..
            echo ""

            # Build
            cmake --build . --parallel $NPROC
            echo ""

            # Run tests
            ctest . --parallel $NPROC
            echo ""
```

### `.github/workflows/par-script.yml`

```yaml
name: hdf5 callable parallel report to cdash

# Triggers hdf5 dev parallel CTest script the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      snap_name:
        description: 'The name in the source tarballs'
        type: string
        required: false
        default: hdfsrc
      file_base:
        description: "The common base name of the source tarballs"
        required: true
        type: string
      use_environ:
        description: 'Environment to locate files'
        type: string
        required: true
        default: snapshots
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string
      cmake_version:
        description: "3.26.0 or later, latest"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  Build_parallel_windows:
    runs-on: windows-latest
    strategy:
      matrix:
        mpi: [ 'msmpi', 'intelmpi']
    name: "Parallel ${{ matrix.mpi }} Windows-${{ inputs.build_mode }}"
    steps:
      - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install Dependencies (Windows)
        run: choco install ninja

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Enable Developer Command Prompt
        uses: ilammy/msvc-dev-cmd@0b201ec74fa43914dc39ae48a89fd1d8cb592756 # v1.13.0

      - name: Setup MPI (${{ matrix.mpi }})
        id: setup-mpi
        uses: mpi4py/setup-mpi@d0a3bf17a182b37921ff27a6737f9009ec76d3b6 # v1
        with:
          mpi: ${{ matrix.mpi }}

      - name: Set MPI Environment Variables (${{ matrix.mpi }})
        run: |
          echo "CC=mpicc" >> $GITHUB_ENV
          echo "FC=mpif90" >> $GITHUB_ENV

      - name: Set file base name (${{ matrix.mpi }})
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT
        shell: bash

      # Get files created by release script
      - name: Get zip-tarball (Windows)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: zip-tarball
              path: ${{ github.workspace }}

      - name: using powershell
        shell: pwsh
        run: Get-Location

      - name: List files for the space (Windows)
        run: |
              Get-ChildItem -Path ${{ github.workspace }}
              Get-ChildItem -Path ${{ runner.workspace }}
        shell: pwsh

      - name: Uncompress source (Windows)
        working-directory: ${{ github.workspace }}
        run: 7z x ${{ steps.set-file-base.outputs.FILE_BASE }}.zip
        shell: bash

      - name: Copy script files for the space (${{ matrix.mpi }})
        run: |
          Copy-Item -Path ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake -Destination ${{ runner.workspace }}/hdf5
          Copy-Item -Path ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake -Destination ${{ runner.workspace }}/hdf5
        shell: pwsh

      - name: List files for the hdf5 (${{ matrix.mpi }})
        run: |
            Get-ChildItem -Path ${{ runner.workspace }}/hdf5
        shell: pwsh

      - name: Create options file (${{ matrix.mpi }})
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
            set (CTEST_DROP_SITE_INIT "my.cdash.org")
            # Change following line to submit to your CDash dashboard to a different CDash project
            #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
            set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
            set (ctest_test_args INCLUDE MPI_TEST)
            set (CTEST_TEST_TIMEOUT 300 CACHE STRING "Maximum test time allowed.")
            set (MODEL "MPI")
            set (GROUP "MPI")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }}")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DMPIEXEC_NUMPROC_FLAG:STRING=-n")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DMPIEXEC_MAX_NUMPROCS:STRING=2")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PARALLEL:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SUBFILING_VFD:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest script (${{ matrix.mpi }})
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=GH-${{ github.event.repository.full_name }}-${{ matrix.mpi }},LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=VS202264,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C ${{ inputs.build_mode }} -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (${{ matrix.mpi }})
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: windows-${{ matrix.mpi }}-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log

  Build_parallel_linux:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        mpi: [ 'mpich', 'openmpi', 'intelmpi']
    name: "Parallel ${{ matrix.mpi }} Linux-${{ inputs.build_mode }}"
    steps:
      - name: Install Dependencies (${{ matrix.mpi }})
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build graphviz curl
          sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
          sudo apt install gcc-12 g++-12 gfortran-12
          sudo apt install libaec0 libaec-dev

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Setup MPI (${{ matrix.mpi }})
        id: setup-mpi
        uses: mpi4py/setup-mpi@d0a3bf17a182b37921ff27a6737f9009ec76d3b6 # v1
        with:
          mpi: ${{ matrix.mpi }}

      - name: Set MPI Environment Variables (${{ matrix.mpi }})
        run: |
          echo "CC=mpicc" >> $GITHUB_ENV
          echo "FC=mpif90" >> $GITHUB_ENV

      - name: Set file base name (${{ matrix.mpi }})
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT
        shell: bash

      # Get files created by release script
      - name: Get tgz-tarball (${{ matrix.mpi }})
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (Linux)
        run: |
              ls -l ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: Uncompress source (Linux)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Copy script files for the space (${{ matrix.mpi }})
        run: |
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

      - name: List files for the hdf5 (${{ matrix.mpi }})
        run: |
            ls ${{ runner.workspace }}/hdf5

      - name: Create options file (${{ matrix.mpi }})
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
            set (CTEST_DROP_SITE_INIT "my.cdash.org")
            # Change following line to submit to your CDash dashboard to a different CDash project
            #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
            set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
            set (ctest_test_args INCLUDE MPI_TEST)
            set (CTEST_TEST_TIMEOUT 600 CACHE STRING "Maximum test time allowed.")
            set (MODEL "MPI")
            set (GROUP "MPI")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }}")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DMPIEXEC_NUMPROC_FLAG:STRING=-n")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DMPIEXEC_MAX_NUMPROCS:STRING=2")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PARALLEL:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SUBFILING_VFD:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest script (${{ matrix.mpi }})
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=GH-${{ github.event.repository.full_name }}-${{ matrix.mpi }},LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C ${{ inputs.build_mode }} -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (${{ matrix.mpi }})
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: linux-${{ matrix.mpi }}-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log

  Build_parallel_intelmpi_macos:
    runs-on: macos-latest
    strategy:
      matrix:
        mpi: [ 'mpich', 'openmpi']
    name: "Parallel ${{ matrix.mpi }} macos-${{ inputs.build_mode }}"
    steps:
      - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install Dependencies (MacOS_latest)
        run: brew install ninja

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Setup MPI (${{ matrix.mpi }})
        id: setup-mpi
        uses: mpi4py/setup-mpi@d0a3bf17a182b37921ff27a6737f9009ec76d3b6 # v1
        with:
          mpi: ${{ matrix.mpi }}

      - name: Set MPI Environment Variables (${{ matrix.mpi }})
        run: |
          echo "CC=mpicc" >> $GITHUB_ENV
          echo "FC=mpif90" >> $GITHUB_ENV

      - name: Set file base name (${{ matrix.mpi }})
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (${{ matrix.mpi }})
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (${{ matrix.mpi }})
        run: |
            ls -l ${{ github.workspace }}
            ls ${{ runner.workspace }}

      - name: Uncompress source (${{ matrix.mpi }})
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Copy script files for the space (${{ matrix.mpi }})
        run: |
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

      - name: List files for the hdf5 (${{ matrix.mpi }})
        run: |
            ls ${{ runner.workspace }}/hdf5

      - name: Create options file (${{ matrix.mpi }})
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
            set (CTEST_DROP_SITE_INIT "my.cdash.org")
            # Change following line to submit to your CDash dashboard to a different CDash project
            #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
            set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
            set (ctest_test_args INCLUDE MPI_TEST)
            set (CTEST_TEST_TIMEOUT 600 CACHE STRING "Maximum test time allowed.")
            set (MODEL "MPI")
            set (GROUP "MPI")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }}")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DMPIEXEC_NUMPROC_FLAG:STRING=-n")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DMPIEXEC_MAX_NUMPROCS:STRING=2")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PARALLEL:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SUBFILING_VFD:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest script (${{ matrix.mpi }})
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=GH-${{ github.event.repository.full_name }}-${{ matrix.mpi }},LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C ${{ inputs.build_mode }} -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (${{ matrix.mpi }})
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: macos-${{ matrix.mpi }}-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log
```

### `.github/workflows/par-source.yml`

```yaml
name: hdf5 dev parallel from source CTest script runs

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      snap_name:
        description: 'The name in the source tarballs'
        type: string
        required: false
        default: hdfsrc
      file_base:
        description: "The common base name of the source tarballs"
        required: true
        type: string
      use_environ:
        description: 'Environment to locate files'
        type: string
        required: true
        default: snapshots
      build_mode:
        description: "release vs. debug build"
        required: true
        type: string
      cmake_version:
        description: "3.26.0 or later"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  Build_openmpi_source:
    name: Build OpenMPI from source
    uses: ./.github/workflows/build_openmpi_source.yml
    with:
      build_mode: ${{ inputs.build_mode }}

  Build_mpich_source:
    name: Build MPICH from source
    uses: ./.github/workflows/build_mpich_source.yml
    with:
      build_mode: ${{ inputs.build_mode }}

  Build_parallel_src_openmpi:
    needs: Build_openmpi_source
    name: "Parallel OpenMPI GCC-${{ inputs.build_mode }}"
    runs-on: ubuntu-latest
    steps:
      - name: Install Linux Dependencies (OpenMPI)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build
          sudo apt install libaec0 libaec-dev

      - name: Get MPI installation (OpenMPI)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: openmpi

      - name: Untar MPI installation (OpenMPI)
        run: |
          tar xvf openmpi.tar -C ${{ runner.workspace }}

      - name: Set path (OpenMPI)
        shell: bash
        run: |
          echo "${{ runner.workspace }}/openmpi/bin" >> $GITHUB_PATH
          echo "LD_LIBRARY_PATH=${{ runner.workspace }}/openmpi/lib:$LD_LIBRARY_PATH" >> $GITHUB_ENV
          echo "CC=${{ runner.workspace }}/openmpi/bin/mpicc" >> $GITHUB_ENV
          echo "FC=${{ runner.workspace }}/openmpi/bin/mpif90" >> $GITHUB_ENV

      - name: Install Doxygen
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Set file base name (OpenMPI)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (OpenMPI)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (OpenMPI)
        run: |
            ls -l ${{ github.workspace }}
            ls ${{ runner.workspace }}

      - name: Uncompress source (OpenMPI)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Copy script files for the space (OpenMPI)
        run: |
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

      - name: List files for the hdf5 (OpenMPI)
        run: |
            ls ${{ runner.workspace }}/hdf5

      - name: Create options file (OpenMPI)
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
            set (CTEST_DROP_SITE_INIT "my.cdash.org")
            # Change following line to submit to your CDash dashboard to a different CDash project
            #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
            set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
            set (ctest_test_args INCLUDE MPI_TEST)
            set (MODEL "MPI")
            set (GROUP "MPI")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DMPIEXEC_NUMPROC_FLAG:STRING=-n")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DMPIEXEC_MAX_NUMPROCS:STRING=2")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PARALLEL:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SUBFILING_VFD:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest script (OpenMPI)
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=GH-${{ github.event.repository.full_name }}-OpenMPI-source,LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C ${{ inputs.build_mode }} -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (OpenMPI)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: openmpi-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

  Build_parallel_src_mpich:
    needs: Build_mpich_source
    name: "Parallel Mpich GCC-${{ inputs.build_mode }}"
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Install Linux Dependencies (MPICH)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build
          sudo apt install libaec0 libaec-dev

      - name: Get MPI installation (MPICH)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: mpich

      - name: Untar MPI installation (MPICH)
        run: |
          tar xvf mpich.tar -C ${{ runner.workspace }}

      - name: Set path (MPICH)
        shell: bash
        run: |
          echo "${{ runner.workspace }}/mpich/bin" >> $GITHUB_PATH
          echo "LD_LIBRARY_PATH=${{ runner.workspace }}/mpich/lib:$LD_LIBRARY_PATH" >> $GITHUB_ENV
          echo "CC=${{ runner.workspace }}/mpich/bin/mpicc" >> $GITHUB_ENV
          echo "FC=${{ runner.workspace }}/mpich/bin/mpif90" >> $GITHUB_ENV

      - name: Install Doxygen
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Set file base name (MPICH)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (MPICH)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (MPICH)
        run: |
            ls -l ${{ github.workspace }}
            ls ${{ runner.workspace }}

      - name: Uncompress source (MPICH)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Copy script files for the space (MPICH)
        run: |
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

      - name: List files for the hdf5 (MPICH)
        run: |
            ls ${{ runner.workspace }}/hdf5

      - name: Create options file (MPICH)
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
            set (CTEST_DROP_SITE_INIT "my.cdash.org")
            # Change following line to submit to your CDash dashboard to a different CDash project
            #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
            set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
            set (ctest_test_args INCLUDE MPI_TEST)
            set (MODEL "MPI")
            set (GROUP "MPI")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DMPIEXEC_NUMPROC_FLAG:STRING=-n")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DMPIEXEC_MAX_NUMPROCS:STRING=2")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PARALLEL:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SUBFILING_VFD:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest script (MPICH)
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=GH-${{ github.event.repository.full_name }}-MPICH-source,LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C ${{ inputs.build_mode }} -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (MPICH)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: mpich-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
```

### `.github/workflows/publish-branch.yml`

```yaml
name: hdf5 publish files in HDF5 folder from branch to S3

# Triggers the workflow on demand
on:
  workflow_dispatch:
    inputs:
      local_dir:
        description: 'HDF5 local directory'
        type: string
        required: true
      target_dir:
        description: 'hdf5 target bucket directory'
        type: string
        required: true

permissions:
  contents: read

jobs:
  publish-tag:
    runs-on: ubuntu-latest
    steps:
        # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
        - name: Get Sources
          uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
          with:
            fetch-depth: 0
            ref: '${{ github.head_ref || github.ref_name }}'

        - name: List files for the space
          run: |
              ls -l ${{ github.workspace }}
              ls ${{ github.workspace }}/HDF5

        - name: Setup AWS CLI
          uses: aws-actions/configure-aws-credentials@61815dcd50bd041e203e49132bacad1fd04d2708 # v5
          with:
                 aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
                 aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
                 aws-region: ${{ secrets.AWS_REGION }}

        - name: Sync dir to S3 bucket
          run: |
                aws s3 sync ./HDF5/${{ inputs.local_dir }} s3://${{ secrets.AWS_S3_BUCKET }}/${{ vars.TARGET_PATH }}/${{ inputs.target_dir }}
```

### `.github/workflows/publish-release.yml`

```yaml
name: hdf5 publish release

# Triggers the workflow on demand
on:
  workflow_dispatch:
    inputs:
      use_tag:
        description: HDF5 Release version tag (e.g., v1.14.0)
        type: string
        required: true
      file_name:
        description: HDF5 Release file name base
        type: string
        required: true
      target_dir:
        description: HDF5 target bucket directory
        type: string
        required: true
      dry_run:
        description: Dry run mode (skip S3 upload)
        type: boolean
        default: false

permissions:
  contents: read

jobs:
  publish-tag:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Validate Inputs
        run: |
          set -euo pipefail
          echo "Validating inputs..."
          if [[ ! "${{ inputs.use_tag }}" =~ ^hdf5[_-][0-9]+[._][0-9]+[._][0-9]+([._][0-9]+)?(-.*)?$ ]]; then
            echo "âŒ Invalid tag format. Expected: hdf5_X.Y.Z, hdf5-X_Y_Z, or hdf5_X.Y.Z.W"
            exit 1
          fi
          if [[ "${{ inputs.target_dir }}" == *".."* ]] || [[ "${{ inputs.target_dir }}" == /* ]]; then
            echo "âŒ Invalid target_dir. Cannot contain '..' or start with '/'"
            exit 1
          fi
          if [[ "${{ inputs.file_name }}" == *".."* ]] || [[ "${{ inputs.file_name }}" == *"/"* ]] || [[ "${{ inputs.file_name }}" =~ [^a-zA-Z0-9._-] ]]; then
            echo "âŒ Invalid file_name. Can only contain alphanumeric, dots, underscores, and hyphens"
            exit 1
          fi
          echo "âœ… Input validation passed"

      # Checks-out your repository under $GITHUB_WORKSPACE
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # Updated to latest stable v6.0.0
        with:
          fetch-depth: 0
          ref: '${{ github.head_ref || github.ref_name }}'

      - name: Create download directory
        run: mkdir -p HDF5

      - name: Get HDF5 release assets
        uses: robinraju/release-downloader@daf26c55d821e836577a15f77d86ddc078948b05 # More reliable alternative v1.12
        with:
          repository: HDFGroup/hdf5
          tag: ${{ inputs.use_tag }}
          fileName: ${{ inputs.file_name }}*
          out-file-path: HDF5
          extract: false

      - name: Verify downloaded files
        run: |
          set -euo pipefail
          echo "ðŸ“ Downloaded files:"
          ls -la HDF5/
          
          # Check for expected files
          EXPECTED_FILES=("${{ inputs.file_name }}.doxygen.zip" "${{ inputs.file_name }}.html.abi.reports.tar.gz")
          for file in "${EXPECTED_FILES[@]}"; do
            if [ ! -f "HDF5/$file" ]; then
              echo "âš ï¸  Warning: Expected file not found: $file"
            else
              echo "âœ… Found: $file"
            fi
          done

      - name: Setup AWS CLI
        if: ${{ !inputs.dry_run }}
        uses: aws-actions/configure-aws-credentials@61815dcd50bd041e203e49132bacad1fd04d2708 # v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Generate index.html for downloads directory
        run: |
          set -euo pipefail
          echo "ðŸ“„ Generating index.html for downloads directory..."
          chmod +x .github/scripts/generate-index-html.sh
          .github/scripts/generate-index-html.sh \
            "./HDF5" \
            "HDF5 ${{ inputs.use_tag }} - Downloads" \
            "Release binaries, source code, and documentation packages for HDF5 ${{ inputs.use_tag }}" \
            "../"
          echo "âœ… Downloads index.html generated"

      - name: Sync release files to S3 bucket
        if: ${{ !inputs.dry_run }}
        run: |
          set -euo pipefail
          echo "ðŸš€ Syncing release files to S3..."
          aws s3 sync ./HDF5 s3://${{ secrets.AWS_S3_BUCKET }}/${{ vars.TARGET_PATH }}/${{ inputs.target_dir }}/downloads \
            --delete \
            --exclude="*" \
            --include="*.tar.gz" \
            --include="*.zip" \
            --include="*.msi" \
            --include="*.dmg" \
            --include="*.exe" \
            --include="*.sha256" \
            --include="index.html" \
            --exact-timestamps

          # Upload index.html with proper content type
          aws s3 cp ./HDF5/index.html \
            s3://${{ secrets.AWS_S3_BUCKET }}/${{ vars.TARGET_PATH }}/${{ inputs.target_dir }}/downloads/index.html \
            --content-type "text/html" \
            --metadata-directive REPLACE
          echo "âœ… Release files sync completed"

      - name: Process documentation
        run: |
          set -euo pipefail
          DOC_FILE="HDF5/${{ inputs.file_name }}.doxygen.zip"
          if [ -f "$DOC_FILE" ]; then
            echo "ðŸ“š Processing documentation..."
            unzip -q "$DOC_FILE"
            if [ -d "${{ inputs.file_name }}.doxygen" ]; then
              echo "âœ… Documentation extracted successfully"
            else
              echo "âŒ Documentation extraction failed"
              exit 1
            fi
          else
            echo "âš ï¸  Documentation file not found, skipping..."
          fi

      - name: Generate index.html for documentation directory
        run: |
          set -euo pipefail
          if [ -d "${{ inputs.file_name }}.doxygen" ]; then
            echo "ðŸ“„ Generating index.html for documentation directory..."
            .github/scripts/generate-index-html.sh \
              "./${{ inputs.file_name }}.doxygen" \
              "HDF5 ${{ inputs.use_tag }} - Documentation" \
              "Doxygen API documentation for HDF5 ${{ inputs.use_tag }}" \
              "../../"
            echo "âœ… Documentation index.html generated"
          else
            echo "âš ï¸  No documentation directory found, skipping index generation..."
          fi

      - name: Sync documentation to S3 bucket
        if: ${{ !inputs.dry_run }}
        run: |
          set -euo pipefail
          if [ -d "${{ inputs.file_name }}.doxygen" ]; then
            echo "ðŸ“š Syncing documentation to S3..."
            aws s3 sync ./${{ inputs.file_name }}.doxygen \
              s3://${{ secrets.AWS_S3_BUCKET }}/${{ vars.TARGET_PATH }}/${{ inputs.target_dir }}/documentation/doxygen \
              --delete \
              --content-type "text/html" \
              --metadata-directive REPLACE
            echo "âœ… Documentation sync completed"
          else
            echo "âš ï¸  No documentation directory found, skipping..."
          fi

      - name: Process compatibility reports
        run: |
          set -euo pipefail
          COMPAT_FILE="HDF5/${{ inputs.file_name }}.html.abi.reports.tar.gz"
          if [ -f "$COMPAT_FILE" ]; then
            echo "ðŸ“Š Processing compatibility reports..."
            tar -xzf "$COMPAT_FILE"
            if [ -d "hdf5" ]; then
              echo "âœ… Compatibility reports extracted successfully"
            else
              echo "âŒ Compatibility reports extraction failed"
              exit 1
            fi
          else
            echo "âš ï¸  Compatibility reports file not found, skipping..."
          fi

      - name: Generate index.html for compatibility reports directory
        run: |
          set -euo pipefail
          if [ -d "hdf5" ]; then
            echo "ðŸ“„ Generating index.html for compatibility reports directory..."
            .github/scripts/generate-index-html.sh \
              "./hdf5" \
              "HDF5 ${{ inputs.use_tag }} - Compatibility Reports" \
              "ABI/API compatibility reports for HDF5 ${{ inputs.use_tag }}" \
              "../"
            echo "âœ… Compatibility reports index.html generated"
          else
            echo "âš ï¸  No compatibility reports directory found, skipping index generation..."
          fi

      - name: Sync compatibility reports to S3 bucket
        if: ${{ !inputs.dry_run }}
        run: |
          set -euo pipefail
          if [ -d "hdf5" ]; then
            echo "ðŸ“Š Syncing compatibility reports to S3..."
            aws s3 sync ./hdf5 \
              s3://${{ secrets.AWS_S3_BUCKET }}/${{ vars.TARGET_PATH }}/${{ inputs.target_dir }}/compat_report \
              --delete \
              --content-type "text/html" \
              --metadata-directive REPLACE
            echo "âœ… Compatibility reports sync completed"
          else
            echo "âš ï¸  No compatibility reports directory found, skipping..."
          fi

      - name: Generate main release directory index.html
        run: |
          set -euo pipefail
          echo "ðŸ“„ Generating main release directory index.html..."

          # Create a temporary directory structure to mimic the S3 layout
          mkdir -p release_root/${{ inputs.target_dir }}/{downloads,documentation,compat_report}

          # Create placeholder files so the script can list them
          touch "release_root/${{ inputs.target_dir }}/downloads/.placeholder"
          touch "release_root/${{ inputs.target_dir }}/documentation/.placeholder"
          touch "release_root/${{ inputs.target_dir }}/compat_report/.placeholder"

          # Generate index for the release directory
          .github/scripts/generate-index-html.sh \
            "release_root/${{ inputs.target_dir }}" \
            "HDF5 ${{ inputs.use_tag }}" \
            "Release files, documentation, and compatibility reports for HDF5 ${{ inputs.use_tag }}" \
            "../"

          echo "âœ… Main release index.html generated"

      - name: Upload main release directory index.html
        if: ${{ !inputs.dry_run }}
        run: |
          set -euo pipefail
          echo "ðŸ“¤ Uploading main release directory index.html..."
          aws s3 cp "release_root/${{ inputs.target_dir }}/index.html" \
            s3://${{ secrets.AWS_S3_BUCKET }}/${{ vars.TARGET_PATH }}/${{ inputs.target_dir }}/index.html \
            --content-type "text/html" \
            --metadata-directive REPLACE
          echo "âœ… Main index.html uploaded"

      - name: Summary
        run: |
          set -euo pipefail
          echo "ðŸŽ‰ HDF5 Release Publication Summary"
          echo "=================================="
          echo "ðŸ·ï¸  Tag: ${{ inputs.use_tag }}"
          echo "ðŸ“ File Base: ${{ inputs.file_name }}"
          echo "ðŸŽ¯ Target Directory: ${{ inputs.target_dir }}"
          echo "ðŸŒ Dry Run: ${{ inputs.dry_run }}"
          echo ""
          if [ "${{ inputs.dry_run }}" == "true" ]; then
            echo "â„¹ï¸  This was a dry run - no files were uploaded to S3"
          else
            echo "âœ… Release published successfully!"
            echo "ðŸ“ Main page: s3://${{ secrets.AWS_S3_BUCKET }}/${{ vars.TARGET_PATH }}/${{ inputs.target_dir }}/index.html"
            echo "ðŸ“ Downloads: s3://${{ secrets.AWS_S3_BUCKET }}/${{ vars.TARGET_PATH }}/${{ inputs.target_dir }}/downloads"
            echo "ðŸ“– Documentation: s3://${{ secrets.AWS_S3_BUCKET }}/${{ vars.TARGET_PATH }}/${{ inputs.target_dir }}/documentation/doxygen"
            echo "ðŸ“Š Reports: s3://${{ secrets.AWS_S3_BUCKET }}/${{ vars.TARGET_PATH }}/${{ inputs.target_dir }}/compat_report"
          fi
```

### `.github/workflows/release-files.yml`

```yaml
name: hdf5 dev release-files

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      use_tag:
        description: 'Release version tag'
        type: string
        required: false
        default: snapshot
      use_environ:
        description: 'Environment to locate files'
        type: string
        required: true
        default: snapshots
      file_base:
        description: "The common base name of the source tarballs"
        required: true
        type: string
      file_branch:
        description: "The branch name for the source tarballs"
        required: true
        type: string
      file_sha:
        description: "The sha for the source tarballs"
        required: true
        type: string

# Minimal permissions to be inherited by any job that doesn't declare its own permissions
permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

# Previous workflows must pass to get here so tag the commit that created the files
jobs:
  create-tag:
    runs-on: ubuntu-latest
    permissions:
        contents: write # In order to allow tag creation
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          fetch-depth: 0
          ref: '${{ github.head_ref || github.ref_name }}'

      - uses: rickstaa/action-create-tag@a1c7777fcb2fee4f19b0f283ba888afa11678b72 # v1.7.2
        id: "tag_create"
        with:
          commit_sha: ${{ inputs.file_sha }}
          tag: "${{ inputs.use_tag }}"
          force_push_tag: false
          tag_exists_error: false
          message: "Latest snapshot"

      # Print result using the action output.
      - run: |
          echo "Tag already present: ${{ steps.tag_create.outputs.tag_exists }}"

  PreRelease-getfiles:
    runs-on: ubuntu-latest
    needs: create-tag
    environment: ${{ inputs.use_environ }}
    permissions:
        contents: write
    steps:
      - name: Get file base name
        id: get-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by tarball script
      - name: Get doxygen (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: docs-doxygen
              path: ${{ github.workspace }}/${{ steps.get-file-base.outputs.FILE_BASE }}.doxygen

      - name: Zip Folder
        run: zip -r ${{ steps.get-file-base.outputs.FILE_BASE }}.doxygen.zip ./${{ steps.get-file-base.outputs.FILE_BASE }}.doxygen

      - name: Get tgz-tarball (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: Get zip-tarball (Windows)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: zip-tarball
              path: ${{ github.workspace }}

      # Get files created by cmake-ctest script
      - name: Get published binary (Windows)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: zip-vs2022_cl-binary
              path: ${{ github.workspace }}

      - name: Get published msi binary (Windows)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: msi-vs2022_cl-binary
              path: ${{ github.workspace }}

      - name: Get published binary (MacOS)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-macos14_clang-binary
              path: ${{ github.workspace }}

      - name: Get published dmg binary (MacOS)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-macos14_clang-dmg-binary
              path: ${{ github.workspace }}

      - name: Get published binary (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-ubuntu-2404_gcc-binary
              path: ${{ github.workspace }}

      - name: Get published deb binary (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: deb-ubuntu-2404_gcc-binary
              path: ${{ github.workspace }}

      - name: Get published rpm binary (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: rpm-ubuntu-2404_gcc-binary
              path: ${{ github.workspace }}

      - name: Get published binary (Linux S3)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-ubuntu-2404_gcc_s3-binary
              path: ${{ github.workspace }}

      - name: Get published binary (Windows_intel)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: zip-vs2022_intel-binary
              path: ${{ github.workspace }}

      - name: Get published msi binary (Windows_intel)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: msi-vs2022_intel-binary
              path: ${{ github.workspace }}

      - name: Get published binary (Linux_intel)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-ubuntu-2404_intel-binary
              path: ${{ github.workspace }}

      - name: Get published abi reports (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: abi-reports
              path: ${{ github.workspace }}

      - name: Get published nonversioned source (tgz)
        if: ${{ (inputs.use_environ == 'release') }}
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-tarball-nover
              path: ${{ github.workspace }}

      - name: Get published nonversioned source (zip)
        if: ${{ (inputs.use_environ == 'release') }}
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: zip-tarball-nover
              path: ${{ github.workspace }}

      - name: Create sha256 sums for files
        run: |
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}.doxygen.zip > ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}.tar.gz >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}.zip >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}-macos14_clang.tar.gz >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}-macos14_clang.dmg >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.tar.gz >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.deb >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.rpm >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc_s3.tar.gz >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_cl.zip >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_cl.msi >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_intel.tar.gz >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_intel.zip >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_intel.msi >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum ${{ steps.get-file-base.outputs.FILE_BASE }}.html.abi.reports.tar.gz >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt

      - name: Create sha256 sums for files  for nonversioned files
        if: ${{ (inputs.use_environ == 'release') }}
        run: |
          sha256sum hdf5.zip >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
          sha256sum hdf5.tar.gz >> ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt

      - name: Store snapshot name
        run: |
          echo "${{ steps.get-file-base.outputs.FILE_BASE }}" > ./last-file.txt

      - name: Get description.txt for release/snapshot
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
                name: description.txt
                path: ${{ github.workspace }}

      - name: PreRelease tag
        id: create_prerelease
        if: ${{ (inputs.use_environ == 'snapshots') }}
        uses: softprops/action-gh-release@a06a81a03ee405af7f2048a818ed3f03bbf83c7b # v2.5.0
        with:
          tag_name: "${{ inputs.use_tag }}"
          prerelease: true
          body_path: description.txt
          files: |
              last-file.txt
              ${{ steps.get-file-base.outputs.FILE_BASE }}.doxygen.zip
              ${{ steps.get-file-base.outputs.FILE_BASE }}.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}.zip
              ${{ steps.get-file-base.outputs.FILE_BASE }}-macos14_clang.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}-macos14_clang.dmg
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.deb
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.rpm
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc_s3.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_cl.zip
              ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_cl.msi
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_intel.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_intel.zip
              ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_intel.msi
              ${{ steps.get-file-base.outputs.FILE_BASE }}.html.abi.reports.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

      - name: Release tag name
        id: create_release_tag_name
        if: ${{ (inputs.use_environ == 'release') }}
        run: |
          if [[ '${{ inputs.use_tag }}' == 'snapshot' ]]
          then
            TAG_NAME_BASE=$(echo "snapshot")
          else
            TAG_NAME_BASE=$(echo "${{ inputs.use_tag }}")
          fi
          echo "TAG_BASE=$TAG_NAME_BASE" >> $GITHUB_OUTPUT
        shell: bash

      - name: Release tag
        id: create_release
        if: ${{ (inputs.use_environ == 'release') }}
        uses: softprops/action-gh-release@a06a81a03ee405af7f2048a818ed3f03bbf83c7b # v2.5.0
        with:
          tag_name: "${{ steps.create_release_tag_name.outputs.TAG_BASE }}"
          name: "HDF5 Release ${{ inputs.use_tag }}"
          prerelease: false
          body_path: description.txt
          files: |
              ${{ steps.get-file-base.outputs.FILE_BASE }}.doxygen.zip
              ${{ steps.get-file-base.outputs.FILE_BASE }}.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}.zip
              hdf5.tar.gz
              hdf5.zip
              ${{ steps.get-file-base.outputs.FILE_BASE }}-macos14_clang.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}-macos14_clang.dmg
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.deb
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.rpm
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc_s3.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_cl.zip
              ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_cl.msi
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_intel.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_intel.zip
              ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_intel.msi
              ${{ steps.get-file-base.outputs.FILE_BASE }}.html.abi.reports.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

      - name: List files for the space (Linux)
        run: |
              ls -l ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: dev-only-docs
        uses: peaceiris/actions-gh-pages@4f9cc6602d3f66b9c108549d475ec49e8ef4d45e # v4.0.0
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ${{ github.workspace }}/${{ steps.get-file-base.outputs.FILE_BASE }}.doxygen
          destination_dir: develop
```

### `.github/workflows/release.yml`

```yaml
name: hdf5 dev release build

# Triggers the workflow on demand
on:
  workflow_dispatch:
    inputs:
      use_tag:
        description: 'Release version tag'
        type: string
        required: false
        default: snapshot
      deploy_maven:
        description: 'Deploy artifacts to Maven repository'
        type: boolean
        required: false
        default: false
      maven_repository:
        description: 'Maven repository type'
        type: choice
        options:
          - github-packages
          - maven-central-staging
        required: false
        default: github-packages

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  log-the-inputs:
    runs-on: ubuntu-latest
    outputs:
      rel_tag: ${{ steps.get-tag-name.outputs.RELEASE_TAG }}
    steps:
    - name: Get tag name
      id: get-tag-name
      env:
        TAG: ${{ inputs.use_tag }}
      run: echo "RELEASE_TAG=$TAG" >> $GITHUB_OUTPUT

  call-workflow-tarball:
    needs: log-the-inputs
    uses: ./.github/workflows/tarball.yml
    with:
      use_tag: ${{ needs.log-the-inputs.outputs.rel_tag }}
      use_environ: release

  call-aws-c-s3-build:
    needs: call-workflow-tarball
    name: "Build aws-c-s3 library"
    uses: ./.github/workflows/build-aws-c-s3.yml
    with:
      build_mode: "Release"
      # Use latest release for building from source on Ubuntu
      # until a package is available to install
      aws_c_s3_tag: "v0.8.0"

  call-workflow-ctest:
    needs: [call-workflow-tarball, call-aws-c-s3-build]
    uses: ./.github/workflows/ctest.yml
    with:
      cmake_version: "latest"
      preset_name: ci-StdShar
      file_base: ${{ needs.call-workflow-tarball.outputs.file_base }}
      snap_name: hdf5-${{ needs.call-workflow-tarball.outputs.source_base }}
      use_environ: release
    secrets:
        APPLE_CERTS_BASE64: ${{ secrets.APPLE_CERTS_BASE64 }}
        APPLE_CERTS_BASE64_PASSWD: ${{ secrets.APPLE_CERTS_BASE64_PASSWD }}
        KEYCHAIN_PASSWD: ${{ secrets.KEYCHAIN_PASSWD }}
        AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
        AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
        AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
        AZURE_ENDPOINT: ${{ secrets.AZURE_ENDPOINT }}
        AZURE_CODE_SIGNING_NAME: ${{ secrets.AZURE_CODE_SIGNING_NAME }}
        AZURE_CERT_PROFILE_NAME: ${{ secrets.AZURE_CERT_PROFILE_NAME }}

  call-workflow-abi:
    needs: [log-the-inputs, call-workflow-tarball, call-workflow-ctest]
    uses: ./.github/workflows/abi-report.yml
    with:
      file_ref: '1.14.5'
      file_base: ${{ needs.call-workflow-tarball.outputs.file_base }}
      use_tag: ${{ needs.log-the-inputs.outputs.rel_tag }}
      use_environ: release

  call-workflow-release:
    needs: [log-the-inputs, call-workflow-tarball, call-workflow-ctest, call-workflow-abi]
    permissions:
      contents: write # In order to allow tag creation
    uses: ./.github/workflows/release-files.yml
    with:
      file_base: ${{ needs.call-workflow-tarball.outputs.file_base }}
      file_branch: ${{ needs.call-workflow-tarball.outputs.file_branch }}
      file_sha: ${{ needs.call-workflow-tarball.outputs.file_sha }}
      use_tag: ${{ needs.log-the-inputs.outputs.rel_tag }}
      use_environ: release

  call-workflow-maven-staging:
    needs: [log-the-inputs, call-workflow-tarball]
    if: ${{ inputs.deploy_maven == true }}
    permissions:
      contents: read
      packages: write
      pull-requests: write
    uses: ./.github/workflows/maven-staging.yml
    with:
      test_maven_deployment: true
      use_snapshot_version: ${{ needs.log-the-inputs.outputs.rel_tag == 'snapshot' }}
      platforms: 'all-platforms'
      java_implementation: 'both'  # Build both FFM and JNI implementations

  deploy-maven-artifacts:
    name: Deploy Maven Artifacts
    needs: [log-the-inputs, call-workflow-tarball, call-workflow-ctest, call-workflow-abi, call-workflow-maven-staging]
    if: ${{ inputs.deploy_maven == true }}
    permissions:
      contents: read
      packages: write
    uses: ./.github/workflows/maven-deploy.yml
    with:
      # Note: file_base and preset_name are legacy parameters and not used by maven-deploy.yml
      repository_url: ${{ inputs.maven_repository == 'github-packages' && format('https://maven.pkg.github.com/{0}', github.repository) || 'https://s01.oss.sonatype.org/service/local/staging/deploy/maven2/' }}
      repository_id: ${{ inputs.maven_repository == 'github-packages' && 'github' || 'ossrh' }}
      deploy_snapshots: ${{ inputs.use_tag == 'snapshot' }}
      dry_run: false
    secrets:
      MAVEN_USERNAME: ${{ inputs.maven_repository == 'github-packages' && github.actor || secrets.MAVEN_CENTRAL_USERNAME }}
      MAVEN_PASSWORD: ${{ inputs.maven_repository == 'github-packages' && secrets.GITHUB_TOKEN || secrets.MAVEN_CENTRAL_PASSWORD }}
      GPG_PRIVATE_KEY: ${{ secrets.GPG_PRIVATE_KEY }}
      GPG_PASSPHRASE: ${{ secrets.GPG_PASSPHRASE }}

  test-deployed-maven-packages:
    name: Test Deployed Maven Packages
    needs: [log-the-inputs, call-workflow-maven-staging, deploy-maven-artifacts]
    if: ${{ inputs.deploy_maven == true && needs.deploy-maven-artifacts.result == 'success' }}
    permissions:
      contents: read
      packages: read
    uses: ./.github/workflows/test-maven-packages.yml
    with:
      version: ${{ needs.call-workflow-maven-staging.outputs.version }}
      java_implementation: 'both'
      repository_url: ${{ inputs.maven_repository == 'github-packages' && format('https://maven.pkg.github.com/{0}', github.repository) || 'https://repo.maven.apache.org/maven2' }}
    secrets: inherit
```

### `.github/workflows/remove-files.yml`

```yaml
name: hdf5 dev remove-files

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      use_tag:
        description: 'Release version tag'
        type: string
        required: false
        default: snapshot
      use_environ:
        description: 'Environment to locate files'
        type: string
        required: true
        default: snapshots
      file_base:
        description: "The common base name of the source tarballs"
        required: true
        type: string

# Minimal permissions to be inherited by any job that doesn't declare its own permissions
permissions:
  contents: read

# Previous workflows must pass to get here so tag the commit that created the files
jobs:
  PreRelease-delfiles:
    runs-on: ubuntu-latest
    environment: ${{ inputs.use_environ }}
    permissions:
        contents: write
    steps:
      - name: Get file base name
        id: get-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT

      - name: PreRelease delete from tag
        id: delete_prerelease
        if: ${{ (inputs.use_environ == 'snapshots') }}
        uses: mknejp/delete-release-assets@6a8fd1fd4ce4296fde58ab811acf5fc13fef3cc9 # v1
        with:
          token: ${{ github.token }}
          tag: "${{ inputs.use_tag }}"
          assets: |
              ${{ steps.get-file-base.outputs.FILE_BASE }}.sha256sums.txt
              ${{ steps.get-file-base.outputs.FILE_BASE }}.html.abi.reports.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}.doxygen.zip
              ${{ steps.get-file-base.outputs.FILE_BASE }}.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}.zip
              ${{ steps.get-file-base.outputs.FILE_BASE }}-macos14_clang.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}-macos14_clang.dmg
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.deb
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc.rpm
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_gcc_s3.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_cl.zip
              ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_cl.msi
              ${{ steps.get-file-base.outputs.FILE_BASE }}-ubuntu-2404_intel.tar.gz
              ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_intel.zip
              ${{ steps.get-file-base.outputs.FILE_BASE }}-win-vs2022_intel.msi
```

### `.github/workflows/scorecard.yml`

```yaml
# This workflow uses actions that are not certified by GitHub. They are provided
# by a third-party and are governed by separate terms of service, privacy
# policy, and support documentation.

name: Scorecard supply-chain security
on:
  # For Branch-Protection check. Only the default branch is supported. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#branch-protection
  branch_protection_rule:
  # To guarantee Maintained check is occasionally updated. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#maintained
  schedule:
    - cron: '36 22 * * 4'
  push:
    branches: [ "develop" ]

# Declare default permissions as read only.
permissions: read-all

jobs:
  analysis:
    name: Scorecard analysis
    runs-on: ubuntu-latest
    permissions:
      # Needed to upload the results to code-scanning dashboard.
      security-events: write
      # Needed to publish results and get a badge (see publish_results below).
      id-token: write
      # Uncomment the permissions below if installing in a private repository.
      # contents: read
      # actions: read

    steps:
      - name: "Checkout code"
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          persist-credentials: false

      - name: "Run analysis"
        uses: ossf/scorecard-action@4eaacf0543bb3f2c246792bd56e8cdeffafb205a # v2.4.3
        with:
          results_file: results.sarif
          results_format: sarif
          # (Optional) "write" PAT token. Uncomment the `repo_token` line below if:
          # - you want to enable the Branch-Protection check on a *public* repository, or
          # - you are installing Scorecard on a *private* repository
          # To create the PAT, follow the steps in https://github.com/ossf/scorecard-action#authentication-with-pat.
          # repo_token: ${{ secrets.SCORECARD_TOKEN }}

          # Public repositories:
          #   - Publish results to OpenSSF REST API for easy access by consumers
          #   - Allows the repository to include the Scorecard badge.
          #   - See https://github.com/ossf/scorecard-action#publishing-results.
          # For private repositories:
          #   - `publish_results` will always be set to `false`, regardless
          #     of the value entered here.
          publish_results: true

      # Upload the results as artifacts (optional). Commenting out will disable uploads of run results in SARIF
      # format to the repository Actions tab.
      - name: "Upload artifact"
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: SARIF file
          path: results.sarif
          retention-days: 5

      # Upload the results to GitHub's code scanning dashboard.
      - name: "Upload to code-scanning"
        uses: github/codeql-action/upload-sarif@fe4161a26a8629af62121b670040955b330f9af2 # v3.29.5
        with:
          sarif_file: results.sarif
```

### `.github/workflows/script.yml`

```yaml
name: hdf5 dev CTest script runs

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      snap_name:
        description: 'The name in the source tarballs'
        type: string
        required: false
        default: hdfsrc
      file_base:
        description: "The common base name of the source tarballs"
        required: true
        type: string
      use_environ:
        description: 'Environment to locate files'
        type: string
        required: true
        default: snapshots
      cmake_version:
        description: "3.26.0 or later, latest"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  build_and_test_win:
  # Windows w/ MSVC
  #
    name: "Windows MSVC CTest"
    runs-on: windows-latest
    steps:
      - name: Install Dependencies (Windows)
        run: choco install ninja

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Enable Developer Command Prompt
        uses: ilammy/msvc-dev-cmd@0b201ec74fa43914dc39ae48a89fd1d8cb592756 # v1.13.0

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Check CMake Version
        shell: bash
        run: |
          which cmake
          cmake --version

      - name: Set file base name (Windows)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT
        shell: bash

      # Get files created by release script
      - name: Get zip-tarball (Windows)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: zip-tarball
              path: ${{ github.workspace }}

      - name: using powershell
        shell: pwsh
        run: Get-Location

      - name: List files for the space (Windows)
        run: |
              Get-ChildItem -Path ${{ github.workspace }}
              Get-ChildItem -Path ${{ runner.workspace }}
        shell: pwsh

      - name: Uncompress source (Windows)
        working-directory: ${{ github.workspace }}
        run: 7z x ${{ steps.set-file-base.outputs.FILE_BASE }}.zip
        shell: bash

      - name: Copy script files for the space (Windows)
        run: |
          Copy-Item -Path ${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake -Destination ${{ runner.workspace }}/hdf5/
          Copy-Item -Path ${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake -Destination ${{ runner.workspace }}/hdf5/
        shell: pwsh

      - name: List files for the hdf5 (Windows)
        run: |
          Get-ChildItem -Path ${{ runner.workspace }}/hdf5
        shell: pwsh

      - name: Create options file (Windows)
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
            path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
            write-mode: overwrite
            contents: |
              set (CTEST_DROP_SITE_INIT "my.cdash.org")
              # Change following line to submit to your CDash dashboard to a different CDash project
              #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
              set (MODEL "GHDaily")
              set (GROUP "GHDaily")
              set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest script (Windows)
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=GH-${{ github.event.repository.full_name }},LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=VS202264,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C Release -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (Windows_intel)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: cl-win-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

  build_and_test_linux:
  # Linux (Ubuntu) w/ gcc
  #
    name: "Ubuntu gcc"
    runs-on: ubuntu-latest
    steps:
      - name: Install Dependencies (Linux)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build graphviz curl

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Set file base name (Linux)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (Linux)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (Linux)
        run: |
            ls -l ${{ github.workspace }}
            ls ${{ runner.workspace }}

      - name: Uncompress source (Linux)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Copy script files for the space (Linux)
        run: |
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

      - name: List files for the hdf5 (Linux)
        run: |
            ls ${{ runner.workspace }}/hdf5

      - name: Create options file (Linux)
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
            set (CTEST_DROP_SITE_INIT "my.cdash.org")
            # Change following line to submit to your CDash dashboard to a different CDash project
            #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
            set (MODEL "GHDaily")
            set (GROUP "GHDaily")
            set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest (Linux)
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=GH-${{ github.event.repository.full_name }}-GCC,LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C Release -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (Linux)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
            name: gcc-ubuntu-log
            path: ${{ runner.workspace }}/hdf5/hdf5.log
            if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

  build_and_test_mac_latest:
  # MacOS w/ Clang
  #
    name: "MacOS Clang"
    runs-on: macos-latest
    steps:
      - name: Install Dependencies (MacOS_latest)
        run: brew install ninja curl

      - name: Install Dependencies
        uses: ssciwr/doxygen-install@501e53b879da7648ab392ee226f5b90e42148449 # v1
        with:
          version: "1.13.2"

      - name: check clang version
        shell: bash
        run: |
          which clang
          clang -v

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Check CMake Version
        shell: bash
        run: |
          which cmake
          cmake --version

      - name: Set up JDK 19
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '21'
          distribution: 'temurin'

      - name: Set file base name (MacOS_latest)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (MacOS_latest)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (MacOS_latest)
        run: |
              ls ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: Uncompress source (MacOS_latest)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Copy script files for the space (MacOS_latest)
        run: |
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

      # symlinks the compiler executables to a common location 
      - name: Setup GNU Fortran
        uses: fortran-lang/setup-fortran@47809fdb6e637da656ce9ada436527b240c1287f # v1
        id: setup-fortran
        with:
          compiler: gcc
          version: 14

      - name: List files for the hdf5 (MacOS_latest)
        run: |
              ls ${{ runner.workspace }}/hdf5

      - name: Create options file (MacOS_latest)
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
            set (CTEST_DROP_SITE_INIT "my.cdash.org")
            # Change following line to submit to your CDash dashboard to a different CDash project
            #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
            set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
            set (MODEL "GHDaily")
            set (GROUP "GHDaily")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest (MacOS_latest)
        id: run-ctest
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=GH-${{ github.event.repository.full_name }}-Clang,LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C Release -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (MacOS_latest)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: macos-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

  build_and_test_S3_linux:
  # Linux S3 (Ubuntu) w/ gcc
  #
    name: "Ubuntu gcc S3"
    runs-on: ubuntu-latest
    steps:
      - name: Install Dependencies (Linux S3)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Install aws-c-s3 (Cached installation)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: libaws-c-s3-Release

      - name: Untar libaws-c-s3 installation
        run: |
          tar xvf libaws-c-s3.tar -C ${{ runner.workspace }}

      - name: List contents of libaws-c-s3 installation
        run: |
          ls -laR ${{ runner.workspace }}/aws-c-s3-build

      - name: Setup environment
        shell: bash
        run: |
          echo "LD_LIBRARY_PATH=${{ runner.workspace }}/aws-c-s3-build/lib:$LD_LIBRARY_PATH" >> $GITHUB_ENV
          echo "CMAKE_PREFIX_PATH=${{ runner.workspace }}/aws-c-s3-build" >> $GITHUB_ENV

      - name: Set file base name (Linux S3)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (Linux S3)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (Linux S3)
        run: |
              ls -l ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: Uncompress source (Linux S3)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Copy script files for the space (Linux S3)
        run: |
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

      - name: List files for the hdf5 (Linux S3)
        run: |
              ls ${{ runner.workspace }}/hdf5

      - name: Create options file (Linux S3)
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
            set (CTEST_DROP_SITE_INIT "my.cdash.org")
            # Change following line to submit to your CDash dashboard to a different CDash project
            #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
            set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
            set (MODEL "GHDaily")
            set (GROUP "GHDaily")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest (Linux S3)
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=GH-${{ github.event.repository.full_name }}-S3,LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C Release -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (Linux S3)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: s3-ubuntu-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

####### intel builds
  build_and_test_win_intel:
  # Windows w/ OneAPI
  #
    name: "Windows Intel CTest"
    runs-on: windows-latest
    steps:
      - name: Install Dependencies (Windows_intel)
        run: choco install ninja

      - name: add oneAPI to env
        uses: fortran-lang/setup-fortran@47809fdb6e637da656ce9ada436527b240c1287f # v1
        id: setup-fortran
        with:
          compiler: intel
          version: '2025.0'

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Set file base name (Windows_intel)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT
        shell: bash

      # Get files created by release script
      - name: Get zip-tarball (Windows_intel)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: zip-tarball
              path: ${{ github.workspace }}

      - name: using powershell
        shell: pwsh
        run: Get-Location

      - name: List files for the space (Windows_intel)
        run: |
              Get-ChildItem -Path ${{ github.workspace }}
              Get-ChildItem -Path ${{ runner.workspace }}
        shell: pwsh

      - name: Uncompress source (Windows_intel)
        working-directory: ${{ github.workspace }}
        run: 7z x ${{ steps.set-file-base.outputs.FILE_BASE }}.zip
        shell: bash

      - name: Copy script files for the space (Windows_intel)
        run: |
          Copy-Item -Path ${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake -Destination ${{ runner.workspace }}/hdf5/
          Copy-Item -Path ${{ runner.workspace }}/hdf5/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake -Destination ${{ runner.workspace }}/hdf5/
        shell: pwsh

      - name: List files for the hdf5 (Windows_intel)
        run: |
              Get-ChildItem -Path ${{ runner.workspace }}/hdf5
        shell: pwsh

      - name: Create options file (Windows_intel)
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
            set (CTEST_DROP_SITE_INIT "my.cdash.org")
            # Change following line to submit to your CDash dashboard to a different CDash project
            #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
            #set (CMAKE_GENERATOR_TOOLSET "Intel C++ Compiler 2024,fortran=ifx")
            set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
            set (MODEL "GHDaily")
            set (GROUP "GHDaily")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DCMAKE_TOOLCHAIN_FILE:STRING=config/toolchain/intel.cmake")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest (Windows_intel) with oneapi
        env:
          FC: ${{ steps.setup-fortran.outputs.fc }}
          CC: ${{ steps.setup-fortran.outputs.cc }}
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=GH-${{ github.event.repository.full_name }}-Intel,LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=VS202264,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C Release -O hdf5.log
        shell: pwsh
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (Windows_intel)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: intel-win-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

  build_and_test_linux_intel:
  # Linux (Ubuntu) w/ OneAPI
  #
    name: "Ubuntu Intel"
    runs-on: ubuntu-latest
    steps:
      - name: Install Dependencies (Linux_intel)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz curl

      - name: add oneAPI to env
        uses: fortran-lang/setup-fortran@47809fdb6e637da656ce9ada436527b240c1287f # v1
        id: setup-fortran
        with:
          compiler: intel
          version: '2025.0'

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Set file base name (Linux_intel)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (Linux_intel)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
              name: tgz-tarball
              path: ${{ github.workspace }}

      - name: List files for the space (Linux_intel)
        run: |
              ls -l ${{ github.workspace }}
              ls ${{ runner.workspace }}

      - name: Uncompress source (Linux_intel)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Copy script files for the space (Linux_intel)
        run: |
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

      - name: List files for the hdf5 (Linux_Linux)
        run: |
          ls ${{ runner.workspace }}/hdf5

      - name: Create options file (Linux_intel)
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
            set (CTEST_DROP_SITE_INIT "my.cdash.org")
            # Change following line to submit to your CDash dashboard to a different CDash project
            #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
            set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
            set (MODEL "GHDaily")
            set (GROUP "GHDaily")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
            set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest (Linux_intel)
        env:
          FC: ${{ steps.setup-fortran.outputs.fc }}
          CC: ${{ steps.setup-fortran.outputs.cc }}
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=GH-${{ github.event.repository.full_name }}-Intel,LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C Release -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (Linux_intel)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: intel-ubuntu-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

####### clang builds
  build_and_test_linux_clang:
  # Linux (Ubuntu) w/ clang
  #
    name: "Ubuntu Clang"
    runs-on: ubuntu-22.04
    steps:
      - name: Install Dependencies (Linux_clang)
        run: |
          sudo apt-get update
          sudo apt-get install ninja-build doxygen graphviz curl libtinfo5

      - name: add clang to env
        uses: KyleMayes/install-llvm-action@98e68e10c96dffcb7bfed8b2144541a66b49aa02 # v2.0.8
        id: setup-clang
        with:
          env: true
          version: '18.1'

      - name: check clang version
        shell: bash
        run: |
          which clang
          clang -v

      - name: Install CMake
        uses: lukka/get-cmake@9e07ecdcee1b12e5037e42f410b67f03e2f626e1 # latest
        with:
            cmakeVersion: ${{ inputs.cmake_version }}
            ninjaVersion: latest

      - name: Check CMake Version
        shell: bash
        run: |
          which cmake
          cmake --version

      - name: Set file base name (Linux_clang)
        id: set-file-base
        run: |
          FILE_NAME_BASE=$(echo "${{ inputs.file_base }}")
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
          if [[ '${{ inputs.use_environ }}' == 'release' ]]
          then
            SOURCE_NAME_BASE=$(echo "${{ inputs.snap_name }}")
          else
            SOURCE_NAME_BASE=$(echo "hdfsrc")
          fi
          echo "SOURCE_BASE=$SOURCE_NAME_BASE" >> $GITHUB_OUTPUT

      # Get files created by release script
      - name: Get tgz-tarball (Linux_clang)
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: tgz-tarball
          path: ${{ github.workspace }}

      - name: List files for the space (Linux_clang)
        run: |
          ls -l ${{ github.workspace }}
          ls ${{ runner.workspace }}

      - name: Uncompress source (Linux_clang)
        run: tar -zxvf ${{ github.workspace }}/${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz

      - name: Copy script files for the space (Linux_clang)
        run: |
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/CTestScript.cmake ${{ runner.workspace }}/hdf5
          cp ${{ github.workspace }}/${{ steps.set-file-base.outputs.SOURCE_BASE }}/config/cmake/scripts/HDF5config.cmake ${{ runner.workspace }}/hdf5

      - name: List files for the hdf5 (Linux_clang)
        run: |
          ls ${{ runner.workspace }}/hdf5

      - name: Create options file (Linux_clang)
        uses: "DamianReeves/write-file-action@1d019960841941be46b139298996df6f139cc7a4" # master
        with:
          path: ${{ runner.workspace }}/hdf5/HDF5options.cmake
          write-mode: overwrite
          contents: |
              set (CTEST_DROP_SITE_INIT "my.cdash.org")
              # Change following line to submit to your CDash dashboard to a different CDash project
              set (SITE_BUILDNAME_SUFFIX "${{ steps.set-file-base.outputs.FILE_BASE }}")
              set (MODEL "GHDaily")
              set (GROUP "GHDaily")
              #set (CTEST_DROP_LOCATION_INIT "/submit.php?project=HDF5")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} --log-level=VERBOSE")
              #set (CMAKE_GENERATOR_TOOLSET "clang")
              #set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DCMAKE_TOOLCHAIN_FILE:STRING=config/toolchain/clang.cmake")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_JAVA:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_CPP_LIB:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_BUILD_FORTRAN:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DLIBAEC_USE_LOCALCONTENT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DZLIB_USE_LOCALCONTENT:BOOL=OFF")
              set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DPLUGIN_USE_LOCALCONTENT:BOOL=OFF")

      - name: Run CTest (Linux_clang)
        run: |
          cd "${{ runner.workspace }}/hdf5"
          ctest -S HDF5config.cmake,CTEST_SITE_EXT=GH-${{ github.event.repository.full_name }}-Clang,LOCAL_SUBMIT=ON,NINJA=TRUE,BUILD_GENERATOR=Unix,CTEST_SOURCE_NAME=${{ steps.set-file-base.outputs.SOURCE_BASE }} -C Release -O hdf5.log
        shell: bash
        continue-on-error: true

      # Save log files created by CTest script
      - name: Save log (Linux_clang)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: clang-ubuntu-log
          path: ${{ runner.workspace }}/hdf5/hdf5.log
          if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
```

### `.github/workflows/tarball.yml`

```yaml
name: hdf5 dev tarball

# Triggers the workflow on a call from another workflow
on:
  workflow_call:
    inputs:
      use_ignore:
        description: 'Ignore has_changes check'
        type: string
        required: false
        default: check
      use_tag:
        description: 'Release version tag'
        type: string
        required: false
        default: snapshot
      use_environ:
        description: 'Environment to locate files'
        type: string
        required: true
        default: snapshots
    outputs:
      has_changes:
        description: "Whether there were changes the previous day"
        value: ${{ jobs.check_commits.outputs.has_changes }}
      source_base:
        description: "The common base name of the source tarballs"
        value: ${{ jobs.create_tarball.outputs.source_base }}
      file_base:
        description: "The common base name of the source tarballs"
        value: ${{ jobs.create_tarball.outputs.file_base }}
      file_branch:
        description: "The branch used for the source tarballs"
        value: ${{ jobs.check_commits.outputs.branch_ref }}
      file_sha:
        description: "The sha used for the source tarballs"
        value: ${{ jobs.check_commits.outputs.branch_sha }}

permissions:
  contents: read

jobs:
  check_commits:
    name: Check for recent commits
    runs-on: ubuntu-latest
    outputs:
      has_changes: ${{ steps.check-new-commits.outputs.has-new-commits }}
      branch_ref: ${{ steps.get-branch-name.outputs.BRANCH_REF }}
      branch_sha: ${{ steps.get-branch-sha.outputs.BRANCH_SHA }}
    steps:
    - uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

    - name: Get branch name
      id: get-branch-name
      env:
        GITHUB_REF: ${{ github.ref }}
        GITHUB_REF_NAME: ${{ github.ref_name }}
        GITHUB_HEAD_REF: ${{ github.head_ref }}
      #run: echo "${{ env.GITHUB_REF_NAME }} | grep -P '[0-9]+/merge' &> /dev/null && BRANCH_REF=${{ env.GITHUB_HEAD_REF }} || BRANCH_REF=${{ env.GITHUB_REF_NAME }}" >> $GITHUB_OUTPUT
      run: echo "BRANCH_REF=${{ env.GITHUB_HEAD_REF || env.GITHUB_REF_NAME }}" >> $GITHUB_OUTPUT

    - name: Get branch sha
      id: get-branch-sha
      env:
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_WF_SHA: ${{ github.workflow_sha }}
      run: |
        SHORT_SHA=$(echo "${{ env.GITHUB_WF_SHA }}" | cut -c1-7)
        echo "BRANCH_SHA=$SHORT_SHA" >> $GITHUB_OUTPUT

    - name: Check for changed source
      id: check-new-commits
      uses: adriangl/check-new-commits-action@e6471f4fda990ebdb3bf44726371e1ad45ac4d37 # v1
      with:
        seconds: 86400 # One day in seconds
        branch: '${{ steps.get-branch-name.outputs.branch_ref }}'
      if: ${{ (inputs.use_environ == 'snapshots' && inputs.use_ignore == 'check') }}

    - run: echo "You have ${{ steps.check-new-commits.outputs.new-commits-number }} new commit(s) in ${{ steps.get-branch-name.outputs.BRANCH_REF }} âœ…!"
      if: ${{ steps.check-new-commits.outputs.has-new-commits == 'true' }}

    - run: echo "Short commit sha is ${{ steps.get-branch-sha.outputs.BRANCH_SHA }}!"

  create_tarball:
    name: Create a source tarball
    runs-on: ubuntu-latest
    needs: check_commits
    if: ${{ ((inputs.use_environ == 'snapshots') && ((needs.check_commits.outputs.has_changes == 'true') || (inputs.use_ignore == 'ignore'))) || (inputs.use_environ  == 'release') }}
    outputs:
      file_base: ${{ steps.set-file-base.outputs.FILE_BASE }}
      source_base: ${{ steps.version.outputs.SOURCE_TAG }}
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          path: hdfsrc
          ref: '${{needs.check_commits.outputs.branch_ref }}'

      - name: Install Linux Dependencies (Linux, serial)
        run: |
             sudo apt update
             sudo apt install gzip dos2unix

      - name: Retrieve version
        id: version
        run: |
          cd "$GITHUB_WORKSPACE/hdfsrc"
          echo "SOURCE_TAG=$(bin/h5vers)" >> $GITHUB_OUTPUT

      - name: Set file base name
        id: set-file-base
        run: |
          if [[ '${{ inputs.use_environ }}' == 'snapshots' ]]
          then
            FILE_NAME_BASE=$(echo "hdf5-${{ needs.check_commits.outputs.branch_ref }}-${{ needs.check_commits.outputs.branch_sha }}")
          else
            if [[ '${{ inputs.use_tag }}' == 'snapshot' ]]
            then
              FILE_NAME_BASE=$(echo "snapshot")
            else
              FILE_NAME_BASE=$(echo "hdf5-${{ steps.version.outputs.SOURCE_TAG }}")
            fi
          fi
          echo "FILE_BASE=$FILE_NAME_BASE" >> $GITHUB_OUTPUT
        shell: bash

      - name: Create snapshot file base name
        id: create-file-base
        if: ${{ (inputs.use_environ == 'snapshots') }}
        run: |
          cd "$GITHUB_WORKSPACE/hdfsrc"
          bin/release -d $GITHUB_WORKSPACE --branch ${{ needs.check_commits.outputs.branch_ref }} --revision gzip zip
        shell: bash

      - name: Create release file base name
        id: create-rel-base
        if: ${{ (inputs.use_environ == 'release') }}
        run: |
          cd "$GITHUB_WORKSPACE/hdfsrc"
          bin/release -d $GITHUB_WORKSPACE gzip zip 
        shell: bash

      - name: Copy the release file source to a non-versioned file name
        id: cp-to-non-versioned
        if: ${{ (inputs.use_environ == 'release') }}
        run: |
          cp hdf5-${{ steps.version.outputs.SOURCE_TAG }}.tar.gz hdf5.tar.gz
          cp hdf5-${{ steps.version.outputs.SOURCE_TAG }}.zip hdf5.zip
        shell: bash

      - name: Rename release file base name
        id: ren-basename
        if: ${{ (inputs.use_environ == 'release') && (inputs.use_tag  == 'snapshot') }}
        run: |
          mv hdf5-${{ steps.version.outputs.SOURCE_TAG }}.tar.gz ${{ inputs.use_tag }}.tar.gz
          mv hdf5-${{ steps.version.outputs.SOURCE_TAG }}.zip ${{ inputs.use_tag }}.zip
        shell: bash

      - name: List files in the repository
        run: |
              ls -l ${{ github.workspace }}
              ls $GITHUB_WORKSPACE

      # Save files created by release script
      - name: Save tgz-tarball
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: tgz-tarball
              path: ${{ steps.set-file-base.outputs.FILE_BASE }}.tar.gz
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

      - name: Save zip-tarball
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: zip-tarball
              path: ${{ steps.set-file-base.outputs.FILE_BASE }}.zip
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

      - name: Save tgz-tarball-nover
        if: ${{ (inputs.use_environ == 'release') }}
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: tgz-tarball-nover
              path: hdf5.tar.gz
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

      - name: Save zip-tarball-nover
        if: ${{ (inputs.use_environ == 'release') }}
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: zip-tarball-nover
              path: hdf5.zip
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`

      - name: Create description for release/snapshot from "Executive Summary" section of CHANGELOG.md
        run: |
          sed -n '/^# .* Executive Summary: HDF5 Version .*/,/^# .* Breaking Changes/p'  ./hdfsrc/release_docs/CHANGELOG.md | sed '$d' > description.txt

      - name: List files in the repository
        run: |
              ls -l ${{ github.workspace }}
              ls $GITHUB_WORKSPACE

      - name: Upload description.txt
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: description.txt
              path: ./description.txt
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
```

### `.github/workflows/test-binary-installation.yml`

```yaml
name: Test Binary Installation

# Tests complete binary installation including:
# - HDF5 native library installation (from system packages or built binaries)
# - Maven package consumption
# - Full integration testing with Java examples

on:
  workflow_dispatch:
    inputs:
      maven_version:
        description: 'Maven artifact version to test'
        type: string
        required: true
        default: '2.0.0-SNAPSHOT'
      maven_repository:
        description: 'Maven repository URL'
        type: string
        required: false
        default: 'https://maven.pkg.github.com/HDFGroup/hdf5'
      java_implementation:
        description: 'Java implementation to test'
        type: choice
        required: false
        default: 'both'
        options:
          - 'both'
          - 'ffm'
          - 'jni'
      install_method:
        description: 'HDF5 native library installation method'
        type: choice
        required: false
        default: 'system-package'
        options:
          - 'system-package'  # apt-get install libhdf5-dev
          - 'from-source'     # Build from source tarball
          - 'from-binary'     # Install from release binary package
  workflow_call:
    inputs:
      maven_version:
        description: 'Maven artifact version to test'
        type: string
        required: true
      maven_repository:
        description: 'Maven repository URL'
        type: string
        required: false
        default: 'https://maven.pkg.github.com/HDFGroup/hdf5'
      java_implementation:
        description: 'Java implementation to test'
        type: string
        required: false
        default: 'both'
      install_method:
        description: 'HDF5 native library installation method'
        type: string
        required: false
        default: 'system-package'

permissions:
  contents: read
  packages: read

jobs:
  test-jni-binary:
    name: Test JNI with Binary Installation
    runs-on: ubuntu-latest
    if: |
      inputs.java_implementation == 'both' ||
      inputs.java_implementation == 'jni'
    steps:
      - name: Checkout HDF5Examples
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: HDFGroup/HDF5Examples
          path: HDF5Examples

      - name: Set up Java 11 for JNI
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '11'
          distribution: 'temurin'
          cache: 'maven'

      - name: Install HDF5 native libraries
        run: |
          echo "=== Installing HDF5 Native Libraries ==="
          echo "Installation method: ${{ inputs.install_method }}"

          if [ "${{ inputs.install_method }}" == "system-package" ]; then
            echo "Installing from system packages..."
            sudo apt-get update
            sudo apt-get install -y libhdf5-dev hdf5-tools

            echo "Installed HDF5 version:"
            h5dump --version || echo "h5dump not found"

            echo "HDF5 library location:"
            find /usr/lib -name "libhdf5.so*" 2>/dev/null || echo "Library not found in /usr/lib"
            find /usr/local/lib -name "libhdf5.so*" 2>/dev/null || true

          elif [ "${{ inputs.install_method }}" == "from-source" ]; then
            echo "Building from source not yet implemented"
            echo "Falling back to system package installation"
            sudo apt-get update
            sudo apt-get install -y libhdf5-dev hdf5-tools

          elif [ "${{ inputs.install_method }}" == "from-binary" ]; then
            echo "Installing from binary package not yet implemented"
            echo "Falling back to system package installation"
            sudo apt-get update
            sudo apt-get install -y libhdf5-dev hdf5-tools
          fi

          echo "Library path configuration:"
          ldconfig -p | grep hdf5 || echo "No HDF5 libraries in ld cache"

      - name: Configure Maven for GitHub Packages
        run: |
          mkdir -p ~/.m2
          cat > ~/.m2/settings.xml << 'SETTINGSEOF'
          <?xml version="1.0" encoding="UTF-8"?>
          <settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"
                    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                    xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0
                    http://maven.apache.org/xsd/settings-1.0.0.xsd">
              <servers>
                  <server>
                      <id>github-hdfgroup-hdf5</id>
                      <username>${env.GITHUB_ACTOR}</username>
                      <password>${env.GITHUB_TOKEN}</password>
                  </server>
              </servers>
              <profiles>
                  <profile>
                      <id>github-packages</id>
                      <repositories>
                          <repository>
                              <id>github-hdfgroup-hdf5</id>
                              <url>MAVEN_REPO_URL_PLACEHOLDER</url>
                              <snapshots><enabled>true</enabled></snapshots>
                              <releases><enabled>true</enabled></releases>
                          </repository>
                      </repositories>
                  </profile>
              </profiles>
              <activeProfiles>
                  <activeProfile>github-packages</activeProfile>
              </activeProfiles>
          </settings>
          SETTINGSEOF

          # Replace placeholder with actual URL
          sed -i "s|MAVEN_REPO_URL_PLACEHOLDER|${{ inputs.maven_repository }}|g" ~/.m2/settings.xml
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Download JNI Maven artifact
        run: |
          echo "Downloading hdf5-java-jni:${{ inputs.maven_version }}"
          mvn dependency:get \
            -Dartifact=org.hdfgroup:hdf5-java-jni:${{ inputs.maven_version }} \
            -DremoteRepositories=${{ inputs.maven_repository }} \
            -Dtransitive=true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Verify JAR and native library integration
        run: |
          echo "=== Testing JNI Integration ==="

          # Find downloaded JAR
          JAR_PATH=$(find ~/.m2/repository/org/hdfgroup/hdf5-java-jni/${{ inputs.maven_version }} -name "*.jar" ! -name "*sources*" ! -name "*javadoc*" | head -1)

          if [ -z "$JAR_PATH" ]; then
            echo "âŒ Could not find downloaded JAR"
            exit 1
          fi

          echo "JAR location: $JAR_PATH"

          # Find native library
          HDF5_LIB=$(find /usr/lib /usr/local/lib -name "libhdf5.so*" 2>/dev/null | head -1)

          if [ -z "$HDF5_LIB" ]; then
            echo "âŒ Could not find HDF5 native library"
            exit 1
          fi

          echo "Native library: $HDF5_LIB"

          # Set library path for testing
          export LD_LIBRARY_PATH=/usr/lib:/usr/local/lib:$LD_LIBRARY_PATH
          export JAVA_LIBRARY_PATH=/usr/lib:/usr/local/lib

          echo "Testing basic H5 initialization..."
          cd HDF5Examples/JAVA

          # Create simple test using printf to avoid heredoc issues in YAML
          printf '%s\n' \
            'import hdf.hdf5lib.H5;' \
            'import hdf.hdf5lib.HDF5Constants;' \
            '' \
            'public class TestH5Init {' \
            '    public static void main(String[] args) {' \
            '        try {' \
            '            H5.H5open();' \
            '            int[] version = new int[3];' \
            '            H5.H5get_libversion(version);' \
            '            System.out.println("âœ… HDF5 library initialized successfully");' \
            '            System.out.println("Library version: " + version[0] + "." + version[1] + "." + version[2]);' \
            '            H5.H5close();' \
            '            System.exit(0);' \
            '        } catch (Exception e) {' \
            '            System.err.println("âŒ Error: " + e.getMessage());' \
            '            e.printStackTrace();' \
            '            System.exit(1);' \
            '        }' \
            '    }' \
            '}' \
            > TestH5Init.java

          # Compile and run
          javac -cp "$JAR_PATH" TestH5Init.java
          java -cp ".:$JAR_PATH" -Djava.library.path="$JAVA_LIBRARY_PATH" TestH5Init

      - name: Run representative examples
        run: |
          cd HDF5Examples/JAVA

          # Find JAR
          JAR_PATH=$(find ~/.m2/repository/org/hdfgroup/hdf5-java-jni/${{ inputs.maven_version }} -name "*.jar" ! -name "*sources*" ! -name "*javadoc*" | head -1)

          # Set library path
          export LD_LIBRARY_PATH=/usr/lib:/usr/local/lib:$LD_LIBRARY_PATH
          export JAVA_LIBRARY_PATH=/usr/lib:/usr/local/lib

          echo "=== Running Example Tests ==="

          # Test a few representative examples from compat directory (JNI compatible)
          for example in compat/TUTR/H5_*.java; do
            if [ -f "$example" ]; then
              example_name=$(basename "$example" .java)
              echo "Testing: $example_name"

              cd "$(dirname "$example")"
              javac -cp "$JAR_PATH" "$example_name.java"

              if timeout 30s java -cp ".:$JAR_PATH" -Djava.library.path="$JAVA_LIBRARY_PATH" "$example_name"; then
                echo "âœ… $example_name passed"
              else
                echo "âŒ $example_name failed"
              fi

              cd - > /dev/null
              break  # Just test one example for now
            fi
          done

  test-ffm-binary:
    name: Test FFM with Binary Installation
    runs-on: ubuntu-latest
    if: |
      inputs.java_implementation == 'both' ||
      inputs.java_implementation == 'ffm'
    steps:
      - name: Checkout HDF5Examples
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: HDFGroup/HDF5Examples
          path: HDF5Examples

      - name: Set up Java 25 for FFM
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '25'
          distribution: 'oracle'
          cache: 'maven'

      - name: Install HDF5 native libraries
        run: |
          echo "=== Installing HDF5 Native Libraries ==="
          sudo apt-get update
          sudo apt-get install -y libhdf5-dev hdf5-tools

          echo "Installed HDF5 version:"
          h5dump --version || echo "h5dump not found"

      - name: Configure Maven for GitHub Packages
        run: |
          mkdir -p ~/.m2
          cat > ~/.m2/settings.xml << 'SETTINGSEOF2'
          <?xml version="1.0" encoding="UTF-8"?>
          <settings xmlns="http://maven.apache.org/SETTINGS/1.0.0">
              <servers>
                  <server>
                      <id>github-hdfgroup-hdf5</id>
                      <username>${env.GITHUB_ACTOR}</username>
                      <password>${env.GITHUB_TOKEN}</password>
                  </server>
              </servers>
              <profiles>
                  <profile>
                      <id>github-packages</id>
                      <repositories>
                          <repository>
                              <id>github-hdfgroup-hdf5</id>
                              <url>MAVEN_REPO_URL_PLACEHOLDER</url>
                              <snapshots><enabled>true</enabled></snapshots>
                              <releases><enabled>true</enabled></releases>
                          </repository>
                      </repositories>
                  </profile>
              </profiles>
              <activeProfiles>
                  <activeProfile>github-packages</activeProfile>
              </activeProfiles>
          </settings>
          SETTINGSEOF2

          # Replace placeholder with actual URL
          sed -i "s|MAVEN_REPO_URL_PLACEHOLDER|${{ inputs.maven_repository }}|g" ~/.m2/settings.xml
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Download FFM Maven artifact
        run: |
          echo "Downloading hdf5-java-ffm:${{ inputs.maven_version }}"
          mvn dependency:get \
            -Dartifact=org.hdfgroup:hdf5-java-ffm:${{ inputs.maven_version }} \
            -DremoteRepositories=${{ inputs.maven_repository }} \
            -Dtransitive=true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Test FFM with system HDF5
        run: |
          echo "=== Testing FFM Integration ==="

          # Find downloaded JAR
          JAR_PATH=$(find ~/.m2/repository/org/hdfgroup/hdf5-java-ffm/${{ inputs.maven_version }} -name "*.jar" ! -name "*sources*" ! -name "*javadoc*" | head -1)

          if [ -z "$JAR_PATH" ]; then
            echo "âŒ Could not find downloaded JAR"
            exit 1
          fi

          echo "JAR location: $JAR_PATH"
          echo "JAR contents sample:"
          jar tf "$JAR_PATH" | grep "org/hdfgroup/javahdf5" | head -5

          echo "âœ… FFM JAR verification complete"
          echo "Note: Full FFM testing requires compatible native library"

  summary:
    name: Binary Installation Test Summary
    runs-on: ubuntu-latest
    needs: [test-jni-binary, test-ffm-binary]
    if: always()
    steps:
      - name: Generate summary
        run: |
          echo "# Binary Installation Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Version:** ${{ inputs.maven_version }}" >> $GITHUB_STEP_SUMMARY
          echo "**Repository:** ${{ inputs.maven_repository }}" >> $GITHUB_STEP_SUMMARY
          echo "**Install Method:** ${{ inputs.install_method }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          JNI_RESULT="${{ needs.test-jni-binary.result }}"
          FFM_RESULT="${{ needs.test-ffm-binary.result }}"

          echo "## Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "$JNI_RESULT" != "skipped" ]; then
            if [ "$JNI_RESULT" == "success" ]; then
              echo "âœ… **JNI Binary Installation:** PASSED" >> $GITHUB_STEP_SUMMARY
            else
              echo "âŒ **JNI Binary Installation:** FAILED" >> $GITHUB_STEP_SUMMARY
            fi
          fi

          if [ "$FFM_RESULT" != "skipped" ]; then
            if [ "$FFM_RESULT" == "success" ]; then
              echo "âœ… **FFM Binary Installation:** PASSED" >> $GITHUB_STEP_SUMMARY
            else
              echo "âŒ **FFM Binary Installation:** FAILED" >> $GITHUB_STEP_SUMMARY
            fi
          fi

          # Fail if any tests failed
          if [ "$JNI_RESULT" == "failure" ] || [ "$FFM_RESULT" == "failure" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âš ï¸ **Some tests failed. Check logs for details.**" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸŽ‰ **All binary installation tests passed!**" >> $GITHUB_STEP_SUMMARY
```

### `.github/workflows/test-maven-deployment.yml`

```yaml
name: Test Maven Deployment

# Manual workflow for testing Maven deployment to HDFGroup packages
on:
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test mode'
        type: choice
        options:
          - dry-run
          - live-deployment
        required: true
        default: dry-run
      target_repository:
        description: 'Maven repository target'
        type: choice
        options:
          - github-packages
          - maven-central-staging
        required: false
        default: github-packages

permissions:
  contents: read
  packages: write

jobs:
  generate-test-artifacts:
    name: Generate Test Artifacts
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Display test configuration
        run: |
          echo "=== Maven Deployment Test Configuration ==="
          echo "Test Mode: ${{ inputs.test_mode }}"
          echo "Target Repository: ${{ inputs.target_repository }}"
          echo "GitHub Actor: ${{ github.actor }}"
          echo "Repository: ${{ github.repository }}"
          echo "Expected packages URL: https://github.com/${{ github.repository }}/packages"
          echo ""

      - name: Check repository permissions
        run: |
          echo "=== Repository Permission Check ==="

          # Check repository context
          if [[ "${{ github.repository }}" == "HDFGroup/hdf5" ]]; then
            echo "âœ“ Running on canonical repository (${{ github.repository }})"
            echo "   Packages will be published to HDFGroup/hdf5"
          else
            echo "âœ“ Running on fork/test repository (${{ github.repository }})"
            echo "   Packages will be published to ${{ github.repository }} for validation"
            echo "   This allows full testing before merging to canonical repository"
          fi

          # Check if we have packages permission
          echo "Checking packages permission..."
          if [[ "${{ github.token }}" != "" ]]; then
            echo "âœ“ GITHUB_TOKEN is available"
          else
            echo "âŒ GITHUB_TOKEN not available"
          fi

      - name: Test GitHub Packages API access
        run: |
          echo "=== Testing GitHub Packages API Access ==="

          # Test basic API access
          echo "Testing GitHub API access..."
          curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
               -H "Accept: application/vnd.github.v3+json" \
               "https://api.github.com/user" | jq '.login // "API_ERROR"'

          # Test packages API access
          echo "Testing GitHub Packages API access..."
          curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
               -H "Accept: application/vnd.github.v3+json" \
               "https://api.github.com/repos/${{ github.repository }}/packages?package_type=maven" \
               | jq 'length // "API_ERROR"' || echo "No packages found yet"

      - name: Generate test Maven artifacts
        run: |
          echo "=== Generating Test Maven Artifacts ==="

          # Create test directory structure
          mkdir -p test-artifacts/maven-staging-artifacts-linux-x86_64

          # Create a minimal test JAR file
          mkdir -p temp-jar/org/hdfgroup/test
          echo 'package org.hdfgroup.test; public class TestClass { }' > temp-jar/org/hdfgroup/test/TestClass.java

          # Compile and create JAR
          cd temp-jar
          javac org/hdfgroup/test/TestClass.java
          jar cf ../test-artifacts/maven-staging-artifacts-linux-x86_64/jarhdf5-2.0.0-test.jar org/hdfgroup/test/TestClass.class
          cd ..

          # Create a test POM file
          cat > test-artifacts/maven-staging-artifacts-linux-x86_64/pom.xml << 'EOF'
          <?xml version="1.0" encoding="UTF-8"?>
          <project xmlns="http://maven.apache.org/POM/4.0.0"
                   xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                   xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
              <modelVersion>4.0.0</modelVersion>
              <groupId>org.hdfgroup</groupId>
              <artifactId>hdf5-java</artifactId>
              <version>2.0.0-test</version>
              <name>HDF5 Java Test</name>
              <description>Test artifact for HDF5 Java Maven deployment</description>
          </project>
          EOF

          # Upload as artifact for the deployment workflow
          echo "Test artifacts created:"
          find test-artifacts -type f -exec ls -la {} \;

      - name: Upload test artifacts
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: maven-staging-artifacts-linux-x86_64
          path: test-artifacts/maven-staging-artifacts-linux-x86_64/

  test-maven-deployment:
    name: Test Maven Deployment to HDFGroup Packages
    needs: generate-test-artifacts
    uses: ./.github/workflows/maven-deploy.yml
    with:
      file_base: "hdf5-test"
      preset_name: "test"
      repository_url: ${{ inputs.target_repository == 'github-packages' && format('https://maven.pkg.github.com/{0}', github.repository) || 'https://s01.oss.sonatype.org/service/local/staging/deploy/maven2/' }}
      repository_id: ${{ inputs.target_repository == 'github-packages' && 'github' || 'ossrh' }}
      deploy_snapshots: false
      dry_run: ${{ inputs.test_mode == 'dry-run' }}
    secrets:
      MAVEN_USERNAME: ${{ inputs.target_repository == 'github-packages' && github.actor || secrets.MAVEN_CENTRAL_USERNAME }}
      MAVEN_PASSWORD: ${{ inputs.target_repository == 'github-packages' && secrets.GITHUB_TOKEN || secrets.MAVEN_CENTRAL_PASSWORD }}
      GPG_PRIVATE_KEY: ${{ secrets.GPG_PRIVATE_KEY }}
      GPG_PASSPHRASE: ${{ secrets.GPG_PASSPHRASE }}

  validate-results:
    name: Validate Deployment Results
    needs: test-maven-deployment
    if: ${{ always() && inputs.test_mode == 'live-deployment' }}
    runs-on: ubuntu-latest
    steps:
      - name: Validate deployment results
        run: |
          echo "=== Validating Deployment Results ==="

          # Wait for packages to be processed
          echo "Waiting 30 seconds for packages to be processed..."
          sleep 30

          # Check GitHub Packages for the deployed artifact
          if [[ "${{ inputs.target_repository }}" == "github-packages" ]]; then
            echo "Checking GitHub Packages..."

            packages=$(curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
                            -H "Accept: application/vnd.github.v3+json" \
                            "https://api.github.com/repos/${{ github.repository }}/packages?package_type=maven")

            echo "Available packages:"
            echo "$packages" | jq '.[] | {name: .name, html_url: .html_url}' || echo "No packages or jq not available"

            # Check for our test package
            if echo "$packages" | jq -r '.[].name' | grep -q "hdf5-java"; then
              echo "âœ“ hdf5-java package found in GitHub Packages"
            else
              echo "âš ï¸ hdf5-java package not found in GitHub Packages"
            fi
          fi

  display-results:
    name: Display Test Results
    needs: [generate-test-artifacts, test-maven-deployment]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Display next steps
        run: |
          echo "=== Test Results and Next Steps ==="
          echo "Generation Status: ${{ needs.generate-test-artifacts.result }}"
          echo "Deployment Status: ${{ needs.test-maven-deployment.result }}"

          if [[ "${{ inputs.test_mode }}" == "dry-run" ]]; then
            echo "ðŸ§ª DRY RUN COMPLETED"
            echo ""
            echo "âœ“ Permission configuration tested"
            echo "âœ“ Workflow logic validated"
            echo "âœ“ No actual artifacts deployed"
            echo ""
            echo "Next steps:"
            echo "1. If no errors above, run with 'live-deployment' mode"
            echo "2. Check https://github.com/${{ github.repository }}/packages for deployed artifacts"
            echo "3. Test consuming the artifacts in a sample Maven project"
          else
            echo "ðŸš€ LIVE DEPLOYMENT COMPLETED"
            echo ""
            echo "Check deployment results at:"
            echo "- GitHub Packages: https://github.com/${{ github.repository }}/packages"
            echo "- Workflow logs above for any deployment errors"
            echo ""
            echo "Next steps:"
            echo "1. Run full release workflow with deploy_maven=true"
            echo "2. Test end-to-end user experience with deployed artifacts"
          fi
```

### `.github/workflows/test-maven-packages.yml`

```yaml
name: Test Maven Packages

# This workflow tests published Maven artifacts by:
# 1. Downloading packages from GitHub Packages
# 2. Verifying JAR contents are complete (not just dependencies)
# 3. Building and running HDF5Examples against the packages
# 4. Testing both JNI and FFM implementations

on:
  workflow_call:
    inputs:
      version:
        description: 'Maven artifact version to test'
        type: string
        required: true
      java_implementation:
        description: 'Java implementation(s) to test: jni, ffm, or both'
        type: string
        required: false
        default: 'both'
      repository_url:
        description: 'Maven repository URL'
        type: string
        required: false
        default: 'https://maven.pkg.github.com/HDFGroup/hdf5'
  workflow_dispatch:
    inputs:
      version:
        description: 'Maven artifact version to test'
        type: string
        required: true
        default: '2.0.0-SNAPSHOT'
      java_implementation:
        description: 'Java implementation(s) to test'
        type: choice
        required: false
        default: 'both'
        options:
          - 'both'
          - 'jni'
          - 'ffm'
      repository_url:
        description: 'Maven repository URL'
        type: string
        required: false
        default: 'https://maven.pkg.github.com/HDFGroup/hdf5'

permissions:
  contents: read
  packages: read

jobs:
  test-jni-package:
    name: Test JNI Maven Package
    runs-on: ubuntu-latest
    if: |
      inputs.java_implementation == 'both' ||
      inputs.java_implementation == 'jni'
    steps:
      - name: Checkout HDF5 repository (contains HDF5Examples)
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Set up Java 21 for JNI
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '21'
          distribution: 'temurin'

      - name: Configure Maven settings for GitHub Packages
        run: |
          mkdir -p ~/.m2
          cat > ~/.m2/settings.xml <<EOF
          <?xml version="1.0" encoding="UTF-8"?>
          <settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"
                    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                    xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0
                    http://maven.apache.org/xsd/settings-1.0.0.xsd">
              <servers>
                  <server>
                      <id>github-hdfgroup-hdf5</id>
                      <username>\${env.GITHUB_ACTOR}</username>
                      <password>\${env.GITHUB_TOKEN}</password>
                  </server>
              </servers>
              <profiles>
                  <profile>
                      <id>github-packages</id>
                      <repositories>
                          <repository>
                              <id>github-hdfgroup-hdf5</id>
                              <url>${{ inputs.repository_url }}</url>
                              <snapshots><enabled>true</enabled></snapshots>
                              <releases><enabled>true</enabled></releases>
                          </repository>
                      </repositories>
                  </profile>
              </profiles>
              <activeProfiles>
                  <activeProfile>github-packages</activeProfile>
              </activeProfiles>
          </settings>
          EOF
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_ACTOR: ${{ github.actor }}

      - name: Verify JNI artifact exists and download
        run: |
          echo "::group::Download JNI artifact"
          # Clear cached SNAPSHOT to force fresh download
          rm -rf ~/.m2/repository/org/hdfgroup/hdf5-java-jni/${{ inputs.version }}

          # Determine platform classifier
          PLATFORM_CLASSIFIER="linux-x86_64"
          ARTIFACT_ID="hdf5-java-jni"
          VERSION="${{ inputs.version }}"
          REPO_URL="${{ inputs.repository_url }}"

          echo "Downloading: org.hdfgroup:${ARTIFACT_ID}:${VERSION} with classifier ${PLATFORM_CLASSIFIER}"

          # For SNAPSHOT versions, get the latest timestamp from maven-metadata.xml
          if [[ "$VERSION" == *"SNAPSHOT"* ]]; then
            echo "Fetching SNAPSHOT metadata..."
            METADATA_URL="${REPO_URL}/org/hdfgroup/${ARTIFACT_ID}/${VERSION}/maven-metadata.xml"

            # Download metadata with retry (GitHub Packages may need time to propagate)
            echo "Downloading metadata (with retry for GitHub Packages propagation)..."
            RETRY_COUNT=0
            MAX_RETRIES=12
            RETRY_DELAY=10

            while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
              if curl -u "$GITHUB_ACTOR:$GITHUB_TOKEN" -fsSL "$METADATA_URL" -o /tmp/maven-metadata.xml; then
                echo "âœ… Metadata downloaded successfully"
                break
              else
                RETRY_COUNT=$((RETRY_COUNT + 1))
                if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                  echo "â³ Attempt $RETRY_COUNT/$MAX_RETRIES failed. Waiting ${RETRY_DELAY}s for GitHub Packages to propagate..."
                  sleep $RETRY_DELAY
                else
                  echo "::error::Failed to download metadata after $MAX_RETRIES attempts"
                  echo "::error::URL: $METADATA_URL"
                  exit 1
                fi
              fi
            done

            # Extract timestamped version from metadata
            # Note: Maven may truncate long classifiers, so we search for partial matches
            # e.g., "linux-x86_64" may be stored as classifier="linux-x" extension="6_64.jar"
            # Maven removes suffixes: 86_64, _64, or 64
            TRUNCATED_CLASSIFIER=$(echo "$PLATFORM_CLASSIFIER" | sed -E 's/(86_64|_64|64)$//')

            # Parse XML using awk (xmllint not available in GitHub Actions by default)
            # Format XML (GitHub Packages returns minified XML on one line)
            # Look for snapshotVersion blocks with matching classifier and jar extension
            echo "Searching for classifier: ${TRUNCATED_CLASSIFIER}"
            TIMESTAMPED_VERSION=$(sed 's/></>\n</g' /tmp/maven-metadata.xml | awk -v search_classifier="${TRUNCATED_CLASSIFIER}" '
              /<snapshotVersion>/ { in_block=1; classifier=""; extension=""; value="" }
              /<\/snapshotVersion>/ {
                if (in_block && classifier == search_classifier && extension ~ /jar/) {
                  print value
                  exit
                }
                in_block=0
              }
              in_block && /<classifier>/ { gsub(/.*<classifier>|<\/classifier>.*/, ""); classifier=$0 }
              in_block && /<extension>/ { gsub(/.*<extension>|<\/extension>.*/, ""); extension=$0 }
              in_block && /<value>/ { gsub(/.*<value>|<\/value>.*/, ""); value=$0 }
            ')

            if [ -z "$TIMESTAMPED_VERSION" ]; then
              echo "::error::Could not extract SNAPSHOT version from metadata"
              echo "::error::Searched for classifier starting with: ${TRUNCATED_CLASSIFIER}"
              echo "::error::Metadata contents:"
              cat /tmp/maven-metadata.xml
              exit 1
            fi

            echo "Latest SNAPSHOT version: ${TIMESTAMPED_VERSION}"
            JAR_FILENAME="${ARTIFACT_ID}-${TIMESTAMPED_VERSION}-${PLATFORM_CLASSIFIER}.jar"
            POM_FILENAME="${ARTIFACT_ID}-${TIMESTAMPED_VERSION}.pom"
          else
            # Release version - use version as-is
            JAR_FILENAME="${ARTIFACT_ID}-${VERSION}-${PLATFORM_CLASSIFIER}.jar"
            POM_FILENAME="${ARTIFACT_ID}-${VERSION}.pom"
          fi

          # Download JAR and POM with retry
          JAR_URL="${REPO_URL}/org/hdfgroup/${ARTIFACT_ID}/${VERSION}/${JAR_FILENAME}"
          POM_URL="${REPO_URL}/org/hdfgroup/${ARTIFACT_ID}/${VERSION}/${POM_FILENAME}"
          mkdir -p /tmp/maven-download

          # Download JAR with retry
          echo "Downloading JAR: ${JAR_URL}"
          RETRY_COUNT=0
          MAX_RETRIES=12
          RETRY_DELAY=10

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if curl -u "$GITHUB_ACTOR:$GITHUB_TOKEN" -fsSL "$JAR_URL" -o "/tmp/maven-download/${JAR_FILENAME}"; then
              echo "âœ… JAR downloaded successfully"
              break
            else
              RETRY_COUNT=$((RETRY_COUNT + 1))
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                echo "â³ JAR download attempt $RETRY_COUNT/$MAX_RETRIES failed. Waiting ${RETRY_DELAY}s..."
                sleep $RETRY_DELAY
              else
                echo "::error::Failed to download JAR after $MAX_RETRIES attempts"
                echo "::error::URL: $JAR_URL"
                exit 1
              fi
            fi
          done

          # Download POM with retry
          echo "Downloading POM: ${POM_URL}"
          RETRY_COUNT=0

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if curl -u "$GITHUB_ACTOR:$GITHUB_TOKEN" -fsSL "$POM_URL" -o "/tmp/maven-download/${POM_FILENAME}"; then
              echo "âœ… POM downloaded successfully"
              break
            else
              RETRY_COUNT=$((RETRY_COUNT + 1))
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                echo "â³ POM download attempt $RETRY_COUNT/$MAX_RETRIES failed. Waiting ${RETRY_DELAY}s..."
                sleep $RETRY_DELAY
              else
                echo "::error::Failed to download POM after $MAX_RETRIES attempts"
                echo "::error::URL: $POM_URL"
                exit 1
              fi
            fi
          done

          # Install to local Maven repository
          echo "Installing artifact to local repository..."
          mvn install:install-file \
            -Dfile="/tmp/maven-download/${JAR_FILENAME}" \
            -DpomFile="/tmp/maven-download/${POM_FILENAME}" \
            -DgroupId=org.hdfgroup \
            -DartifactId=${ARTIFACT_ID} \
            -Dversion=${VERSION} \
            -Dpackaging=jar \
            -Dclassifier=${PLATFORM_CLASSIFIER}

          echo "âœ… Artifact installed successfully"
          echo "::endgroup::"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_ACTOR: ${{ github.actor }}

      - name: Verify JAR contents (JNI)
        run: |
          echo "::group::Verify JAR contents"
          JAR_PATH=$(find ~/.m2/repository/org/hdfgroup/hdf5-java-jni/${{ inputs.version }} -name "*.jar" ! -name "*sources*" ! -name "*javadoc*" | head -1)

          if [ -z "$JAR_PATH" ]; then
            echo "::error::Could not find downloaded JAR file"
            exit 1
          fi

          echo "JAR location: $JAR_PATH"
          echo "JAR size: $(du -h "$JAR_PATH" | cut -f1)"

          # Verify HDF5 classes are present
          if jar tf "$JAR_PATH" | grep -q "hdf/hdf5lib/H5.class"; then
            echo "âœ… JAR contains HDF5 JNI classes"
            CLASS_COUNT=$(jar tf "$JAR_PATH" | grep "hdf/hdf5lib.*\.class" | wc -l)
            echo "Found $CLASS_COUNT HDF5 classes"
          else
            echo "::error::JAR does not contain HDF5 classes!"
            echo "JAR contents (first 50 files):"
            jar tf "$JAR_PATH" | head -50
            exit 1
          fi
          echo "::endgroup::"

      - name: Download HDF5 installation from workflow
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
          name: hdf5-install-linux-x86_64-jni
          path: ${{ runner.workspace }}/hdf5-install

      - name: Verify HDF5 installation
        run: |
          echo "HDF5 installation contents:"
          ls -la "${{ runner.workspace }}/hdf5-install"
          if [ -d "${{ runner.workspace }}/hdf5-install/lib" ]; then
            echo "Libraries:"
            ls -la "${{ runner.workspace }}/hdf5-install/lib" | grep -E '\.so' || echo "No shared libraries found"
          fi

      - name: Test JNI examples
        run: |
          cd HDF5Examples/JAVA
          ./test-maven-jni.sh "${{ inputs.version }}" "${{ inputs.repository_url }}"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_ACTOR: ${{ github.actor }}
          HDF5_HOME: ${{ runner.workspace }}/hdf5-install

      - name: Upload test artifacts (JNI)
        if: always()
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: jni-test-results-${{ inputs.version }}
          path: |
            HDF5Examples/JAVA/build/maven-test-jni/

  test-ffm-package:
    name: Test FFM Maven Package
    runs-on: ubuntu-latest
    if: |
      inputs.java_implementation == 'both' ||
      inputs.java_implementation == 'ffm'
    steps:
      - name: Checkout HDF5 repository (contains HDF5Examples)
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Set up Java 25 for FFM
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          java-version: '25'
          distribution: 'oracle'

      - name: Configure Maven settings for GitHub Packages
        run: |
          mkdir -p ~/.m2
          cat > ~/.m2/settings.xml <<EOF
          <?xml version="1.0" encoding="UTF-8"?>
          <settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"
                    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                    xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0
                    http://maven.apache.org/xsd/settings-1.0.0.xsd">
              <servers>
                  <server>
                      <id>github-hdfgroup-hdf5</id>
                      <username>\${env.GITHUB_ACTOR}</username>
                      <password>\${env.GITHUB_TOKEN}</password>
                  </server>
              </servers>
              <profiles>
                  <profile>
                      <id>github-packages</id>
                      <repositories>
                          <repository>
                              <id>github-hdfgroup-hdf5</id>
                              <url>${{ inputs.repository_url }}</url>
                              <snapshots><enabled>true</enabled></snapshots>
                              <releases><enabled>true</enabled></releases>
                          </repository>
                      </repositories>
                  </profile>
              </profiles>
              <activeProfiles>
                  <activeProfile>github-packages</activeProfile>
              </activeProfiles>
          </settings>
          EOF
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_ACTOR: ${{ github.actor }}

      - name: Verify FFM artifact exists and download
        run: |
          echo "::group::Download FFM artifact"
          # Clear cached SNAPSHOT to force fresh download
          rm -rf ~/.m2/repository/org/hdfgroup/hdf5-java-ffm/${{ inputs.version }}

          # Determine platform classifier
          PLATFORM_CLASSIFIER="linux-x86_64"
          ARTIFACT_ID="hdf5-java-ffm"
          VERSION="${{ inputs.version }}"
          REPO_URL="${{ inputs.repository_url }}"

          echo "Downloading: org.hdfgroup:${ARTIFACT_ID}:${VERSION} with classifier ${PLATFORM_CLASSIFIER}"

          # For SNAPSHOT versions, get the latest timestamp from maven-metadata.xml
          if [[ "$VERSION" == *"SNAPSHOT"* ]]; then
            echo "Fetching SNAPSHOT metadata..."
            METADATA_URL="${REPO_URL}/org/hdfgroup/${ARTIFACT_ID}/${VERSION}/maven-metadata.xml"

            # Download metadata with retry (GitHub Packages may need time to propagate)
            echo "Downloading metadata (with retry for GitHub Packages propagation)..."
            RETRY_COUNT=0
            MAX_RETRIES=12
            RETRY_DELAY=10

            while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
              if curl -u "$GITHUB_ACTOR:$GITHUB_TOKEN" -fsSL "$METADATA_URL" -o /tmp/maven-metadata.xml; then
                echo "âœ… Metadata downloaded successfully"
                break
              else
                RETRY_COUNT=$((RETRY_COUNT + 1))
                if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                  echo "â³ Attempt $RETRY_COUNT/$MAX_RETRIES failed. Waiting ${RETRY_DELAY}s for GitHub Packages to propagate..."
                  sleep $RETRY_DELAY
                else
                  echo "::error::Failed to download metadata after $MAX_RETRIES attempts"
                  echo "::error::URL: $METADATA_URL"
                  exit 1
                fi
              fi
            done

            # Extract timestamped version from metadata
            # Note: Maven may truncate long classifiers, so we search for partial matches
            # e.g., "linux-x86_64" may be stored as classifier="linux-x" extension="6_64.jar"
            # Maven removes suffixes: 86_64, _64, or 64
            TRUNCATED_CLASSIFIER=$(echo "$PLATFORM_CLASSIFIER" | sed -E 's/(86_64|_64|64)$//')

            # Parse XML using awk (xmllint not available in GitHub Actions by default)
            # Format XML (GitHub Packages returns minified XML on one line)
            # Look for snapshotVersion blocks with matching classifier and jar extension
            echo "Searching for classifier: ${TRUNCATED_CLASSIFIER}"
            TIMESTAMPED_VERSION=$(sed 's/></>\n</g' /tmp/maven-metadata.xml | awk -v search_classifier="${TRUNCATED_CLASSIFIER}" '
              /<snapshotVersion>/ { in_block=1; classifier=""; extension=""; value="" }
              /<\/snapshotVersion>/ {
                if (in_block && classifier == search_classifier && extension ~ /jar/) {
                  print value
                  exit
                }
                in_block=0
              }
              in_block && /<classifier>/ { gsub(/.*<classifier>|<\/classifier>.*/, ""); classifier=$0 }
              in_block && /<extension>/ { gsub(/.*<extension>|<\/extension>.*/, ""); extension=$0 }
              in_block && /<value>/ { gsub(/.*<value>|<\/value>.*/, ""); value=$0 }
            ')

            if [ -z "$TIMESTAMPED_VERSION" ]; then
              echo "::error::Could not extract SNAPSHOT version from metadata"
              echo "::error::Searched for classifier starting with: ${TRUNCATED_CLASSIFIER}"
              echo "::error::Metadata contents:"
              cat /tmp/maven-metadata.xml
              exit 1
            fi

            echo "Latest SNAPSHOT version: ${TIMESTAMPED_VERSION}"
            JAR_FILENAME="${ARTIFACT_ID}-${TIMESTAMPED_VERSION}-${PLATFORM_CLASSIFIER}.jar"
            POM_FILENAME="${ARTIFACT_ID}-${TIMESTAMPED_VERSION}.pom"
          else
            # Release version - use version as-is
            JAR_FILENAME="${ARTIFACT_ID}-${VERSION}-${PLATFORM_CLASSIFIER}.jar"
            POM_FILENAME="${ARTIFACT_ID}-${VERSION}.pom"
          fi

          # Download JAR and POM with retry
          JAR_URL="${REPO_URL}/org/hdfgroup/${ARTIFACT_ID}/${VERSION}/${JAR_FILENAME}"
          POM_URL="${REPO_URL}/org/hdfgroup/${ARTIFACT_ID}/${VERSION}/${POM_FILENAME}"
          mkdir -p /tmp/maven-download

          # Download JAR with retry
          echo "Downloading JAR: ${JAR_URL}"
          RETRY_COUNT=0
          MAX_RETRIES=12
          RETRY_DELAY=10

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if curl -u "$GITHUB_ACTOR:$GITHUB_TOKEN" -fsSL "$JAR_URL" -o "/tmp/maven-download/${JAR_FILENAME}"; then
              echo "âœ… JAR downloaded successfully"
              break
            else
              RETRY_COUNT=$((RETRY_COUNT + 1))
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                echo "â³ JAR download attempt $RETRY_COUNT/$MAX_RETRIES failed. Waiting ${RETRY_DELAY}s..."
                sleep $RETRY_DELAY
              else
                echo "::error::Failed to download JAR after $MAX_RETRIES attempts"
                echo "::error::URL: $JAR_URL"
                exit 1
              fi
            fi
          done

          # Download POM with retry
          echo "Downloading POM: ${POM_URL}"
          RETRY_COUNT=0

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if curl -u "$GITHUB_ACTOR:$GITHUB_TOKEN" -fsSL "$POM_URL" -o "/tmp/maven-download/${POM_FILENAME}"; then
              echo "âœ… POM downloaded successfully"
              break
            else
              RETRY_COUNT=$((RETRY_COUNT + 1))
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                echo "â³ POM download attempt $RETRY_COUNT/$MAX_RETRIES failed. Waiting ${RETRY_DELAY}s..."
                sleep $RETRY_DELAY
              else
                echo "::error::Failed to download POM after $MAX_RETRIES attempts"
                echo "::error::URL: $POM_URL"
                exit 1
              fi
            fi
          done

          # Install to local Maven repository
          echo "Installing artifact to local repository..."
          mvn install:install-file \
            -Dfile="/tmp/maven-download/${JAR_FILENAME}" \
            -DpomFile="/tmp/maven-download/${POM_FILENAME}" \
            -DgroupId=org.hdfgroup \
            -DartifactId=${ARTIFACT_ID} \
            -Dversion=${VERSION} \
            -Dpackaging=jar \
            -Dclassifier=${PLATFORM_CLASSIFIER}

          echo "âœ… Artifact installed successfully"
          echo "::endgroup::"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_ACTOR: ${{ github.actor }}

      - name: Verify JAR contents (FFM)
        run: |
          echo "::group::Verify JAR contents"
          JAR_PATH=$(find ~/.m2/repository/org/hdfgroup/hdf5-java-ffm/${{ inputs.version }} -name "*.jar" ! -name "*sources*" ! -name "*javadoc*" | head -1)

          if [ -z "$JAR_PATH" ]; then
            echo "::error::Could not find downloaded JAR file"
            exit 1
          fi

          echo "JAR location: $JAR_PATH"
          echo "JAR size: $(du -h "$JAR_PATH" | cut -f1)"

          # Verify HDF5 FFM classes are present
          if jar tf "$JAR_PATH" | grep -q "org/hdfgroup/javahdf5/hdf5_h.class"; then
            echo "âœ… JAR contains HDF5 FFM classes"
            CLASS_COUNT=$(jar tf "$JAR_PATH" | grep "org/hdfgroup/javahdf5.*\.class" | wc -l)
            echo "Found $CLASS_COUNT HDF5 FFM classes"
          else
            echo "::error::JAR does not contain HDF5 FFM classes!"
            echo "JAR contents (first 50 files):"
            jar tf "$JAR_PATH" | head -50
            exit 1
          fi
          echo "::endgroup::"

      - name: Download HDF5 installation from workflow
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6
        with:
          name: hdf5-install-linux-x86_64-ffm
          path: ${{ runner.workspace }}/hdf5-install

      - name: Verify HDF5 installation
        run: |
          echo "HDF5 installation contents:"
          ls -la "${{ runner.workspace }}/hdf5-install"
          if [ -d "${{ runner.workspace }}/hdf5-install/lib" ]; then
            echo "Libraries:"
            ls -la "${{ runner.workspace }}/hdf5-install/lib" | grep -E '\.so' || echo "No shared libraries found"
          fi

      - name: Test FFM examples
        run: |
          cd HDF5Examples/JAVA
          ./test-maven-ffm.sh "${{ inputs.version }}" "${{ inputs.repository_url }}"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_ACTOR: ${{ github.actor }}
          HDF5_HOME: ${{ runner.workspace }}/hdf5-install

      - name: Upload test artifacts (FFM)
        if: always()
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: ffm-test-results-${{ inputs.version }}
          path: |
            HDF5Examples/JAVA/build/maven-test-ffm/

  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [test-jni-package, test-ffm-package]
    if: always()
    steps:
      - name: Check test results
        run: |
          echo "## Maven Package Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Version:** ${{ inputs.version }}" >> $GITHUB_STEP_SUMMARY
          echo "**Repository:** ${{ inputs.repository_url }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          JNI_RESULT="${{ needs.test-jni-package.result }}"
          FFM_RESULT="${{ needs.test-ffm-package.result }}"

          echo "### Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "$JNI_RESULT" != "skipped" ]; then
            if [ "$JNI_RESULT" == "success" ]; then
              echo "âœ… **JNI Package:** PASSED" >> $GITHUB_STEP_SUMMARY
            else
              echo "âŒ **JNI Package:** FAILED" >> $GITHUB_STEP_SUMMARY
            fi
          fi

          if [ "$FFM_RESULT" != "skipped" ]; then
            if [ "$FFM_RESULT" == "success" ]; then
              echo "âœ… **FFM Package:** PASSED" >> $GITHUB_STEP_SUMMARY
            else
              echo "âŒ **FFM Package:** FAILED" >> $GITHUB_STEP_SUMMARY
            fi
          fi

          # Fail if any tests failed
          if [ "$JNI_RESULT" == "failure" ] || [ "$FFM_RESULT" == "failure" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âš ï¸ **One or more tests failed. Check the logs above for details.**" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸŽ‰ **All tests passed!**" >> $GITHUB_STEP_SUMMARY
```

### `.github/workflows/testxpr.yml`

```yaml
name: hdf5 TestExpress CI

on:
  workflow_call:

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  build_and_test:
    strategy:
      matrix:
        build_mode: ["Release", "Debug"]
        include:
          - build_mode: "Release"
          - build_mode: "Debug"

    name: "${{ matrix.build_mode }} Express Test Workflows"

    # Don't run the action if the commit message says to skip CI
    if: "!contains(github.event.head_commit.message, 'skip-ci')"

    runs-on: ubuntu-latest
    steps:
      - name: Install Linux Dependencies
        run: |
           sudo apt-get update
           sudo apt-get install ninja-build doxygen graphviz
           sudo apt install libssl3 libssl-dev libcurl4 libcurl4-openssl-dev
           sudo apt install libaec0 libaec-dev
           sudo apt install gcc-12 g++-12 gfortran-12
           echo "CC=gcc-12" >> $GITHUB_ENV
           echo "CXX=g++-12" >> $GITHUB_ENV
           echo "FC=gfortran-12" >> $GITHUB_ENV

      - name: Get Sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Configure
        shell: bash
        run: |
           mkdir "${{ runner.workspace }}/build"
           cd "${{ runner.workspace }}/build"
           cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
                -G Ninja \
                -DCMAKE_BUILD_TYPE=${{ matrix.build_mode }} \
                -DBUILD_SHARED_LIBS=ON \
                -DHDF5_ENABLE_ALL_WARNINGS=ON \
                -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
                -DHDF5_BUILD_CPP_LIB:BOOL=OFF \
                -DHDF5_BUILD_FORTRAN=OFF \
                -DHDF5_BUILD_JAVA=OFF \
                -DHDF5_BUILD_DOC=OFF \
                -DLIBAEC_USE_LOCALCONTENT=OFF \
                -DZLIB_USE_LOCALCONTENT=OFF \
                -DHDF_TEST_EXPRESS=0 \
                $GITHUB_WORKSPACE

      - name: Build
        run: cmake --build . --parallel 3 --config ${{ matrix.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      - name: Run Tests
        env:
          HDF5TestExpress: 0
        run: ctest . --parallel 2 -C ${{ matrix.build_mode }} -R H5TESTXPR
        working-directory: ${{ runner.workspace }}/build
```

### `.github/workflows/update-progress.py`

```python
#!/usr/bin/env python3
"""
GitHub Project Release Blocker Progress Tracker
Fetches release blocker issues from the HDF5 project and calculates completion percentage.
"""

import requests
from typing import Dict, Any, Optional

class GitHubProjectTracker:
    """Tracks release blocker progress in GitHub projects."""
    
    def __init__(self, token: str, owner: str, project_number: int):
        self.api_url = "https://api.github.com/graphql"
        self.headers = {
            "Authorization": f"bearer {token}",
            "Content-Type": "application/json"
        }
        self.owner = owner
        self.project_number = project_number
        
    def _get_query(self) -> str:
        """Returns the GraphQL query for fetching project items."""
        return """
        query($owner: String!, $projectNumber: Int!, $cursor: String) {
          organization(login: $owner) {
            projectV2(number: $projectNumber) {
              items(first: 100, after: $cursor) {
                pageInfo { hasNextPage, endCursor }
                nodes {
                  id
                  fieldValues(first: 20) {
                    nodes {
                      __typename
                      ... on ProjectV2ItemFieldTextValue { 
                        text, field { ... on ProjectV2Field { name } } 
                      }
                      ... on ProjectV2ItemFieldSingleSelectValue { 
                        name, field { ... on ProjectV2SingleSelectField { name } } 
                      }
                      ... on ProjectV2ItemFieldIterationValue { 
                        title, field { ... on ProjectV2IterationField { name } } 
                      }
                      ... on ProjectV2ItemFieldNumberValue { 
                        number, field { ... on ProjectV2Field { name } } 
                      }
                      ... on ProjectV2ItemFieldDateValue { 
                        date, field { ... on ProjectV2Field { name } } 
                      }
                    }
                  }
                  content { ... on Issue { id, title, url } }
                }
              }
            }
          }
        }
        """
    
    def _extract_field_value(self, field_data: Dict[str, Any]) -> Optional[str]:
        """Extracts value from a field based on its type."""
        type_name = field_data.get("__typename")
        
        value_map = {
            "ProjectV2ItemFieldSingleSelectValue": "name",
            "ProjectV2ItemFieldIterationValue": "title", 
            "ProjectV2ItemFieldTextValue": "text",
            "ProjectV2ItemFieldNumberValue": "number",
            "ProjectV2ItemFieldDateValue": "date"
        }
        
        value_key = value_map.get(type_name)
        return field_data.get(value_key) if value_key else None
    
    def _parse_item_fields(self, item: Dict[str, Any]) -> Dict[str, str]:
        """Parses field values from a project item."""
        fields = {}
        
        for field_data in item.get("fieldValues", {}).get("nodes", []):
            field_name = field_data.get("field", {}).get("name")
            if not field_name:
                continue
                
            value = self._extract_field_value(field_data)
            if value is not None:
                fields[field_name] = str(value)
                
        return fields
    
    def fetch_release_blocker_stats(self) -> Dict[str, int]:
        """
        Fetches release blocker statistics from the GitHub project.
        
        Returns:
            Dict with 'total', 'done', and 'percentage' keys
        """
        total = 0
        done = 0
        cursor = None
        
        while True:
            variables = {
                "owner": self.owner,
                "projectNumber": self.project_number,
                "cursor": cursor
            }
            
            try:
                response = requests.post(
                    self.api_url,
                    json={'query': self._get_query(), 'variables': variables},
                    headers=self.headers,
                    timeout=30
                )
                response.raise_for_status()
                result = response.json()
                
                if "errors" in result:
                    raise Exception(f"GraphQL errors: {result['errors']}")
                    
            except requests.RequestException as e:
                raise Exception(f"API request failed: {e}")
            
            # Parse response
            project = result.get("data", {}).get("organization", {}).get("projectV2", {})
            items = project.get("items", {})
            
            for item in items.get("nodes", []):
                if not item.get("content"):
                    continue
                    
                fields = self._parse_item_fields(item)
                
                if fields.get("Release gating") == "Release_Blocker":
                    total += 1
                    if fields.get("Status") == "Done":
                        done += 1
            
            # Check for next page
            page_info = items.get("pageInfo", {})
            if not page_info.get("hasNextPage", False):
                break
            cursor = page_info.get("endCursor")
        
        percentage = round((done / total * 100), 1) if total > 0 else 0
        
        return {
            'total': total,
            'done': done,
            'percentage': percentage
        }


def main():
    """Main function to run the tracker."""
    import os
    import sys
    
    # Configuration - can be overridden by environment variables
    TOKEN = os.getenv("GITHUB_TOKEN")
    OWNER = os.getenv("GITHUB_OWNER", "HDFGroup")
    PROJECT_NUMBER = int(os.getenv("GITHUB_PROJECT_NUMBER", "39"))
    
    try:
        tracker = GitHubProjectTracker(TOKEN, OWNER, PROJECT_NUMBER)
        stats = tracker.fetch_release_blocker_stats()
        
        # Output for GitHub Actions
        github_output = os.getenv("GITHUB_OUTPUT")
        if github_output:
            with open(github_output, "a") as f:
                f.write(f"percentage={stats['percentage']}\n")
                f.write(f"done={stats['done']}\n")
                f.write(f"total={stats['total']}\n")
        
        # Also output to stdout for local testing
        print(f"percentage={stats['percentage']}")
        print(f"Calculated progress: {stats['percentage']}%")
        print(f"Done / Total: {stats['done']} / {stats['total']}")
        
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
```

### `.github/workflows/update-progress.yml`

```yaml
name: Release Progress

on:
  schedule:
    - cron: '0 */4 * * *'  # Run every 4 hours
  workflow_dispatch:  # Allow manual triggering

jobs:
  check-progress:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    if: github.repository_owner == 'HDFGROUP'
    steps:
    - name: Checkout repository
      uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        
    - name: Set up Python
      uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548 # v6
      with:
        python-version: '3.11'  # Use specific version for consistency
        
    - name: Cache pip dependencies
      uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
        
    - name: Install dependencies
      run: |
        pip install requests
        
    - name: Validate required secrets
      run: |
        if [ -z "${{ secrets.GIST_TOKEN }}" ]; then
          echo "::error::GIST_TOKEN secret is required"
          exit 1
        fi
        if [ -z "${{ secrets.GIST_ID }}" ]; then
          echo "::error::GIST_ID secret is required"
          exit 1
        fi
        echo "::notice::All required secrets are present"
        
    - name: Debug information
      if: runner.debug == '1'
      run: |
        echo "Repository: $GITHUB_REPOSITORY"
        echo "Workflow: $GITHUB_WORKFLOW"
        echo "Run ID: $GITHUB_RUN_ID"
        echo "Python version: $(python --version)"
        echo "Environment variables:"
        env | grep -E '^GITHUB_' | sort
        
    - name: Calculate progress
      id: progress
      run: |
        cd .github/workflows
        # Check if Python script exists
        if [ ! -f "update-progress.py" ]; then
          echo "::error::update-progress.py not found in .github/workflows directory"
          exit 1
        fi
        
        echo "::notice::Running progress calculation script..."
        
        # Run the Python script with error handling
        if ! python update-progress.py > progress_output.txt 2>&1; then
          echo "::error::Python script execution failed"
          echo "Script output:"
          cat progress_output.txt
          exit 1
        fi
        
        # Display the captured output for debugging
        echo "=== Python Script Output ==="
        cat progress_output.txt
        echo "=========================="
        
        # Extract and validate percentage
        PERCENTAGE=$(grep "^percentage=" progress_output.txt | cut -d'=' -f2 | head -1)
        if [ -z "$PERCENTAGE" ] || ! [[ "$PERCENTAGE" =~ ^[0-9]+(\.[0-9]+)?$ ]]; then
          echo "::error::Invalid or missing percentage value: '$PERCENTAGE'"
          echo "Expected format: percentage=XX.X"
          exit 1
        fi
        
        # Extract and validate done/total counts
        DONE_TOTAL=$(grep "^Done / Total:" progress_output.txt | cut -d':' -f2 | xargs | head -1)
        if [ -z "$DONE_TOTAL" ]; then
          echo "::error::Missing 'Done / Total:' line in script output"
          exit 1
        fi
        
        DONE=$(echo "$DONE_TOTAL" | cut -d' ' -f1)
        TOTAL=$(echo "$DONE_TOTAL" | cut -d' ' -f3)  # Account for "XX / YY" format
        
        # Validate numeric values
        if ! [[ "$DONE" =~ ^[0-9]+$ ]] || ! [[ "$TOTAL" =~ ^[0-9]+$ ]]; then
          echo "::error::Invalid done/total values: done='$DONE', total='$TOTAL'"
          echo "Expected format: 'Done / Total: XX / YY'"
          exit 1
        fi
        
        # Validate logical consistency
        if [ "$TOTAL" -eq 0 ]; then
          echo "::error::Total count cannot be zero"
          exit 1
        fi
        
        if [ "$DONE" -gt "$TOTAL" ]; then
          echo "::error::Done count ($DONE) cannot exceed total count ($TOTAL)"
          exit 1
        fi
        
        # Set outputs for use in subsequent steps
        echo "percentage=$PERCENTAGE" >> $GITHUB_OUTPUT
        echo "done=$DONE" >> $GITHUB_OUTPUT
        echo "total=$TOTAL" >> $GITHUB_OUTPUT
        
        echo "::notice::Progress calculation successful: ${PERCENTAGE}% (${DONE}/${TOTAL})"
        
        # Clean up
        rm -f progress_output.txt
        
      env:
        GITHUB_TOKEN: ${{ secrets.GIST_TOKEN }}
        GITHUB_OWNER: "HDFGroup"
        GITHUB_PROJECT_NUMBER: "39"

    - name: Update progress badge Gist
      env:
        GITHUB_TOKEN: ${{ secrets.GIST_TOKEN }}
        GIST_ID: ${{ secrets.GIST_ID }}
      run: |
        PERCENTAGE="${{ steps.progress.outputs.percentage }}"
        DONE="${{ steps.progress.outputs.done }}"
        TOTAL="${{ steps.progress.outputs.total }}"
        
        echo "::notice::Updating badge with: ${PERCENTAGE}% (${DONE}/${TOTAL})"
        
        # Use integer comparison for more reliable thresholds
        PERCENTAGE_INT="${PERCENTAGE%.*}"
        # Determine badge color and status with cleaner logic
        if [ "$PERCENTAGE_INT" -ge 90 ]; then
          COLOR="brightgreen"
          STATUS="ðŸŸ¢ Readying for Deployment"
        elif [ "$PERCENTAGE_INT" -ge 60 ]; then
          COLOR="yellow"
          STATUS="ðŸŸ¡ Nearing Completion"
        elif [ "$PERCENTAGE_INT" -ge 40 ]; then
          COLOR="orange"
          STATUS="ðŸŸ  In Development"
        else
          COLOR="red"
          STATUS="ðŸ”´ Initial Phase"
        fi
        
        echo "::notice title=Release Progress::${PERCENTAGE}% Complete (${DONE}/${TOTAL}) - ${STATUS}"
        
        # Create badge JSON for shields.io endpoint with proper escaping (\($done)/\($total))
        BADGE_JSON=$(jq -n \
          --arg percentage "$PERCENTAGE" \
          --arg done "$DONE" \
          --arg total "$TOTAL" \
          --arg color "$COLOR" \
          '{
            "schemaVersion": 1,
            "label": "Release Progress",
            "message": "\($percentage)%",
            "color": $color,
            "style": "flat-square"
          }')
          
        # Validate JSON was created successfully
        if [ -z "$BADGE_JSON" ] || ! echo "$BADGE_JSON" | jq empty 2>/dev/null; then
          echo "::error::Failed to generate valid badge JSON"
          exit 1
        fi
          
        # The filename in the Gist must match the one created manually
        GIST_NAME="release-progress-${GITHUB_REPOSITORY##*/}.json"
        echo "::notice::Updating Gist file: $GIST_NAME"
        
        # Create the request payload
        REQUEST_PAYLOAD=$(jq -n \
          --arg filename "$GIST_NAME" \
          --argjson content "$BADGE_JSON" \
          '{
            "files": {
              ($filename): {
                "content": ($content | tostring)
              }
            }
          }')
        
        # Update the existing Gist with response validation
        echo "Updating Gist: ${GIST_ID}"
        RESPONSE=$(curl -s -w "\n%{http_code}" -L -X PATCH \
          -H "Authorization: token ${GITHUB_TOKEN}" \
          -H "Accept: application/vnd.github.v3+json" \
          "https://api.github.com/gists/${GIST_ID}" \
          -d "$REQUEST_PAYLOAD")
        
        # Extract HTTP status code (last line) and response body
        HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
        RESPONSE_BODY=$(echo "$RESPONSE" | head -n -1)
        
        # Validate API response
        if [ "$HTTP_CODE" != "200" ]; then
          echo "::error::Gist update failed with HTTP status $HTTP_CODE"
          echo "Response body: $RESPONSE_BODY"
          exit 1
        fi
        
        echo "::notice::Gist updated successfully"
        
        # Generate badge URL for use in README
        BADGE_URL="https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/${GITHUB_REPOSITORY_OWNER}-Bot/${GIST_ID}/raw/${GIST_NAME}"
        PROJECT_URL="https://github.com/${GITHUB_REPOSITORY}/projects/39"
        
        echo "::notice::Badge URL generated: $BADGE_URL"
        echo "badge_url=$BADGE_URL" >> $GITHUB_OUTPUT
        echo "project_url=$PROJECT_URL" >> $GITHUB_OUTPUT
          
        # Create enhanced step summary
        echo "## ðŸ“Š Release Progress: ${PERCENTAGE}%" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${STATUS}" >> $GITHUB_STEP_SUMMARY
        echo "**Progress:** ${DONE} of ${TOTAL} release blockers completed" >> $GITHUB_STEP_SUMMARY
        echo "**Badge Color:** ${COLOR}" >> $GITHUB_STEP_SUMMARY
        echo "**Gist ID:** ${GIST_ID}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Badge URLs" >> $GITHUB_STEP_SUMMARY
        echo "**Markdown:** \`[![Release Progress](${BADGE_URL})](${PROJECT_URL})\`" >> $GITHUB_STEP_SUMMARY
        echo "**Image Only:** \`![Release Progress](${BADGE_URL})\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Badge JSON Preview" >> $GITHUB_STEP_SUMMARY
        echo '```json' >> $GITHUB_STEP_SUMMARY
        echo "$BADGE_JSON" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        
    - name: Cleanup on failure
      if: failure()
      run: |
        echo "::warning::Workflow failed - cleaning up temporary files"
        rm -f progress_output.txt
        echo "::notice::Check the logs above for specific error details"
```

### `.github/workflows/vfd-main.yml`

```yaml
name: hdf5 VFD CI main

on:
  workflow_call:
    inputs:
      build_mode:
        description: "Build type (CMAKE_BUILD_TYPE)"
        required: true
        type: string

permissions:
  contents: read

jobs:
  # Test HDF5 VFDs that are always built by default
  #hdf5_vfd_standard:
  #  uses: ./.github/workflows/vfd-standard.yml
  #  with:
  #    build_mode: "${{ inputs.build_mode }}"

  # Test HDF5 MPI I/O VFD
  #hdf5_vfd_mpiio:
  #  uses: ./.github/workflows/vfd-mpiio.yml
  #  with:
  #    build_mode: "${{ inputs.build_mode }}"

  # Test HDF5 Direct VFD
  #hdf5_vfd_direct:
  #  uses: ./.github/workflows/vfd-direct.yml
  #  with:
  #    build_mode: "${{ inputs.build_mode }}"

  # Test HDF5 Mirror VFD
  #hdf5_vfd_mirror:
  #  uses: ./.github/workflows/vfd-mirror.yml
  #  with:
  #    build_mode: "${{ inputs.build_mode }}"

  # Build aws-c-s3 library for ROS3 VFD testing
  build_aws_c_s3:
    uses: ./.github/workflows/build-aws-c-s3.yml
    with:
      build_mode: "${{ inputs.build_mode }}"
      aws_c_s3_tag: "main"

  # Test HDF5 ROS3 VFD
  hdf5_vfd_ros3:
    needs: build_aws_c_s3
    uses: ./.github/workflows/vfd-ros3.yml
    with:
      build_mode: "${{ inputs.build_mode }}"
      aws_c_s3_build_type: "source"

  # Test HDF5 HDFS VFD
  #hdf5_vfd_hdfs:
  #  uses: ./.github/workflows/vfd-hdfs.yml
  #  with:
  #    build_mode: "${{ inputs.build_mode }}"

  # Test HDF5 Subfiling VFD
  hdf5_vfd_subfiling:
    uses: ./.github/workflows/vfd-subfiling.yml
    with:
      build_mode: "${{ inputs.build_mode }}"
```

### `.github/workflows/vfd-ros3.yml`

```yaml
name: Build and test HDF5 ROS3 VFD

on:
  workflow_call:
    inputs:
      build_mode:
        description: "Build type (CMAKE_BUILD_TYPE)"
        required: true
        type: string
      aws_c_s3_build_type:
        description: "Install aws-c-s3 from a package manager ('package') or build from source ('source')"
        required: true
        type: string
      save_binary:
        description: "binary-ext-name or missing"
        required: false
        default: "skip"
        type: string
      java_version:
        description: "Java version for testing (11, 17, 21, 24, latest, auto)"
        required: false
        default: "auto"
        type: string
      force_java_implementation:
        description: "Force specific Java implementation (auto, ffm, jni)"
        required: false
        default: "jni"
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  # NOTE: The aws-c-s3 library build is now handled by the parent workflow
  # that calls this workflow. The parent must call build-aws-c-s3.yml before
  # calling vfd-ros3.yml to ensure the libaws-c-s3 artifact is available.

  build_from_package_managers:
    if: ${{ inputs.aws_c_s3_build_type == 'package' }}

    # Ubuntu doesn't have a package for aws-c-s3 yet, so use a
    # built from source version until then

    strategy:
      # Let jobs run to completion even if one fails
      fail-fast: false
      matrix:
        os_name: ["Windows MSVC", "Ubuntu GCC", "MacOS Clang"]
        include:
          - os_name: "Windows MSVC"
            os: windows-latest
            test_ros3: OFF
          - os_name: "Ubuntu GCC"
            os: ubuntu-latest
            test_ros3: ON
          - os_name: "MacOS Clang"
            os: macos-latest
            test_ros3: OFF

    name: "ROS3 VFD (${{ matrix.os_name }} ${{ inputs.build_mode }}-${{ inputs.force_java_implementation }})"
    runs-on: ${{ matrix.os }}
    steps:
      - name: Install aws-c-s3 (Windows)
        if: ${{ matrix.os_name == 'Windows MSVC' }}
        run: vcpkg install aws-c-s3

      # Ubuntu doesn't have a package for aws-c-s3 yet, so use a
      # built from source version until then
      # - name: Install aws-c-s3 (Ubuntu)
      #   if: ${{ matrix.os_name == 'Ubuntu GCC' }}
      #   run: |
      #     sudo apt-get update
      #     sudo apt-get install aws-c-s3

      - name: Install libaws-c-s3 (Ubuntu) (Cached installation)
        if: ${{ matrix.os_name == 'Ubuntu GCC' }}
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: libaws-c-s3-${{ inputs.build_mode }}

      - name: Untar libaws-c-s3 installation (Ubuntu)
        if: ${{ matrix.os_name == 'Ubuntu GCC' }}
        run: |
          tar xvf libaws-c-s3.tar -C ${{ runner.workspace }}

      - name: List contents of libaws-c-s3 installation (Ubuntu)
        if: ${{ matrix.os_name == 'Ubuntu GCC' }}
        run: |
          ls -lR ${{ runner.workspace }}/aws-c-s3-build

      - name: Setup environment (Ubuntu)
        if: ${{ matrix.os_name == 'Ubuntu GCC' }}
        shell: bash
        run: |
          echo "LD_LIBRARY_PATH=${{ runner.workspace }}/aws-c-s3-build/lib:$LD_LIBRARY_PATH" >> $GITHUB_ENV
          echo "CMAKE_PREFIX_PATH=${{ runner.workspace }}/aws-c-s3-build" >> $GITHUB_ENV

      - name: Install aws-c-s3 (MacOS)
        if: ${{ matrix.os_name == 'MacOS Clang' }}
        run: brew install aws-c-s3

      - name: Set up Java (if specified or FFM required)
        if: inputs.java_version != 'auto' || inputs.force_java_implementation == 'ffm'
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          distribution: ${{ inputs.force_java_implementation == 'ffm' && 'oracle' || 'temurin' }}
          java-version: |
            ${{
              inputs.force_java_implementation == 'ffm' && '25' ||
              inputs.java_version == 'latest' && '24' ||
              inputs.java_version
            }}

      - name: Verify Java Setup
        if: inputs.java_version != 'auto' || inputs.force_java_implementation == 'ffm'
        run: |
          java -version
          echo "JAVA_HOME=$JAVA_HOME"
          echo "Selected Java implementation: ${{ inputs.force_java_implementation }}"

      - name: Set environment for MSVC (Windows)
        if: ${{ matrix.os_name == 'Windows MSVC' }}
        run: |
          # Set these environment variables so CMake picks the correct compiler
          echo "CXX=cl.exe" >> $GITHUB_ENV
          echo "CC=cl.exe" >> $GITHUB_ENV

      - name: Get HDF5 sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Setup jextract (FFM builds only)
        if: ${{ inputs.force_java_implementation == 'ffm' }}
        uses: ./.github/actions/setup-jextract
        with:
          java-version: '25'

      # For Windows, use vcpkg toolchain file to allow find_package() calls to resolve
      - name: "Configure (vcpkg; ROS3 testing: ${{ matrix.test_ros3 }})"
        if: ${{ matrix.os_name == 'Windows MSVC' }}
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            --log-level=VERBOSE \
            -DCMAKE_TOOLCHAIN_FILE=C:/vcpkg/scripts/buildsystems/vcpkg.cmake \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_WARNINGS_AS_ERRORS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON \
            -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON \
            -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_ROS3_VFD:BOOL=ON \
            -DHDF5_ENABLE_ROS3_VFD_DOCKER_PROXY=${{ matrix.test_ros3 }} \
            -DHDF5_PACK_EXAMPLES:BOOL=ON \
            -DHDF5_ENABLE_JNI:BOOL=${{ inputs.force_java_implementation == 'jni' }} \
            $GITHUB_WORKSPACE
        shell: bash

      - name: "Configure (ROS3 testing: ${{ matrix.test_ros3 }})"
        if: ${{ matrix.os_name != 'Windows MSVC' }}
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_WARNINGS_AS_ERRORS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON \
            -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON \
            -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_ROS3_VFD:BOOL=ON \
            -DHDF5_ENABLE_ROS3_VFD_DOCKER_PROXY=${{ matrix.test_ros3 }} \
            -DHDF5_PACK_EXAMPLES:BOOL=ON \
            -DHDF5_ENABLE_JNI:BOOL=${{ inputs.force_java_implementation == 'jni' }} \
            $GITHUB_WORKSPACE
        shell: bash

      - name: Build
        run: cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      - name: Run Tests
        if: matrix.test_ros3 == 'ON'
        run: |
          # For now, just run S3 tests
          ctest . -C ${{ inputs.build_mode }} -R "S3TEST"
        working-directory: ${{ runner.workspace }}/build

  # Build and test the ROS3 VFD using aws-c-s3 from source (Linux) or package managers (Windows/macOS)
  build_and_test_vfd:
    if: ${{ inputs.aws_c_s3_build_type == 'source' }}

    strategy:
      # Let jobs run to completion even if one fails
      fail-fast: false
      matrix:
        os_name: ["Windows MSVC", "Ubuntu GCC", "MacOS Clang"]
        include:
          - os_name: "Windows MSVC"
            os: windows-latest
            ostype: windows
            test_ros3: OFF
          - os_name: "Ubuntu GCC"
            os: ubuntu-latest
            ostype: ubuntu
            test_ros3: ON
          - os_name: "MacOS Clang"
            os: macos-latest
            ostype: macos
            test_ros3: OFF

    name: "ROS3 VFD Source (${{ matrix.os_name }} ${{ inputs.build_mode }})"
    runs-on: ${{ matrix.os }}
    steps:
      # Linux: Download aws-c-s3 artifact from build-aws-c-s3 workflow
      - name: Install libaws-c-s3 (Ubuntu) (Cached installation)
        if: ${{ matrix.os_name == 'Ubuntu GCC' }}
        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6.0.0
        with:
          name: libaws-c-s3-${{ inputs.build_mode }}

      - name: Untar libaws-c-s3 installation (Ubuntu)
        if: ${{ matrix.os_name == 'Ubuntu GCC' }}
        run: |
          tar xvf libaws-c-s3.tar -C ${{ runner.workspace }}

      - name: List contents of libaws-c-s3 installation (Ubuntu)
        if: ${{ matrix.os_name == 'Ubuntu GCC' }}
        run: |
          ls -lR ${{ runner.workspace }}/aws-c-s3-build

      - name: Setup environment (Ubuntu)
        if: ${{ matrix.os_name == 'Ubuntu GCC' }}
        shell: bash
        run: |
          echo "LD_LIBRARY_PATH=${{ runner.workspace }}/aws-c-s3-build/lib:$LD_LIBRARY_PATH" >> $GITHUB_ENV
          echo "CMAKE_PREFIX_PATH=${{ runner.workspace }}/aws-c-s3-build" >> $GITHUB_ENV

      # Windows: Install aws-c-s3 via vcpkg
      - name: Install aws-c-s3 (Windows)
        if: ${{ matrix.os_name == 'Windows MSVC' }}
        run: vcpkg install aws-c-s3

      # macOS: Install aws-c-s3 via Homebrew
      - name: Install aws-c-s3 (MacOS)
        if: ${{ matrix.os_name == 'MacOS Clang' }}
        run: brew install aws-c-s3

      - name: Set up Java (if specified or FFM required)
        if: inputs.java_version != 'auto' || inputs.force_java_implementation == 'ffm'
        uses: actions/setup-java@f2beeb24e141e01a676f977032f5a29d81c9e27e # v5
        with:
          distribution: ${{ inputs.force_java_implementation == 'ffm' && 'oracle' || 'temurin' }}
          java-version: |
            ${{
              inputs.force_java_implementation == 'ffm' && '25' ||
              inputs.java_version == 'latest' && '24' ||
              inputs.java_version
            }}

      - name: Verify Java Setup
        if: inputs.java_version != 'auto' || inputs.force_java_implementation == 'ffm'
        run: |
          java -version
          echo "JAVA_HOME=$JAVA_HOME"
          echo "Selected Java implementation: ${{ inputs.force_java_implementation }}"

      - name: Set environment for MSVC (Windows)
        if: ${{ matrix.os_name == 'Windows MSVC' }}
        run: |
          # Set these environment variables so CMake picks the correct compiler
          echo "CXX=cl.exe" >> $GITHUB_ENV
          echo "CC=cl.exe" >> $GITHUB_ENV

      - name: Get HDF5 sources
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Setup jextract (FFM builds only)
        if: ${{ inputs.force_java_implementation == 'ffm' }}
        uses: ./.github/actions/setup-jextract
        with:
          java-version: '25'

      # For Windows, use vcpkg toolchain file to allow find_package() calls to resolve
      - name: "Configure (vcpkg; ROS3 testing: ${{ matrix.test_ros3 }})"
        if: ${{ matrix.os_name == 'Windows MSVC' }}
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            --log-level=VERBOSE \
            -DCMAKE_TOOLCHAIN_FILE=C:/vcpkg/scripts/buildsystems/vcpkg.cmake \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_WARNINGS_AS_ERRORS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON \
            -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON \
            -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_ROS3_VFD:BOOL=ON \
            -DHDF5_ENABLE_ROS3_VFD_DOCKER_PROXY=${{ matrix.test_ros3 }} \
            -DHDF5_PACK_EXAMPLES:BOOL=ON \
            -DHDF5_ENABLE_JNI:BOOL=${{ inputs.force_java_implementation == 'jni' }} \
            $GITHUB_WORKSPACE
        shell: bash

      - name: "Configure (ROS3 testing: ${{ matrix.test_ros3 }})"
        if: ${{ matrix.os_name != 'Windows MSVC' }}
        run: |
          mkdir "${{ runner.workspace }}/build"
          cd "${{ runner.workspace }}/build"
          cmake -C $GITHUB_WORKSPACE/config/cmake/cacheinit.cmake \
            --log-level=VERBOSE \
            -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DBUILD_SHARED_LIBS=ON \
            -DHDF5_ENABLE_ALL_WARNINGS=ON \
            -DHDF5_ENABLE_WARNINGS_AS_ERRORS=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=OFF \
            -DHDF5_BUILD_FORTRAN:BOOL=OFF \
            -DHDF5_BUILD_CPP_LIB:BOOL=ON \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=ON \
            -DHDF5_ENABLE_SZIP_ENCODING:BOOL=ON \
            -DHDF5_ENABLE_PLUGIN_SUPPORT:BOOL=ON \
            -DLIBAEC_USE_LOCALCONTENT=OFF \
            -DZLIB_USE_LOCALCONTENT=OFF \
            -DHDF5_ENABLE_ROS3_VFD:BOOL=ON \
            -DHDF5_ENABLE_ROS3_VFD_DOCKER_PROXY=${{ matrix.test_ros3 }} \
            -DHDF5_PACK_EXAMPLES:BOOL=ON \
            -DHDF5_ENABLE_JNI:BOOL=${{ inputs.force_java_implementation == 'jni' }} \
            $GITHUB_WORKSPACE
        shell: bash

      - name: Build
        run: cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
        working-directory: ${{ runner.workspace }}/build

      - name: Run Tests
        if: matrix.test_ros3 == 'ON'
        run: |
          # For now, just run S3 tests
          ctest . -C ${{ inputs.build_mode }} -R "S3TEST"
        working-directory: ${{ runner.workspace }}/build

      - name: Run Package
        run: cpack -C ${{ inputs.build_mode }} -V
        working-directory: ${{ runner.workspace }}/build

      - name: List files in the space
        run: |
              ls -l ${{ runner.workspace }}/build

      # Save files created by CTest script
      - name: Save published binary (Windows)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: zip-vs2022_cl-S3-${{ inputs.build_mode }}-${{ inputs.save_binary }}-binary
              path: ${{ runner.workspace }}/build/HDF5-*-win64.zip
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
        if:  ${{ (matrix.ostype == 'windows') && ( inputs.save_binary != 'skip') }}

      - name: Save published binary (linux)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: tgz-ubuntu-2404_gcc-S3-${{ inputs.build_mode }}-${{ inputs.save_binary }}-binary
              path: ${{ runner.workspace }}/build/HDF5-*-Linux.tar.gz
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
        if:  ${{ (matrix.ostype == 'ubuntu') && ( inputs.save_binary != 'skip') }}

      - name: Save published binary (Mac_latest)
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
              name: tgz-macos14_clang-S3-${{ inputs.build_mode }}-${{ inputs.save_binary }}-binary
              path: ${{ runner.workspace }}/build/HDF5-*-Darwin.tar.gz
              if-no-files-found: error # 'warn' or 'ignore' are also available, defaults to `warn`
        if: ${{ (matrix.ostype == 'macos') && ( inputs.save_binary != 'skip') }}
```

### `.github/workflows/vfd-subfiling.yml`

```yaml
name: Test HDF5 Subfiling VFD

on:
  workflow_call:
    inputs:
      build_mode:
        description: "Build type (CMAKE_BUILD_TYPE)"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  build_and_test:
    strategy:
      # Let jobs run to completion even if one fails
      fail-fast: false
      matrix:
        os_name: ["Ubuntu"]
#        os_name: ["Ubuntu", "MacOS"]
        mpi_lib: ["OpenMPI"]
#        mpi_lib: ["OpenMPI", "MPICH"]
        include:
          - os_name: "Ubuntu"
            os: ubuntu-latest
            mpi_lib: "OpenMPI"
#          - os_name: "Ubuntu"
#            os: ubuntu-latest
#            mpi_lib: "MPICH"
#          - os_name: "MacOS"
#            os: macos-latest
#            mpi_lib: "OpenMPI"
#          - os_name: "MacOS"
#            os: macos-latest
#            mpi_lib: "MPICH"

    # Sets the job's name from the properties
    name: "Test HDF5 Subfiling VFD (${{ inputs.build_mode }}) on ${{ matrix.os_name }} with ${{ matrix.mpi_lib }}"

    runs-on: ${{ matrix.os }}

    steps:
      # - name: Install Linux Dependencies
      #  run: |
      #    sudo apt-get update
      #    sudo apt-get install 
      #  if: ${{ matrix.os == 'ubuntu-latest' }}

      # For now, just install OpenMPI or MPICH with the package
      # manager. Eventually, we should pick one or 2 release
      # versions of each, then build and cache those installations
      - name: Install OpenMPI
        run: |
          sudo apt-get update
          sudo apt-get install libopenmpi-dev
          echo "CC=mpicc" >> $GITHUB_ENV
        if: ${{ matrix.os == 'ubuntu-latest' && matrix.mpi_lib == 'OpenMPI' }}

      - name: Install MPICH
        run: |
          sudo apt-get update
          sudo apt-get install libmpich-dev
          echo "CC=mpicc" >> $GITHUB_ENV
        if: ${{ matrix.os == 'ubuntu-latest' && matrix.mpi_lib == 'MPICH' }}

      - name: Checkout HDF5
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Configure HDF5 with Subfiling VFD
        shell: bash
        run: |
          mkdir ${{ runner.workspace }}/build
          cd ${{ runner.workspace }}/build
          cmake -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/hdf5_build \
            -DBUILD_STATIC_LIBS=OFF \
            -DHDF5_TEST_VFD:BOOL=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=ON \
            -DHDF5_ENABLE_SUBFILING_VFD:BOOL=ON \
            -DMPIEXEC_MAX_NUMPROCS=2 \
            $GITHUB_WORKSPACE
          cat src/libhdf5.settings

      - name: Build HDF5
        shell: bash
        working-directory: ${{ runner.workspace }}/build
        run: |
          cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
          echo "LD_LIBRARY_PATH=${{ runner.workspace }}/build/bin" >> $GITHUB_ENV

      - name: Test HDF5 Subfiling VFD
        working-directory: ${{ runner.workspace }}/build
        run: |
          # For now, just run the tests directly setup for use with the
          # Subfiling VFD. We can expand on this once the library's tests
          # are better separated into categories for VFD testing.
          ctest --build-config ${{ inputs.build_mode }} \
              -R "MPI_TEST_t_subfiling_vfd|MPI_TEST_t_vfd|H5_ph5_subfiling"
```

### `.github/workflows/vfd.yml`

```yaml
name: hdf5 VFD CI

# Run VFD CI daily at 07:00 CDT (12:00 UTC) or on demand
on:
  workflow_dispatch:
  schedule:
    - cron: "0 12 * * *"

permissions:
  contents: read

jobs:
  build_and_test:
    strategy:
      matrix:
        build_mode: ["Release", "Debug"]

    # Sets the job's name from the properties
    name: "${{ matrix.build_mode }} Workflows"

    # Don't run the action if the commit message says to skip CI
    if: "!contains(github.event.head_commit.message, 'skip-ci')"

    uses: ./.github/workflows/vfd-main.yml
    with:
      build_mode: ${{ matrix.build_mode }}
```

### `.github/workflows/vol.yml`

```yaml
name: hdf5 VOL connectors CI

# Run VOL connector CI daily at 06:00 CDT (11:00 UTC) or on demand.
on:
  workflow_dispatch:
  schedule:
    - cron: "0 11 * * *"

permissions:
  contents: read

jobs:
  # Build and test individual VOL connectors by using HDF5's
  # CMake FetchContent functionality. 
  #hdf5_vol_daos_fetchcontent:
  #  uses: ./.github/workflows/vol_daos.yml
  #  with:
  #    build_mode: "Release"

  hdf5_vol_rest_fetchcontent:
    uses: ./.github/workflows/vol_rest.yml
    with:
      build_mode: "Release"

  hdf5_vol_ext_passthru_fetchcontent:
    uses: ./.github/workflows/vol_ext_passthru.yml
    with:
      build_mode: "Release"

  hdf5_vol_async_fetchcontent:
    uses: ./.github/workflows/vol_async.yml
    with:
      build_mode: "Release"

  hdf5_vol_cache_fetchcontent:
    uses: ./.github/workflows/vol_cache.yml
    with:
      build_mode: "Release"

  hdf5_vol_adios2:
    uses: ./.github/workflows/vol_adios2.yml
    with:
      build_mode: "Release"

  hdf5_vol_log:
    uses: ./.github/workflows/vol_log.yml
    with:
      build_mode: "Release"
```

### `.github/workflows/vol_adios2.yml`

```yaml
name: Test HDF5 ADIOS2 VOL

on:
  workflow_call:
    inputs:
      build_mode:
        description: "Build type"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1
  ADIOS2_COMMIT: 3adf20a929b69c23312a6b5f3cccc49376df77e8
  ADIOS2_COMMIT_SHORT: 3adf20a

jobs:
  build_and_test:
    name: Test HDF5 ADIOS2 VOL connector
    runs-on: ubuntu-latest
    steps:
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install libopenmpi-dev

      - name: Checkout HDF5
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          path: hdf5

      - name: Configure HDF5
        shell: bash
        run: |
          mkdir ${{ github.workspace }}/hdf5/build
          cd ${{ github.workspace }}/hdf5/build
          cmake -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/hdf5_build \
            -DBUILD_STATIC_LIBS=OFF \
            -DHDF5_TEST_API:BOOL=ON \
            -DHDF5_TEST_API_ENABLE_ASYNC:BOOL=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=ON \
            -DHDF5_ENABLE_THREADSAFE:BOOL=ON \
            -DHDF5_ALLOW_UNSUPPORTED:BOOL=ON \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF \
            ${{ github.workspace }}/hdf5
          cat src/libhdf5.settings

      - name: Build and install HDF5
        shell: bash
        working-directory: ${{ github.workspace }}/hdf5/build
        run: |
          cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
          cmake --install .
          echo "LD_LIBRARY_PATH=${{ github.workspace }}/hdf5/build/bin" >> $GITHUB_ENV
          echo "PATH=${{ runner.workspace }}/hdf5_build/bin:${PATH}" >> $GITHUB_ENV

      # Since the HDF5 ADIOS2 VOL connector is part of the ADIOS2 repository,
      # it is difficult to use CMake's FetchContent functionality to fetch
      # and build the ADIOS2 connector. Also, since building of ADIOS2 takes
      # a while, it isn't ideal to have to rebuild it every time we want to
      # test against changes in HDF5 or the VOL connector. Therefore, just
      # use a fixed commit for the build of ADIOS2 so we can cache that and
      # still test the connector against changes in HDF5.
      - name: Restore ADIOS2 (${{ env.ADIOS2_COMMIT_SHORT }}) installation cache
        id: cache-adios2
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: ${{ runner.workspace }}/adios2-${{ env.ADIOS2_COMMIT_SHORT }}-install
          key: ${{ runner.os }}-${{ runner.arch }}-adios2-${{ env.ADIOS2_COMMIT }}-${{ inputs.build_mode }}-cache

      - if: ${{ steps.cache-adios2.outputs.cache-hit != 'true' }}
        name: Checkout ADIOS2 (${{ env.ADIOS2_COMMIT_SHORT }})
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: ornladios/ADIOS2
          ref: ${{ env.ADIOS2_COMMIT }}
          path: adios2

      - if: ${{ steps.cache-adios2.outputs.cache-hit != 'true' }}
        name: Install ADIOS2 (${{ env.ADIOS2_COMMIT_SHORT }})
        env:
          CXX: mpic++
          CC: mpicc
        run: |
          mkdir adios2/build
          cd adios2/build
          cmake -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/adios2-${{ env.ADIOS2_COMMIT_SHORT }}-install \
            -DADIOS2_USE_HDF5:BOOL=ON \
            -DHDF5_ROOT=${{ runner.workspace }}/hdf5_build/ \
            ..
          make -j2
          make -j2 install

      - name: Cache ADIOS2 (${{ env.ADIOS2_COMMIT_SHORT }}) installation
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        if: ${{ steps.cache-adios2.outputs.cache-hit != 'true' }}
        with:
          path: ${{ runner.workspace }}/adios2-${{ env.ADIOS2_COMMIT_SHORT }}-install
          key: ${{ runner.os }}-${{ runner.arch }}-adios2-${{ env.ADIOS2_COMMIT }}-${{ inputs.build_mode }}-cache

      - name: Set environment variables for tests
        run: |
          echo "HDF5_PLUGIN_PATH=${{ runner.workspace }}/adios2-${{ env.ADIOS2_COMMIT_SHORT }}-install/lib" >> $GITHUB_ENV
          echo "HDF5_VOL_CONNECTOR=ADIOS2_VOL" >> $GITHUB_ENV

      # Skip parallel testing for now as it appears to hang
      - name: Test HDF5 ADIOS2 VOL connector with HDF5 API tests
        working-directory: ${{ github.workspace }}/hdf5/build
        # Don't test the ADIOS2 VOL connector with the HDF5 API tests yet,
        # as it doesn't currently pass all the tests. Leave the step in,
        # but skip it to leave an indication that this should be re-enabled
        # in the future.
        if: false
        run: |
          ctest --build-config ${{ inputs.build_mode }} -R "h5_api" -E "parallel" .
```

### `.github/workflows/vol_async.yml`

```yaml
name: Test HDF5 async VOL

on:
  workflow_call:
    inputs:
      build_mode:
        description: "Build type"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  build_and_test:
    name: Test HDF5 asynchronous I/O VOL connector
    runs-on: ubuntu-latest
    steps:
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install automake autoconf libtool libtool-bin libopenmpi-dev

      - name: Checkout HDF5
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          path: hdf5

      - name: Checkout Argobots
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: pmodels/argobots
          path: abt

      # Argobots builds and installs fairly quickly,
      # so no caching is currently performed here
      - name: Install Argobots
        working-directory: ${{ github.workspace }}/abt
        run: |
          ./autogen.sh
          ./configure --prefix=/usr/local
          make -j2
          sudo make -j2 install

      - name: Configure HDF5 with asynchronous I/O VOL connector
        shell: bash
        run: |
          mkdir ${{ github.workspace }}/hdf5/build
          cd ${{ github.workspace }}/hdf5/build
          cmake -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/hdf5_build \
            -DBUILD_STATIC_LIBS=OFF \
            -DHDF5_TEST_API:BOOL=ON \
            -DHDF5_TEST_API_ENABLE_ASYNC:BOOL=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=ON \
            -DHDF5_ENABLE_THREADSAFE:BOOL=ON \
            -DHDF5_ALLOW_UNSUPPORTED:BOOL=ON \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF \
            -DHDF5_VOL_ALLOW_EXTERNAL:STRING="GIT" \
            -DHDF5_VOL_URL01:STRING="https://github.com/HDFGroup/vol-async.git" \
            -DHDF5_VOL_VOL-ASYNC_BRANCH:STRING="develop" \
            -DHDF5_VOL_VOL-ASYNC_NAME:STRING="async under_vol=0\;under_info={}" \
            -DHDF5_VOL_VOL-ASYNC_TEST_PARALLEL:BOOL=ON \
            ${{ github.workspace }}/hdf5
          cat src/libhdf5.settings

      - name: Build HDF5 and asynchronous I/O VOL connector
        shell: bash
        working-directory: ${{ github.workspace }}/hdf5/build
        run: |
          cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
          echo "LD_LIBRARY_PATH=/usr/local/lib:${{ github.workspace }}/hdf5/build/bin" >> $GITHUB_ENV

      # Workaround for asynchronous I/O VOL CMake issue
      - name: Copy testing files
        working-directory: ${{ github.workspace }}/hdf5/build
        run: |
          cp bin/async_test* ./_deps/vol-async-build/test

      - name: Test HDF5 asynchronous I/O VOL connector with external tests
        working-directory: ${{ github.workspace }}/hdf5/build
        run: |
          ctest --build-config ${{ inputs.build_mode }} -R "async_test" .

      - name: Test HDF5 asynchronous I/O VOL connector with HDF5 API tests
        working-directory: ${{ github.workspace }}/hdf5/build
        # Don't test the Async VOL connector with the HDF5 API tests yet,
        # as it doesn't currently pass all the tests. Leave the step in,
        # but skip it to leave an indication that this should be re-enabled
        # in the future.
        if: false
        run: |
          ctest --build-config ${{ inputs.build_mode }} -R "HDF5_VOL_vol-async" .
```

### `.github/workflows/vol_cache.yml`

```yaml
name: Test HDF5 cache VOL

on:
  workflow_call:
    inputs:
      build_mode:
        description: "Build type"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  build_and_test:
    strategy:
      matrix:
        name:
          - "Test HDF5 cache VOL connector"
          - "Test HDF5 cache VOL connector atop async VOL connector"
        async: [false, true]
        exclude:
          - name: "Test HDF5 cache VOL connector"
            async: true

          - name: "Test HDF5 cache VOL connector atop async VOL connector"
            async: false


    runs-on: ubuntu-latest
    steps:
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install automake autoconf libtool libtool-bin libopenmpi-dev

      - name: Checkout HDF5
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          path: hdf5

      - name: Checkout Argobots
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: pmodels/argobots
          path: abt

      # Argobots builds and installs fairly quickly,
      # so no caching is currently performed here
      - name: Install Argobots
        working-directory: ${{ github.workspace }}/abt
        run: |
          ./autogen.sh
          ./configure --prefix=/usr/local
          make -j2
          sudo make -j2 install

      - name: Set environment variables for configuration (cache VOL only)
        run: |
          echo "HDF5_VOL_CACHE_TEST_NAME=cache_ext config=$GITHUB_WORKSPACE/config1.cfg\;under_vol=0\;under_info={}\;" >> $GITHUB_ENV
        if: ${{ ! matrix.async }}

      - name: Set environment variables for configuration (cache VOL atop async VOL)
        run: |
          echo "HDF5_VOL_CACHE_TEST_NAME=cache_ext config=$GITHUB_WORKSPACE/config1.cfg\;under_vol=512\;under_info={under_vol=0\;under_info={}}\;" >> $GITHUB_ENV
        if: ${{ matrix.async }}

      # Define ASYNC_INCLUDE_DIR, ASYNC_INCLUDE_DIRS and ASYNC_LIBRARIES to
      # patch around having the cache VOL find the async VOL when they're built
      # at the same time. Once the Async and Cache VOLs create CMake .config
      # files, this should no longer be needed with CMake 3.24 and newer (see
      # FetchContent's OVERRIDE_FIND_PACKAGE)
      - name: Configure HDF5 with cache VOL connector
        shell: bash
        run: |
          mkdir ${{ github.workspace }}/hdf5/build
          cd ${{ github.workspace }}/hdf5/build
          cmake -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/hdf5_build \
            -DBUILD_STATIC_LIBS=OFF \
            -DHDF5_TEST_API:BOOL=ON \
            -DHDF5_TEST_API_ENABLE_ASYNC:BOOL=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=ON \
            -DHDF5_ENABLE_THREADSAFE:BOOL=ON \
            -DHDF5_ALLOW_UNSUPPORTED:BOOL=ON \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF \
            -DHDF5_VOL_ALLOW_EXTERNAL:STRING="GIT" \
            -DHDF5_VOL_URL01:STRING="https://github.com/HDFGroup/vol-async.git" \
            -DHDF5_VOL_VOL-ASYNC_BRANCH:STRING="develop" \
            -DHDF5_VOL_VOL-ASYNC_NAME:STRING="async under_vol=0\;under_info={}" \
            -DHDF5_VOL_VOL-ASYNC_TEST_PARALLEL:BOOL=ON \
            -DHDF5_VOL_URL02:STRING="https://github.com/HDFGroup/vol-cache.git" \
            -DHDF5_VOL_VOL-CACHE_BRANCH:STRING="develop" \
            -DHDF5_VOL_VOL-CACHE_NAME:STRING="$HDF5_VOL_CACHE_TEST_NAME" \
            -DHDF5_VOL_VOL-CACHE_TEST_PARALLEL:BOOL=ON \
            -DASYNC_INCLUDE_DIR=${{ github.workspace }}/hdf5/build/_deps/vol-async-src/src \
            -DASYNC_INCLUDE_DIRS=${{ github.workspace }}/hdf5/build/_deps/vol-async-src/src \
            -DASYNC_LIBRARIES=${{ github.workspace }}/hdf5/build/bin/libasynchdf5.a\;${{ github.workspace }}/hdf5/build/bin/libh5async.so \
            ${{ github.workspace }}/hdf5
          cat src/libhdf5.settings

      - name: Build HDF5 and cache VOL connector
        shell: bash
        working-directory: ${{ github.workspace }}/hdf5/build
        run: |
          cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
          echo "LD_LIBRARY_PATH=/usr/local/lib:${{ github.workspace }}/hdf5/build/bin" >> $GITHUB_ENV

      - name: Create cache VOL connector configuration file for testing
        shell: bash
        run: |
          mkdir -p $GITHUB_WORKSPACE/scratch
          touch $GITHUB_WORKSPACE/config1.cfg
          echo "HDF5_CACHE_STORAGE_SCOPE: LOCAL" >> $GITHUB_WORKSPACE/config1.cfg
          echo "HDF5_CACHE_STORAGE_PATH: $GITHUB_WORKSPACE/scratch" >> $GITHUB_WORKSPACE/config1.cfg
          echo "HDF5_CACHE_STORAGE_SIZE: 4294967296" >> $GITHUB_WORKSPACE/config1.cfg
          echo "HDF5_CACHE_STORAGE_TYPE: SSD" >> $GITHUB_WORKSPACE/config1.cfg
          echo "HDF5_CACHE_REPLACEMENT_POLICY: LRU" >> $GITHUB_WORKSPACE/config1.cfg

      # Workaround for cache VOL CMake issue
      - name: Copy testing files
        working-directory: ${{ github.workspace }}/hdf5/build
        run: |
          cp bin/test_file.exe ./_deps/vol-cache-build/tests
          cp bin/test_group.exe ./_deps/vol-cache-build/tests
          cp bin/test_dataset.exe ./_deps/vol-cache-build/tests
          cp bin/test_dataset_async_api.exe ./_deps/vol-cache-build/tests
          cp bin/test_write_multi.exe ./_deps/vol-cache-build/tests
          cp bin/test_multdset.exe ./_deps/vol-cache-build/tests

      - name: Set environment variables for external tests (cache VOL only)
        run: |
          echo "HDF5_PLUGIN_PATH=${{ github.workspace }}/hdf5/build/bin/" >> $GITHUB_ENV
          echo "HDF5_VOL_CONNECTOR=cache_ext config=$GITHUB_WORKSPACE/config1.cfg;under_vol=0;under_info={};" >> $GITHUB_ENV
        if: ${{ ! matrix.async }}

      - name: Set environment variables for external tests (cache VOL atop async VOL)
        run: |
          echo "HDF5_PLUGIN_PATH=${{ github.workspace }}/hdf5/build/bin/" >> $GITHUB_ENV
          echo "HDF5_VOL_CONNECTOR=cache_ext config=$GITHUB_WORKSPACE/config1.cfg;under_vol=512;under_info={under_vol=0;under_info={}};" >> $GITHUB_ENV
        if: ${{ matrix.async }}

      # Until cache VOL tests are namespaced properly, run them directly
      - name: Test HDF5 cache VOL connector with external tests
        working-directory: ${{ github.workspace }}/hdf5/build
        run: |
          ctest --build-config ${{ inputs.build_mode }} -R "^test_file$" .
          ctest --build-config ${{ inputs.build_mode }} -R "^test_group$" .
          ctest --build-config ${{ inputs.build_mode }} -R "^test_dataset$" .
          ctest --build-config ${{ inputs.build_mode }} -R "^test_dataset_async_api$" .
          ctest --build-config ${{ inputs.build_mode }} -R "^test_write_multi$" .
          ctest --build-config ${{ inputs.build_mode }} -R "^test_multdset$" .

      - name: Test HDF5 cache VOL connector with HDF5 API tests
        working-directory: ${{ github.workspace }}/hdf5/build
        # Don't test the Cache VOL connector with the HDF5 API tests yet
        # when it's stacked on top of the Async connector, as it doesn't
        # currently pass all the tests due to the Async connector not passing
        # all the tests. Leave the step in, but skip it to leave an indication
        # that this should be re-enabled in the future.
        if: ${{ ! matrix.async }}
        run: |
          ctest --build-config ${{ inputs.build_mode }} -R "HDF5_VOL_vol-cache" -E "H5DIFF" .
```

### `.github/workflows/vol_ext_passthru.yml`

```yaml
name: Test HDF5 external pass-through VOL

on:
  workflow_call:
    inputs:
      build_mode:
        description: "Build type"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  build_and_test:
    name: Test HDF5 external passthrough VOL connector
    runs-on: ubuntu-latest
    steps:
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install libopenmpi-dev

      - name: Checkout HDF5
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          path: hdf5

      - name: Checkout vol-external-passthrough
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: hpc-io/vol-external-passthrough
          path: vol-external-passthrough

      - name: Configure HDF5 with external passthrough VOL connector
        shell: bash
        run: |
          mkdir ${{ github.workspace }}/hdf5/build
          cd ${{ github.workspace }}/hdf5/build
          cmake -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/hdf5_build \
            -DBUILD_STATIC_LIBS=OFF \
            -DHDF5_TEST_API:BOOL=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=ON \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF \
            -DHDF5_VOL_ALLOW_EXTERNAL:STRING="GIT" \
            -DHDF5_VOL_URL01:STRING="https://github.com/hpc-io/vol-external-passthrough.git" \
            -DHDF5_VOL_VOL-EXTERNAL-PASSTHROUGH_BRANCH:STRING="develop" \
            -DHDF5_VOL_VOL-EXTERNAL-PASSTHROUGH_NAME:STRING="pass_through_ext under_vol=0\;under_info={}\;" \
            -DHDF5_VOL_VOL-EXTERNAL-PASSTHROUGH_TEST_PARALLEL:BOOL=ON \
            ${{ github.workspace }}/hdf5
          cat src/libhdf5.settings

      - name: Build HDF5 and external passthrough VOL connector
        shell: bash
        working-directory: ${{ github.workspace }}/hdf5/build
        run: |
          cmake --build . --parallel 3 --config ${{ inputs.build_mode }}

      - name: Test HDF5 external passthrough VOL connector with HDF5 API tests
        working-directory: ${{ github.workspace }}/hdf5/build
        run: |
          ctest --build-config ${{ inputs.build_mode }} -R "HDF5_VOL_vol-external-passthrough" .
```

### `.github/workflows/vol_log.yml`

```yaml
name: Test HDF5 Log-based VOL

on:
  workflow_call:
    inputs:
      build_mode:
        description: "Build type"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1

jobs:
  build_and_test:
    name: Test HDF5 Log-based VOL connector
    runs-on: ubuntu-latest
    steps:
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install automake autoconf libtool libtool-bin libopenmpi-dev zlib1g-dev
          #mpich

      - name: Checkout HDF5
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          path: hdf5

      # Log-based VOL currently doesn't have CMake support
      - name: Configure HDF5
        shell: bash
        run: |
          mkdir ${{ github.workspace }}/hdf5/build
          cd ${{ github.workspace }}/hdf5/build
          cmake -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/hdf5_build \
            -DBUILD_STATIC_LIBS=OFF \
            -DHDF5_TEST_API:BOOL=ON \
            -DHDF5_TEST_API_ENABLE_ASYNC:BOOL=ON \
            -DHDF5_ENABLE_PARALLEL:BOOL=ON \
            -DHDF5_ENABLE_THREADSAFE:BOOL=ON \
            -DHDF5_ALLOW_UNSUPPORTED:BOOL=ON \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=ON \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF \
            ${{ github.workspace }}/hdf5
          cat src/libhdf5.settings

      - name: Build and install HDF5
        shell: bash
        working-directory: ${{ github.workspace }}/hdf5/build
        run: |
          cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
          cmake --install .
          echo "LD_LIBRARY_PATH=${{ github.workspace }}/hdf5/build/bin" >> $GITHUB_ENV
          echo "PATH=${{ runner.workspace }}/hdf5_build/bin:${PATH}" >> $GITHUB_ENV

      - name: Checkout Log-based VOL
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: HDFGroup/vol-log-based
          path: vol-log-based

      - name: Build HDF5 Log-based VOL connector and test with external tests 
        env:
          CXX: mpic++
          CC: mpicc
          LD_LIBRARY_PATH: ${{ runner.workspace }}/hdf5_build/lib
        run: |
          cd vol-log-based
          autoreconf -i
          ./configure --prefix=${{ runner.workspace }}/vol-log-based-build --with-hdf5=${{ runner.workspace }}/hdf5_build/ --enable-shared --enable-zlib
          make -j2 && make install
          export HDF5_PLUGIN_PATH="${{ runner.workspace }}/vol-log-based-build/lib"
          export HDF5_VOL_CONNECTOR="LOG under_vol=0;under_info={}"
          make check
          echo "HDF5_PLUGIN_PATH=${HDF5_PLUGIN_PATH}" >> $GITHUB_ENV
          echo "HDF5_VOL_CONNECTOR=${HDF5_VOL_CONNECTOR}" >> $GITHUB_ENV

      # Skip parallel testing for now as it appears to hang
      - name: Test HDF5 Log-based VOL connector with HDF5 API tests
        working-directory: ${{ github.workspace }}/hdf5/build
        # Don't test the Log-based VOL connector with the HDF5 API tests yet,
        # as it doesn't currently pass all the tests. Leave the step in,
        # but skip it to leave an indication that this should be re-enabled
        # in the future.
        if: false
        run: |
          ctest --build-config ${{ inputs.build_mode }} -R "h5_api" -E "parallel" .
```

### `.github/workflows/vol_rest.yml`

```yaml
name: Test HDF5 REST VOL

on:
  workflow_call:
    inputs:
      build_mode:
        description: "Build type"
        required: true
        type: string

permissions:
  contents: read

env:
  CTEST_OUTPUT_ON_FAILURE: 1
  ADMIN_PASSWORD: admin
  ADMIN_USERNAME: admin
  USER_NAME: test_user1
  USER_PASSWORD: test
  USER2_NAME: test_user2
  USER2_PASSWORD: test
  HSDS_USERNAME: test_user1
  HSDS_PASSWORD: test
  HSDS_PATH: /home/test_user1/
  HDF5_API_TEST_PATH_PREFIX: /home/test_user1/
  HSDS_ENDPOINT: http+unix://%2Ftmp%2Fhs%2Fsn_1.sock
  HDF5_VOL_CONNECTOR: REST
  ROOT_DIR: ${{github.workspace}}/hsdsdata
  BUCKET_NAME: hsdstest

jobs:
  build_and_test:
    name: Test HDF5 REST VOL connector
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10"]

    steps:
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install libcurl4-openssl-dev libyajl-dev

      - name: Checkout HDF5
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          path: hdf5

      - name: Configure HDF5 with REST VOL connector
        shell: bash
        run: |
          mkdir ${{ github.workspace }}/hdf5/build
          cd ${{ github.workspace }}/hdf5/build
          cmake -DCMAKE_BUILD_TYPE=${{ inputs.build_mode }} \
            -DCMAKE_INSTALL_PREFIX=${{ runner.workspace }}/hdf5_build \
            -DBUILD_STATIC_LIBS=OFF \
            -DHDF5_BUILD_HL_LIB:BOOL=ON \
            -DHDF5_TEST_API:BOOL=ON \
            -DHDF5_ALLOW_UNSUPPORTED:BOOL=ON \
            -DHDF5_ENABLE_ZLIB_SUPPORT:BOOL=OFF \
            -DHDF5_ENABLE_SZIP_SUPPORT:BOOL=OFF \
            -DHDF5_VOL_ALLOW_EXTERNAL:STRING="GIT" \
            -DHDF5_VOL_URL01:STRING="https://github.com/HDFGroup/vol-rest.git" \
            -DHDF5_VOL_VOL-REST_BRANCH:STRING="master" \
            -DHDF5_VOL_VOL-REST_NAME:STRING="REST" \
            -DHDF5_VOL_VOL-REST_TEST_PARALLEL:BOOL=OFF \
            -DHDF5_VOL_REST_ENABLE_EXAMPLES=ON \
            ${{ github.workspace }}/hdf5
          cat src/libhdf5.settings

      - name: Build and install HDF5 and REST VOL connector
        shell: bash
        working-directory: ${{ github.workspace }}/hdf5/build
        run: |
          cmake --build . --parallel 3 --config ${{ inputs.build_mode }}
          cmake --install .
          echo "LD_LIBRARY_PATH=${{ github.workspace }}/hdf5/build/bin" >> $GITHUB_ENV

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548 # v6
        with:
          python-version: ${{ matrix.python-version }}

      - name: Checkout HSDS
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0
        with:
          repository: HDFGroup/hsds
          path: ${{github.workspace}}/hsds

      - name: Get HSDS HEAD commit SHA
        shell: bash
        working-directory: ${{github.workspace}}/hsds
        run: |
          export HSDS_COMMIT=`git rev-parse HEAD`
          export HSDS_COMMIT_SHORT=`git rev-parse --short HEAD`
          echo "HSDS_COMMIT=${HSDS_COMMIT}" >> $GITHUB_ENV
          echo "HSDS_COMMIT_SHORT=${HSDS_COMMIT_SHORT}" >> $GITHUB_ENV

      # Note that we don't currently cache HSDS, as we would need
      # to pick a fixed commit/tag in order to generate a reasonable
      # key to use for caching/restoring from the cache
      #- name: Restore HSDS (${{ env.HSDS_COMMIT_SHORT }}) installation cache
      #  id: restore-hsds
      #  uses: actions/cache@v3
      #  with:
      #    path: ${{ runner.workspace }}/hsds-${{ env.HSDS_COMMIT_SHORT }}-install
      #    key: ${{ runner.os }}-${{ runner.arch }}-hsds-${{ env.HSDS_COMMIT }}-${{ inputs.build_mode }}-cache

      - name: Install HSDS (${{ env.HSDS_COMMIT_SHORT }}) dependencies
        shell: bash
        working-directory: ${{github.workspace}}/hsds
        run: |
          python -m pip install --upgrade pip
          python -m pip install pytest
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Install HSDS (${{ env.HSDS_COMMIT_SHORT }}) package
        #if: ${{ ! steps.restore-hsds.outputs.cache-hit }}
        shell: bash
        run: |
          cd ${{github.workspace}}/hsds
          pip install -e .

      #- name: Cache HSDS (${{ env.HSDS_COMMIT_SHORT }}) installation
      #  uses: actions/cache/save@v3
      #  if: ${{ ! steps.restore-hsds.outputs.cache-hit }}
      #  with:
      #    path: ${{ runner.workspace }}/hsds-${{ env.HSDS_COMMIT_SHORT }}-install
      #    key: ${{ runner.os }}-${{ runner.arch }}-hsds-${{ env.HSDS_COMMIT }}-${{ inputs.build_mode }}-cache

      # Requests 2.32.0 breaks requests-unixsocket, used by HSDS for socket connections
      - name: Fix requests version
        run: |
          pip install requests==2.31.0

      - name: Run HSDS unit tests
        shell: bash
        working-directory: ${{github.workspace}}/hsds
        run: |
          pytest

      - name: Start HSDS
        working-directory: ${{github.workspace}}/hsds
        run: |
          mkdir ${{github.workspace}}/hsdsdata &&
          mkdir ${{github.workspace}}/hsdsdata/hsdstest &&
          cp admin/config/groups.default admin/config/groups.txt &&
          cp admin/config/passwd.default admin/config/passwd.txt &&
          cp admin/config/groups.default admin/config/groups.txt &&
          cp admin/config/passwd.default admin/config/passwd.txt
          ROOT_DIR=${{github.workspace}}/hsdsdata ./runall.sh --no-docker 1 &
          sleep 10

      - name: Test HSDS
        working-directory: ${{github.workspace}}/hsds
        run: |
          python tests/integ/setup_test.py

      - name: Test HDF5 REST VOL connector with external tests
        working-directory: ${{github.workspace}}/hdf5/build/
        run: |
          sudo \
          HDF5_PLUGIN_PATH="${{ runner.workspace }}/hdf5_build/lib" \
          HDF5_VOL_CONNECTOR=REST \
          ADMIN_USERNAME=admin ADMIN_PASSWORD=admin \
          USER_NAME=test_user1 USER_PASSWORD=test \
          USER2_NAME=test_user2 USER2_PASSWORD=test \
          HSDS_USERNAME=test_user1 HSDS_PASSWORD=test \
          HSDS_PATH=/home/test_user1/ HDF5_API_TEST_PATH_PREFIX=/home/test_user1/ \
          HSDS_ENDPOINT=http+unix://%2Ftmp%2Fhs%2Fsn_1.sock \
          ROOT_DIR=${{github.workspace}}/hsdsdata \
          BUCKET_NAME=hsdstest \
          ctest --build-config ${{ inputs.build_mode }} -R "test_rest_vol" .

      - name: Test HDF5 REST VOL connector with HDF5 API tests
        working-directory: ${{github.workspace}}/hdf5/build/
        # Don't test the REST VOL connector with the HDF5 API tests yet,
        # as it doesn't currently pass all the tests. Leave the step in,
        # but skip it to leave an indication that this should be re-enabled
        # in the future.
        if: false
        run: |
          sudo \
          HDF5_PLUGIN_PATH="${{ runner.workspace }}/hdf5_build/lib" \
          HDF5_VOL_CONNECTOR=REST \
          ADMIN_USERNAME=admin ADMIN_PASSWORD=admin \
          USER_NAME=test_user1 USER_PASSWORD=test \
          USER2_NAME=test_user2 USER2_PASSWORD=test \
          HSDS_USERNAME=test_user1 HSDS_PASSWORD=test \
          HSDS_PATH=/home/test_user1/ HDF5_API_TEST_PATH_PREFIX=/home/test_user1/ \
          HSDS_ENDPOINT=http+unix://%2Ftmp%2Fhs%2Fsn_1.sock \
          ROOT_DIR=${{github.workspace}}/hsdsdata \
          BUCKET_NAME=hsdstest \
          ctest --build-config ${{ inputs.build_mode }} -R "h5_api" -E "H5COPY|H5DIFF|H5LS|H5DUMP|H5REPACK" .
```

### `.h5chkright.ini`

```ini
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.

# Initialization files for the Copyright Checker, chkcopyright.
# Each line is a keyword for action and the rest are values.
# Keywords:
# '#'		Comments
# skip		Files to be skipped
# prune		Directories to be skipped.  Notice this prunes all directories
#		with the same name.  E.g.,
#		"prune test" skips test, fortran/test, c++/test, ...

# Skip LICENSE since it is the detail Copyright notice.
skip LICENSE

# Sort of strange to have a copyright notice in README
skip README

# Generated files in src.
skip H5config.h.in

# Generated files in fortran/src.
skip H5match_types.c
skip H5test_kind.f90

# Ignore this expected output file in windows/examples.
skip testExamples_exp_output.txt

# Skip all testfiles/* and tfiles/* since if we insert a copyright notice in the expected
# data files, we would have to spend extra effort to filter them out.
prune testfiles
prune tfiles
```

### `CMakeBuildOptions.cmake`

```cmake
# Put all top-level build options into one place
# This file will be included at the beginning of the root CMakeLists.txt
option (HDF5_USE_FOLDERS "Enable folder grouping of projects in IDEs." ON)
mark_as_advanced (HDF5_USE_FOLDERS)

option (HDF5_NO_PACKAGES "CPACK - Disable packaging" OFF)
mark_as_advanced (HDF5_NO_PACKAGES)
option (HDF5_ALLOW_UNSUPPORTED "Allow unsupported combinations of configure options" OFF)
mark_as_advanced (HDF5_ALLOW_UNSUPPORTED)

option (HDF5_ONLY_SHARED_LIBS "Only Build Shared Libraries" OFF)
mark_as_advanced (HDF5_ONLY_SHARED_LIBS)
option (BUILD_STATIC_LIBS "Build Static Libraries" ON)
option (BUILD_SHARED_LIBS "Build Shared Libraries" ON)

option (HDF5_BUILD_STATIC_TOOLS "Build Static Tools NOT Shared Tools" OFF)
mark_as_advanced (HDF5_BUILD_STATIC_TOOLS)

option (BUILD_STATIC_EXECS "Build Static Executables" OFF)
mark_as_advanced (BUILD_STATIC_EXECS)

option (HDF5_ENABLE_ANALYZER_TOOLS "enable the use of Clang tools" OFF)
mark_as_advanced (HDF5_ENABLE_ANALYZER_TOOLS)
option (HDF5_ENABLE_SANITIZERS "execute the Clang sanitizer" OFF)
mark_as_advanced (HDF5_ENABLE_SANITIZERS)
option (HDF5_ENABLE_FORMATTERS "format source files" OFF)
mark_as_advanced (HDF5_ENABLE_FORMATTERS)

option (HDF5_ENABLE_COVERAGE "Enable code coverage for Libraries and Programs" OFF)
mark_as_advanced (HDF5_ENABLE_COVERAGE)

option (HDF5_ENABLE_USING_MEMCHECKER "Indicate that a memory checker is used" OFF)
mark_as_advanced (HDF5_ENABLE_USING_MEMCHECKER)

option (HDF5_ENABLE_PREADWRITE "Use pread/pwrite in sec2/log/core VFDs in place of read/write (when available)" ON)
mark_as_advanced (HDF5_ENABLE_PREADWRITE)

option (HDF5_ENABLE_DEPRECATED_SYMBOLS "Enable deprecated public API symbols" ON)

option (HDF5_MINGW_STATIC_GCC_LIBS "Statically link libgcc/libstdc++" OFF)
mark_as_advanced (HDF5_MINGW_STATIC_GCC_LIBS)

option (HDF5_ENABLE_TRACE "Enable API tracing capability" OFF)
mark_as_advanced (HDF5_ENABLE_TRACE)

option (HDF5_ENABLE_EMBEDDED_LIBINFO "Embed library info into executables" ON)
mark_as_advanced (HDF5_ENABLE_EMBEDDED_LIBINFO)

option (HDF5_ENABLE_HDFS "Enable HDFS" OFF)

option (HDF5_ENABLE_PARALLEL "Enable parallel build (requires MPI)" OFF)

option (HDF5_ENABLE_SZIP_SUPPORT "Use SZip Filter" OFF)
option (HDF5_ENABLE_ZLIB_SUPPORT "Enable Zlib Filters" OFF)

option (HDF5_PACKAGE_EXTLIBS "CPACK - include external libraries" OFF)
mark_as_advanced (HDF5_PACKAGE_EXTLIBS)

option (HDF5_ENABLE_THREADSAFE "Enable thread-safety" OFF)

option (HDF5_ENABLE_CONCURRENCY "Enable multi-threaded concurrency" OFF)

option (HDF5_ENABLE_MAP_API "Build the map API" OFF)
mark_as_advanced (HDF5_ENABLE_MAP_API)

option (HDF5_BUILD_DOC "Build documentation" OFF)

option (HDF5_BUILD_PARALLEL_TOOLS "Build Parallel HDF5 Tools" OFF)
mark_as_advanced (HDF5_BUILD_PARALLEL_TOOLS)

option (HDF5_BUILD_TOOLS "Build HDF5 Tools" ON)

option (HDF5_ENABLE_PLUGIN_SUPPORT "Enable PLUGIN Filters" OFF)

option (HDF5_BUILD_HL_LIB "Build HIGH Level HDF5 Library" ON)

option (HDF5_BUILD_FORTRAN "Build FORTRAN support" OFF)

option (HDF5_BUILD_CPP_LIB "Build HDF5 C++ Library" OFF)

option (HDF5_BUILD_JAVA "Build Java HDF5 Library" OFF)
cmake_dependent_option (HDF5_ENABLE_JNI "Force JNI implementation instead of FFM for Java bindings when Java 25+ is available" ON "HDF5_BUILD_JAVA" OFF)
mark_as_advanced (HDF5_ENABLE_JNI)
cmake_dependent_option (HDF5_ENABLE_MAVEN_DEPLOY "Enable Maven repository deployment support" OFF "HDF5_BUILD_JAVA" OFF)
mark_as_advanced (HDF5_ENABLE_MAVEN_DEPLOY)
cmake_dependent_option (HDF5_MAVEN_SNAPSHOT "Build Maven snapshot versions with -SNAPSHOT suffix" OFF "HDF5_BUILD_JAVA" OFF)
mark_as_advanced (HDF5_MAVEN_SNAPSHOT)

option (HDF5_BUILD_EXAMPLES "Build HDF5 Library Examples" ON)

option (BUILD_TESTING "Build HDF5 Unit Testing" ON)
```

### `CMakeFilters.cmake`

```cmake
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.
#

# -----------------------------------------------------------------------------
# HDF5 CMake Filter Support Configuration
# -----------------------------------------------------------------------------
# This CMake module configures support for external compression filters in HDF5,
# specifically ZLIB (including zlib-ng) and SZIP (libaec). It provides options
# for enabling/disabling filter support, selecting static/shared builds, and
# controlling how dependencies are found or built (external, local, or via GIT/TGZ).
#
# Key Features:
# - Options to enable/disable ZLIB and SZIP support, and select static/shared linking.
# - Support for using zlib-ng as a drop-in replacement for zlib.
# - Support for building dependencies externally (via GIT or TGZ) or using system libraries.
# - Handles configuration of include directories, library targets, and CMake variables
#   for downstream use.
# - Sets up required variables for HDF5 to use the DEFLATE and SZIP filters.
#
# Usage:
#   HDF5 includes this file from the main CMakeLists.txt if ZLIB or SZIP filter support in HDF5
#   is enabled. Configure options as needed before including this file.
#
# See comments throughout for details on each option and logic branch.
# -----------------------------------------------------------------------------

# Specify major options at the top of the file
# -----------------------------------------------------------------------------
cmake_dependent_option (HDF5_USE_ZLIB_NG "Use zlib-ng library as zlib library" OFF HDF5_ENABLE_ZLIB_SUPPORT OFF)
cmake_dependent_option (HDF5_USE_ZLIB_STATIC "Find static zlib library" OFF HDF5_ENABLE_ZLIB_SUPPORT OFF)
cmake_dependent_option (HDF5_USE_LIBAEC_STATIC "Find static AEC library" OFF HDF5_ENABLE_SZIP_SUPPORT OFF)
option (ZLIB_USE_EXTERNAL "Use External Library Building for ZLIB" OFF)
mark_as_advanced (ZLIB_USE_EXTERNAL)
option (SZIP_USE_EXTERNAL "Use External Library Building for SZIP" OFF)
mark_as_advanced (SZIP_USE_EXTERNAL)
cmake_dependent_option (ZLIB_USE_LOCALCONTENT "Use local file for ZLIB FetchContent" OFF HDF5_ENABLE_ZLIB_SUPPORT OFF)
mark_as_advanced (ZLIB_USE_LOCALCONTENT)
cmake_dependent_option (LIBAEC_USE_LOCALCONTENT "Use local file for LIBAEC FetchContent" OFF HDF5_ENABLE_SZIP_SUPPORT OFF)
mark_as_advanced (LIBAEC_USE_LOCALCONTENT)


# -----------------------------------------------------------------------------
# the ExternalProject module is needed for building compression libraries from source
include (ExternalProject)

# If compression libraries will be built from source, then choose which method and
# source location.
#option (HDF5_ALLOW_EXTERNAL_SUPPORT "Allow External Library Building (NO GIT TGZ)" "NO")
set (HDF5_ALLOW_EXTERNAL_SUPPORT "NO" CACHE STRING "Allow External Library Building (NO GIT TGZ)")
set_property (CACHE HDF5_ALLOW_EXTERNAL_SUPPORT PROPERTY STRINGS NO GIT TGZ)
if (HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "GIT" OR HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "TGZ")
  set (ZLIB_USE_EXTERNAL ON CACHE BOOL "Use External Library Building for ZLIB else search" FORCE)
  set (SZIP_USE_EXTERNAL ON CACHE BOOL "Use External Library Building for SZIP else search" FORCE)
  if (HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "GIT")
    if (HDF5_USE_ZLIB_NG)
      set (ZLIB_URL ${ZLIBNG_GIT_URL} CACHE STRING "Path to zlib-ng git repository")
      set (ZLIB_BRANCH ${ZLIBNG_GIT_BRANCH})
    else ()
      set (ZLIB_URL ${ZLIB_GIT_URL} CACHE STRING "Path to zlib git repository")
      set (ZLIB_BRANCH ${ZLIB_GIT_BRANCH})
    endif ()
    
    set (SZIP_URL ${LIBAEC_GIT_URL} CACHE STRING "Path to szip git repository")
    set (SZIP_BRANCH ${LIBAEC_GIT_BRANCH})
  elseif (HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "TGZ")
    if (NOT TGZPATH)
      set (TGZPATH ${HDF5_SOURCE_DIR})
    endif ()
    if (NOT ZLIB_USE_LOCALCONTENT)
      if (HDF5_USE_ZLIB_NG)
        set (ZLIB_URL ${ZLIBNG_TGZ_ORIGPATH}/${ZLIBNG_TGZ_NAME})
      else ()
        set (ZLIB_URL ${ZLIB_TGZ_ORIGPATH}/${ZLIB_TGZ_NAME})
      endif ()
    else ()
      if (HDF5_USE_ZLIB_NG)
        set (ZLIB_URL ${TGZPATH}/${ZLIBNG_TGZ_NAME})
      else ()
        set (ZLIB_URL ${TGZPATH}/${ZLIB_TGZ_NAME})
      endif ()
      if (NOT EXISTS "${ZLIB_URL}")
        set (HDF5_ENABLE_ZLIB_SUPPORT OFF CACHE BOOL "" FORCE)
        message (VERBOSE "Filter ZLIB file ${ZLIB_URL} not found")
      endif ()
    endif ()
    message (VERBOSE "Filter ZLIB URL is ${ZLIB_URL}")

    if (NOT LIBAEC_USE_LOCALCONTENT)
      set (SZIP_URL ${LIBAEC_TGZ_ORIGPATH}/${LIBAEC_TGZ_NAME})
    else ()
      set (SZIP_URL ${TGZPATH}/${LIBAEC_TGZ_NAME})
      if (NOT EXISTS "${SZIP_URL}")
        set (HDF5_ENABLE_SZIP_SUPPORT OFF CACHE BOOL "" FORCE)
        message (VERBOSE "Filter SZIP file ${SZIP_URL} not found")
      endif ()
    endif ()
    message (VERBOSE "Filter SZIP URL is ${SZIP_URL}")

  else ()
    set (HDF5_ENABLE_ZLIB_SUPPORT OFF CACHE BOOL "" FORCE)
    set (ZLIB_USE_EXTERNAL OFF CACHE BOOL "Use External Library Building for ZLIB else search")
    set (HDF5_ENABLE_SZIP_SUPPORT OFF CACHE BOOL "" FORCE)
    set (SZIP_USE_EXTERNAL OFF CACHE BOOL "Use External Library Building for SZIP else search")
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Option for ZLib support
#-----------------------------------------------------------------------------
set (H5_ZLIB_FOUND FALSE)
# Choose which zlib package to use by name
if (NOT DEFINED ZLIB_PACKAGE_NAME)
  set (ZLIB_PACKAGE_NAME "zlib")
endif ()
if (NOT DEFINED ZLIBNG_PACKAGE_NAME)
  set (ZLIBNG_PACKAGE_NAME "zlib-ng")
endif ()
if (HDF5_ENABLE_ZLIB_SUPPORT)
  if (NOT H5_ZLIB_HEADER) # This checks if zlib has already been found/built
    if (NOT ZLIB_USE_EXTERNAL) # This checks if zlib should be found on the system or built from an external source
      cmake_dependent_option (HDF5_MODULE_MODE_ZLIB "Prefer module mode to find ZLIB" ON "NOT ZLIB_USE_EXTERNAL" OFF)
      mark_as_advanced (HDF5_MODULE_MODE_ZLIB)
      if (HDF5_USE_ZLIB_NG)
        set (HDF5_MODULE_MODE_ZLIB OFF CACHE BOOL "" FORCE)
        set (Z_PACKAGE_NAME ${ZLIBNG_PACKAGE_NAME}${HDF_PACKAGE_EXT})
      else ()
        set (Z_PACKAGE_NAME ${ZLIB_PACKAGE_NAME}${HDF_PACKAGE_EXT})
      endif ()
      set (ZLIB_FOUND FALSE)
      message (VERBOSE "Filter HDF5_ZLIB package name:${Z_PACKAGE_NAME}")
      if (HDF5_MODULE_MODE_ZLIB)
        # Expect that the default shared library is expected with FindZLIB.cmake
        find_package (ZLIB MODULE)
      else ()
        # Expect that a correctly built library with CMake config files is available
        if (HDF5_USE_ZLIB_STATIC)
          set (ZLIB_SEARCH_TYPE "static")
          if (CMAKE_VERSION VERSION_GREATER_EQUAL "3.24.0")
            set (ZLIB_USE_STATIC_LIBS  ${HDF5_USE_ZLIB_STATIC})
          endif ()
        else ()
          set (ZLIB_SEARCH_TYPE "shared")
        endif ()
        find_package (ZLIB NAMES ${Z_PACKAGE_NAME} CONFIG OPTIONAL_COMPONENTS ${ZLIB_SEARCH_TYPE})
      endif ()
      set (H5_ZLIB_FOUND ${ZLIB_FOUND})
      if (H5_ZLIB_FOUND)
        if (HDF5_USE_ZLIB_NG)
          set (H5_ZLIB_HEADER "zlib-ng.h")
        else ()
          set (H5_ZLIB_HEADER "zlib.h")
        endif ()
        set (H5_ZLIB_INCLUDE_DIR_GEN ${ZLIB_INCLUDE_DIR})
        set (H5_ZLIB_INCLUDE_DIRS ${H5_ZLIB_INCLUDE_DIRS} ${ZLIB_INCLUDE_DIR})
        if (NOT WIN32) #windows has a list of names
          # The FindZLIB.cmake module does not set an OUTPUT_NAME
          # on the target. The target returned is: ZLIB::ZLIB
          get_filename_component (libname ${ZLIB_LIBRARIES} NAME_WLE)
          string (REGEX REPLACE "^lib" "" libname ${libname})
          set_target_properties (ZLIB::ZLIB PROPERTIES OUTPUT_NAME ${libname})
        endif ()
        set (LINK_COMP_LIBS ${LINK_COMP_LIBS} ZLIB::ZLIB)
      endif ()
    else ()
      if (HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "GIT" OR HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "TGZ")
        EXTERNAL_ZLIB_LIBRARY (${HDF5_ALLOW_EXTERNAL_SUPPORT})
        message (VERBOSE "Filter HDF5_ZLIB is built")
        set (LINK_COMP_LIBS ${LINK_COMP_LIBS} ${H5_ZLIB_STATIC_LIBRARY})
      endif ()
    endif ()
  else ()
    # This project is being called from within another and ZLib is already configured
    set (H5_ZLIB_FOUND TRUE)
  endif ()
  if (H5_ZLIB_FOUND)
    set (H5_HAVE_FILTER_DEFLATE 1)
    set (H5_HAVE_ZLIB_H 1)
    if (HDF5_USE_ZLIB_NG AND NOT ZLIB_COMPAT)
      set (H5_HAVE_ZLIBNG_H 1)
    endif ()
    set (H5_HAVE_LIBZ 1)
    if (H5_HAVE_FILTER_DEFLATE)
      set (EXTERNAL_FILTERS "${EXTERNAL_FILTERS} DEFLATE")
    endif ()
    set (HDF5_COMP_INCLUDE_DIRECTORIES "${HDF5_COMP_INCLUDE_DIRECTORIES};${H5_ZLIB_INCLUDE_DIRS}")
    message (VERBOSE "Filter HDF5_ZLIB is ON")
  else ()
    set (HDF5_ENABLE_ZLIB_SUPPORT OFF CACHE BOOL "" FORCE)
    message (FATAL_ERROR " ZLib support in HDF5 was enabled but not found")
  endif ()
  message (VERBOSE "H5_ZLIB_HEADER=${H5_ZLIB_HEADER}")
endif ()

#-----------------------------------------------------------------------------
# Option for SzLib support
#-----------------------------------------------------------------------------
set (H5_SZIP_FOUND FALSE)
# Choose which szip package to use by name
if (NOT DEFINED LIBAEC_PACKAGE_NAME)
  set (LIBAEC_PACKAGE_NAME "libaec")
endif ()
if (HDF5_ENABLE_SZIP_SUPPORT)
  cmake_dependent_option (HDF5_ENABLE_SZIP_ENCODING "Use SZip Encoding" ON HDF5_ENABLE_SZIP_SUPPORT OFF)
  if (NOT SZIP_USE_EXTERNAL) # This checks if szip should be found on the system or built from an external source
    if (HDF5_USE_LIBAEC_STATIC)
      set (LIBAEC_SEARCH_TYPE "static")
    else ()
      set (LIBAEC_SEARCH_TYPE "shared")
    endif ()
    set (libaec_USE_STATIC_LIBS ${HDF5_USE_LIBAEC_STATIC})
    set (SZIP_FOUND FALSE)
    # Search pure Config mode, there is not a FindSZIP module available
    find_package (${LIBAEC_PACKAGE_NAME} NAMES ${LIBAEC_PACKAGE_NAME}${HDF_PACKAGE_EXT} OPTIONAL_COMPONENTS ${LIBAEC_SEARCH_TYPE})
    set (H5_SZIP_FOUND ${${LIBAEC_PACKAGE_NAME}_FOUND})
    if (H5_SZIP_FOUND)
      set (H5_SZIP_INCLUDE_DIR_GEN ${SZIP_INCLUDE_DIR})
      set (H5_SZIP_INCLUDE_DIRS ${H5_SZIP_INCLUDE_DIRS} ${SZIP_INCLUDE_DIR})
      if (LIBAEC_PACKAGE_NAME STREQUAL "libaec")
        set (LINK_COMP_LIBS ${LINK_COMP_LIBS} libaec::sz libaec::aec)
      else ()
        set (LINK_COMP_LIBS ${LINK_COMP_LIBS} ${SZIP_LIBRARIES})
      endif ()
    endif ()
    message (VERBOSE "H5_SZIP_FOUND=${SZIP_FOUND} and LINK_COMP_LIBS=${LINK_COMP_LIBS}")
  else ()
    if (HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "GIT" OR HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "TGZ")
      EXTERNAL_SZIP_LIBRARY (${HDF5_ALLOW_EXTERNAL_SUPPORT} ${HDF5_ENABLE_SZIP_ENCODING})
      message (VERBOSE "Filter SZIP is built using library AEC")
      set (LINK_COMP_LIBS ${LINK_COMP_LIBS} ${H5_SZIP_STATIC_LIBRARY})
    endif ()
  endif ()
  message (VERBOSE "LINK_COMP_LIBS=${LINK_COMP_LIBS}")
  if (H5_SZIP_FOUND)
    set (H5_HAVE_FILTER_SZIP 1)
    set (H5_HAVE_SZLIB_H 1)
    set (H5_HAVE_LIBSZ 1)
    set (HDF5_COMP_INCLUDE_DIRECTORIES "${HDF5_COMP_INCLUDE_DIRECTORIES};${H5_SZIP_INCLUDE_DIRS}")
    message (VERBOSE "Filter SZIP is ON")
    if (H5_HAVE_FILTER_SZIP)
      set (EXTERNAL_FILTERS "${EXTERNAL_FILTERS} DECODE")
    endif ()
    if (HDF5_ENABLE_SZIP_ENCODING)
      set (H5_HAVE_SZIP_ENCODER 1)
      set (EXTERNAL_FILTERS "${EXTERNAL_FILTERS} ENCODE")
    endif ()
  else ()
    set (HDF5_ENABLE_SZIP_SUPPORT OFF CACHE BOOL "" FORCE)
    message (FATAL_ERROR "SZIP support in HDF5 was enabled but not found")
  endif ()
endif ()
```

### `CMakeInstallation.cmake`

```cmake
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.
#

# -----------------------------------------------------------------------------
# HDF5 CMake Installation and Packaging Configuration
# -----------------------------------------------------------------------------
# This CMake module handles installation and packaging for the HDF5 library and
# its components. It configures install targets, generates CMake config files,
# sets up packaging with CPack, and manages platform-specific installer options.
#
# Key Features:
# - Installs HDF5 libraries, headers, utilities, documentation, and CMake config files.
# - Generates hdf5-config.cmake and version files for build and install trees.
# - Supports Windows (NSIS, WiX), macOS (DMG, Framework), and Linux (DEB, RPM, TGZ) packaging.
# - Handles installation of example files and release documentation.
# - Configures CPack variables and component groups for flexible packaging.
#
# Usage:
#   HDF5 includes this file from the main CMakeLists.txt to enable installation and
#   packaging for HDF5. Adjust options and variables as needed for your platform
#   and distribution requirements.
#
# See comments throughout for details on each section and option.
# -----------------------------------------------------------------------------

include (CMakePackageConfigHelpers)

#-----------------------------------------------------------------------------
# Check for Installation Utilities
#-----------------------------------------------------------------------------
if (WIN32)
  set (PF_ENV_EXT "(x86)")
  find_program (NSIS_EXECUTABLE NSIS.exe PATHS "$ENV{ProgramFiles}\\NSIS" "$ENV{ProgramFiles${PF_ENV_EXT}}\\NSIS")
  if(NOT CPACK_WIX_ROOT)
    file(TO_CMAKE_PATH "$ENV{WIX}" CPACK_WIX_ROOT)
  endif ()
  find_program (WIX_EXECUTABLE candle  PATHS "${CPACK_WIX_ROOT}/bin")
endif ()


#-----------------------------------------------------------------------------
# Add Target(s) to CMake Install for import into other projects
#-----------------------------------------------------------------------------
if (NOT HDF5_EXTERNALLY_CONFIGURED)
  if (HDF5_EXPORTED_TARGETS)
    install (
        EXPORT ${HDF5_EXPORTED_TARGETS}
        DESTINATION ${HDF5_INSTALL_CMAKE_DIR}
        FILE ${HDF5_PACKAGE}${HDF_PACKAGE_EXT}-targets.cmake
        NAMESPACE ${HDF_PACKAGE_NAMESPACE}
        COMPONENT configinstall
    )
  endif ()

  #-----------------------------------------------------------------------------
  # Export all exported targets to the build tree for use by parent project
  #-----------------------------------------------------------------------------
  export (
      TARGETS ${HDF5_LIBRARIES_TO_EXPORT} ${HDF5_LIB_DEPENDENCIES} ${HDF5_UTILS_TO_EXPORT}
      FILE ${HDF5_PACKAGE}${HDF_PACKAGE_EXT}-targets.cmake
      NAMESPACE ${HDF_PACKAGE_NAMESPACE}
  )
endif ()

#-----------------------------------------------------------------------------
# Set includes needed for build
#-----------------------------------------------------------------------------
set (HDF5_INCLUDES_BUILD_TIME
    ${HDF5_SRC_INCLUDE_DIRS} ${HDF5_CPP_SRC_DIR} ${HDF5_HL_SRC_DIR}
    ${HDF5_TOOLS_SRC_DIR} ${HDF5_SRC_BINARY_DIR}
)

#-----------------------------------------------------------------------------
# Set Java JAR names for config file (with Maven SNAPSHOT suffix if enabled)
#-----------------------------------------------------------------------------
if (HDF5_BUILD_JAVA)
  if (HDF5_ENABLE_MAVEN_DEPLOY AND HDF5_MAVEN_SNAPSHOT)
    set (HDF5_JARHDF5_JAR_NAME "jarhdf5-${HDF5_PACKAGE_VERSION}-SNAPSHOT.jar")
    set (HDF5_JAVAHDF5_JAR_NAME "javahdf5-${HDF5_PACKAGE_VERSION}-SNAPSHOT.jar")
  else ()
    set (HDF5_JARHDF5_JAR_NAME "jarhdf5-${HDF5_PACKAGE_VERSION}.jar")
    set (HDF5_JAVAHDF5_JAR_NAME "javahdf5-${HDF5_PACKAGE_VERSION}.jar")
  endif ()
  # slf4j JAR names (these are dependencies, version shouldn't change with SNAPSHOT)
  set (HDF5_SLF4J_API_JAR_NAME "slf4j-api-2.0.16.jar")
  set (HDF5_SLF4J_NOP_JAR_NAME "slf4j-nop-2.0.16.jar")
endif ()

#-----------------------------------------------------------------------------
# Configure the hdf5-config.cmake file for the build directory
#-----------------------------------------------------------------------------
set (INCLUDE_INSTALL_DIR ${HDF5_INSTALL_INCLUDE_DIR})
set (SHARE_INSTALL_DIR "${CMAKE_CURRENT_BINARY_DIR}/${HDF5_INSTALL_CMAKE_DIR}" )
set (CURRENT_BUILD_DIR "${CMAKE_CURRENT_BINARY_DIR}" )
configure_package_config_file (
    ${HDF_CONFIG_DIR}/install/hdf5-config.cmake.in
    "${HDF5_BINARY_DIR}/${HDF5_PACKAGE}${HDF_PACKAGE_EXT}-config.cmake"
    INSTALL_DESTINATION "${HDF5_INSTALL_CMAKE_DIR}"
    PATH_VARS INCLUDE_INSTALL_DIR SHARE_INSTALL_DIR CURRENT_BUILD_DIR
    INSTALL_PREFIX "${CMAKE_CURRENT_BINARY_DIR}"
)

#-----------------------------------------------------------------------------
# Configure the hdf5-config.cmake file for the install directory
#-----------------------------------------------------------------------------
set (INCLUDE_INSTALL_DIR ${HDF5_INSTALL_INCLUDE_DIR})
set (SHARE_INSTALL_DIR "${CMAKE_INSTALL_PREFIX}/${HDF5_INSTALL_CMAKE_DIR}" )
set (CURRENT_BUILD_DIR "${CMAKE_INSTALL_PREFIX}" )
configure_package_config_file (
    ${HDF_CONFIG_DIR}/install/hdf5-config.cmake.in
    "${HDF5_BINARY_DIR}${CMAKE_FILES_DIRECTORY}/${HDF5_PACKAGE}${HDF_PACKAGE_EXT}-config.cmake"
    INSTALL_DESTINATION "${HDF5_INSTALL_CMAKE_DIR}"
    PATH_VARS INCLUDE_INSTALL_DIR SHARE_INSTALL_DIR CURRENT_BUILD_DIR
)

if (NOT HDF5_EXTERNALLY_CONFIGURED)
  install (
      FILES ${HDF5_BINARY_DIR}${CMAKE_FILES_DIRECTORY}/${HDF5_PACKAGE}${HDF_PACKAGE_EXT}-config.cmake
      DESTINATION ${HDF5_INSTALL_CMAKE_DIR}
      COMPONENT configinstall
  )
endif ()

#-----------------------------------------------------------------------------
# Configure the hdf5-config-version .cmake file for the install directory
#-----------------------------------------------------------------------------
if (NOT HDF5_EXTERNALLY_CONFIGURED)
  write_basic_package_version_file (
    "${HDF5_BINARY_DIR}${CMAKE_FILES_DIRECTORY}/${HDF5_PACKAGE}${HDF_PACKAGE_EXT}-config-version.cmake"
    VERSION ${HDF5_PACKAGE_VERSION}
    COMPATIBILITY SameMinorVersion
  )
  #configure_file (
  #    ${HDF_CONFIG_DIR}/install/hdf5-config-version.cmake.in
  #    ${HDF5_BINARY_DIR}${CMAKE_FILES_DIRECTORY}/${HDF5_PACKAGE}${HDF_PACKAGE_EXT}-config-version.cmake @ONLY
  #)
  install (
      FILES ${HDF5_BINARY_DIR}${CMAKE_FILES_DIRECTORY}/${HDF5_PACKAGE}${HDF_PACKAGE_EXT}-config-version.cmake
      DESTINATION ${HDF5_INSTALL_CMAKE_DIR}
      COMPONENT configinstall
  )
endif ()

#-----------------------------------------------------------------------------
# Configure the libhdf5.settings file with library info
#-----------------------------------------------------------------------------
if (H5_WORDS_BIGENDIAN)
  set (BYTESEX big-endian)
else ()
  set (BYTESEX little-endian)
endif ()
configure_file (
    ${HDF5_SOURCE_DIR}/src/libhdf5.settings.in
    ${HDF5_SRC_BINARY_DIR}/libhdf5.settings ESCAPE_QUOTES @ONLY
)
install (
    FILES ${HDF5_SRC_BINARY_DIR}/libhdf5.settings
    DESTINATION ${HDF5_INSTALL_LIB_DIR}
    COMPONENT libraries
)

#-----------------------------------------------------------------------------
# Configure the HDF5_Examples.cmake file and the examples
#-----------------------------------------------------------------------------
option (HDF5_PACK_EXAMPLES  "Package the HDF5 Library Examples Compressed File" OFF)
if (HDF5_PACK_EXAMPLES)
  if (DEFINED CMAKE_TOOLCHAIN_FILE)
    get_filename_component(TOOLCHAIN ${CMAKE_TOOLCHAIN_FILE} NAME)
    set(CTEST_TOOLCHAIN_FILE "\${CTEST_SOURCE_DIRECTORY}/config/toolchain/${TOOLCHAIN}")
  endif ()
  configure_file (
      ${HDF_CONFIG_DIR}/examples/HDF5_Examples.cmake.in
      ${HDF5_BINARY_DIR}/HDF5_Examples.cmake @ONLY
  )
  install (
      FILES ${HDF5_BINARY_DIR}/HDF5_Examples.cmake
      DESTINATION ${HDF5_INSTALL_DATA_DIR}
      COMPONENT hdfdocuments
  )

  install (
    DIRECTORY ${HDF5_SOURCE_DIR}/HDF5Examples
    DESTINATION ${HDF5_INSTALL_DATA_DIR}
    USE_SOURCE_PERMISSIONS
    COMPONENT hdfdocuments
  )
  install (
      FILES
          ${HDF5_SOURCE_DIR}/release_docs/USING_CMake_Examples.txt
      DESTINATION ${HDF5_INSTALL_DATA_DIR}
      COMPONENT hdfdocuments
  )
  install (
      FILES
          ${HDF_CONFIG_DIR}/examples/CTestScript.cmake
      DESTINATION ${HDF5_INSTALL_DATA_DIR}
      COMPONENT hdfdocuments
  )
  install (
      FILES
          ${HDF_CONFIG_DIR}/examples/HDF5_Examples_options.cmake
      DESTINATION ${HDF5_INSTALL_DATA_DIR}
      COMPONENT hdfdocuments
  )
endif ()

#-----------------------------------------------------------------------------
# Configure the README.md file for the binary package
#-----------------------------------------------------------------------------
HDF_README_PROPERTIES(HDF5_BUILD_FORTRAN)

#-----------------------------------------------------------------------------
# Configure the LICENSE.txt file for the windows binary package
#-----------------------------------------------------------------------------
if (WIN32)
  configure_file (${HDF5_SOURCE_DIR}/LICENSE ${HDF5_BINARY_DIR}/LICENSE.txt @ONLY)
endif ()

#-----------------------------------------------------------------------------
# Add Document File(s) to CMake Install
#-----------------------------------------------------------------------------
if (NOT HDF5_EXTERNALLY_CONFIGURED)
  install (
      FILES ${HDF5_SOURCE_DIR}/LICENSE
      DESTINATION ${HDF5_INSTALL_DATA_DIR}
      COMPONENT hdfdocuments
  )
  if (EXISTS "${HDF5_SOURCE_DIR}/release_docs" AND IS_DIRECTORY "${HDF5_SOURCE_DIR}/release_docs")
    set (release_files
        ${HDF5_SOURCE_DIR}/release_docs/USING_HDF5_CMake.txt
        ${HDF5_SOURCE_DIR}/release_docs/CHANGELOG.md
    )
    if (WIN32)
      set (release_files
          ${release_files}
          ${HDF5_SOURCE_DIR}/release_docs/USING_HDF5_VS.txt
      )
    endif ()
    if (HDF5_PACK_INSTALL_DOCS)
      set (release_files
          ${release_files}
          ${HDF5_SOURCE_DIR}/release_docs/INSTALL_CMake.txt
          ${HDF5_SOURCE_DIR}/release_docs/HISTORY-1_8.txt
          ${HDF5_SOURCE_DIR}/release_docs/INSTALL
      )
      if (WIN32)
        set (release_files
            ${release_files}
            ${HDF5_SOURCE_DIR}/release_docs/INSTALL_Windows.txt
        )
      endif ()
      if (CYGWIN)
        set (release_files
            ${release_files}
            ${HDF5_SOURCE_DIR}/release_docs/INSTALL_Cygwin.txt
        )
      endif ()
      if (HDF5_ENABLE_PARALLEL)
        set (release_files
            ${release_files}
            ${HDF5_SOURCE_DIR}/release_docs/README_HPC.md
        )
      endif ()
    endif ()
    install (
        FILES ${release_files}
        DESTINATION ${HDF5_INSTALL_DOC_DIR}
        COMPONENT hdfdocuments
    )
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Set the cpack variables
#-----------------------------------------------------------------------------
if (NOT HDF5_EXTERNALLY_CONFIGURED AND NOT HDF5_NO_PACKAGES)
  set (CPACK_PACKAGE_VENDOR "HDF_Group")
  set (CPACK_PACKAGE_NAME "${HDF5_PACKAGE_NAME}")
  if (NOT WIN32 OR HDF5_VERS_SUBRELEASE MATCHES "^[0-9]+$")
    set (CPACK_PACKAGE_VERSION "${HDF5_PACKAGE_VERSION_STRING}")
  else ()
    set (CPACK_PACKAGE_VERSION "${HDF5_PACKAGE_VERSION}")
  endif ()
  set (CPACK_PACKAGE_VERSION_MAJOR "${HDF5_PACKAGE_VERSION_MAJOR}")
  set (CPACK_PACKAGE_VERSION_MINOR "${HDF5_PACKAGE_VERSION_MINOR}")
  set (CPACK_PACKAGE_VERSION_PATCH "")
  set (CPACK_RESOURCE_FILE_LICENSE "${CMAKE_CURRENT_SOURCE_DIR}/LICENSE")
  if (EXISTS "${HDF5_SOURCE_DIR}/release_docs")
    set (CPACK_PACKAGE_DESCRIPTION_FILE "${CMAKE_CURRENT_SOURCE_DIR}/release_docs/CHANGELOG.md")
    set (CPACK_RESOURCE_FILE_README "${CMAKE_CURRENT_SOURCE_DIR}/release_docs/CHANGELOG.md")
  endif ()
  set (CPACK_PACKAGE_RELOCATABLE TRUE)
  if (OVERRIDE_INSTALL_VERSION)
    set (CPACK_PACKAGE_INSTALL_DIRECTORY "${CPACK_PACKAGE_VENDOR}/${CPACK_PACKAGE_NAME}/${OVERRIDE_INSTALL_VERSION}")
  else ()
    set (CPACK_PACKAGE_INSTALL_DIRECTORY "${CPACK_PACKAGE_VENDOR}/${CPACK_PACKAGE_NAME}/${CPACK_PACKAGE_VERSION}")
  endif ()
  set (CPACK_PACKAGE_ICON "${HDF_CONFIG_DIR}/install/hdf.bmp")

  set (CPACK_ORIG_SOURCE_DIR ${CMAKE_SOURCE_DIR})
  if ("$ENV{BINSIGN}" STREQUAL "exists")
    set (CPACK_PRE_BUILD_SCRIPTS ${CMAKE_SOURCE_DIR}/config/install/SignPackageFiles.cmake)
  endif ()

  # Add the package types to the list of generators, TGZ is the default on all platforms
  set (CPACK_GENERATOR "TGZ")
  if (WIN32)
    set (CPACK_GENERATOR "ZIP")

    # Installers for 32- vs. 64-bit CMake:
    #  - Root install directory (displayed to end user at installer-run time)
    #  - "NSIS package/display name" (text used in the installer GUI)
    #  - Registry key used to store info about the installation
    set (CPACK_NSIS_PACKAGE_NAME "${HDF5_PACKAGE_STRING}")
    if (CMAKE_CL_64)
      set (CPACK_NSIS_INSTALL_ROOT "$PROGRAMFILES64")
      set (CPACK_PACKAGE_INSTALL_REGISTRY_KEY "${CPACK_PACKAGE_NAME}-${CPACK_PACKAGE_VERSION} (Win64)")
    else ()
      set (CPACK_NSIS_INSTALL_ROOT "$PROGRAMFILES")
      set (CPACK_PACKAGE_INSTALL_REGISTRY_KEY "${CPACK_PACKAGE_NAME}-${CPACK_PACKAGE_VERSION}")
    endif ()
    # set the install/uninstall icon used for the installer itself
    # There is a bug in NSI that does not handle full unix paths properly.
    set (CPACK_NSIS_MUI_ICON "${HDF_CONFIG_DIR}\\\\install\\\\hdf.ico")
    set (CPACK_NSIS_MUI_UNIICON "${HDF_CONFIG_DIR}\\\\install\\\\hdf.ico")
    # set the package header icon for MUI
    set (CPACK_PACKAGE_ICON "${HDF_CONFIG_DIR}\\\\install\\\\hdf.bmp")
    set (CPACK_NSIS_DISPLAY_NAME "${CPACK_NSIS_PACKAGE_NAME}")
    if (OVERRIDE_INSTALL_VERSION)
      set (CPACK_PACKAGE_INSTALL_DIRECTORY "${CPACK_PACKAGE_VENDOR}\\\\${CPACK_PACKAGE_NAME}\\\\${OVERRIDE_INSTALL_VERSION}")
    else ()
      set (CPACK_PACKAGE_INSTALL_DIRECTORY "${CPACK_PACKAGE_VENDOR}\\\\${CPACK_PACKAGE_NAME}\\\\${CPACK_PACKAGE_VERSION}")
    endif ()
    set (CPACK_NSIS_CONTACT "${HDF5_PACKAGE_BUGREPORT}")
    set (CPACK_NSIS_MODIFY_PATH ON)

    if (WIX_EXECUTABLE)
      list (APPEND CPACK_GENERATOR "WIX")
    endif ()
#WiX variables
    set (CPACK_WIX_UNINSTALL "1")
# .. variable:: CPACK_WIX_LICENSE_RTF
#  RTF License File
#
#  If CPACK_RESOURCE_FILE_LICENSE has an .rtf extension it is used as-is.
#
#  If CPACK_RESOURCE_FILE_LICENSE has an .txt extension it is implicitly
#  converted to RTF by the WiX Generator.
#  The expected encoding of the .txt file is UTF-8.
#
#  With CPACK_WIX_LICENSE_RTF you can override the license file used by the
#  WiX Generator in case CPACK_RESOURCE_FILE_LICENSE is in an unsupported
#  format or the .txt -> .rtf conversion does not work as expected.
    set (CPACK_RESOURCE_FILE_LICENSE "${HDF5_BINARY_DIR}/LICENSE.txt")
# .. variable:: CPACK_WIX_PRODUCT_ICON
#  The Icon shown next to the program name in Add/Remove programs.
    set(CPACK_WIX_PRODUCT_ICON "${HDF_CONFIG_DIR}\\\\install\\\\hdf.ico")
#
# .. variable:: CPACK_WIX_UI_BANNER
#
#  The bitmap will appear at the top of all installer pages other than the
#  welcome and completion dialogs.
#
#  If set, this image will replace the default banner image.
#
#  This image must be 493 by 58 pixels.
#
# .. variable:: CPACK_WIX_UI_DIALOG
#
#  Background bitmap used on the welcome and completion dialogs.
#
#  If this variable is set, the installer will replace the default dialog
#  image.
#
#  This image must be 493 by 312 pixels.
#
    set(CPACK_WIX_PROPERTY_ARPCOMMENTS "HDF5 (Hierarchical Data Format 5) Software Library and Utilities")
    set(CPACK_WIX_PROPERTY_ARPURLINFOABOUT "${HDF5_PACKAGE_URL}")
    set(CPACK_WIX_PROPERTY_ARPHELPLINK "${HDF5_PACKAGE_BUGREPORT}")
    if (BUILD_SHARED_LIBS)
      if (${HDF_CFG_NAME} MATCHES "Debug" OR ${HDF_CFG_NAME} MATCHES "Developer")
        set (WIX_CMP_NAME "${HDF5_LIB_NAME}${CMAKE_DEBUG_POSTFIX}")
      else ()
        set (WIX_CMP_NAME "${HDF5_LIB_NAME}")
      endif ()
      configure_file (${HDF_CONFIG_DIR}/install/patch.xml.in ${HDF5_BINARY_DIR}/patch.xml @ONLY)
      set(CPACK_WIX_PATCH_FILE "${HDF5_BINARY_DIR}/patch.xml")
    endif ()
  elseif (APPLE)
    list (APPEND CPACK_GENERATOR "STGZ")
    option (HDF5_PACK_MACOSX_DMG  "Package the HDF5 Library using DragNDrop" ON)
    if (HDF5_PACK_MACOSX_DMG)
      list (APPEND CPACK_GENERATOR "DragNDrop")
    endif ()
    set (CPACK_COMPONENTS_ALL_IN_ONE_PACKAGE ON)
    set (CPACK_PACKAGING_INSTALL_PREFIX "/${CPACK_PACKAGE_INSTALL_DIRECTORY}")
    set (CPACK_PACKAGE_ICON "${HDF_CONFIG_DIR}/install/hdf.icns")

    option (HDF5_PACK_MACOSX_FRAMEWORK  "Package the HDF5 Library in a Frameworks" OFF)
    if (HDF5_PACK_MACOSX_FRAMEWORK AND HDF5_BUILD_FRAMEWORKS)
      set (CPACK_BUNDLE_NAME "${HDF5_PACKAGE_STRING}")
      set (CPACK_BUNDLE_LOCATION "/")    # make sure CMAKE_INSTALL_PREFIX ends in /
      set (CMAKE_INSTALL_PREFIX "/${CPACK_BUNDLE_NAME}.framework/Versions/${CPACK_PACKAGE_VERSION}/${CPACK_PACKAGE_NAME}/")
      set (CPACK_BUNDLE_ICON "${HDF_CONFIG_DIR/install}/hdf.icns")
      set (CPACK_BUNDLE_PLIST "${HDF5_BINARY_DIR}/CMakeFiles/Info.plist")
      set (CPACK_SHORT_VERSION_STRING "${CPACK_PACKAGE_VERSION}")
      #-----------------------------------------------------------------------------
      # Configure the Info.plist file for the install bundle
      #-----------------------------------------------------------------------------
      configure_file (
          ${HDF_CONFIG_DIR}/install/CPack.Info.plist.in
          ${HDF5_BINARY_DIR}/CMakeFiles/Info.plist @ONLY
      )
      configure_file (
          ${HDF_CONFIG_DIR}/install/PkgInfo.in
          ${HDF5_BINARY_DIR}/CMakeFiles/PkgInfo @ONLY
      )
      configure_file (
          ${HDF_CONFIG_DIR}/install/version.plist.in
          ${HDF5_BINARY_DIR}/CMakeFiles/version.plist @ONLY
      )
      install (
          FILES ${HDF5_BINARY_DIR}/CMakeFiles/PkgInfo
          DESTINATION ..
      )
    endif ()
  else ()
    list (APPEND CPACK_GENERATOR "STGZ")
    set (CPACK_PACKAGING_INSTALL_PREFIX "/${CPACK_PACKAGE_INSTALL_DIRECTORY}")
    set (CPACK_COMPONENTS_ALL_IN_ONE_PACKAGE ON)

    find_program (DPKGSHLIB_EXE dpkg-shlibdeps)
    if (DPKGSHLIB_EXE)
      list (APPEND CPACK_GENERATOR "DEB")
      set (CPACK_DEBIAN_PACKAGE_SECTION "Libraries")
      set (CPACK_DEBIAN_PACKAGE_MAINTAINER "${HDF5_PACKAGE_BUGREPORT}")
    endif ()

    find_program (RPMBUILD_EXE rpmbuild)
    if (RPMBUILD_EXE AND NOT HDF5_ENABLE_PARALLEL)
      list (APPEND CPACK_GENERATOR "RPM")
      set (CPACK_RPM_PACKAGE_RELEASE "1")
      set (CPACK_RPM_PACKAGE_RELEASE_DIST ON)
      set (CPACK_RPM_COMPONENT_INSTALL ON)
      set (CPACK_RPM_PACKAGE_RELOCATABLE ON)
      set (CPACK_RPM_FILE_NAME "RPM-DEFAULT")
      set (CPACK_RPM_PACKAGE_NAME "${CPACK_PACKAGE_NAME}")
      set (CPACK_RPM_PACKAGE_VERSION "${CPACK_PACKAGE_VERSION}")
      set (CPACK_RPM_PACKAGE_VENDOR "${CPACK_PACKAGE_VENDOR}")
      set (CPACK_RPM_PACKAGE_LICENSE "BSD-style")
      set (CPACK_RPM_PACKAGE_GROUP "Development/Libraries")
      set (CPACK_RPM_PACKAGE_URL "${HDF5_PACKAGE_URL}")
      set (CPACK_RPM_PACKAGE_SUMMARY "HDF5 is a unique technology suite that makes possible the management of extremely large and complex data collections.")
      set (CPACK_RPM_PACKAGE_DESCRIPTION
        "The HDF5 technology suite includes:

    * A versatile data model that can represent very complex data objects and a wide variety of metadata.

    * A completely portable file format with no limit on the number or size of data objects in the collection.

    * A software library that runs on a range of computational platforms, from laptops to massively parallel systems, and implements a high-level API with C, C++, Fortran 90, and Java interfaces.

    * A rich set of integrated performance features that allow for access time and storage space optimizations.

    * Tools and applications for managing, manipulating, viewing, and analyzing the data in the collection.

The HDF5 data model, file format, API, library, and tools are open and distributed without charge.
"
      )

      #-----------------------------------------------------------------------------
      # Configure the spec file for the install RPM
      #-----------------------------------------------------------------------------
#      configure_file ("${HDF_CONFIG_DIR}/install/hdf5.spec.in" "${CMAKE_CURRENT_BINARY_DIR}/${HDF5_PACKAGE_NAME}.spec" @ONLY IMMEDIATE)
#      set (CPACK_RPM_USER_BINARY_SPECFILE "${CMAKE_CURRENT_BINARY_DIR}/${HDF5_PACKAGE_NAME}.spec")
    endif ()
  endif ()

  # By default, do not warn when built on machines using only VS Express:
  if (NOT DEFINED CMAKE_INSTALL_SYSTEM_RUNTIME_LIBS_NO_WARNINGS)
    set (CMAKE_INSTALL_SYSTEM_RUNTIME_LIBS_NO_WARNINGS ON)
  endif ()
  include (InstallRequiredSystemLibraries)

  set (CPACK_INSTALL_CMAKE_PROJECTS "${HDF5_BINARY_DIR};HDF5;ALL;/")

  if (HDF5_PACKAGE_EXTLIBS)
    if (HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "GIT" OR HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "TGZ")
      if (H5_ZLIB_FOUND AND ZLIB_USE_EXTERNAL)
        if (WIN32)
          set (CPACK_INSTALL_CMAKE_PROJECTS "${CPACK_INSTALL_CMAKE_PROJECTS};${H5_ZLIB_INCLUDE_DIR_GEN};HDF5_ZLIB;ALL;/")
        else ()
          set (CPACK_INSTALL_CMAKE_PROJECTS "${CPACK_INSTALL_CMAKE_PROJECTS};${H5_ZLIB_INCLUDE_DIR_GEN};HDF5_ZLIB;libraries;/")
          set (CPACK_INSTALL_CMAKE_PROJECTS "${CPACK_INSTALL_CMAKE_PROJECTS};${H5_ZLIB_INCLUDE_DIR_GEN};HDF5_ZLIB;configinstall;/")
        endif ()
      endif ()
      if (H5_SZIP_FOUND AND SZIP_USE_EXTERNAL)
        set (SZIP_PROJNAME "${LIBAEC_PACKAGE_NAME}")
        if (WIN32)
          set (CPACK_INSTALL_CMAKE_PROJECTS "${CPACK_INSTALL_CMAKE_PROJECTS};${H5_SZIP_INCLUDE_DIR_GEN};${SZIP_PROJNAME};ALL;/")
        else ()
          set (CPACK_INSTALL_CMAKE_PROJECTS "${CPACK_INSTALL_CMAKE_PROJECTS};${H5_SZIP_INCLUDE_DIR_GEN};${SZIP_PROJNAME};libraries;/")
          set (CPACK_INSTALL_CMAKE_PROJECTS "${CPACK_INSTALL_CMAKE_PROJECTS};${H5_SZIP_INCLUDE_DIR_GEN};${SZIP_PROJNAME};configinstall;/")
        endif ()
      endif ()
      if (PLUGIN_FOUND AND PLUGIN_USE_EXTERNAL)
        if (WIN32)
          set (CPACK_INSTALL_CMAKE_PROJECTS "${CPACK_INSTALL_CMAKE_PROJECTS};${PLUGIN_BINARY_DIR};PLUGIN;ALL;/")
        else ()
          set (CPACK_INSTALL_CMAKE_PROJECTS "${CPACK_INSTALL_CMAKE_PROJECTS};${PLUGIN_BINARY_DIR};PLUGIN;libraries;/")
        endif ()
      endif ()
    endif ()
  endif ()

  include (CPack)

  # The following sets packaging specific categories and descriptions
  cpack_add_install_type(Full DISPLAY_NAME "Everything")
  cpack_add_install_type(Developer)

  cpack_add_component_group(Runtime)

  cpack_add_component_group(Documents
      EXPANDED
      DESCRIPTION "Release notes for developing HDF5 applications"
  )

  cpack_add_component_group(Development
      EXPANDED
      DESCRIPTION "All of the tools you'll need to develop HDF5 applications"
  )

  cpack_add_component_group(Applications
      EXPANDED
      DESCRIPTION "Tools for HDF5 files"
  )

  #---------------------------------------------------------------------------
  # Now list the cpack commands
  #---------------------------------------------------------------------------
  cpack_add_component (libraries
      DISPLAY_NAME "HDF5 Libraries"
      GROUP Runtime
      INSTALL_TYPES Full Developer User
  )
  cpack_add_component (headers
      DISPLAY_NAME "HDF5 Headers"
      DEPENDS libraries
      GROUP Development
      INSTALL_TYPES Full Developer
  )
  cpack_add_component (hdfdocuments
      DISPLAY_NAME "HDF5 Documents"
      GROUP Documents
      INSTALL_TYPES Full Developer
  )
  cpack_add_component (configinstall
      DISPLAY_NAME "HDF5 CMake files"
      HIDDEN
      DEPENDS libraries
      GROUP Development
      INSTALL_TYPES Full Developer User
  )

  if (HDF5_BUILD_FORTRAN)
    cpack_add_component (fortlibraries
        DISPLAY_NAME "HDF5 Fortran Libraries"
        DEPENDS libraries
        GROUP Runtime
        INSTALL_TYPES Full Developer User
    )
    cpack_add_component (fortheaders
        DISPLAY_NAME "HDF5 Fortran Headers"
        DEPENDS fortlibraries
        GROUP Development
        INSTALL_TYPES Full Developer
    )
  endif ()

  if (HDF5_BUILD_CPP_LIB)
    cpack_add_component (cpplibraries
        DISPLAY_NAME "HDF5 C++ Libraries"
        DEPENDS libraries
        GROUP Runtime
        INSTALL_TYPES Full Developer User
    )
    cpack_add_component (cppheaders
        DISPLAY_NAME "HDF5 C++ Headers"
        DEPENDS cpplibraries
        GROUP Development
        INSTALL_TYPES Full Developer
    )
  endif ()

  cpack_add_component (utilsapplications
      DISPLAY_NAME "HDF5 Utility Applications"
      DEPENDS libraries
      GROUP Applications
      INSTALL_TYPES Full Developer User
  )

  if (HDF5_BUILD_TOOLS)
    cpack_add_component (toolsapplications
        DISPLAY_NAME "HDF5 Tools Applications"
        DEPENDS toolslibraries
        GROUP Applications
        INSTALL_TYPES Full Developer User
    )
    cpack_add_component (toolslibraries
        DISPLAY_NAME "HDF5 Tools Libraries"
        DEPENDS libraries
        GROUP Runtime
        INSTALL_TYPES Full Developer User
    )
    cpack_add_component (toolsheaders
        DISPLAY_NAME "HDF5 Tools Headers"
        DEPENDS toolslibraries
        GROUP Development
        INSTALL_TYPES Full Developer
    )
  endif ()

  if (HDF5_BUILD_HL_LIB)
    cpack_add_component (hllibraries
        DISPLAY_NAME "HDF5 HL Libraries"
        DEPENDS libraries
        GROUP Runtime
        INSTALL_TYPES Full Developer User
    )
    cpack_add_component (hlheaders
        DISPLAY_NAME "HDF5 HL Headers"
        DEPENDS hllibraries
        GROUP Development
        INSTALL_TYPES Full Developer
    )
    cpack_add_component (hltoolsapplications
        DISPLAY_NAME "HDF5 HL Tools Applications"
        DEPENDS hllibraries
        GROUP Applications
        INSTALL_TYPES Full Developer User
    )
    if (HDF5_BUILD_CPP_LIB)
      cpack_add_component (hlcpplibraries
          DISPLAY_NAME "HDF5 HL C++ Libraries"
          DEPENDS hllibraries
          GROUP Runtime
          INSTALL_TYPES Full Developer User
      )
      cpack_add_component (hlcppheaders
          DISPLAY_NAME "HDF5 HL C++ Headers"
          DEPENDS hlcpplibraries
          GROUP Development
          INSTALL_TYPES Full Developer
      )
    endif ()
    if (HDF5_BUILD_FORTRAN)
      cpack_add_component (hlfortlibraries
          DISPLAY_NAME "HDF5 HL Fortran Libraries"
          DEPENDS fortlibraries
          GROUP Runtime
          INSTALL_TYPES Full Developer User
      )
    endif ()
  endif ()

endif ()
```

### `CMakeLists.txt`

```
# -----------------------------------------------------------------------------
# HDF5 CMake Top-Level Build Configuration
# -----------------------------------------------------------------------------
# This is the main CMakeLists.txt file for configuring, building, and installing
# the HDF5 library and its components. It supports building HDF5 as a standalone
# project or as a subproject within a larger CMake build. Key features include:
#
# - Enforces out-of-source builds for safety and cleanliness.
# - Supports configuration as a subproject (HDF5_EXTERNALLY_CONFIGURED).
# - Provides options for building shared/static libraries, tools, examples, and bindings (C++, Fortran, Java).
# - Handles versioning, installation paths, and library naming customization.
# - Integrates with external dependencies (zlib, szip, MPI, HDFS, etc.).
# - Supports advanced features: thread-safety, concurrency, plugins, code coverage, and more.
# - Includes options for documentation and testing.
#
# Usage examples:
#   1. Presets:
#      cd <path-to-hdf5-source>
#      cmake --workflow --preset <preset-name>
#
#   2. Command line:
#      mkdir build && cd build
#      cmake [options] <path-to-hdf5-source>
#      cmake --build .
#      cmake --install .
#
# See comments throughout this file for details on configuration variables and options.
# -----------------------------------------------------------------------------

cmake_minimum_required (VERSION 3.26)
project (HDF5 C)

if (POLICY CMP0074)
  # find_package() uses <PackageName>_ROOT variables.
  cmake_policy (SET CMP0074 NEW)
endif ()

if (POLICY CMP0144)
  # <PACKAGENAME> is the upper-cased package name.
  cmake_policy (SET CMP0144 NEW)
endif ()

if (POLICY CMP0083)
  # To control generation of Position Independent Executable (PIE) or not,
  # some flags are required at link time.
  cmake_policy (SET CMP0083 NEW)
endif ()

if (POLICY CMP0127)
  # to evaluate each condition as if(<condition>), where <condition> is re-parsed
  # as if literally written in a call to if().
  cmake_policy (SET CMP0127 NEW)
endif ()

# Avoid warning about DOWNLOAD_EXTRACT_TIMESTAMP in CMake 3.24:
if (CMAKE_VERSION VERSION_GREATER_EQUAL "3.24.0")
    cmake_policy(SET CMP0135 NEW)
endif()

#-----------------------------------------------------------------------------
# Instructions for use : Normal Build
#
# For standard build of HDF5 libraries,tests and tools.
# Run cmake using the HDF5 source tree to generate a build tree.
# Enable/Disable options according to requirements and
# set CMAKE_INSTALL_PREFIX to the required install path.
# Make install can be used to install all components for system-wide use.
#
if (CMAKE_CURRENT_SOURCE_DIR STREQUAL CMAKE_CURRENT_BINARY_DIR)
    message (FATAL_ERROR "\nERROR! ${PROJECT_NAME} DOES NOT SUPPORT IN SOURCE BUILDS!\n"
      "CMAKE_CURRENT_SOURCE_DIR=${CMAKE_CURRENT_SOURCE_DIR}"
      " == CMAKE_CURRENT_BINARY_DIR=${CMAKE_CURRENT_BINARY_DIR}\n"
      "NEXT STEPS:\n"
      "(1) Delete the CMakeCache.txt file and the CMakeFiles/ directory\n"
      "     under the source directory for ${PROJECT_NAME}, otherwise you\n"
      "     will not be able to configure ${PROJECT_NAME} correctly!\n"
      "      * For example, on linux machines do:\n"
      "        $ rm -r CMakeCache.txt CMakeFiles/\n"
      "(2) Create a different directory and configure ${PROJECT_NAME} in that directory.\n"
      "      * For example, on linux machines do:\n"
      "        $ mkdir MY_BUILD\n"
      "        $ cd MY_BUILD\n"
      "        $ cmake [OPTIONS] ..\n"
      )
endif ()

# Whether the most recently called project() command, in the current scope or above,
# was in the top level CMakeLists.txt file.
if (CMAKE_VERSION VERSION_GREATER_EQUAL "3.21.0")
  if(NOT PROJECT_IS_TOP_LEVEL)
    set (HDF5_EXTERNALLY_CONFIGURED 1)
  endif()
else()
  if (NOT CMAKE_SOURCE_DIR STREQUAL PROJECT_SOURCE_DIR)
    set (HDF5_EXTERNALLY_CONFIGURED 1)
  endif()
endif()

#-----------------------------------------------------------------------------
# Enable the use of the cmake_dependent_option() function.
include (CMakeDependentOption)

#-----------------------------------------------------------------------------
# Instructions for use : Sub-Project Build
#
# To include HDF5 as a sub-project within another project.
# Set HDF5_EXTERNALLY_CONFIGURED to 1 in the parent project and
# supply values for the following variables...
#
# HDF5_EXPORTED_TARGETS :
#   Set this to the name of the targets variable which controls exports
#   If unset (because parent project does not support/use the
#   install (EXPORT target...) syntax), then targets are not configured
#   for export during install.
#
# HDF5_LIB_DEPENDENCIES :
#   If the build of HDF5 libs is being customized, then rules for the
#   dependencies of the HDF5 libs may be 'incomplete', add additional
#   dependencies to this variable so that external projects pick them up
#
# Put all global top-level build options into one place at the beginning
#-----------------------------------------------------------------------------
include (CMakeBuildOptions.cmake)

#option (HDF5_EXTERNAL_LIB_PREFIX "Use prefix for custom library naming." "")
set (HDF5_EXTERNAL_LIB_PREFIX "" CACHE STRING "Use prefix for custom library naming.")
mark_as_advanced (HDF5_EXTERNAL_LIB_PREFIX)
# HDF5_EXTERNAL_LIB_PREFIX :
#   If the parent project needs to install hdf libraries, but avoid
#   name conflicts with system versions, then a prefix may be added
#   to ensure that the correct versions configured are used.
set (HDF5_LIB_INFIX "" CACHE STRING "Use infix for custom library naming.")
mark_as_advanced (HDF5_LIB_INFIX)
# HDF5_LIB_INFIX :
#   This infix is added to all library names after 'hdf5'.
#   e.g. the infix '_openmpi' results in the library name 'libhdf5_openmpi.so'
#   This name is used in packages on debian based systems.
#   (see https://packages.debian.org/jessie/amd64/libhdf5-openmpi-8/filelist)
#option (HDF5_EXTERNAL_LIB_SUFFIX "Use prefix for custom library naming." "")
set (HDF5_EXTERNAL_LIB_SUFFIX "" CACHE STRING "Use suffix for custom library naming.")
mark_as_advanced (HDF5_EXTERNAL_LIB_SUFFIX)
# HDF5_EXTERNAL_LIB_SUFFIX :
#   If the parent project needs to install hdf libraries, but avoid
#   name conflicts with system versions, then a suffix may be added
#   to ensure that the correct versions configured are used.
#
# HDF5_INSTALL_BIN_DIR, HDF5_INSTALL_LIB_DIR, HDF5_INSTALL_INCLUDE_DIR, HDF5_INSTALL_DATA_DIR :
#   Customize the 'bin', 'lib', 'include', and 'share' installation directories.
#
# HDF5_INSTALL_NO_DEVELOPMENT :
#   Set to true to skip installation of headers and CMake package files.
#
# Consider this example from the ParaView project, it builds its own zlib
# library and tells HDF5 to add it as a dependency - this ensures that
# any project making use of this build of HDF5 will use the correct zlib
#
#   # Tell hdf5 that we are manually overriding certain settings
#   set (HDF5_EXTERNALLY_CONFIGURED 1)
#   # Avoid duplicating names of installed libraries
#   set (HDF5_EXTERNAL_LIB_PREFIX "vtk")
#   # Export configuration to this export variable
#   set (HDF5_EXPORTED_TARGETS "paraview-targets")
#
#   # Setup all necessary overrides for zlib so that HDF5 uses our
#   # internally compiled zlib rather than any other version
#   if (HDF5_ENABLE_ZLIB_SUPPORT)
#     # We must tell the main HDF5 library that it depends on our zlib
#     set (HDF5_LIB_DEPENDENCIES vtkzlib)
#     # Override the zlib header file
#     if (VTK_USE_SYSTEM_ZLIB)
#       set (H5_ZLIB_HEADER "zlib.h")
#     else ()
#       set (H5_ZLIB_HEADER "vtk_zlib.h")
#       # Set vars that FindZlib would have set if used in sub project
#       set (H5_ZLIB_INCLUDE_DIRS "${VTK_H5_ZLIB_INCLUDE_DIRS}")
#       set (H5_ZLIB_LIBRARIES vtkzlib)
#     endif ()
#   endif ()
#
#   # Add the sub project
#   add_subdirectory (Utilities/hdf5-1.8)
#-----------------------------------------------------------------------------
string (TIMESTAMP CONFIG_DATE "%Y-%m-%d")

#-----------------------------------------------------------------------------
# Allow Visual Studio solution directories
#-----------------------------------------------------------------------------
# Provide a way for Visual Studio Express users to turn OFF the new FOLDER
# organization feature. Default to ON for non-Express users. Express users must
# explicitly turn off this option to build HDF5 in the Express IDE...
#
if (HDF5_USE_FOLDERS)
  set_property (GLOBAL PROPERTY USE_FOLDERS ON)
endif ()

#-----------------------------------------------------------------------------
# Set the core names of all the libraries CORENAME is the base library name
# for targets, BASE_CORE
# filename are made of PREFIX_BASE_INFIX_CORE_SUFFIX
#-----------------------------------------------------------------------------
set (HDF5_LIB_BASE              "hdf5")

set (HDF5_LIB_CORE              "")
set (HDF5_TEST_LIB_CORE         "_test")
set (HDF5_TEST_PAR_LIB_CORE     "_testpar")
set (HDF5_CPP_LIB_CORE          "_cpp")
set (HDF5_HL_LIB_CORE           "_hl")
set (HDF5_HL_CPP_LIB_CORE       "_hl_cpp")
set (HDF5_TOOLS_LIB_CORE        "_tools")
set (HDF5_UTILS_LIB_CORE        "_utils")
set (HDF5_F90_LIB_CORE          "_fortran")
set (HDF5_F90_C_LIB_CORE        "_f90cstub")
set (HDF5_F90_TEST_LIB_CORE     "_test_fortran")
set (HDF5_F90_C_TEST_LIB_CORE   "_test_f90cstub")
set (HDF5_HL_F90_LIB_CORE       "_hl_fortran")
set (HDF5_HL_F90_C_LIB_CORE     "_hl_f90cstub")
set (HDF5_JAVA_JNI_LIB_CORE     "_java")

set (HDF5_LIB_CORENAME              "${HDF5_LIB_BASE}")
set (HDF5_TEST_LIB_CORENAME         "${HDF5_LIB_BASE}${HDF5_TEST_LIB_CORE}")
set (HDF5_TEST_PAR_LIB_CORENAME     "${HDF5_LIB_BASE}${HDF5_TEST_PAR_LIB_CORE}")
set (HDF5_CPP_LIB_CORENAME          "${HDF5_LIB_BASE}${HDF5_CPP_LIB_CORE}")
set (HDF5_HL_LIB_CORENAME           "${HDF5_LIB_BASE}${HDF5_HL_LIB_CORE}")
set (HDF5_HL_CPP_LIB_CORENAME       "${HDF5_LIB_BASE}${HDF5_HL_CPP_LIB_CORE}")
set (HDF5_TOOLS_LIB_CORENAME        "${HDF5_LIB_BASE}${HDF5_TOOLS_LIB_CORE}")
set (HDF5_UTILS_LIB_CORENAME        "${HDF5_LIB_BASE}${HDF5_UTILS_LIB_CORE}")
set (HDF5_F90_LIB_CORENAME          "${HDF5_LIB_BASE}${HDF5_F90_LIB_CORE}")
set (HDF5_F90_C_LIB_CORENAME        "${HDF5_LIB_BASE}${HDF5_F90_C_LIB_CORE}")
set (HDF5_F90_TEST_LIB_CORENAME     "${HDF5_LIB_BASE}${HDF5_F90_TEST_LIB_CORE}")
set (HDF5_F90_C_TEST_LIB_CORENAME   "${HDF5_LIB_BASE}${HDF5_F90_C_TEST_LIB_CORE}")
set (HDF5_HL_F90_LIB_CORENAME       "${HDF5_LIB_BASE}${HDF5_HL_F90_LIB_CORE}")
set (HDF5_HL_F90_C_LIB_CORENAME     "${HDF5_LIB_BASE}${HDF5_HL_F90_C_LIB_CORE}")
set (HDF5_JAVA_JNI_LIB_CORENAME     "${HDF5_LIB_BASE}${HDF5_JAVA_JNI_LIB_CORE}")
set (HDF5_JAVA_HDF5_LIB_CORENAME    "jarhdf5")
set (HDF5_JAVA_TEST_LIB_CORENAME    "jartest5")
set (HDF5_JAVA_JSRC_LIB_CORENAME    "javahdf5")
set (HDF5_JAVA_JTEST_LIB_CORENAME   "javatest5")

#-----------------------------------------------------------------------------
# Set the true names of all the libraries if customized by external project
#-----------------------------------------------------------------------------
set (HDF5_LIB_NAME              "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_TEST_LIB_NAME         "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_TEST_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_TEST_PAR_LIB_NAME     "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_TEST_PAR_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_CPP_LIB_NAME          "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_CPP_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_HL_LIB_NAME           "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_HL_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_HL_CPP_LIB_NAME       "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_HL_CPP_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_TOOLS_LIB_NAME        "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_TOOLS_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_UTILS_LIB_NAME        "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_UTILS_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_F90_LIB_NAME          "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_F90_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_F90_C_LIB_NAME        "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_F90_C_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_F90_TEST_LIB_NAME     "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_F90_TEST_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_F90_C_TEST_LIB_NAME   "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_F90_C_TEST_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_HL_F90_LIB_NAME       "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_HL_F90_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_HL_F90_C_LIB_NAME     "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_HL_F90_C_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
#Because the loading mechanism of the JNI library requires a fixed name, the name of the JNI library cannot be customized
#set (HDF5_JAVA_JNI_LIB_NAME     "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_LIB_BASE}${HDF5_LIB_INFIX}${HDF5_JAVA_JNI_LIB_CORE}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_JAVA_JNI_LIB_NAME     "${HDF5_LIB_BASE}${HDF5_JAVA_JNI_LIB_CORE}")
set (HDF5_JAVA_HDF5_LIB_NAME    "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_JAVA_HDF5_LIB_CORENAME}${HDF5_LIB_INFIX}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_JAVA_TEST_LIB_NAME    "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_JAVA_TEST_LIB_CORENAME}${HDF5_LIB_INFIX}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_JAVA_JSRC_LIB_NAME    "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_JAVA_JSRC_LIB_CORENAME}${HDF5_LIB_INFIX}${HDF5_EXTERNAL_LIB_SUFFIX}")
set (HDF5_JAVA_JTEST_LIB_NAME   "${HDF5_EXTERNAL_LIB_PREFIX}${HDF5_JAVA_JTEST_LIB_CORENAME}${HDF5_LIB_INFIX}${HDF5_EXTERNAL_LIB_SUFFIX}")

#-----------------------------------------------------------------------------
# Set the target names of all the libraries
#-----------------------------------------------------------------------------
set (HDF5_LIB_TARGET              "${HDF5_LIB_CORENAME}-static")
set (HDF5_TEST_LIB_TARGET         "${HDF5_TEST_LIB_CORENAME}-static")
set (HDF5_TEST_PAR_LIB_TARGET     "${HDF5_TEST_PAR_LIB_CORENAME}-static")
set (HDF5_CPP_LIB_TARGET          "${HDF5_CPP_LIB_CORENAME}-static")
set (HDF5_HL_LIB_TARGET           "${HDF5_HL_LIB_CORENAME}-static")
set (HDF5_HL_CPP_LIB_TARGET       "${HDF5_HL_CPP_LIB_CORENAME}-static")
set (HDF5_TOOLS_LIB_TARGET        "${HDF5_TOOLS_LIB_CORENAME}-static")
set (HDF5_UTILS_LIB_TARGET        "${HDF5_UTILS_LIB_CORENAME}-static")
set (HDF5_F90_LIB_TARGET          "${HDF5_F90_LIB_CORENAME}-static")
set (HDF5_F90_C_LIB_TARGET        "${HDF5_F90_C_LIB_CORENAME}-static")
set (HDF5_F90_TEST_LIB_TARGET     "${HDF5_F90_TEST_LIB_CORENAME}-static")
set (HDF5_F90_C_TEST_LIB_TARGET   "${HDF5_F90_C_TEST_LIB_CORENAME}-static")
set (HDF5_HL_F90_LIB_TARGET       "${HDF5_HL_F90_LIB_CORENAME}-static")
set (HDF5_HL_F90_C_LIB_TARGET     "${HDF5_HL_F90_C_LIB_CORENAME}-static")
set (HDF5_LIBSH_TARGET            "${HDF5_LIB_CORENAME}-shared")
set (HDF5_TEST_LIBSH_TARGET       "${HDF5_TEST_LIB_CORENAME}-shared")
set (HDF5_TEST_PAR_LIBSH_TARGET   "${HDF5_TEST_PAR_LIB_CORENAME}-shared")
set (HDF5_CPP_LIBSH_TARGET        "${HDF5_CPP_LIB_CORENAME}-shared")
set (HDF5_HL_LIBSH_TARGET         "${HDF5_HL_LIB_CORENAME}-shared")
set (HDF5_HL_CPP_LIBSH_TARGET     "${HDF5_HL_CPP_LIB_CORENAME}-shared")
set (HDF5_TOOLS_LIBSH_TARGET      "${HDF5_TOOLS_LIB_CORENAME}-shared")
set (HDF5_UTILS_LIBSH_TARGET      "${HDF5_UTILS_LIB_CORENAME}-shared")
set (HDF5_F90_LIBSH_TARGET        "${HDF5_F90_LIB_CORENAME}-shared")
set (HDF5_F90_C_LIBSH_TARGET      "${HDF5_F90_C_LIB_CORENAME}-shared")
set (HDF5_F90_TEST_LIBSH_TARGET   "${HDF5_F90_TEST_LIB_CORENAME}-shared")
set (HDF5_F90_C_TEST_LIBSH_TARGET "${HDF5_F90_C_TEST_LIB_CORENAME}-shared")
set (HDF5_HL_F90_LIBSH_TARGET     "${HDF5_HL_F90_LIB_CORENAME}-shared")
set (HDF5_HL_F90_C_LIBSH_TARGET   "${HDF5_HL_F90_C_LIB_CORENAME}-shared")
set (HDF5_JAVA_JNI_LIB_TARGET     "${HDF5_JAVA_JNI_LIB_CORENAME}")
set (HDF5_JAVA_HDF5_LIB_TARGET    "${HDF5_JAVA_HDF5_LIB_CORENAME}")
set (HDF5_JAVA_TEST_LIB_TARGET    "${HDF5_JAVA_TEST_LIB_CORENAME}")
set (HDF5_JAVA_JSRC_LIB_TARGET    "${HDF5_JAVA_JSRC_LIB_CORENAME}")
set (HDF5_JAVA_JTEST_LIB_TARGET   "${HDF5_JAVA_JTEST_LIB_CORENAME}")

#-----------------------------------------------------------------------------
# Define some CMake variables for use later in the project
#-----------------------------------------------------------------------------
set (HDF_CONFIG_DIR            ${HDF5_SOURCE_DIR}/config)
set (HDF_RESOURCES_DIR         ${HDF5_SOURCE_DIR}/config/cmake)
set (HDF5_SRC_DIR              ${HDF5_SOURCE_DIR}/src)
set (HDF5_TEST_SRC_DIR         ${HDF5_SOURCE_DIR}/test)
set (HDF5_TEST_PAR_DIR         ${HDF5_SOURCE_DIR}/testpar)
set (HDF5_TEST_API_SRC_DIR     ${HDF5_SOURCE_DIR}/test/API)
set (HDF5_TEST_API_PAR_SRC_DIR ${HDF5_SOURCE_DIR}/testpar/API)
set (HDF5_CPP_SRC_DIR          ${HDF5_SOURCE_DIR}/c++/src)
set (HDF5_CPP_TST_DIR          ${HDF5_SOURCE_DIR}/c++/test)
set (HDF5_HL_SRC_DIR           ${HDF5_SOURCE_DIR}/hl/src)
set (HDF5_HL_TST_DIR           ${HDF5_SOURCE_DIR}/hl/test)
set (HDF5_HL_CPP_SRC_DIR       ${HDF5_SOURCE_DIR}/hl/c++/src)
set (HDF5_HL_CPP_TST_DIR       ${HDF5_SOURCE_DIR}/hl/c++/test)
set (HDF5_HL_TOOLS_DIR         ${HDF5_SOURCE_DIR}/hl/tools)
set (HDF5_TOOLS_ROOT_DIR       ${HDF5_SOURCE_DIR}/tools)
set (HDF5_TOOLS_SRC_DIR        ${HDF5_SOURCE_DIR}/tools/src)
set (HDF5_TOOLS_TST_DIR        ${HDF5_SOURCE_DIR}/tools/test)
set (HDF5_UTILS_DIR            ${HDF5_SOURCE_DIR}/utils)
set (HDF5_F90_SRC_DIR          ${HDF5_SOURCE_DIR}/fortran)
set (HDF5_JAVA_SRC_PATH            ${HDF5_SOURCE_DIR}/java)
set (HDF5_JAVA_SRCJNI_PATH         ${HDF5_SOURCE_DIR}/java/src-jni)
set (HDF5_JAVA_SRCJNI_JNI_SRC_DIR  ${HDF5_SOURCE_DIR}/java/src-jni/jni)
set (HDF5_JAVA_LIB_DIR             ${HDF5_SOURCE_DIR}/java/lib)
set (HDF5_JAVA_JSRC_DIR            ${HDF5_SOURCE_DIR}/java/jsrc)
set (HDF5_JAVA_LOGGING_JAR         ${HDF5_SOURCE_DIR}/java/lib/slf4j-api-2.0.16.jar)
set (HDF5_JAVA_LOGGING_NOP_JAR     ${HDF5_SOURCE_DIR}/java/lib/ext/slf4j-nop-2.0.16.jar)
set (HDF5_JAVA_LOGGING_SIMPLE_JAR  ${HDF5_SOURCE_DIR}/java/lib/ext/slf4j-simple-2.0.16.jar)
set (HDF5_DOXYGEN_DIR              ${HDF5_SOURCE_DIR}/doxygen)

set (HDF5_SRC_INCLUDE_DIRS ${HDF5_SRC_DIR})

if (HDF5_USE_PREGEN)
  set (HDF5_GENERATED_SOURCE_DIR ${HDF5_USE_PREGEN_DIR})
else ()
  set (HDF5_GENERATED_SOURCE_DIR ${HDF5_F90_SRC_BINARY_DIR})
endif ()

#-----------------------------------------------------------------------------
# parse the full version number from H5public.h and include in H5_VERS_INFO
#-----------------------------------------------------------------------------
file (READ ${HDF5_SRC_DIR}/H5public.h _h5public_h_contents)
string (REGEX REPLACE ".*#define[ \t]+H5_VERS_MAJOR[ \t]+([0-9]*).*$"
    "\\1" H5_VERS_MAJOR ${_h5public_h_contents})
string (REGEX REPLACE ".*#define[ \t]+H5_VERS_MINOR[ \t]+([0-9]*).*$"
    "\\1" H5_VERS_MINOR ${_h5public_h_contents})
string (REGEX REPLACE ".*#define[ \t]+H5_VERS_RELEASE[ \t]+([0-9]*).*$"
    "\\1" H5_VERS_RELEASE ${_h5public_h_contents})
string (REGEX REPLACE ".*#define[ \t]+H5_VERS_SUBRELEASE[ \t]+\"([0-9A-Za-z._-]*)\".*$"
    "\\1" H5_VERS_SUBRELEASE ${_h5public_h_contents})
message (TRACE "VERSION: ${H5_VERS_MAJOR}.${H5_VERS_MINOR}.${H5_VERS_RELEASE}-${H5_VERS_SUBRELEASE}")

#-----------------------------------------------------------------------------
# parse the full soversion number from config/lt_vers.am and include in H5_SOVERS_INFO
#-----------------------------------------------------------------------------
file (READ ${HDF_CONFIG_DIR}/lt_vers.am _lt_vers_am_contents)
string (REGEX REPLACE ".*LT_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$"
    "\\1" H5_LIB_SOVERS_INTERFACE ${_lt_vers_am_contents})
string (REGEX REPLACE ".*LT_VERS_REVISION[ \t]+=[ \t]+([0-9]*).*$"
    "\\1" H5_LIB_SOVERS_MINOR ${_lt_vers_am_contents})
string (REGEX REPLACE ".*LT_VERS_AGE[ \t]+=[ \t]+([0-9]*).*$"
    "\\1" H5_LIB_SOVERS_RELEASE ${_lt_vers_am_contents})
math (EXPR H5_LIB_SOVERS_MAJOR ${H5_LIB_SOVERS_INTERFACE}-${H5_LIB_SOVERS_RELEASE})
message (VERBOSE "SOVERSION: ${H5_LIB_SOVERS_MAJOR}.${H5_LIB_SOVERS_RELEASE}.${H5_LIB_SOVERS_MINOR}")
string (REGEX MATCH ".*LT_TOOLS_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$" H5_TOOLS_SOVERS_EXISTS ${_lt_vers_am_contents})
if (H5_TOOLS_SOVERS_EXISTS)
  string (REGEX REPLACE ".*LT_TOOLS_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_TOOLS_SOVERS_INTERFACE ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_TOOLS_VERS_REVISION[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_TOOLS_SOVERS_MINOR ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_TOOLS_VERS_AGE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_TOOLS_SOVERS_RELEASE ${_lt_vers_am_contents})
  math (EXPR H5_TOOLS_SOVERS_MAJOR ${H5_TOOLS_SOVERS_INTERFACE}-${H5_TOOLS_SOVERS_RELEASE})
  message (VERBOSE "SOVERSION_TOOLS: ${H5_TOOLS_SOVERS_MAJOR}.${H5_TOOLS_SOVERS_RELEASE}.${H5_TOOLS_SOVERS_MINOR}")
endif ()
string (REGEX MATCH ".*LT_CXX_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$" H5_CXX_SOVERS_EXISTS ${_lt_vers_am_contents})
if (H5_CXX_SOVERS_EXISTS)
  string (REGEX REPLACE ".*LT_CXX_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_CXX_SOVERS_INTERFACE ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_CXX_VERS_REVISION[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_CXX_SOVERS_MINOR ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_CXX_VERS_AGE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_CXX_SOVERS_RELEASE ${_lt_vers_am_contents})
  math (EXPR H5_CXX_SOVERS_MAJOR ${H5_CXX_SOVERS_INTERFACE}-${H5_CXX_SOVERS_RELEASE})
  message (VERBOSE "SOVERSION_CXX: ${H5_CXX_SOVERS_MAJOR}.${H5_CXX_SOVERS_RELEASE}.${H5_CXX_SOVERS_MINOR}")
endif ()
string (REGEX MATCH ".*LT_F_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$" H5_F_SOVERS_EXISTS ${_lt_vers_am_contents})
if (H5_F_SOVERS_EXISTS)
  string (REGEX REPLACE ".*LT_F_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_F_SOVERS_INTERFACE ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_F_VERS_REVISION[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_F_SOVERS_MINOR ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_F_VERS_AGE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_F_SOVERS_RELEASE ${_lt_vers_am_contents})
  math (EXPR H5_F_SOVERS_MAJOR ${H5_F_SOVERS_INTERFACE}-${H5_F_SOVERS_RELEASE})
  message (VERBOSE "SOVERSION_F: ${H5_F_SOVERS_MAJOR}.${H5_F_SOVERS_RELEASE}.${H5_F_SOVERS_MINOR}")
endif ()
string (REGEX MATCH ".*LT_HL_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$" H5_HL_SOVERS_EXISTS ${_lt_vers_am_contents})
if (H5_HL_SOVERS_EXISTS)
  string (REGEX REPLACE ".*LT_HL_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_HL_SOVERS_INTERFACE ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_HL_VERS_REVISION[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_HL_SOVERS_MINOR ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_HL_VERS_AGE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_HL_SOVERS_RELEASE ${_lt_vers_am_contents})
  math (EXPR H5_HL_SOVERS_MAJOR ${H5_HL_SOVERS_INTERFACE}-${H5_HL_SOVERS_RELEASE})
  message (VERBOSE "SOVERSION_HL: ${H5_HL_SOVERS_MAJOR}.${H5_HL_SOVERS_RELEASE}.${H5_HL_SOVERS_MINOR}")
endif ()
string (REGEX MATCH ".*LT_HL_CXX_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$" H5_HL_CXX_SOVERS_EXISTS ${_lt_vers_am_contents})
if (H5_HL_CXX_SOVERS_EXISTS)
  string (REGEX REPLACE ".*LT_HL_CXX_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_HL_CXX_SOVERS_INTERFACE ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_HL_CXX_VERS_REVISION[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_HL_CXX_SOVERS_MINOR ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_HL_CXX_VERS_AGE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_HL_CXX_SOVERS_RELEASE ${_lt_vers_am_contents})
  math (EXPR H5_HL_CXX_SOVERS_MAJOR ${H5_HL_CXX_SOVERS_INTERFACE}-${H5_HL_CXX_SOVERS_RELEASE})
  message (VERBOSE "SOVERSION_HL_CXX: ${H5_HL_CXX_SOVERS_MAJOR}.${H5_HL_CXX_SOVERS_RELEASE}.${H5_HL_CXX_SOVERS_MINOR}")
endif ()
string (REGEX MATCH ".*LT_HL_F_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$" H5_HL_F_SOVERS_EXISTS ${_lt_vers_am_contents})
if (H5_HL_F_SOVERS_EXISTS)
  string (REGEX REPLACE ".*LT_HL_F_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_HL_F_SOVERS_INTERFACE ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_HL_F_VERS_REVISION[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_HL_F_SOVERS_MINOR ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_HL_F_VERS_AGE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_HL_F_SOVERS_RELEASE ${_lt_vers_am_contents})
  math (EXPR H5_HL_F_SOVERS_MAJOR ${H5_HL_F_SOVERS_INTERFACE}-${H5_HL_F_SOVERS_RELEASE})
  message (VERBOSE "SOVERSION_HL_F: ${H5_HL_F_SOVERS_MAJOR}.${H5_HL_F_SOVERS_RELEASE}.${H5_HL_F_SOVERS_MINOR}")
endif ()
string (REGEX MATCH ".*LT_JAVA_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$" H5_JAVA_SOVERS_EXISTS ${_lt_vers_am_contents})
if(H5_JAVA_SOVERS_EXISTS)
  string (REGEX REPLACE ".*LT_JAVA_VERS_INTERFACE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_JAVA_SOVERS_INTERFACE ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_JAVA_VERS_REVISION[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_JAVA_SOVERS_MINOR ${_lt_vers_am_contents})
  string (REGEX REPLACE ".*LT_JAVA_VERS_AGE[ \t]+=[ \t]+([0-9]*).*$"
      "\\1" H5_JAVA_SOVERS_RELEASE ${_lt_vers_am_contents})
  math (EXPR H5_JAVA_SOVERS_MAJOR ${H5_JAVA_SOVERS_INTERFACE}-${H5_JAVA_SOVERS_RELEASE})
  message (VERBOSE "SOVERSION_JAVA: ${H5_JAVA_SOVERS_MAJOR}.${H5_JAVA_SOVERS_RELEASE}.${H5_JAVA_SOVERS_MINOR}")
endif ()

#-----------------------------------------------------------------------------
# Basic HDF5 stuff here
#-----------------------------------------------------------------------------
set (HDF5_PACKAGE "hdf5")
set (HDF5_PACKAGE_NAME "HDF5")
set (HDF5_PACKAGE_VERSION "${H5_VERS_MAJOR}.${H5_VERS_MINOR}.${H5_VERS_RELEASE}")
set (HDF5_PACKAGE_VERSION_MAJOR "${H5_VERS_MAJOR}.${H5_VERS_MINOR}")
set (HDF5_PACKAGE_VERSION_MINOR "${H5_VERS_RELEASE}")
if (H5_VERS_SUBRELEASE)
  set (HDF5_PACKAGE_VERSION_STRING "${HDF5_PACKAGE_VERSION}.${H5_VERS_SUBRELEASE}")
  set (HDF5_RELEASE_VERSION_STRING "${HDF5_PACKAGE_VERSION}-${H5_VERS_SUBRELEASE}")
else ()
  set (HDF5_PACKAGE_VERSION_STRING "${HDF5_PACKAGE_VERSION}")
  set (HDF5_RELEASE_VERSION_STRING "${HDF5_PACKAGE_VERSION}")
endif ()
set (HDF5_LIB_PACKAGE_SOVERSION "${H5_LIB_SOVERS_MAJOR}.${H5_LIB_SOVERS_RELEASE}.${H5_LIB_SOVERS_MINOR}")
set (HDF5_LIB_PACKAGE_SOVERSION_MAJOR "${H5_LIB_SOVERS_MAJOR}")
if (H5_TOOLS_SOVERS_EXISTS)
  set (HDF5_TOOLS_PACKAGE_SOVERSION "${H5_TOOLS_SOVERS_MAJOR}.${H5_TOOLS_SOVERS_RELEASE}.${H5_TOOLS_SOVERS_MINOR}")
  set (HDF5_TOOLS_PACKAGE_SOVERSION_MAJOR "${H5_TOOLS_SOVERS_MAJOR}")
else ()
  set (HDF5_TOOLS_PACKAGE_SOVERSION "${H5_LIB_SOVERS_MAJOR}.${H5_LIB_SOVERS_RELEASE}.${H5_LIB_SOVERS_MINOR}")
  set (HDF5_TOOLS_PACKAGE_SOVERSION_MAJOR "${H5_LIB_SOVERS_MAJOR}")
endif ()
if (H5_CXX_SOVERS_EXISTS)
  set (HDF5_CXX_PACKAGE_SOVERSION "${H5_CXX_SOVERS_MAJOR}.${H5_CXX_SOVERS_RELEASE}.${H5_CXX_SOVERS_MINOR}")
  set (HDF5_CXX_PACKAGE_SOVERSION_MAJOR "${H5_CXX_SOVERS_MAJOR}")
else ()
  set (HDF5_CXX_PACKAGE_SOVERSION "${H5_LIB_SOVERS_MAJOR}.${H5_LIB_SOVERS_RELEASE}.${H5_LIB_SOVERS_MINOR}")
  set (HDF5_CXX_PACKAGE_SOVERSION_MAJOR "${H5_LIB_SOVERS_MAJOR}")
endif ()
if (H5_F_SOVERS_EXISTS)
  set (HDF5_F_PACKAGE_SOVERSION "${H5_F_SOVERS_MAJOR}.${H5_F_SOVERS_RELEASE}.${H5_F_SOVERS_MINOR}")
  set (HDF5_F_PACKAGE_SOVERSION_MAJOR "${H5_F_SOVERS_MAJOR}")
else ()
  set (HDF5_F_PACKAGE_SOVERSION "${H5_LIB_SOVERS_MAJOR}.${H5_LIB_SOVERS_RELEASE}.${H5_LIB_SOVERS_MINOR}")
  set (HDF5_F_PACKAGE_SOVERSION_MAJOR "${H5_LIB_SOVERS_MAJOR}")
endif ()
if (H5_HL_SOVERS_EXISTS)
  set (HDF5_HL_PACKAGE_SOVERSION "${H5_HL_SOVERS_MAJOR}.${H5_HL_SOVERS_RELEASE}.${H5_HL_SOVERS_MINOR}")
  set (HDF5_HL_PACKAGE_SOVERSION_MAJOR "${H5_HL_SOVERS_MAJOR}")
else ()
  set (HDF5_HL_PACKAGE_SOVERSION "${H5_LIB_SOVERS_MAJOR}.${H5_LIB_SOVERS_RELEASE}.${H5_LIB_SOVERS_MINOR}")
  set (HDF5_HL_PACKAGE_SOVERSION_MAJOR "${H5_LIB_SOVERS_MAJOR}")
endif ()
if (H5_HL_F_SOVERS_EXISTS)
  set (HDF5_HL_CXX_PACKAGE_SOVERSION "${H5_HL_CXX_SOVERS_MAJOR}.${H5_HL_CXX_SOVERS_RELEASE}.${H5_HL_CXX_SOVERS_MINOR}")
  set (HDF5_HL_CXX_PACKAGE_SOVERSION_MAJOR "${H5_HL_CXX_SOVERS_MAJOR}")
else ()
  set (HDF5_HL_CXX_PACKAGE_SOVERSION "${H5_LIB_SOVERS_MAJOR}.${H5_LIB_SOVERS_RELEASE}.${H5_LIB_SOVERS_MINOR}")
  set (HDF5_HL_CXX_PACKAGE_SOVERSION_MAJOR "${H5_LIB_SOVERS_MAJOR}")
endif ()
if (H5_HL_F_SOVERS_EXISTS)
  set (HDF5_HL_F_PACKAGE_SOVERSION "${H5_HL_F_SOVERS_MAJOR}.${H5_HL_F_SOVERS_RELEASE}.${H5_HL_F_SOVERS_MINOR}")
  set (HDF5_HL_F_PACKAGE_SOVERSION_MAJOR "${H5_HL_F_SOVERS_MAJOR}")
else ()
  set (HDF5_HL_F_PACKAGE_SOVERSION "${H5_LIB_SOVERS_MAJOR}.${H5_LIB_SOVERS_RELEASE}.${H5_LIB_SOVERS_MINOR}")
  set (HDF5_HL_F_PACKAGE_SOVERSION_MAJOR "${H5_LIB_SOVERS_MAJOR}")
endif ()
if (H5_JAVA_SOVERS_EXISTS)
  set (HDF5_JAVA_PACKAGE_SOVERSION "${H5_JAVA_SOVERS_MAJOR}.${H5_JAVA_SOVERS_RELEASE}.${H5_JAVA_SOVERS_MINOR}")
  set (HDF5_JAVA_PACKAGE_SOVERSION_MAJOR "${H5_JAVA_SOVERS_MAJOR}")
else ()
  set (HDF5_JAVA_PACKAGE_SOVERSION "${H5_LIB_SOVERS_MAJOR}.${H5_LIB_SOVERS_RELEASE}.${H5_LIB_SOVERS_MINOR}")
  set (HDF5_JAVA_PACKAGE_SOVERSION_MAJOR "${H5_LIB_SOVERS_MAJOR}")
endif ()
set (HDF5_PACKAGE_STRING "${HDF5_PACKAGE_NAME} ${HDF5_PACKAGE_VERSION_STRING}")
set (HDF5_PACKAGE_TARNAME "${HDF5_PACKAGE}${HDF_PACKAGE_EXT}")
set (HDF5_PACKAGE_URL "https://www.hdfgroup.org")
set (HDF5_PACKAGE_BUGREPORT "help@hdfgroup.org")

#-----------------------------------------------------------------------------
# Set variables needed for installation
#-----------------------------------------------------------------------------
set (HDF5_VERSION_STRING ${HDF5_PACKAGE_VERSION})
set (HDF5_VERSION_MAJOR  ${HDF5_PACKAGE_VERSION_MAJOR})
set (HDF5_VERSION_MINOR  ${HDF5_PACKAGE_VERSION_MINOR})
if (H5_VERS_MAJOR GREATER 1)
  if (H5_VERS_MINOR GREATER 9)
    set (H5_LIBVER_DIR ${H5_VERS_MAJOR}${H5_VERS_MINOR})
  else ()
    set (H5_LIBVER_DIR ${H5_VERS_MAJOR}0${H5_VERS_MINOR})
  endif()
else ()
  set (H5_LIBVER_DIR ${H5_VERS_MAJOR}${H5_VERS_MINOR})
endif()
#-----------------------------------------------------------------------------
# Include some macros for reusable code
#-----------------------------------------------------------------------------

get_property(IS_MULTI GLOBAL PROPERTY GENERATOR_IS_MULTI_CONFIG)

if(NOT CMAKE_BUILD_TYPE AND NOT IS_MULTI)
    message(STATUS "No CMAKE_BUILD_TYPE set -- using Release")
    set(CMAKE_BUILD_TYPE Release)
endif()

include (${HDF_CONFIG_DIR}/HDFMacros.cmake)

HDF_DIR_PATHS(${HDF5_PACKAGE_NAME})

include (${HDF_RESOURCES_DIR}/HDFLibMacros.cmake)
include (${HDF_RESOURCES_DIR}/HDF5PluginMacros.cmake)
include (${HDF_CONFIG_DIR}/HDF5Macros.cmake)

#-----------------------------------------------------------------------------
# Targets built within this project are exported at Install time for use
# by other projects.
#-----------------------------------------------------------------------------
if (NOT HDF5_EXPORTED_TARGETS)
  set (HDF5_EXPORTED_TARGETS "hdf5-targets")
endif ()

#-----------------------------------------------------------------------------
# To include a library in the list exported by the project AT BUILD TIME,
# add it to this variable. This is NOT used by Make Install, but for projects
# which include hdf5 as a sub-project within their build tree
#-----------------------------------------------------------------------------
set_global_variable (HDF5_LIBRARIES_TO_EXPORT "")
set_global_variable (HDF5_UTILS_TO_EXPORT "")

set (EXTERNAL_HEADER_LIST "")
set (EXTERNAL_LIBRARY_LIST "")
set (EXTERNAL_LIBRARYDLL_LIST "")

#-----------------------------------------------------------------------------
# Run all the CMake configuration tests for our build environment
#-----------------------------------------------------------------------------
include (${HDF_CONFIG_DIR}/ConfigureChecks.cmake)

set (CMAKE_INCLUDE_CURRENT_DIR_IN_INTERFACE ON)

#-----------------------------------------------------------------------------
# Include directories in the source or build tree should come before other
# directories to prioritize headers in the sources over installed ones.
#-----------------------------------------------------------------------------
set (CMAKE_INCLUDE_DIRECTORIES_PROJECT_BEFORE ON)
set (HDF5_COMP_INCLUDE_DIRECTORIES)

#-----------------------------------------------------------------------------
# Mac OS X Options
#-----------------------------------------------------------------------------
if (HDF5_BUILD_FRAMEWORKS AND NOT BUILD_SHARED_LIBS)
  set (BUILD_SHARED_LIBS ON CACHE BOOL "Build Shared Libraries" FORCE)
endif ()

#-----------------------------------------------------------------------------
# Option to Build Shared and Static libs, default is both
#-----------------------------------------------------------------------------
set (H5_ENABLE_STATIC_LIB NO)
set (H5_ENABLE_SHARED_LIB NO)

# only shared libraries/tools is true if user forces static OFF
if (NOT BUILD_STATIC_LIBS)
  set (HDF5_ONLY_SHARED_LIBS ON CACHE BOOL "Only Build Shared Libraries" FORCE)
endif ()

# only shared libraries is set ON by user then force settings
if (HDF5_ONLY_SHARED_LIBS)
  set (H5_ENABLE_STATIC_LIB NO)
  set (BUILD_SHARED_LIBS ON CACHE BOOL "Build Shared Libraries" FORCE)
  set (BUILD_STATIC_LIBS OFF CACHE BOOL "Build Static Libraries" FORCE)
  if (HDF5_BUILD_STATIC_TOOLS)
    message (WARNING "Cannot build static tools without static libraries. Building shared tools.")
  endif ()
  set (HDF5_BUILD_STATIC_TOOLS OFF CACHE BOOL "Build Static Tools NOT Shared Tools" FORCE)
endif ()

if (NOT BUILD_SHARED_LIBS AND NOT HDF5_BUILD_STATIC_TOOLS)
  message (VERBOSE "Cannot build shared tools without shared libraries. Building static tools.")
  set (HDF5_BUILD_STATIC_TOOLS ON CACHE BOOL "Build Static Tools NOT Shared Tools" FORCE)
endif ()

if (BUILD_STATIC_LIBS)
  set (H5_ENABLE_STATIC_LIB YES)
endif ()
if (BUILD_SHARED_LIBS)
  set (H5_ENABLE_SHARED_LIB YES)
endif ()

set (CMAKE_POSITION_INDEPENDENT_CODE ON)

#-----------------------------------------------------------------------------
# perl is used in some optional src and tests, check availability
find_package (Perl)
if (PERL_FOUND)
  set (H5_PERL_FOUND YES)
endif ()
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Option to Build Static executables
#-----------------------------------------------------------------------------
if (BUILD_STATIC_EXECS)
  if (NOT WIN32)
    set (CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -static")
    set (CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -static")
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Options to analyze the code with various tools
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_ANALYZER_TOOLS)
  include (${HDF_CONFIG_DIR}/sanitizer/tools.cmake)
endif ()
if (HDF5_ENABLE_SANITIZERS)
  include (${HDF_CONFIG_DIR}/sanitizer/sanitizers.cmake)
endif ()
if (HDF5_ENABLE_FORMATTERS)
  include (${HDF_CONFIG_DIR}/sanitizer/formatting.cmake)
endif ()

#-----------------------------------------------------------------------------
# Option to use code coverage
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_COVERAGE)
  include (${HDF_CONFIG_DIR}/sanitizer/code-coverage.cmake)
  if (CODE_COVERAGE AND CODE_COVERAGE_ADDED)
    message (VERBOSE "Add instrumentation to all targets")
    add_code_coverage () # Adds instrumentation to all targets
  else ()
    message (VERBOSE "Use --coverage option")
    set (CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -g -O0 --coverage -fprofile-arcs -ftest-coverage")
    set (CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -g --coverage -O0 -fprofile-arcs -ftest-coverage")
    if (CMAKE_C_COMPILER_ID STREQUAL "GNU")
      set (LDFLAGS "${LDFLAGS} -fprofile-arcs -ftest-coverage")
      link_libraries (gcov)
    else ()
      set (CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} --coverage")
    endif ()
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Option to indicate using a memory checker
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_USING_MEMCHECKER)
  set (H5_USING_MEMCHECKER 1)
endif ()

#-----------------------------------------------------------------------------
# Option to enable/disable using pread/pwrite for VFDs
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_PREADWRITE AND H5_HAVE_PREAD AND H5_HAVE_PWRITE)
  set (H5_HAVE_PREADWRITE 1)
endif ()

#-----------------------------------------------------------------------------
# Option to use deprecated public API symbols
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_DEPRECATED_SYMBOLS)
  set (H5_NO_DEPRECATED_SYMBOLS 0)
else ()
  set (H5_NO_DEPRECATED_SYMBOLS 1)
endif ()

#-----------------------------------------------------------------------------
# When building utility executables that generate other (source) files :
# we make use of the following variables defined in the root CMakeLists.
# Certain systems may add /Debug or /Release to output paths
# and we need to call the executable from inside the CMake configuration
#-----------------------------------------------------------------------------
if (WIN32)
  add_compile_definitions (_CRT_SECURE_NO_WARNINGS)
  if (MSVC)
    add_compile_definitions (_BIND_TO_CURRENT_VCLIBS_VERSION=1 _CONSOLE)
  endif ()
endif ()

if (MSVC)
  set (CMAKE_MFC_FLAG 0)
  set (WIN_COMPILE_FLAGS "")
  set (WIN_LINK_FLAGS "")
endif ()

#-----------------------------------------------------------------------------
# Add some definitions for Debug Builds
#-----------------------------------------------------------------------------
if (${HDF_CFG_NAME} MATCHES "Debug" OR ${HDF_CFG_NAME} MATCHES "Developer")
  # Enable instrumenting of the library's internal operations
  cmake_dependent_option (HDF5_ENABLE_INSTRUMENT "Instrument The library" OFF HDF5_ENABLE_TRACE OFF)

  # Instrumenting is enabled by default for parallel debug builds
  if (HDF5_ENABLE_PARALLEL)
    set (HDF5_ENABLE_INSTRUMENT ON CACHE BOOL "Instrument The library" FORCE)
  endif ()

  if (HDF5_ENABLE_INSTRUMENT)
    set (H5_HAVE_INSTRUMENTED_LIBRARY 1)
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Add some definitions for Developer Builds
#-----------------------------------------------------------------------------
if (${HDF_CFG_NAME} MATCHES "Developer")
  include (${HDF_RESOURCES_DIR}/HDF5DeveloperBuild.cmake)
endif ()

#-----------------------------------------------------------------------------
# Option to embed library info into executables
#-----------------------------------------------------------------------------
if (CMAKE_SYSTEM_NAME STREQUAL "Emscripten")
  set (H5_HAVE_EMBEDDED_LIBINFO 0)
else ()
  if (HDF5_ENABLE_EMBEDDED_LIBINFO)
    set (H5_HAVE_EMBEDDED_LIBINFO 1)
  endif ()
endif ()

include (${HDF_CONFIG_DIR}/flags/HDFCompilerFlags.cmake)
set (CMAKE_MODULE_PATH ${HDF_CONFIG_DIR} ${HDF_RESOURCES_DIR} ${CMAKE_MODULE_PATH})

#-----------------------------------------------------------------------------
# Option to Enable HDFS
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_HDFS)
  find_package (JNI REQUIRED)
  if (JNI_FOUND)
    set (H5_HAVE_LIBJVM 1)
  endif ()
  find_package (HDFS REQUIRED)
  if (HDFS_FOUND)
    set (H5_HAVE_LIBHDFS 1)
    set (H5_HAVE_HDFS_H 1)
    if (NOT MSVC)
      list (APPEND LINK_LIBS -pthread)
    endif ()
  else ()
    set (HDF5_ENABLE_HDFS OFF CACHE BOOL "Enable HDFS" FORCE)
    message (FATAL_ERROR "Set to use libhdfs library, but could not find or use libhdfs. Please verify that the path to HADOOP_HOME is valid, and/or reconfigure without HDF5_ENABLE_HDFS")
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Option to Enable MPI Parallel
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_PARALLEL)
  find_package (MPI REQUIRED)
  if (MPI_C_FOUND)
    set (H5_HAVE_PARALLEL 1)

    # Require MPI standard 3.0 and greater
    if (MPI_VERSION LESS 3)
      message (FATAL_ERROR "HDF5 requires MPI standard 3.0 or greater")
    endif ()

    # MPI checks, only do these if MPI_C_FOUND is true, otherwise they always fail
    # and once set, they are cached as false and not regenerated
    set (CMAKE_REQUIRED_LIBRARIES "${MPI_C_LIBRARIES}")
    set (CMAKE_REQUIRED_INCLUDES "${MPI_C_INCLUDE_DIRS}")
    # Used by Fortran + MPI
    CHECK_SYMBOL_EXISTS (MPI_Comm_c2f "mpi.h"  H5_HAVE_MPI_MULTI_LANG_Comm)
    CHECK_SYMBOL_EXISTS (MPI_Info_c2f "mpi.h"  H5_HAVE_MPI_MULTI_LANG_Info)

    # Used by Parallel Compression feature
    set (PARALLEL_FILTERED_WRITES ON)
    CHECK_SYMBOL_EXISTS (MPI_Ibarrier "mpi.h" H5_HAVE_MPI_Ibarrier)
    CHECK_SYMBOL_EXISTS (MPI_Issend "mpi.h" H5_HAVE_MPI_Issend)
    CHECK_SYMBOL_EXISTS (MPI_Iprobe "mpi.h" H5_HAVE_MPI_Iprobe)
    CHECK_SYMBOL_EXISTS (MPI_Irecv "mpi.h" H5_HAVE_MPI_Irecv)
    if (H5_HAVE_MPI_Ibarrier AND H5_HAVE_MPI_Issend AND H5_HAVE_MPI_Iprobe AND H5_HAVE_MPI_Irecv)
      set (H5_HAVE_PARALLEL_FILTERED_WRITES 1)
    else ()
      message (WARNING "The MPI_Ibarrier/MPI_Issend/MPI_Iprobe/MPI_Irecv functions could not be located.
               Parallel writes of filtered data will be disabled.")
      set (PARALLEL_FILTERED_WRITES OFF)
    endif ()

    # Used by big I/O feature
    set (LARGE_PARALLEL_IO ON)
    CHECK_SYMBOL_EXISTS (MPI_Get_elements_x "mpi.h" H5_HAVE_MPI_Get_elements_x)
    CHECK_SYMBOL_EXISTS (MPI_Type_size_x "mpi.h" H5_HAVE_MPI_Type_size_x)
    if (NOT H5_HAVE_MPI_Get_elements_x OR NOT H5_HAVE_MPI_Type_size_x)
      message (WARNING "The MPI_Get_elements_x and/or MPI_Type_size_x functions could not be located.
               Reading/Writing >2GB of data in a single parallel I/O operation will be disabled.")
      set (LARGE_PARALLEL_IO OFF)
    endif ()

    # Used by Subfiling VFD feature
    CHECK_SYMBOL_EXISTS (MPI_Comm_split_type "mpi.h" H5_HAVE_MPI_Comm_split_type)
  else ()
    message (FATAL_ERROR "Parallel libraries not found")
  endif ()
endif ()

# Parallel IO usage requires MPI to be Linked and Included
if (H5_HAVE_PARALLEL)
  list (APPEND LINK_PUB_LIBS MPI::MPI_C)
  if (MPI_C_LINK_FLAGS)
    set (CMAKE_EXE_LINKER_FLAGS "${MPI_C_LINK_FLAGS} ${CMAKE_EXE_LINKER_FLAGS}")
  endif ()
endif ()

# Determine if a threading package is available on this system
set (THREADS_PREFER_PTHREAD_FLAG ON)
find_package (Threads)
if (Threads_FOUND)
  set (H5_HAVE_THREADS 1)
  set (CMAKE_REQUIRED_LIBRARIES ${CMAKE_THREAD_LIBS_INIT})

  # Determine which threading package to use
  # Comment out check for C11 threads for now, since it conflicts with the
  # current --std=c99 compile flags at configuration time.  When we switch to
  # --std=c11, this can be uncommented.
  #CHECK_INCLUDE_FILE("threads.h" HAVE_THREADS_H)
  if (WIN32)
    # When Win32 is available, we use those threads
    set (H5_HAVE_WIN_THREADS 1)
  elseif (HAVE_THREADS_H)
    # When C11 threads are available, those are the top choice
    set (H5_HAVE_C11_THREADS 1)
  elseif (CMAKE_USE_PTHREADS_INIT)
    set (H5_HAVE_PTHREAD_H 1)
  else ()
    message (FATAL_ERROR " **** thread support requires C11 threads, Win32 threads or Pthreads **** ")
  endif ()
  set (HDF5_THREADS_ENABLED ON) # Used to init hdf5-config.cmake
  list (APPEND LINK_LIBS Threads::Threads)

  # Check for compiler support for atomic variables
  CHECK_INCLUDE_FILE("stdatomic.h" HAVE_STDATOMIC_H)
  if (HAVE_STDATOMIC_H)
    set (H5_HAVE_STDATOMIC_H 1)
  endif()
else ()
  set (HDF5_THREADS_ENABLED OFF) # Used to init hdf5-config.cmake
endif ()

# Determine whether to build the HDF5 Subfiling VFD
set (H5FD_SUBFILING_DIR ${HDF5_SRC_DIR}/H5FDsubfiling)
set (HDF5_SRC_INCLUDE_DIRS
    ${HDF5_SRC_INCLUDE_DIRS}
    ${H5FD_SUBFILING_DIR}
)

cmake_dependent_option (HDF5_ENABLE_SUBFILING_VFD "Build Parallel HDF5 Subfiling VFD" OFF "HDF5_ENABLE_PARALLEL;NOT WIN32" OFF)
if (HDF5_ENABLE_SUBFILING_VFD)
  # Make sure we found MPI_Comm_split_type previously
  if (NOT H5_HAVE_MPI_Comm_split_type)
    message (FATAL_ERROR "Subfiling VFD requires MPI-3 support for MPI_Comm_split_type")
  endif ()

  # Subfiling requires thread operations
  if (NOT Threads_FOUND)
    message (FATAL_ERROR "Subfiling requires thread operations support")
  endif ()

  set (H5_HAVE_SUBFILING_VFD 1)
  # IOC VFD is currently only built when subfiling is enabled
  set (H5_HAVE_IOC_VFD 1)
endif()

message (VERBOSE "LINK_LIBS=${LINK_LIBS}")
message (VERBOSE "LINK_PUB_LIBS=${LINK_PUB_LIBS}")

set (HDF5_DEFAULT_API_VERSION "v200" CACHE STRING "Enable v2.0 API (v16, v18, v110, v112, v114, v200)")
set_property (CACHE HDF5_DEFAULT_API_VERSION PROPERTY STRINGS v16 v18 v110 v112 v114 v200)
#-----------------------------------------------------------------------------
# Option to use 1.6.x API
#-----------------------------------------------------------------------------
set (H5_USE_16_API_DEFAULT 0)
if (HDF5_DEFAULT_API_VERSION MATCHES "v16")
  set (H5_USE_16_API_DEFAULT 1)
endif ()

#-----------------------------------------------------------------------------
# Option to use 1.8.x API
#-----------------------------------------------------------------------------
set (H5_USE_18_API_DEFAULT 0)
if (HDF5_DEFAULT_API_VERSION MATCHES "v18")
  set (H5_USE_18_API_DEFAULT 1)
endif ()

#-----------------------------------------------------------------------------
# Option to use 1.10.x API
#-----------------------------------------------------------------------------
set (H5_USE_110_API_DEFAULT 0)
if (HDF5_DEFAULT_API_VERSION MATCHES "v110")
  set (H5_USE_110_API_DEFAULT 1)
endif ()

#-----------------------------------------------------------------------------
# Option to use 1.12.x API
#-----------------------------------------------------------------------------
set (H5_USE_112_API_DEFAULT 0)
if (HDF5_DEFAULT_API_VERSION MATCHES "v112")
  set (H5_USE_112_API_DEFAULT 1)
endif ()

#-----------------------------------------------------------------------------
# Option to use 1.14.x API
#-----------------------------------------------------------------------------
set (H5_USE_114_API_DEFAULT 0)
if (HDF5_DEFAULT_API_VERSION MATCHES "v114")
  set (H5_USE_114_API_DEFAULT 1)
endif ()

#-----------------------------------------------------------------------------
# Option to use 2.x.y API
#-----------------------------------------------------------------------------
set (H5_USE_200_API_DEFAULT 0)
if (NOT HDF5_DEFAULT_API_VERSION)
  set (HDF5_DEFAULT_API_VERSION "v200")
endif ()
if (DEFAULT_API_VERSION MATCHES "v200")
  set (H5_USE_200_API_DEFAULT 1)
endif ()

#-----------------------------------------------------------------------------
# Include user macros
#-----------------------------------------------------------------------------
include (UserMacros.cmake)
include (config/CacheURLs.cmake)

#-----------------------------------------------------------------------------
# Include filter (zlib, szip, etc.) macros
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_ZLIB_SUPPORT OR HDF5_ENABLE_SZIP_SUPPORT)
  include (CMakeFilters.cmake)
endif ()

#-----------------------------------------------------------------------------
# Include external VOL connectors
#-----------------------------------------------------------------------------
include (CMakeVOL.cmake)

#-----------------------------------------------------------------------------
# Option for external libraries on windows
#-----------------------------------------------------------------------------
if (NOT HDF5_EXTERNALLY_CONFIGURED)
  if (HDF5_PACKAGE_EXTLIBS)
    set (HDF5_NO_PACKAGES OFF CACHE BOOL "CPACK - Disable packaging" FORCE)
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Option to use threadsafe
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_THREADSAFE)
  # check for unsupported options
  if (WIN32)
    if (BUILD_STATIC_LIBS)
      message (FATAL_ERROR " **** thread-safety option not supported with static library **** ")
    endif ()
  endif ()
  if (HDF5_ENABLE_PARALLEL)
    if (NOT HDF5_ALLOW_UNSUPPORTED)
      message (FATAL_ERROR " **** Parallel and thread-safety options are not supported, override with HDF5_ALLOW_UNSUPPORTED option **** ")
    else ()
      message (VERBOSE " **** Allowing unsupported parallel and thread-safety options **** ")
    endif ()
  endif ()
  if (HDF5_BUILD_FORTRAN)
    if (NOT HDF5_ALLOW_UNSUPPORTED)
      message (FATAL_ERROR " **** Fortran and thread-safety options are not supported, override with HDF5_ALLOW_UNSUPPORTED option **** ")
    else ()
      message (VERBOSE " **** Allowing unsupported Fortran and thread-safety options **** ")
    endif ()
  endif ()
  if (HDF5_BUILD_CPP_LIB)
    if (NOT HDF5_ALLOW_UNSUPPORTED)
      message (FATAL_ERROR " **** C++ and thread-safety options are not supported, override with HDF5_ALLOW_UNSUPPORTED option **** ")
    else ()
      message (VERBOSE " **** Allowing unsupported C++ and thread-safety options **** ")
    endif ()
  endif ()
  if (HDF5_BUILD_HL_LIB)
    if (NOT HDF5_ALLOW_UNSUPPORTED)
      message (FATAL_ERROR " **** HL and thread-safety options are not supported, override with HDF5_ALLOW_UNSUPPORTED option **** ")
    else ()
      message (VERBOSE " **** Allowing unsupported HL and thread-safety options **** ")
    endif ()
  endif ()

  # Check for threading package
  if (NOT Threads_FOUND)
    message (FATAL_ERROR " **** thread-safety option requires a threading package and none was found **** ")
  endif ()

  set (H5_HAVE_THREADSAFE 1)
endif ()

#-----------------------------------------------------------------------------
# Option to use multi-threaded concurrency
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_CONCURRENCY)
  # check for unsupported options
  if (WIN32)
    if (BUILD_STATIC_LIBS)
      message (FATAL_ERROR " **** multi-threaded concurrency option not supported with static library **** ")
    endif ()
  endif ()
  if (HDF5_ENABLE_PARALLEL)
    if (NOT HDF5_ALLOW_UNSUPPORTED)
      message (FATAL_ERROR " **** Parallel and multi-threaded concurrency options are not supported, override with HDF5_ALLOW_UNSUPPORTED option **** ")
    else ()
      message (VERBOSE " **** Allowing unsupported parallel and multi-threaded concurrency options **** ")
    endif ()
  endif ()
  if (HDF5_BUILD_FORTRAN)
    if (NOT HDF5_ALLOW_UNSUPPORTED)
      message (FATAL_ERROR " **** Fortran and multi-threaded concurrency options are not supported, override with HDF5_ALLOW_UNSUPPORTED option **** ")
    else ()
      message (VERBOSE " **** Allowing unsupported Fortran and multi-threaded concurrency options **** ")
    endif ()
  endif ()
  if (HDF5_BUILD_CPP_LIB)
    if (NOT HDF5_ALLOW_UNSUPPORTED)
      message (FATAL_ERROR " **** C++ and multi-threaded concurrency options are not supported, override with HDF5_ALLOW_UNSUPPORTED option **** ")
    else ()
      message (VERBOSE " **** Allowing unsupported C++ and multi-threaded concurrency options **** ")
    endif ()
  endif ()
  if (HDF5_BUILD_HL_LIB)
    if (NOT HDF5_ALLOW_UNSUPPORTED)
      message (FATAL_ERROR " **** HL and multi-threaded concurrency options are not supported, override with HDF5_ALLOW_UNSUPPORTED option **** ")
    else ()
      message (VERBOSE " **** Allowing unsupported HL and multi-threaded concurrency options **** ")
    endif ()
  endif ()

  # Check for threading package
  if (NOT Threads_FOUND)
    message (FATAL_ERROR " **** multi-threaded concurrency option requires a threading package and none was found **** ")
  endif ()

  # Multi-threaded concurrency and threadsafe options are mutually exclusive
  if (HDF5_ENABLE_THREADSAFE)
    message (FATAL_ERROR " **** multi-threaded concurrency and threadsafe options are mutually exclusive **** ")
  endif ()

  set (H5_HAVE_CONCURRENCY 1)
endif ()

#-----------------------------------------------------------------------------
# Option to build the map API
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_MAP_API)
  set (H5_HAVE_MAP_API 1)
else ()
  set (HDF5_ENABLE_MAP_API OFF CACHE BOOL "Build the map API" FORCE)
endif ()

#-----------------------------------------------------------------------------
# Option to override the compiler for h5cc
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_PARALLEL AND MPI_C_FOUND)
  set (_HDF5_H5CC_C_COMPILER ${MPI_C_COMPILER})
else ()
  set (_HDF5_H5CC_C_COMPILER ${CMAKE_C_COMPILER})
endif ()
set (HDF5_H5CC_C_COMPILER ${_HDF5_H5CC_C_COMPILER} CACHE STRING "C compiler to use in h5cc")

#-----------------------------------------------------------------------------
# Java implementation detection for version value
# Note: Java version detection happens after find_package(Java)
# Everything else is set by the Java subdirectory based on actual Java version
#-----------------------------------------------------------------------------
if (HDF5_BUILD_JAVA)
  if (NOT BUILD_SHARED_LIBS)
    message (FATAL_ERROR "\nJava requires shared libraries!\n")
  else ()
    find_package (Java)
    message (VERBOSE "Java version is ${Java_VERSION_STRING}")
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Add the HDF5 Library Target to the build
#-----------------------------------------------------------------------------
add_subdirectory (src)

if (HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "GIT" OR HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "TGZ")
  if ((H5_ZLIB_FOUND AND ZLIB_USE_EXTERNAL) OR (H5_SZIP_FOUND AND SZIP_USE_EXTERNAL))
    if (BUILD_STATIC_LIBS)
      add_dependencies (${HDF5_LIB_TARGET} ${LINK_COMP_LIBS})
    endif ()
    if (BUILD_SHARED_LIBS)
      add_dependencies (${HDF5_LIBSH_TARGET} ${LINK_COMP_LIBS})
    endif ()
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Option to build documentation
#-----------------------------------------------------------------------------
if (HDF5_BUILD_DOC AND EXISTS "${HDF5_DOXYGEN_DIR}" AND IS_DIRECTORY "${HDF5_DOXYGEN_DIR}")
# check if Doxygen is installed
  find_package (Doxygen)
  if (DOXYGEN_FOUND)
    option (HDF5_ENABLE_DOXY_WARNINGS "Enable fail if doxygen parsing has warnings." OFF)
    mark_as_advanced (HDF5_ENABLE_DOXY_WARNINGS)
    if (HDF5_ENABLE_DOXY_WARNINGS)
      set (HDF5_DOXY_WARNINGS "FAIL_ON_WARNINGS_PRINT")
    else ()
      set (HDF5_DOXY_WARNINGS "NO")
    endif ()
    message (VERBOSE "Doxygen version: ${DOXYGEN_VERSION}")

    # Set the default directory for the Java sources for doxygen
    if (Java_VERSION_STRING VERSION_GREATER_EQUAL "25.0.0")
      if (HDF5_ENABLE_JNI)
        set (DOXYGEN_JAVA_DIR ${HDF5_JAVA_SRCJNI_PATH})
     else ()
        set (DOXYGEN_JAVA_DIR ${HDF5_JAVA_SRC_PATH})
      endif ()
    else ()
      set (DOXYGEN_JAVA_DIR ${HDF5_JAVA_SRCJNI_PATH})
    endif ()

    add_subdirectory (doxygen)
  else ()
    message (WARNING "Doxygen needs to be installed to generate the doxygen documentation")
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Dashboard and Testing Settings
#-----------------------------------------------------------------------------
if (BUILD_TESTING)
  include (CMakeTests.cmake)
endif ()

#-----------------------------------------------------------------------------
# Option to build HDF5 Utilities
#-----------------------------------------------------------------------------
if (EXISTS "${HDF5_SOURCE_DIR}/utils" AND IS_DIRECTORY "${HDF5_SOURCE_DIR}/utils")
  if (HDF5_BUILD_PARALLEL_TOOLS AND HDF5_ENABLE_PARALLEL)
    set (CMAKE_PREFIX_PATH "$HDF_RESOURCES_DIR")
    find_package (MFU REQUIRED)
    if (MFU_FOUND)
      message (VERBOSE "LL_PATH=${LL_PATH}")
      set (H5_HAVE_LIBMFU 1)
      set (H5_HAVE_MFU_H 1)
      set (CMAKE_REQUIRED_INCLUDES "${MFU_INCLUDE_DIR}")
      set (MFU_LIBRARY_DEBUG "$MFU_LIBRARY")
      set (MFU_LIBRARY_RELEASE "$MFU_LIBRARY")
    endif ()
    find_package (CIRCLE REQUIRED)
    if (CIRCLE_FOUND)
      set (H5_HAVE_LIBCIRCLE 1)
      set (H5_HAVE_CIRCLE_H 1)
      set (CMAKE_REQUIRED_INCLUDES "${CIRCLE_INCLUDE_DIR}")
    endif ()
    find_package (DTCMP REQUIRED)
    if (DTCMP_FOUND)
      set (H5_HAVE_LIBDTCMP 1)
      set (H5_HAVE_DTCMP_H 1)
      set (CMAKE_REQUIRED_INCLUDES "${DTCMP_INCLUDE_DIR}")
    endif ()
  endif ()
  add_subdirectory (utils)
endif ()

#-----------------------------------------------------------------------------
# Option to build HDF5 Tools
#-----------------------------------------------------------------------------
if (EXISTS "${HDF5_SOURCE_DIR}/tools" AND IS_DIRECTORY "${HDF5_SOURCE_DIR}/tools")
  if (HDF5_BUILD_TOOLS)
    add_subdirectory (tools)
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Include filter plugins
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_PLUGIN_SUPPORT)
  if (${H5_LIBVER_DIR} EQUAL 16 OR HDF5_DEFAULT_API_VERSION MATCHES "v16")
    set (HDF5_ENABLE_PLUGIN_SUPPORT OFF CACHE BOOL "" FORCE)
    message (VERBOSE "Filter PLUGINs cannot be used with 1.6 API")
  else ()
    include (CMakePlugins.cmake)

    if (HDF5_PACKAGE_EXTLIBS AND NOT HDF5_NO_PACKAGES AND PLUGIN_FOUND)
      PACKAGE_PLUGIN_LIBRARY (${HDF5_ALLOW_EXTERNAL_SUPPORT})
    endif ()
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Option to build High Level API's
#-----------------------------------------------------------------------------
if (EXISTS "${HDF5_SOURCE_DIR}/hl" AND IS_DIRECTORY "${HDF5_SOURCE_DIR}/hl")
  if (HDF5_BUILD_HL_LIB)
    set (H5_INCLUDE_HL 1)
    add_subdirectory (hl)
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Option to build Fortran bindings/tests
# Make sure this appears before the CONFIGURE_FILE step
# so that fortran name mangling is detected before writing H5pubconf.h
#-----------------------------------------------------------------------------
# Set default name mangling : overridden by Fortran detection in fortran dir
set (H5_FC_FUNC  "H5_FC_FUNC(name,NAME) name ## _" CACHE INTERNAL "Fortran name mangling macro for C identifiers without underscores")
set (H5_FC_FUNC_ "H5_FC_FUNC_(name,NAME) name ## _" CACHE INTERNAL "Fortran name mangling macro for C identifiers with underscores")
if (EXISTS "${HDF5_SOURCE_DIR}/fortran" AND IS_DIRECTORY "${HDF5_SOURCE_DIR}/fortran")
  if (HDF5_BUILD_FORTRAN)
    add_subdirectory (fortran)
    if (HDF5_BUILD_HL_LIB)
      if (EXISTS "${HDF5_SOURCE_DIR}/hl/fortran" AND IS_DIRECTORY "${HDF5_SOURCE_DIR}/hl/fortran")
        #-- Build the High Level Fortran source codes
        add_subdirectory (hl/fortran)
      endif ()
    endif ()
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Option to build HDF5 C++ Library
#-----------------------------------------------------------------------------
if (EXISTS "${HDF5_SOURCE_DIR}/c++" AND IS_DIRECTORY "${HDF5_SOURCE_DIR}/c++")
  if (HDF5_BUILD_CPP_LIB)
    # check for unsupported options
    if (HDF5_ENABLE_PARALLEL)
      if (NOT HDF5_ALLOW_UNSUPPORTED)
        message (FATAL_ERROR " **** Parallel and C++ options are mutually exclusive, override with HDF5_ALLOW_UNSUPPORTED option **** ")
      else ()
        message (VERBOSE " **** Allowing unsupported Parallel and C++ options **** ")
      endif ()
    endif ()

    include (${HDF_CONFIG_DIR}/flags/HDFCompilerCXXFlags.cmake)

    add_subdirectory (c++)
    if (HDF5_BUILD_HL_LIB)
      if (EXISTS "${HDF5_SOURCE_DIR}/hl/c++" AND IS_DIRECTORY "${HDF5_SOURCE_DIR}/hl/c++")
        #-- Build the High Level Fortran source codes
        add_subdirectory (hl/c++)
      endif ()
    endif ()
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Generate the H5pubconf.h file containing user settings needed by compilation
#-----------------------------------------------------------------------------
configure_file (${HDF5_SRC_DIR}/H5pubconf.h.in ${HDF5_SRC_BINARY_DIR}/H5pubconf.h @ONLY)

#-----------------------------------------------------------------------------
# Option to build HDF5 Java Library
#-----------------------------------------------------------------------------
if (EXISTS "${HDF5_SOURCE_DIR}/java" AND IS_DIRECTORY "${HDF5_SOURCE_DIR}/java")
  if (HDF5_BUILD_JAVA)
    if (NOT BUILD_SHARED_LIBS)
      message (FATAL_ERROR "\nJava requires shared libraries!\n")
    else ()
      if (HDF5_ENABLE_MAVEN_DEPLOY)
        message (STATUS "Maven deployment enabled for ${HDF5_JAVA_ARTIFACT_ID}")
      endif ()
      add_subdirectory (java)
    endif ()
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Option to build examples
#-----------------------------------------------------------------------------
if (EXISTS "${HDF5_SOURCE_DIR}/HDF5Examples" AND IS_DIRECTORY "${HDF5_SOURCE_DIR}/HDF5Examples")
  if (HDF5_BUILD_EXAMPLES AND NOT HDF5_USE_SANITIZER)
    include (${HDF_RESOURCES_DIR}/HDF5ExampleCache.cmake)
    set (HDF5_VERSION ${HDF5_PACKAGE_VERSION})
    add_subdirectory (HDF5Examples)
  endif ()
endif ()

include (CMakeInstallation.cmake)
```

### `CMakePlugins.cmake`

```cmake
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.
#

# -----------------------------------------------------------------------------
# HDF5 CMake Filter Plugin Support Configuration
# -----------------------------------------------------------------------------
# This CMake module configures support for external filter plugins in HDF5.
# It provides options for enabling/disabling plugin support, selecting
# static/shared builds, and controlling how plugin dependencies are found or
# built (external, local, or via GIT/TGZ).
#
# Key Features:
# - Options to enable/disable plugin support and select external or local builds.
# - Support for building plugins externally (via GIT or TGZ) or using system libraries.
# - Handles configuration of plugin include directories, library targets, and CMake variables.
# - Sets up required variables for HDF5 to use filter plugins.
#
# Usage:
#   HDF5 includes this file from the main CMakeLists.txt if filter plugin support
#   in HDF5 is enabled (HDF5_ENABLE_PLUGIN_SUPPORT). Configure options as needed before
#   including this file.
#
# See comments throughout for details on each option and logic branch.
# -----------------------------------------------------------------------------

option (PLUGIN_USE_EXTERNAL "Use External Library Building for filter PLUGIN else search" OFF)
cmake_dependent_option (PLUGIN_USE_LOCALCONTENT "Use local file for PLUGIN FetchContent" OFF PLUGIN_USE_EXTERNAL OFF)

include (ExternalProject)

# -----------------------------------------------------------------------------
# Option for enabling filter plugin support by building the plugins from external sources
# -----------------------------------------------------------------------------
#option (HDF5_ALLOW_EXTERNAL_SUPPORT "Allow External Library Building (NO GIT TGZ)" "NO")
set (HDF5_ALLOW_EXTERNAL_SUPPORT "NO" CACHE STRING "Allow External Library Building (NO GIT TGZ)")
set_property (CACHE HDF5_ALLOW_EXTERNAL_SUPPORT PROPERTY STRINGS NO GIT TGZ)
if (HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "GIT" OR HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "TGZ")
  set (PLUGIN_USE_EXTERNAL ON CACHE BOOL "Use External Library Building for PLUGIN else search" FORCE)
  if (HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "GIT")
    set (PLUGIN_URL ${PLUGIN_GIT_URL} CACHE STRING "Path to PLUGIN git repository")
    set (PLUGIN_BRANCH ${PLUGIN_GIT_BRANCH})
  elseif (HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "TGZ")
    if (NOT H5PL_TGZPATH)
      set (H5PL_TGZPATH ${TGZPATH})
    endif ()
    if (NOT PLUGIN_USE_LOCALCONTENT)
      set (PLUGIN_URL ${PLUGIN_TGZ_ORIGPATH}/${PLUGIN_TGZ_NAME})
    else ()
      if (NOT H5PL_TGZPATH)
        set (H5PL_TGZPATH ${TGZPATH})
      endif ()
      set (PLUGIN_URL ${H5PL_TGZPATH}/${PLUGIN_TGZ_NAME})
      if (NOT EXISTS "${PLUGIN_URL}")
        set (HDF5_ENABLE_PLUGIN_SUPPORT OFF CACHE BOOL "" FORCE)
        message (VERBOSE "Filter PLUGIN file ${PLUGIN_URL} not found")
      endif ()
    endif ()
    message (VERBOSE "Filter PLUGIN file is ${PLUGIN_URL}")
  else ()
    set (PLUGIN_USE_EXTERNAL OFF CACHE BOOL "Use External Library Building for PLUGIN else search")
    message (VERBOSE "Filter PLUGIN not built")
  endif ()
endif ()

#-----------------------------------------------------------------------------
# Option for PLUGIN support
#-----------------------------------------------------------------------------
if (HDF5_ENABLE_PLUGIN_SUPPORT)
  if (NOT PLUGIN_USE_EXTERNAL) # This checks if plugins should be found on the system or built from an external source
    find_package (PLUGIN NAMES ${PLUGIN_PACKAGE_NAME}${HDF_PACKAGE_EXT})
    if (NOT PLUGIN_FOUND)
      find_package (PLUGIN) # Legacy find
    endif ()
  else ()
    if (HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "GIT" OR HDF5_ALLOW_EXTERNAL_SUPPORT MATCHES "TGZ")
      EXTERNAL_PLUGIN_LIBRARY (${HDF5_ALLOW_EXTERNAL_SUPPORT})
      message (STATUS "Filter PLUGIN is built")
    endif ()
  endif ()
  if (PLUGIN_FOUND)
    message (STATUS "Filter PLUGIN is ON")
  else ()
    set (HDF5_ENABLE_PLUGIN_SUPPORT OFF CACHE BOOL "" FORCE)
    message (FATAL_ERROR " PLUGIN support in HDF5 was enabled but not found")
  endif ()
endif ()
```

### `CMakePresets.json`

```json
{
  "version": 6,
  "include": [
    "config/cmake-presets/hidden-presets.json"
  ],
  "configurePresets": [
    {
      "name": "ci-base-tgz",
      "hidden": true,
      "inherits": "ci-base",
      "cacheVariables": {
        "HDF5_ALLOW_EXTERNAL_SUPPORT": {
          "type": "STRING",
          "value": "TGZ"
        },
        "TGZPATH": {
          "type": "PATH",
          "value": "${sourceParentDir}/temp"
        }
      }
    },
    {
      "name": "ci-CompressionVars",
      "hidden": true,
      "cacheVariables": {
        "ZLIB_PACKAGE_NAME": {
          "type": "STRING",
          "value": "zlib"
        },
        "ZLIB_TGZ_ORIGPATH": {
          "type": "STRING",
          "value": "https://github.com/madler/zlib/releases/download/v1.3.1"
        },
        "ZLIB_TGZ_NAME": {
          "type": "STRING",
          "value": "zlib-1.3.1.tar.gz"
        },
        "ZLIBNG_PACKAGE_NAME": {
          "type": "STRING",
          "value": "zlib-ng"
        },
        "ZLIBNG_TGZ_ORIGPATH": {
          "type": "STRING",
          "value": "https://github.com/zlib-ng/zlib-ng/archive/refs/tags"
        },
        "ZLIBNG_TGZ_NAME": {
          "type": "STRING",
          "value": "2.2.4.tar.gz"
        },
        "LIBAEC_PACKAGE_NAME": {
          "type": "STRING",
          "value": "libaec"
        },
        "LIBAEC_TGZ_ORIGPATH": {
          "type": "STRING",
          "value": "https://github.com/MathisRosenhauer/libaec/releases/download/v1.1.3"
        },
        "LIBAEC_TGZ_NAME": {
          "type": "STRING",
          "value": "libaec-1.1.3.tar.gz"
        }
      }
    },
    {
      "name": "ci-StdCompression",
      "hidden": true,
      "inherits": [
        "ci-base-tgz",
        "ci-CompressionVars"
      ],
      "cacheVariables": {
        "HDF5_PACKAGE_EXTLIBS": "ON",
        "HDF5_USE_ZLIB_NG": "OFF",
        "ZLIB_USE_LOCALCONTENT": "OFF",
        "LIBAEC_USE_LOCALCONTENT": "OFF",
        "HDF5_USE_ZLIB_STATIC": "ON",
        "HDF5_USE_LIBAEC_STATIC": "ON",
        "HDF5_ENABLE_SZIP_SUPPORT": "ON",
        "HDF5_ENABLE_ZLIB_SUPPORT": "ON"
      }
    },
    {
      "name": "ci-base-plugins",
      "hidden": true,
      "cacheVariables": {
        "BITGROOM_PACKAGE_NAME": {
          "type": "STRING",
          "value": "bitgroom"
        },
        "BITROUND_PACKAGE_NAME": {
          "type": "STRING",
          "value": "bitround"
        },
        "BSHUF_TGZ_NAME": {
          "type": "STRING",
          "value": "bitshuffle-0.5.2.tar.gz"
        },
        "BSHUF_PACKAGE_NAME": {
          "type": "STRING",
          "value": "bshuf"
        },
        "BLOSC_TGZ_NAME": {
          "type": "STRING",
          "value": "c-blosc-1.21.6.tar.gz"
        },
        "BLOSC_PACKAGE_NAME": {
          "type": "STRING",
          "value": "blosc"
        },
        "BLOSC_ZLIB_TGZ_NAME": {
          "type": "STRING",
          "value": "zlib-1.3.1.tar.gz"
        },
        "BLOSC_ZLIB_PACKAGE_NAME": {
          "type": "STRING",
          "value": "zlib"
        },
        "BLOSC2_TGZ_NAME": {
          "type": "STRING",
          "value": "c-blosc2-2.17.1.tar.gz"
        },
        "BLOSC2_PACKAGE_NAME": {
          "type": "STRING",
          "value": "blosc2"
        },
        "BLOSC2_ZLIB_TGZ_NAME": {
          "type": "STRING",
          "value": "zlib-1.3.1.tar.gz"
        },
        "BLOSC2_ZLIB_PACKAGE_NAME": {
          "type": "STRING",
          "value": "zlib"
        },
        "BZ2_TGZ_NAME": {
          "type": "STRING",
          "value": "bzip2-bzip2-1.0.8.tar.gz"
        },
        "BZ2_PACKAGE_NAME": {
          "type": "STRING",
          "value": "bz2"
        },
        "FPZIP_TGZ_NAME": {
          "type": "STRING",
          "value": "fpzip-1.3.0.tar.gz"
        },
        "FPZIP_PACKAGE_NAME": {
          "type": "STRING",
          "value": "fpzip"
        },
        "JPEG_TGZ_NAME": {
          "type": "STRING",
          "value": "jpegsrc.v9e.tar.gz"
        },
        "JPEG_PACKAGE_NAME": {
          "type": "STRING",
          "value": "jpeg"
        },
        "BUILD_LZ4_LIBRARY_SOURCE": "ON",
        "LZ4_TGZ_NAME": {
          "type": "STRING",
          "value": "lz4-1.10.0.tar.gz"
        },
        "LZ4_PACKAGE_NAME": {
          "type": "STRING",
          "value": "lz4"
        },
        "LZF_TGZ_NAME": {
          "type": "STRING",
          "value": "liblzf-3.6.tar.gz"
        },
        "LZF_PACKAGE_NAME": {
          "type": "STRING",
          "value": "lzf"
        },
        "SZ_TGZ_NAME": {
          "type": "STRING",
          "value": "SZ-2.1.12.5.tar.gz"
        },
        "SZ_PACKAGE_NAME": {
          "type": "STRING",
          "value": "SZ"
        },
        "ZFP_TGZ_NAME": {
          "type": "STRING",
          "value": "zfp-1.0.1.tar.gz"
        },
        "ZFP_PACKAGE_NAME": {
          "type": "STRING",
          "value": "zfp"
        },
        "ZSTD_TGZ_NAME": {
          "type": "STRING",
          "value": "zstd-1.5.7.tar.gz"
        },
        "ZSTD_PACKAGE_NAME": {
          "type": "STRING",
          "value": "zstd"
        }
      }
    },
    {
      "name": "ci-PluginsVars",
      "hidden": true,
      "cacheVariables": {
        "HDF5_ENABLE_PLUGIN_SUPPORT": "ON",
        "H5PL_ALLOW_EXTERNAL_SUPPORT": {
          "type": "STRING",
          "value": "TGZ"
        },
        "PLUGIN_PACKAGE_NAME": {
          "type": "STRING",
          "value": "pl"
        },
        "PLUGIN_TGZ_ORIGPATH": {
          "type": "STRING",
          "value": "https://github.com/HDFGroup/hdf5_plugins/releases/download/snapshot"
        },
        "PLUGIN_TGZ_NAME": {
          "type": "STRING",
          "value": "hdf5_plugins-master.tar.gz"
        }
      }
    },
    {
      "name": "ci-StdPlugins",
      "hidden": true,
      "inherits": [
        "ci-base-plugins",
        "ci-PluginsVars",
        "ci-base-tgz"
      ],
      "cacheVariables": {
        "PLUGIN_USE_LOCALCONTENT": "OFF"
      }
    },
    {
      "name": "ci-StdExamples",
      "hidden": true,
      "inherits": [
        "ci-base",
        "ci-base-tgz"
      ],
      "cacheVariables": {
        "HDF5_PACK_EXAMPLES": "ON",
        "EXAMPLES_DOWNLOAD": "ON"
      }
    },
    {
      "name": "ci-S3",
      "hidden": true,
      "cacheVariables": {
        "HDF5_ENABLE_ROS3_VFD": "ON",
        "HDF5_ENABLE_HDFS": "OFF"
      }
    },
    {
      "name": "ci-S3-proxy",
      "hidden": true,
      "cacheVariables": {
        "HDF5_ENABLE_DOCKER_PROXY": "ON",
        "HDF5_ENABLE_ROS3_VFD": "ON",
        "HDF5_ENABLE_HDFS": "OFF"
      }
    },
    {
      "name": "ci-Maven",
      "hidden": true,
      "cacheVariables": {
        "HDF5_ENABLE_MAVEN_DEPLOY": "ON",
        "HDF5_MAVEN_SNAPSHOT": "OFF"
      }
    },
    {
      "name": "ci-Maven-Snapshot",
      "hidden": true,
      "cacheVariables": {
        "HDF5_ENABLE_MAVEN_DEPLOY": "ON",
        "HDF5_MAVEN_SNAPSHOT": "ON"
      }
    },
    {
      "name": "ci-Maven-Minimal",
      "hidden": true,
      "cacheVariables": {
        "HDF5_BUILD_EXAMPLES": "OFF",
        "BUILD_TESTING": "OFF",
        "HDF5_BUILD_TOOLS": "OFF",
        "HDF5_BUILD_FORTRAN": "OFF",
        "HDF5_BUILD_CPP_LIB": "OFF",
        "HDF5_BUILD_JAVA": "ON",
        "HDF5_ENABLE_MAVEN_DEPLOY": "ON",
        "HDF5_MAVEN_SNAPSHOT": "OFF"
      }
    },
    {
      "name": "ci-Maven-Minimal-Snapshot",
      "hidden": true,
      "inherits": [
        "ci-Maven-Minimal"
      ],
      "cacheVariables": {
        "HDF5_MAVEN_SNAPSHOT": "ON"
      }
    },
    {
      "name": "ci-Maven-Testing",
      "description": "Maven Configuration with Testing Enabled - For debugging FFM/JNI issues",
      "hidden": true,
      "cacheVariables": {
        "HDF5_BUILD_EXAMPLES": "ON",
        "BUILD_TESTING": "ON",
        "HDF5_BUILD_TOOLS": "ON",
        "HDF5_BUILD_FORTRAN": "OFF",
        "HDF5_BUILD_CPP_LIB": "OFF",
        "HDF5_BUILD_JAVA": "ON",
        "HDF5_ENABLE_MAVEN_DEPLOY": "OFF",
        "HDF5_MAVEN_SNAPSHOT": "OFF"
      }
    },
    {
      "name": "ci-StdShar",
      "hidden": true,
      "inherits": [
        "ci-StdCompression",
        "ci-StdExamples",
        "ci-StdPlugins"
      ],
      "cacheVariables": {
        "HDF_PACKAGE_NAMESPACE": {
          "type": "STRING",
          "value": "hdf5::"
        },
        "HDF5_INSTALL_MOD_FORTRAN": "NO",
        "HDF5_ENABLE_ALL_WARNINGS": "ON",
        "HDF5_MINGW_STATIC_GCC_LIBS": "ON",
        "HDF_TEST_EXPRESS": "2"
      }
    },
    {
      "name": "ci-StdShar-MSVC",
      "description": "MSVC Standard Config for x64 (Release)",
      "inherits": [
        "ci-x64-Release-MSVC",
        "ci-CPP",
        "ci-Java",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-MSVC-FFM",
      "description": "MSVC Standard Config for x64 with Java FFM (Requires Java 25+) (Release)",
      "inherits": [
        "ci-x64-Release-MSVC",
        "ci-CPP",
        "ci-Java-FFM",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-MSVC-Fortran",
      "description": "MSVC Standard Config for x64 (Release)",
      "inherits": [
        "ci-x64-Release-MSVC",
        "ci-CPP",
        "ci-Fortran",
        "ci-Java",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-MSVC-Fortran-FFM",
      "description": "MSVC Standard Config for x64 with Java FFM (Requires Java 25+) (Release)",
      "inherits": [
        "ci-x64-Release-MSVC",
        "ci-CPP",
        "ci-Fortran",
        "ci-Java-FFM",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-Clang",
      "description": "Clang Standard Config for x64 (Release)",
      "inherits": [
        "ci-x64-Release-Clang",
        "ci-CPP",
        "ci-Fortran",
        "ci-Java",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-Clang-FFM",
      "description": "Clang Standard Config for x64 with Java FFM (Requires Java 25+) (Release)",
      "inherits": [
        "ci-x64-Release-Clang",
        "ci-CPP",
        "ci-Fortran",
        "ci-Java-FFM",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-macos-Clang",
      "description": "Clang Standard Config for macos (Release)",
      "inherits": [
        "ci-macos-Release-Clang",
        "ci-CPP",
        "ci-Java",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-macos-Clang-FFM",
      "description": "Clang Standard Config for macos for x64 with Java FFM (Requires Java 25+) (Release)",
      "inherits": [
        "ci-macos-Release-Clang",
        "ci-CPP",
        "ci-Java-FFM",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-macos-GNUC",
      "description": "GNUC Standard Config for macos (Release)",
      "inherits": [
        "ci-macos-Release-GNUC",
        "ci-CPP",
        "ci-Java",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-macos-GNUC-FFM",
      "description": "GNUC Standard Config for macos for x64 with Java FFM (Requires Java 25+) (Release)",
      "inherits": [
        "ci-macos-Release-GNUC",
        "ci-CPP",
        "ci-Java-FFM",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-GNUC",
      "description": "GNUC Standard Config for x64 with Java JNI (Java 11+)",
      "inherits": [
        "ci-x64-Release-GNUC",
        "ci-CPP",
        "ci-Fortran",
        "ci-Java",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-GNUC-FFM",
      "description": "GNUC Standard Config for x64 with Java FFM (Requires Java 25+)",
      "inherits": [
        "ci-x64-Release-GNUC",
        "ci-CPP",
        "ci-Fortran",
        "ci-Java-FFM",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-GNUC-S3",
      "description": "GNUC S3 Config for x64 (Release)",
      "inherits": [
        "ci-StdShar-GNUC",
        "ci-S3"
      ]
    },
    {
      "name": "ci-StdShar-Intel",
      "description": "Intel Standard Config for x64 (Release)",
      "inherits": [
        "ci-x64-Release-Intel",
        "ci-CPP",
        "ci-Fortran",
        "ci-Java",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-Intel-FFM",
      "description": "Intel Standard Config for x64 with Java FFM (Requires Java 25+) (Release)",
      "inherits": [
        "ci-x64-Release-Intel",
        "ci-CPP",
        "ci-Fortran",
        "ci-Java-FFM",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven",
      "description": "GNUC Minimal Config for x64 with Maven deployment (JNI - Java 8+)",
      "inherits": [
        "ci-x64-Release-GNUC",
        "ci-Java",
        "ci-Maven-Minimal"
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven-Snapshot",
      "description": "GNUC Minimal Config for x64 with Maven snapshot deployment (JNI - Java 8+)",
      "inherits": [
        "ci-x64-Release-GNUC",
        "ci-Java",
        "ci-Maven-Minimal-Snapshot"
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven",
      "description": "MSVC Minimal Config for x64 with Maven deployment (JNI - Java 8+)",
      "inherits": [
        "ci-x64-Release-MSVC",
        "ci-Java",
        "ci-Maven-Minimal"
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven-Snapshot",
      "description": "MSVC Minimal Config for x64 with Maven snapshot deployment (JNI - Java 8+)",
      "inherits": [
        "ci-x64-Release-MSVC",
        "ci-Java",
        "ci-Maven-Minimal-Snapshot"
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven",
      "description": "Clang Minimal Config for x64 with Maven deployment (JNI - Java 8+)",
      "inherits": [
        "ci-x64-Release-Clang",
        "ci-Java",
        "ci-Maven-Minimal"
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven-Snapshot",
      "description": "Clang Minimal Config for x64 with Maven snapshot deployment (JNI - Java 8+)",
      "inherits": [
        "ci-x64-Release-Clang",
        "ci-Java",
        "ci-Maven-Minimal-Snapshot"
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven-FFM",
      "description": "GNUC Minimal Config for x64 with Maven deployment (FFM - Requires Java 25+)",
      "inherits": [
        "ci-x64-Release-GNUC",
        "ci-Java-FFM",
        "ci-Maven-Minimal"
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven-FFM-Snapshot",
      "description": "GNUC Minimal Config for x64 with Maven snapshot deployment (FFM - Requires Java 25+)",
      "inherits": [
        "ci-x64-Release-GNUC",
        "ci-Java-FFM",
        "ci-Maven-Minimal-Snapshot"
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven-FFM",
      "description": "MSVC Minimal Config for x64 with Maven deployment (FFM - Requires Java 25+)",
      "inherits": [
        "ci-x64-Release-MSVC",
        "ci-Java-FFM",
        "ci-Maven-Minimal"
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven-FFM-Snapshot",
      "description": "MSVC Minimal Config for x64 with Maven snapshot deployment (FFM - Requires Java 25+)",
      "inherits": [
        "ci-x64-Release-MSVC",
        "ci-Java-FFM",
        "ci-Maven-Minimal-Snapshot"
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven-FFM",
      "description": "Clang Minimal Config for x64 with Maven deployment (FFM - Requires Java 25+)",
      "inherits": [
        "ci-x64-Release-Clang",
        "ci-Java-FFM",
        "ci-Maven-Minimal"
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven-FFM-Snapshot",
      "description": "Clang Minimal Config for x64 with Maven snapshot deployment (FFM - Requires Java 25+)",
      "inherits": [
        "ci-x64-Release-Clang",
        "ci-Java-FFM",
        "ci-Maven-Minimal-Snapshot"
      ]
    },
    {
      "name": "ci-Testing-GNUC-FFM",
      "description": "GNUC Test-Enabled Config for FFM (Java 25+) - For debugging Windows FFM issues",
      "inherits": [
        "ci-x64-Release-GNUC",
        "ci-Java-FFM",
        "ci-Maven-Testing"
      ]
    },
    {
      "name": "ci-Testing-MSVC-FFM",
      "description": "MSVC Test-Enabled Config for FFM (Java 25+) - For debugging Windows FFM issues",
      "inherits": [
        "ci-x64-Release-MSVC",
        "ci-Java-FFM",
        "ci-Maven-Testing"
      ]
    },
    {
      "name": "ci-Testing-Clang-FFM",
      "description": "Clang Test-Enabled Config for FFM (Java 25+) - For debugging macOS FFM issues",
      "inherits": [
        "ci-x64-Release-Clang",
        "ci-Java-FFM",
        "ci-Maven-Testing"
      ]
    },
    {
      "name": "ci-Testing-GNUC-JNI",
      "description": "GNUC Test-Enabled Config for JNI (Java 11+) - For debugging JNI issues",
      "inherits": [
        "ci-x64-Release-GNUC",
        "ci-Java",
        "ci-Maven-Testing"
      ]
    },
    {
      "name": "ci-Testing-MSVC-JNI",
      "description": "MSVC Test-Enabled Config for JNI (Java 11+) - For debugging Windows JNI issues",
      "inherits": [
        "ci-x64-Release-MSVC",
        "ci-Java",
        "ci-Maven-Testing"
      ]
    },
    {
      "name": "ci-Testing-Clang-JNI",
      "description": "Clang Test-Enabled Config for JNI (Java 11+) - For debugging macOS JNI issues",
      "inherits": [
        "ci-x64-Release-Clang",
        "ci-Java",
        "ci-Maven-Testing"
      ]
    }
  ],
  "buildPresets": [
    {
      "name": "ci-StdShar-MSVC",
      "description": "MSVC Standard Build for x64 (Release)",
      "configurePreset": "ci-StdShar-MSVC",
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-StdShar-MSVC-FFM",
      "description": "MSVC Standard Build for x64 with Java FFM (Requires Java 25+) (Release)",
      "configurePreset": "ci-StdShar-MSVC-FFM",
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-StdShar-Clang",
      "description": "Clang Standard Build for x64 (Release)",
      "configurePreset": "ci-StdShar-Clang",
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-StdShar-Clang-FFM",
      "description": "Clang Standard Build for x64 with Java FFM (Requires Java 25+) (Release)",
      "configurePreset": "ci-StdShar-Clang-FFM",
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-StdShar-macos-Clang",
      "description": "Clang Standard Build for macos (Release)",
      "configurePreset": "ci-StdShar-macos-Clang",
      "inherits": [
        "ci-macos-Release-Clang"
      ]
    },
    {
      "name": "ci-StdShar-macos-Clang-FFM",
      "description": "Clang Standard Build for macos with Java FFM (Requires Java 25+) (Release)",
      "configurePreset": "ci-StdShar-macos-Clang-FFM",
      "inherits": [
        "ci-macos-Release-Clang"
      ]
    },
    {
      "name": "ci-StdShar-macos-GNUC-FFM",
      "description": "GNUC Standard Build for macos (Release)",
      "configurePreset": "ci-StdShar-macos-GNUC-FFM",
      "verbose": true,
      "inherits": [
        "ci-macos-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-macos-GNUC",
      "description": "GNUC Standard Build for macos with Java FFM (Requires Java 25+) (Release)",
      "configurePreset": "ci-StdShar-macos-GNUC",
      "verbose": true,
      "inherits": [
        "ci-macos-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-GNUC",
      "description": "GNUC Standard Build for x64 (Release)",
      "configurePreset": "ci-StdShar-GNUC",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-GNUC-FFM",
      "description": "GNUC Standard Build for x64 with Java FFM (Requires Java 25+) (Release)",
      "configurePreset": "ci-StdShar-GNUC-FFM",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-GNUC-S3",
      "description": "GNUC S3 Build for x64 (Release)",
      "configurePreset": "ci-StdShar-GNUC-S3",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-Intel",
      "description": "Intel Standard Build for x64 (Release)",
      "configurePreset": "ci-StdShar-Intel",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-Intel"
      ]
    },
    {
      "name": "ci-StdShar-Intel-FFM",
      "description": "Intel Standard Build for x64 with Java FFM (Requires Java 25+) (Release)",
      "configurePreset": "ci-StdShar-Intel-FFM",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-Intel"
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven",
      "description": "GNUC Minimal Build for x64 with Maven deployment (JNI - Java 8+)",
      "configurePreset": "ci-MinShar-GNUC-Maven",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven-Snapshot",
      "description": "GNUC Minimal Build for x64 with Maven snapshot deployment (JNI - Java 8+)",
      "configurePreset": "ci-MinShar-GNUC-Maven-Snapshot",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven",
      "description": "MSVC Minimal Build for x64 with Maven deployment (JNI - Java 8+)",
      "configurePreset": "ci-MinShar-MSVC-Maven",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven-Snapshot",
      "description": "MSVC Minimal Build for x64 with Maven snapshot deployment (JNI - Java 8+)",
      "configurePreset": "ci-MinShar-MSVC-Maven-Snapshot",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven",
      "description": "Clang Minimal Build for x64 with Maven deployment (JNI - Java 8+)",
      "configurePreset": "ci-MinShar-Clang-Maven",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven-Snapshot",
      "description": "Clang Minimal Build for x64 with Maven snapshot deployment (JNI - Java 8+)",
      "configurePreset": "ci-MinShar-Clang-Maven-Snapshot",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven-FFM",
      "description": "GNUC Minimal Build for x64 with Maven deployment (FFM - Requires Java 25+)",
      "configurePreset": "ci-MinShar-GNUC-Maven-FFM",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven-FFM-Snapshot",
      "description": "GNUC Minimal Build for x64 with Maven snapshot deployment (FFM - Requires Java 25+)",
      "configurePreset": "ci-MinShar-GNUC-Maven-FFM-Snapshot",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven-FFM",
      "description": "MSVC Minimal Build for x64 with Maven deployment (FFM - Requires Java 25+)",
      "configurePreset": "ci-MinShar-MSVC-Maven-FFM",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven-FFM-Snapshot",
      "description": "MSVC Minimal Build for x64 with Maven snapshot deployment (FFM - Requires Java 25+)",
      "configurePreset": "ci-MinShar-MSVC-Maven-FFM-Snapshot",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven-FFM",
      "description": "Clang Minimal Build for x64 with Maven deployment (FFM - Requires Java 25+)",
      "configurePreset": "ci-MinShar-Clang-Maven-FFM",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven-FFM-Snapshot",
      "description": "Clang Minimal Build for x64 with Maven snapshot deployment (FFM - Requires Java 25+)",
      "configurePreset": "ci-MinShar-Clang-Maven-FFM-Snapshot",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-Testing-GNUC-FFM",
      "description": "GNUC Test-Enabled Build for FFM (Java 25+)",
      "configurePreset": "ci-Testing-GNUC-FFM",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-Testing-MSVC-FFM",
      "description": "MSVC Test-Enabled Build for FFM (Java 25+)",
      "configurePreset": "ci-Testing-MSVC-FFM",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-Testing-Clang-FFM",
      "description": "Clang Test-Enabled Build for FFM (Java 25+)",
      "configurePreset": "ci-Testing-Clang-FFM",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-Testing-GNUC-JNI",
      "description": "GNUC Test-Enabled Build for JNI (Java 11+)",
      "configurePreset": "ci-Testing-GNUC-JNI",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-Testing-MSVC-JNI",
      "description": "MSVC Test-Enabled Build for JNI (Java 11+)",
      "configurePreset": "ci-Testing-MSVC-JNI",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-Testing-Clang-JNI",
      "description": "Clang Test-Enabled Build for JNI (Java 11+)",
      "configurePreset": "ci-Testing-Clang-JNI",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    }
  ],
  "testPresets": [
    {
      "name": "ci-StdShar-MSVC",
      "configurePreset": "ci-StdShar-MSVC",
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-StdShar-MSVC-FFM",
      "configurePreset": "ci-StdShar-MSVC-FFM",
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-StdShar-Clang",
      "configurePreset": "ci-StdShar-Clang",
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-StdShar-Clang-FFM",
      "configurePreset": "ci-StdShar-Clang-FFM",
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-StdShar-macos-Clang",
      "configurePreset": "ci-StdShar-macos-Clang",
      "inherits": [
        "ci-macos-Release-Clang"
      ],
      "execution": {
        "noTestsAction": "error",
        "timeout": 180,
        "jobs": 2
      }
    },
    {
      "name": "ci-StdShar-macos-Clang-FFM",
      "configurePreset": "ci-StdShar-macos-Clang-FFM",
      "inherits": [
        "ci-macos-Release-Clang"
      ],
      "execution": {
        "noTestsAction": "error",
        "timeout": 180,
        "jobs": 2
      }
    },
    {
      "name": "ci-StdShar-macos-GNUC",
      "configurePreset": "ci-StdShar-macos-GNUC",
      "inherits": [
        "ci-macos-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-macos-GNUC-FFM",
      "configurePreset": "ci-StdShar-macos-GNUC-FFM",
      "inherits": [
        "ci-macos-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-GNUC",
      "configurePreset": "ci-StdShar-GNUC",
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-GNUC-FFM",
      "configurePreset": "ci-StdShar-GNUC-FFM",
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-GNUC-S3",
      "configurePreset": "ci-StdShar-GNUC-S3",
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-win-Intel",
      "configurePreset": "ci-StdShar-Intel",
      "inherits": [
        "ci-x64-Release-Intel"
      ],
      "condition": {
        "type": "equals",
        "lhs": "${hostSystemName}",
        "rhs": "Windows"
      }
    },
    {
      "name": "ci-StdShar-win-Intel-FFM",
      "configurePreset": "ci-StdShar-Intel-FFM",
      "inherits": [
        "ci-x64-Release-Intel"
      ],
      "condition": {
        "type": "equals",
        "lhs": "${hostSystemName}",
        "rhs": "Windows"
      }
    },
    {
      "name": "ci-StdShar-Intel",
      "configurePreset": "ci-StdShar-Intel",
      "inherits": [
        "ci-x64-Release-Intel"
      ]
    },
    {
      "name": "ci-StdShar-Intel-FFM",
      "configurePreset": "ci-StdShar-Intel-FFM",
      "inherits": [
        "ci-x64-Release-Intel"
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven",
      "configurePreset": "ci-MinShar-GNUC-Maven",
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven-Snapshot",
      "configurePreset": "ci-MinShar-GNUC-Maven-Snapshot",
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven",
      "configurePreset": "ci-MinShar-MSVC-Maven",
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven-Snapshot",
      "configurePreset": "ci-MinShar-MSVC-Maven-Snapshot",
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven",
      "configurePreset": "ci-MinShar-Clang-Maven",
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven-Snapshot",
      "configurePreset": "ci-MinShar-Clang-Maven-Snapshot",
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven-FFM",
      "configurePreset": "ci-MinShar-GNUC-Maven-FFM",
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven-FFM-Snapshot",
      "configurePreset": "ci-MinShar-GNUC-Maven-FFM-Snapshot",
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven-FFM",
      "configurePreset": "ci-MinShar-MSVC-Maven-FFM",
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven-FFM-Snapshot",
      "configurePreset": "ci-MinShar-MSVC-Maven-FFM-Snapshot",
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven-FFM",
      "configurePreset": "ci-MinShar-Clang-Maven-FFM",
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven-FFM-Snapshot",
      "configurePreset": "ci-MinShar-Clang-Maven-FFM-Snapshot",
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-Testing-GNUC-FFM",
      "configurePreset": "ci-Testing-GNUC-FFM",
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-Testing-MSVC-FFM",
      "configurePreset": "ci-Testing-MSVC-FFM",
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-Testing-Clang-FFM",
      "configurePreset": "ci-Testing-Clang-FFM",
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-Testing-GNUC-JNI",
      "configurePreset": "ci-Testing-GNUC-JNI",
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-Testing-MSVC-JNI",
      "configurePreset": "ci-Testing-MSVC-JNI",
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-Testing-Clang-JNI",
      "configurePreset": "ci-Testing-Clang-JNI",
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    }
  ],
  "packagePresets": [
    {
      "name": "ci-StdShar-MSVC",
      "configurePreset": "ci-StdShar-MSVC",
      "inherits": "ci-x64-Release-MSVC"
    },
    {
      "name": "ci-StdShar-MSVC-FFM",
      "configurePreset": "ci-StdShar-MSVC-FFM",
      "inherits": "ci-x64-Release-MSVC"
    },
    {
      "name": "ci-StdShar-Clang",
      "configurePreset": "ci-StdShar-Clang",
      "inherits": "ci-x64-Release-Clang"
    },
    {
      "name": "ci-StdShar-Clang-FFM",
      "configurePreset": "ci-StdShar-Clang-FFM",
      "inherits": "ci-x64-Release-Clang"
    },
    {
      "name": "ci-StdShar-macos-Clang",
      "configurePreset": "ci-StdShar-macos-Clang",
      "inherits": "ci-macos-Release-Clang"
    },
    {
      "name": "ci-StdShar-macos-Clang-FFM",
      "configurePreset": "ci-StdShar-macos-Clang-FFM",
      "inherits": "ci-macos-Release-Clang"
    },
    {
      "name": "ci-StdShar-macos-GNUC",
      "configurePreset": "ci-StdShar-macos-GNUC",
      "inherits": "ci-macos-Release-GNUC"
    },
    {
      "name": "ci-StdShar-macos-GNUC-FFM",
      "configurePreset": "ci-StdShar-macos-GNUC-FFM",
      "inherits": "ci-macos-Release-GNUC"
    },
    {
      "name": "ci-StdShar-GNUC",
      "configurePreset": "ci-StdShar-GNUC",
      "inherits": "ci-x64-Release-GNUC"
    },
    {
      "name": "ci-StdShar-GNUC-FFM",
      "configurePreset": "ci-StdShar-GNUC-FFM",
      "inherits": "ci-x64-Release-GNUC"
    },
    {
      "name": "ci-StdShar-GNUC-S3",
      "configurePreset": "ci-StdShar-GNUC-S3",
      "inherits": "ci-x64-Release-GNUC"
    },
    {
      "name": "ci-StdShar-Intel",
      "configurePreset": "ci-StdShar-Intel",
      "inherits": "ci-x64-Release-Intel"
    },
    {
      "name": "ci-StdShar-Intel-FFM",
      "configurePreset": "ci-StdShar-Intel-FFM",
      "inherits": "ci-x64-Release-Intel"
    },
    {
      "name": "ci-MinShar-GNUC-Maven",
      "configurePreset": "ci-MinShar-GNUC-Maven",
      "inherits": "ci-x64-Release-GNUC"
    },
    {
      "name": "ci-MinShar-GNUC-Maven-Snapshot",
      "configurePreset": "ci-MinShar-GNUC-Maven-Snapshot",
      "inherits": "ci-x64-Release-GNUC"
    },
    {
      "name": "ci-MinShar-MSVC-Maven",
      "configurePreset": "ci-MinShar-MSVC-Maven",
      "inherits": "ci-x64-Release-MSVC"
    },
    {
      "name": "ci-MinShar-MSVC-Maven-Snapshot",
      "configurePreset": "ci-MinShar-MSVC-Maven-Snapshot",
      "inherits": "ci-x64-Release-MSVC"
    },
    {
      "name": "ci-MinShar-Clang-Maven",
      "configurePreset": "ci-MinShar-Clang-Maven",
      "inherits": "ci-x64-Release-Clang"
    },
    {
      "name": "ci-MinShar-Clang-Maven-Snapshot",
      "configurePreset": "ci-MinShar-Clang-Maven-Snapshot",
      "inherits": "ci-x64-Release-Clang"
    },
    {
      "name": "ci-MinShar-GNUC-Maven-FFM",
      "configurePreset": "ci-MinShar-GNUC-Maven-FFM",
      "inherits": "ci-x64-Release-GNUC"
    },
    {
      "name": "ci-MinShar-GNUC-Maven-FFM-Snapshot",
      "configurePreset": "ci-MinShar-GNUC-Maven-FFM-Snapshot",
      "inherits": "ci-x64-Release-GNUC"
    },
    {
      "name": "ci-MinShar-MSVC-Maven-FFM",
      "configurePreset": "ci-MinShar-MSVC-Maven-FFM",
      "inherits": "ci-x64-Release-MSVC"
    },
    {
      "name": "ci-MinShar-MSVC-Maven-FFM-Snapshot",
      "configurePreset": "ci-MinShar-MSVC-Maven-FFM-Snapshot",
      "inherits": "ci-x64-Release-MSVC"
    },
    {
      "name": "ci-MinShar-Clang-Maven-FFM",
      "configurePreset": "ci-MinShar-Clang-Maven-FFM",
      "inherits": "ci-x64-Release-Clang"
    },
    {
      "name": "ci-MinShar-Clang-Maven-FFM-Snapshot",
      "configurePreset": "ci-MinShar-Clang-Maven-FFM-Snapshot",
      "inherits": "ci-x64-Release-Clang"
    }
  ],
  "workflowPresets": [
    {
      "name": "ci-StdShar-MSVC",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-MSVC"
        },
        {
          "type": "build",
          "name": "ci-StdShar-MSVC"
        },
        {
          "type": "test",
          "name": "ci-StdShar-MSVC"
        },
        {
          "type": "package",
          "name": "ci-StdShar-MSVC"
        }
      ]
    },
   {
      "name": "ci-StdShar-MSVC-FFM",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-MSVC-FFM"
        },
        {
          "type": "build",
          "name": "ci-StdShar-MSVC-FFM"
        },
        {
          "type": "test",
          "name": "ci-StdShar-MSVC-FFM"
        },
        {
          "type": "package",
          "name": "ci-StdShar-MSVC-FFM"
        }
      ]
    },
    {
      "name": "ci-StdShar-Clang",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-Clang"
        },
        {
          "type": "build",
          "name": "ci-StdShar-Clang"
        },
        {
          "type": "test",
          "name": "ci-StdShar-Clang"
        },
        {
          "type": "package",
          "name": "ci-StdShar-Clang"
        }
      ]
    },
    {
      "name": "ci-StdShar-Clang-FFM",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-Clang-FFM"
        },
        {
          "type": "build",
          "name": "ci-StdShar-Clang-FFM"
        },
        {
          "type": "test",
          "name": "ci-StdShar-Clang-FFM"
        },
        {
          "type": "package",
          "name": "ci-StdShar-Clang-FFM"
        }
      ]
    },
    {
      "name": "ci-StdShar-macos-Clang",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-macos-Clang"
        },
        {
          "type": "build",
          "name": "ci-StdShar-macos-Clang"
        },
        {
          "type": "test",
          "name": "ci-StdShar-macos-Clang"
        },
        {
          "type": "package",
          "name": "ci-StdShar-macos-Clang"
        }
      ]
    },
    {
      "name": "ci-StdShar-macos-Clang-FFM",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-macos-Clang-FFM"
        },
        {
          "type": "build",
          "name": "ci-StdShar-macos-Clang-FFM"
        },
        {
          "type": "test",
          "name": "ci-StdShar-macos-Clang-FFM"
        },
        {
          "type": "package",
          "name": "ci-StdShar-macos-Clang-FFM"
        }
      ]
    },
    {
      "name": "ci-StdShar-macos-GNUC",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-macos-GNUC"
        },
        {
          "type": "build",
          "name": "ci-StdShar-macos-GNUC"
        },
        {
          "type": "test",
          "name": "ci-StdShar-macos-GNUC"
        },
        {
          "type": "package",
          "name": "ci-StdShar-macos-GNUC"
        }
      ]
    },
    {
      "name": "ci-StdShar-macos-GNUC-FFM",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-macos-GNUC-FFM"
        },
        {
          "type": "build",
          "name": "ci-StdShar-macos-GNUC-FFM"
        },
        {
          "type": "test",
          "name": "ci-StdShar-macos-GNUC-FFM"
        },
        {
          "type": "package",
          "name": "ci-StdShar-macos-GNUC-FFM"
        }
      ]
    },
    {
      "name": "ci-StdShar-GNUC",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-GNUC"
        },
        {
          "type": "build",
          "name": "ci-StdShar-GNUC"
        },
        {
          "type": "test",
          "name": "ci-StdShar-GNUC"
        },
        {
          "type": "package",
          "name": "ci-StdShar-GNUC"
        }
      ]
    },
    {
      "name": "ci-StdShar-GNUC-FFM",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-GNUC-FFM"
        },
        {
          "type": "build",
          "name": "ci-StdShar-GNUC-FFM"
        },
        {
          "type": "test",
          "name": "ci-StdShar-GNUC-FFM"
        },
        {
          "type": "package",
          "name": "ci-StdShar-GNUC-FFM"
        }
      ]
    },
    {
      "name": "ci-StdShar-GNUC-S3",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-GNUC-S3"
        },
        {
          "type": "build",
          "name": "ci-StdShar-GNUC-S3"
        },
        {
          "type": "test",
          "name": "ci-StdShar-GNUC-S3"
        },
        {
          "type": "package",
          "name": "ci-StdShar-GNUC-S3"
        }
      ]
    },
    {
      "name": "ci-StdShar-Intel",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-Intel"
        },
        {
          "type": "build",
          "name": "ci-StdShar-Intel"
        },
        {
          "type": "test",
          "name": "ci-StdShar-Intel"
        },
        {
          "type": "package",
          "name": "ci-StdShar-Intel"
        }
      ]
    },
    {
      "name": "ci-StdShar-Intel-FFM",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-Intel-FFM"
        },
        {
          "type": "build",
          "name": "ci-StdShar-Intel-FFM"
        },
        {
          "type": "test",
          "name": "ci-StdShar-Intel-FFM"
        },
        {
          "type": "package",
          "name": "ci-StdShar-Intel-FFM"
        }
      ]
    },
    {
      "name": "ci-StdShar-win-Intel",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-Intel"
        },
        {
          "type": "build",
          "name": "ci-StdShar-Intel"
        },
        {
          "type": "test",
          "name": "ci-StdShar-win-Intel"
        },
        {
          "type": "package",
          "name": "ci-StdShar-Intel"
        }
      ]
    },
    {
      "name": "ci-StdShar-win-Intel-FFM",
      "steps": [
        {
          "type": "configure",
          "name": "ci-StdShar-Intel-FFM"
        },
        {
          "type": "build",
          "name": "ci-StdShar-Intel-FFM"
        },
        {
          "type": "test",
          "name": "ci-StdShar-win-Intel-FFM"
        },
        {
          "type": "package",
          "name": "ci-StdShar-Intel-FFM"
        }
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven",
      "steps": [
        {
          "type": "configure",
          "name": "ci-MinShar-GNUC-Maven"
        },
        {
          "type": "build",
          "name": "ci-MinShar-GNUC-Maven"
        },
        {
          "type": "package",
          "name": "ci-MinShar-GNUC-Maven"
        }
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven-Snapshot",
      "steps": [
        {
          "type": "configure",
          "name": "ci-MinShar-GNUC-Maven-Snapshot"
        },
        {
          "type": "build",
          "name": "ci-MinShar-GNUC-Maven-Snapshot"
        },
        {
          "type": "package",
          "name": "ci-MinShar-GNUC-Maven-Snapshot"
        }
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven",
      "steps": [
        {
          "type": "configure",
          "name": "ci-MinShar-MSVC-Maven"
        },
        {
          "type": "build",
          "name": "ci-MinShar-MSVC-Maven"
        },
        {
          "type": "package",
          "name": "ci-MinShar-MSVC-Maven"
        }
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven-Snapshot",
      "steps": [
        {
          "type": "configure",
          "name": "ci-MinShar-MSVC-Maven-Snapshot"
        },
        {
          "type": "build",
          "name": "ci-MinShar-MSVC-Maven-Snapshot"
        },
        {
          "type": "package",
          "name": "ci-MinShar-MSVC-Maven-Snapshot"
        }
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven",
      "steps": [
        {
          "type": "configure",
          "name": "ci-MinShar-Clang-Maven"
        },
        {
          "type": "build",
          "name": "ci-MinShar-Clang-Maven"
        },
        {
          "type": "package",
          "name": "ci-MinShar-Clang-Maven"
        }
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven-Snapshot",
      "steps": [
        {
          "type": "configure",
          "name": "ci-MinShar-Clang-Maven-Snapshot"
        },
        {
          "type": "build",
          "name": "ci-MinShar-Clang-Maven-Snapshot"
        },
        {
          "type": "package",
          "name": "ci-MinShar-Clang-Maven-Snapshot"
        }
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven-FFM",
      "steps": [
        {
          "type": "configure",
          "name": "ci-MinShar-GNUC-Maven-FFM"
        },
        {
          "type": "build",
          "name": "ci-MinShar-GNUC-Maven-FFM"
        },
        {
          "type": "package",
          "name": "ci-MinShar-GNUC-Maven-FFM"
        }
      ]
    },
    {
      "name": "ci-MinShar-GNUC-Maven-FFM-Snapshot",
      "steps": [
        {
          "type": "configure",
          "name": "ci-MinShar-GNUC-Maven-FFM-Snapshot"
        },
        {
          "type": "build",
          "name": "ci-MinShar-GNUC-Maven-FFM-Snapshot"
        },
        {
          "type": "package",
          "name": "ci-MinShar-GNUC-Maven-FFM-Snapshot"
        }
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven-FFM",
      "steps": [
        {
          "type": "configure",
          "name": "ci-MinShar-MSVC-Maven-FFM"
        },
        {
          "type": "build",
          "name": "ci-MinShar-MSVC-Maven-FFM"
        },
        {
          "type": "package",
          "name": "ci-MinShar-MSVC-Maven-FFM"
        }
      ]
    },
    {
      "name": "ci-MinShar-MSVC-Maven-FFM-Snapshot",
      "steps": [
        {
          "type": "configure",
          "name": "ci-MinShar-MSVC-Maven-FFM-Snapshot"
        },
        {
          "type": "build",
          "name": "ci-MinShar-MSVC-Maven-FFM-Snapshot"
        },
        {
          "type": "package",
          "name": "ci-MinShar-MSVC-Maven-FFM-Snapshot"
        }
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven-FFM",
      "steps": [
        {
          "type": "configure",
          "name": "ci-MinShar-Clang-Maven-FFM"
        },
        {
          "type": "build",
          "name": "ci-MinShar-Clang-Maven-FFM"
        },
        {
          "type": "package",
          "name": "ci-MinShar-Clang-Maven-FFM"
        }
      ]
    },
    {
      "name": "ci-MinShar-Clang-Maven-FFM-Snapshot",
      "steps": [
        {
          "type": "configure",
          "name": "ci-MinShar-Clang-Maven-FFM-Snapshot"
        },
        {
          "type": "build",
          "name": "ci-MinShar-Clang-Maven-FFM-Snapshot"
        },
        {
          "type": "package",
          "name": "ci-MinShar-Clang-Maven-FFM-Snapshot"
        }
      ]
    },
    {
      "name": "ci-Testing-GNUC-FFM",
      "steps": [
        {
          "type": "configure",
          "name": "ci-Testing-GNUC-FFM"
        },
        {
          "type": "build",
          "name": "ci-Testing-GNUC-FFM"
        },
        {
          "type": "test",
          "name": "ci-Testing-GNUC-FFM"
        }
      ]
    },
    {
      "name": "ci-Testing-MSVC-FFM",
      "steps": [
        {
          "type": "configure",
          "name": "ci-Testing-MSVC-FFM"
        },
        {
          "type": "build",
          "name": "ci-Testing-MSVC-FFM"
        },
        {
          "type": "test",
          "name": "ci-Testing-MSVC-FFM"
        }
      ]
    },
    {
      "name": "ci-Testing-Clang-FFM",
      "steps": [
        {
          "type": "configure",
          "name": "ci-Testing-Clang-FFM"
        },
        {
          "type": "build",
          "name": "ci-Testing-Clang-FFM"
        },
        {
          "type": "test",
          "name": "ci-Testing-Clang-FFM"
        }
      ]
    },
    {
      "name": "ci-Testing-GNUC-JNI",
      "steps": [
        {
          "type": "configure",
          "name": "ci-Testing-GNUC-JNI"
        },
        {
          "type": "build",
          "name": "ci-Testing-GNUC-JNI"
        },
        {
          "type": "test",
          "name": "ci-Testing-GNUC-JNI"
        }
      ]
    },
    {
      "name": "ci-Testing-MSVC-JNI",
      "steps": [
        {
          "type": "configure",
          "name": "ci-Testing-MSVC-JNI"
        },
        {
          "type": "build",
          "name": "ci-Testing-MSVC-JNI"
        },
        {
          "type": "test",
          "name": "ci-Testing-MSVC-JNI"
        }
      ]
    },
    {
      "name": "ci-Testing-Clang-JNI",
      "steps": [
        {
          "type": "configure",
          "name": "ci-Testing-Clang-JNI"
        },
        {
          "type": "build",
          "name": "ci-Testing-Clang-JNI"
        },
        {
          "type": "test",
          "name": "ci-Testing-Clang-JNI"
        }
      ]
    }
  ]
}
```

### `CMakeTests.cmake`

```cmake
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.
#

# -----------------------------------------------------------------------------
# HDF5 CMake Testing and Dashboard Configuration
# -----------------------------------------------------------------------------
# This CMake module configures the testing and dashboard infrastructure for HDF5.
# It sets up test timeouts, test options, and enables/disables various test suites
# (API, VFD, VOL, serial, parallel, Fortran, C++, Java, tools, examples, SWMR, etc.).
# It also configures CTest integration, test express levels, and test directories.
#
# Key Features:
# - Configures DART/CTest timeouts and test express levels.
# - Provides options to enable/disable specific test suites and features.
# - Supports advanced test options (e.g., API async, driver, VFD lists, passthrough VOL).
# - Integrates with CTest and dashboard tools for automated testing.
# - Handles test directory setup for serial, parallel, and API tests.
#
# Usage:
#   HDF5 includes this file from the main CMakeLists.txt to enable and configure
#   HDF5 testing, if testing is enabled (BUILD_TESTING). Adjust options as needed
#   for your build and test requirements.
#
# See comments throughout for details on each option and logic branch.
# -----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Dashboard and Testing Settings
#-----------------------------------------------------------------------------
  set (DART_TESTING_TIMEOUT 1200
      CACHE STRING
      "Timeout in seconds for each test (default 1200=20minutes)"
  )

  # Generate a list of timeouts based on DART_TESTING_TIMEOUT
  math (EXPR CTEST_SHORT_TIMEOUT "${DART_TESTING_TIMEOUT} / 2")
  math (EXPR CTEST_LONG_TIMEOUT "${DART_TESTING_TIMEOUT} * 2")
  math (EXPR CTEST_VERY_LONG_TIMEOUT "${DART_TESTING_TIMEOUT} * 3")

  option (HDF5_DISABLE_TESTS_REGEX "Regex pattern to set execution of specific tests to DISABLED" "")
  mark_as_advanced (HDF5_DISABLE_TESTS_REGEX)

  if (HDF5_ENABLE_ROS3_VFD)
    if (HDF5_ENABLE_ROS3_VFD_DOCKER_PROXY)
      # Create a test credentials file
      file (WRITE "${CMAKE_BINARY_DIR}/credentials" "[default]\naws_access_key_id = remote-identity\naws_secret_access_key = remote-credential\nregion = us-east-2\n\n[ros3_vfd_test]\naws_access_key_id = remote-identity\naws_secret_access_key = remote-credential\nregion = us-east-2\n")
    endif ()
  endif ()

  option (HDF5_TEST_API "Execute HDF5 API tests" ON)
  mark_as_advanced (HDF5_TEST_API)
  cmake_dependent_option (HDF5_TEST_API_INSTALL "Install HDF5 API tests" OFF HDF5_TEST_API OFF)
  mark_as_advanced (HDF5_TEST_API_INSTALL)

  # Enable HDF5 Async API tests
  cmake_dependent_option (HDF5_TEST_API_ENABLE_ASYNC "Enable HDF5 Async API tests" OFF HDF5_TEST_API OFF)
  mark_as_advanced (HDF5_TEST_API_ENABLE_ASYNC)

  # Build and use HDF5 test driver program for API tests
  cmake_dependent_option (HDF5_TEST_API_ENABLE_DRIVER "Enable HDF5 API test driver program" OFF HDF5_TEST_API OFF)
  mark_as_advanced (HDF5_TEST_API_ENABLE_DRIVER)
  if (HDF5_TEST_API_ENABLE_DRIVER)
    set (HDF5_TEST_API_SERVER "" CACHE STRING "Server executable for running API tests")
    mark_as_advanced (HDF5_TEST_API_SERVER)
  endif ()

  option (HDF5_TEST_VFD "Execute tests with different VFDs" OFF)
  mark_as_advanced (HDF5_TEST_VFD)
  cmake_dependent_option (HDF5_TEST_FHEAP_VFD "Execute tests with different VFDs" ON HDF5_TEST_VFD OFF)
  mark_as_advanced (HDF5_TEST_FHEAP_VFD)

  if (HDF5_TEST_VFD)
    # Initialize the list of VFDs to be used for testing and create a test folder for each VFD
    H5_SET_VFD_LIST ()
  endif ()

  option (HDF5_TEST_PASSTHROUGH_VOL "Execute tests with different passthrough VOL connectors" OFF)
  mark_as_advanced (HDF5_TEST_PASSTHROUGH_VOL)
  cmake_dependent_option (HDF5_TEST_FHEAP_PASSTHROUGH_VOL "Execute fheap test with different passthrough VOL connectors" ON HDF5_TEST_PASSTHROUGH_VOL OFF)
  mark_as_advanced (HDF5_TEST_FHEAP_PASSTHROUGH VOL)

  set (H5_TEST_EXPRESS_LEVEL_DEFAULT "3")
  set (HDF_TEST_EXPRESS "${H5_TEST_EXPRESS_LEVEL_DEFAULT}"
      CACHE STRING "Control testing framework (0-3) (0 = exhaustive testing; 3 = quicker testing)")
  mark_as_advanced (HDF_TEST_EXPRESS)
  if (NOT "${HDF_TEST_EXPRESS}" STREQUAL "")
    set (H5_TEST_EXPRESS_LEVEL_DEFAULT "${HDF_TEST_EXPRESS}")
  endif ()

  enable_testing ()
  include (CTest)

  include (${HDF5_SOURCE_DIR}/CTestConfig.cmake)
  configure_file (${HDF_CONFIG_DIR}/CTestCustom.cmake ${HDF5_BINARY_DIR}/CTestCustom.ctest @ONLY)

  option (HDF5_TEST_SERIAL "Execute non-parallel tests" ON)
  mark_as_advanced (HDF5_TEST_SERIAL)

  cmake_dependent_option (HDF5_TEST_TOOLS "Execute tools tests" ON "HDF5_BUILD_TOOLS" OFF)

  cmake_dependent_option (HDF5_TEST_EXAMPLES "Execute tests on examples" ON "HDF5_BUILD_EXAMPLES" OFF)
  mark_as_advanced (HDF5_TEST_EXAMPLES)

  option (HDF5_TEST_SWMR "Execute SWMR tests" ON)
  mark_as_advanced (HDF5_TEST_SWMR)

  cmake_dependent_option (HDF5_TEST_PARALLEL "Execute parallel tests" ON "HDF5_ENABLE_PARALLEL" OFF)
  mark_as_advanced (HDF5_TEST_PARALLEL)

  cmake_dependent_option (HDF5_TEST_FORTRAN "Execute fortran tests" ON "HDF5_BUILD_FORTRAN" OFF)
  mark_as_advanced (HDF5_TEST_FORTRAN)

  cmake_dependent_option (HDF5_TEST_CPP "Execute cpp tests" ON "HDF5_BUILD_CPP_LIB" OFF)
  mark_as_advanced (HDF5_TEST_CPP)

  cmake_dependent_option (HDF5_TEST_JAVA "Execute java tests" ON "HDF5_BUILD_JAVA" OFF)
  mark_as_advanced (HDF5_TEST_JAVA)

  if (NOT HDF5_EXTERNALLY_CONFIGURED)
    if (EXISTS "${HDF5_TEST_SRC_DIR}" AND IS_DIRECTORY "${HDF5_TEST_SRC_DIR}")
      add_subdirectory (test)
    endif ()
    if (H5_HAVE_PARALLEL)
      if (EXISTS "${HDF5_TEST_PAR_DIR}" AND IS_DIRECTORY "${HDF5_TEST_PAR_DIR}")
        add_subdirectory (testpar)
      endif ()
    endif ()
  endif ()
```

### `CMakeVOL.cmake`

```cmake
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.
#

include (FetchContent)

# Function to retrieve all of the CMake targets generated
# in a directory and all its subdirectories
function (get_generated_cmake_targets out_var dir)
  get_directory_property (dir_targets DIRECTORY "${dir}" BUILDSYSTEM_TARGETS)
  get_directory_property (dir_subdirs DIRECTORY "${dir}" SUBDIRECTORIES)

  foreach (subdir ${dir_subdirs})
    get_generated_cmake_targets (subdir_targets "${subdir}")
    list (APPEND dir_targets "${subdir_targets}")
  endforeach()

  set (${out_var} "${dir_targets}" PARENT_SCOPE)
endfunction ()

# Function to apply connector-specify workarounds to build
# code once a connector has been populated through FetchContent
function (apply_connector_workarounds connector_name source_dir)
  # For the cache VOL, remove the call to find_package(ASYNC).
  # Eventually, the FetchContent OVERRIDE_FIND_PACKAGE should be
  # able to fulfill this dependency when building the cache VOL,
  # but for now we have to hack around this until the async and
  # cache VOLs create CMake .config files
  if ("${connector_name}" MATCHES "vol-cache")
    # Remove find_package(ASYNC) call from connector's CMake code
    file (READ "${source_dir}/CMakeLists.txt" vol_cmake_contents)
    string (REGEX REPLACE "[ \t]*find_package[ \t]*\\([ \t]*ASYNC[^\r\n\\)]*\\)[ \t]*[\r\n]+" "" vol_cmake_contents "${vol_cmake_contents}")
    file (WRITE "${source_dir}/CMakeLists.txt" "${vol_cmake_contents}")

    # Remove setting of HDF5_VOL_CONNECTOR and HDF5_PLUGIN_PATH
    # in connector's external tests CMake code
    file (STRINGS "${source_dir}/tests/CMakeLists.txt" file_lines)
    file (WRITE "${source_dir}/tests/CMakeLists.txt" "")
    foreach (line IN LISTS file_lines)
      set (stripped_line "${line}")
      string (REGEX MATCH "^[ \t]*set_tests_properties\\([ \t]*[\r\n]?" match_string "${line}")
      if (NOT "${match_string}" STREQUAL "")
        string (REGEX REPLACE "^[ \t]*set_tests_properties\\([ \t]*[\r\n]?" "" stripped_line "${line}")
      endif ()
      string (REGEX MATCH "^[ \t]*.\\{test\\}[ \t]*[\r\n]?" match_string "${line}")
      if (NOT "${match_string}" STREQUAL "")
        string (REGEX REPLACE "^[ \t]*.\\{[A-Za-z]*\\}[ \t]*[\r\n]?" "" stripped_line "${line}")
      endif ()
      string (REGEX MATCH "^[ \t]*PROPERTIES[ \t]*[\r\n]?" match_string "${line}")
      if (NOT "${match_string}" STREQUAL "")
        string (REGEX REPLACE "^[ \t]*PROPERTIES[ \t]*[\r\n]?" "" stripped_line "${line}")
      endif ()
      string (REGEX MATCH "^[ \t]*ENVIRONMENT[ \t]*.*[\r\n]?" match_string "${line}")
      if (NOT "${match_string}" STREQUAL "")
        string (REGEX REPLACE "^[ \t]*ENVIRONMENT[ \t]*.*[\r\n]?" "" stripped_line "${line}")
      endif ()
      file (APPEND "${source_dir}/tests/CMakeLists.txt" "${stripped_line}\n")
    endforeach ()
  endif ()
endfunction ()

set (HDF5_VOL_ALLOW_EXTERNAL "NO" CACHE STRING "Allow building of external HDF5 VOL connectors with FetchContent")
set_property (CACHE HDF5_VOL_ALLOW_EXTERNAL PROPERTY STRINGS NO GIT LOCAL_DIR)
mark_as_advanced (HDF5_VOL_ALLOW_EXTERNAL)
if (HDF5_VOL_ALLOW_EXTERNAL MATCHES "GIT" OR HDF5_VOL_ALLOW_EXTERNAL MATCHES "LOCAL_DIR")
  # For compatibility, set some variables that projects would
  # typically look for after calling find_package(HDF5)
  set (HDF5_FOUND 1)
  set (HDF5_LIBRARIES "${HDF5_LIBSH_TARGET};${LINK_LIBS};${LINK_COMP_LIBS};$<$<BOOL:${HDF5_ENABLE_PARALLEL}>:MPI::MPI_C>")
  set (HDF5_INCLUDE_DIRS "${HDF5_SRC_INCLUDE_DIRS};${HDF5_SRC_BINARY_DIR};$<$<BOOL:${HDF5_ENABLE_PARALLEL}>:${MPI_C_INCLUDE_DIRS}>")
  set (HDF5_IS_PARALLEL ${H5_HAVE_PARALLEL})
  set (HDF5_VERSION ${HDF5_PACKAGE_VERSION})

  set (HDF5_C_LIBRARIES "${HDF5_LIBRARIES}")
  
  if (HDF5_BUILD_HL_LIB)
    set (HDF5_C_HL_LIBRARIES "${HDF5_HL_LIBSH_TARGET}")
  endif()

  set (HDF5_MAX_EXTERNAL_VOLS 10)
  set (HDF5_EXTERNAL_VOL_TARGETS "")

  foreach (vol_idx RANGE 1 ${HDF5_MAX_EXTERNAL_VOLS})
    # Generate fixed-width index number prepended with 0s
    # so VOL sources come in order from 1 - HDF5_MAX_EXTERNAL_VOLS
    set (vol_idx_num_digits 2) # Based on HDF5_MAX_EXTERNAL_VOLS
    set (vol_idx_fixed "${vol_idx}")
    string (LENGTH "${vol_idx_fixed}" vol_idx_len)
    while (vol_idx_len LESS vol_idx_num_digits)
      string (PREPEND vol_idx_fixed "0")
      math (EXPR vol_idx_len "${vol_idx_len}+1")
    endwhile ()

    if (HDF5_VOL_ALLOW_EXTERNAL MATCHES "GIT")
      set (HDF5_VOL_URL${vol_idx_fixed} "" CACHE STRING "Git repository URL of an external HDF5 VOL connector to build")
      mark_as_advanced (HDF5_VOL_URL${vol_idx_fixed})
      set (HDF5_VOL_SOURCE "${HDF5_VOL_URL${vol_idx_fixed}}")
    elseif(HDF5_VOL_ALLOW_EXTERNAL MATCHES "LOCAL_DIR")
      set (HDF5_VOL_PATH${vol_idx_fixed} "" CACHE STRING "Path to the source directory of an external HDF5 VOL connector to build")
      mark_as_advanced (HDF5_VOL_PATH${vol_idx_fixed})
      set (HDF5_VOL_SOURCE "${HDF5_VOL_PATH${vol_idx_fixed}}")
    endif()

    if (NOT "${HDF5_VOL_SOURCE}" STREQUAL "")
      # Deal with trailing slash in path for LOCAL_DIR case
      if (HDF5_VOL_ALLOW_EXTERNAL MATCHES "LOCAL_DIR")
        # Erase trailing slash
        string (REGEX REPLACE "/$" "" HDF5_VOL_SOURCE ${HDF5_VOL_SOURCE})
      endif()

      # Extract the name of the VOL connector
      string (FIND "${HDF5_VOL_SOURCE}" "/" hdf5_vol_name_pos REVERSE)
      if (hdf5_vol_name_pos EQUAL -1)
        if (HDF5_VOL_ALLOW_EXTERNAL MATCHES "GIT")
          message (SEND_ERROR "Invalid URL '${HDF5_VOL_SOURCE}' specified for HDF5_VOL_URL${vol_idx_fixed}")
        elseif (HDF5_VOL_ALLOW_EXTERNAL MATCHES "LOCAL_DIR")
          message (SEND_ERROR "Invalid source path '${HDF5_VOL_SOURCE}' specified for HDF5_VOL_PATH${vol_idx_fixed}")
        endif()
      endif ()

      math (EXPR hdf5_vol_name_pos "${hdf5_vol_name_pos}+1")

      string (SUBSTRING "${HDF5_VOL_SOURCE}" ${hdf5_vol_name_pos} -1 hdf5_vol_name)
      string (REPLACE ".git" "" hdf5_vol_name "${hdf5_vol_name}")
      string (STRIP "${hdf5_vol_name}" hdf5_vol_name)
      string (TOUPPER "${hdf5_vol_name}" hdf5_vol_name_upper)
      string (TOLOWER "${hdf5_vol_name}" hdf5_vol_name_lower)

      message (VERBOSE "Building VOL connector '${hdf5_vol_name}' with FetchContent from source ${HDF5_VOL_SOURCE}")

      # Set some cache variables that can be set by users when building
      if (HDF5_VOL_ALLOW_EXTERNAL MATCHES "GIT")
        set ("HDF5_VOL_${hdf5_vol_name_upper}_BRANCH" "main" CACHE STRING "Git branch (or tag) to use when building VOL connector '${hdf5_vol_name}'")
        mark_as_advanced ("HDF5_VOL_${hdf5_vol_name_upper}_BRANCH")
      endif()

      set ("HDF5_VOL_${hdf5_vol_name_upper}_CMAKE_PACKAGE_NAME"
        "${hdf5_vol_name_lower}"
        CACHE
        STRING
        "CMake package name used by find_package(...) calls for VOL connector '${hdf5_vol_name}'"
      )

      set ("HDF5_VOL_${hdf5_vol_name_upper}_NAME" "" CACHE STRING "Name of VOL connector to set for the HDF5_VOL_CONNECTOR environment variable")
      option ("HDF5_VOL_${hdf5_vol_name_upper}_TEST_PARALLEL" "Whether to test VOL connector '${hdf5_vol_name}' against the parallel API tests" OFF)

      mark_as_advanced ("HDF5_VOL_${hdf5_vol_name_upper}_NAME")
      mark_as_advanced ("HDF5_VOL_${hdf5_vol_name_upper}_TEST_PARALLEL")

      if (HDF5_TEST_API)
        if ("${HDF5_VOL_${hdf5_vol_name_upper}_NAME}" STREQUAL "")
          message (SEND_ERROR "HDF5_VOL_${hdf5_vol_name_upper}_NAME must be set to a valid connector name to use VOL connector '${hdf5_vol_name}' for testing")
        endif ()
      endif ()

      if ((HDF5_VOL_ALLOW_EXTERNAL MATCHES "GIT") AND ("${HDF5_VOL_${hdf5_vol_name_upper}_BRANCH}" STREQUAL ""))
        message (SEND_ERROR "HDF5_VOL_${hdf5_vol_name_upper}_BRANCH must be set to a valid git branch name (or git tag) to build VOL connector '${hdf5_vol_name}'")
      endif ()

      if ((HDF5_VOL_ALLOW_EXTERNAL MATCHES "LOCAL_DIR")
        AND NOT (EXISTS ${HDF5_VOL_SOURCE} AND IS_DIRECTORY ${HDF5_VOL_SOURCE}))
          message (FATAL_ERROR "HDF5_VOL_PATH${vol_idx_fixed} must be an absolute path to a valid directory")
      endif ()

      # Set internal convenience variables for FetchContent dependency name
      set (hdf5_vol_depname "${HDF5_VOL_${hdf5_vol_name_upper}_CMAKE_PACKAGE_NAME}")
      string (TOLOWER "${hdf5_vol_depname}" hdf5_vol_depname_lower)

      if (${CMAKE_VERSION} VERSION_GREATER_EQUAL "3.24")
        set("OVERRIDE_FIND_PACKAGE_OPT" "OVERRIDE_FIND_PACKAGE")
      endif()

      if (HDF5_VOL_ALLOW_EXTERNAL MATCHES "GIT")
        FetchContent_Declare (${hdf5_vol_depname}
            GIT_REPOSITORY "${HDF5_VOL_SOURCE}"
            GIT_TAG "${HDF5_VOL_${hdf5_vol_name_upper}_BRANCH}"
            "${OVERRIDE_FIND_PACKAGE_OPT}"
        )
      elseif (HDF5_VOL_ALLOW_EXTERNAL MATCHES "LOCAL_DIR")
        FetchContent_Declare (${hdf5_vol_depname}
            SOURCE_DIR "${HDF5_VOL_SOURCE}"
        )
      endif ()

      FetchContent_GetProperties (${hdf5_vol_depname})
      if (NOT ${hdf5_vol_depname}_POPULATED)
        FetchContent_Populate (${hdf5_vol_depname})

        # Now that content has been populated, set other internal
        # convenience variables for FetchContent dependency
        set (hdf5_vol_depname_source_dir "${${hdf5_vol_depname_lower}_SOURCE_DIR}")
        set (hdf5_vol_depname_binary_dir "${${hdf5_vol_depname_lower}_BINARY_DIR}")

        if (NOT EXISTS "${hdf5_vol_depname_source_dir}/CMakeLists.txt")
          if (HDF5_VOL_ALLOW_EXTERNAL MATCHES "GIT")
            message (SEND_ERROR "The git repository branch '${HDF5_VOL_${hdf5_vol_name_upper}_BRANCH}' for VOL connector '${hdf5_vol_name}' does not appear to contain a CMakeLists.txt file")
          elseif (HDF5_VOL_ALLOW_EXTERNAL MATCHES "LOCAL_DIR")
            message (SEND_ERROR "The local directory '${HDF5_VOL_SOURCE}' for VOL connector '${hdf5_vol_name}' does not appear to contain a CMakeLists.txt file")
          endif ()
        endif ()

        # If there are any calls to find_package(HDF5) in the connector's
        # CMakeLists.txt files, remove those since any found HDF5 targets
        # will conflict with targets being generated by this build of HDF5
        if (EXISTS "${hdf5_vol_depname_source_dir}/CMakeLists.txt")
          file (READ "${hdf5_vol_depname_source_dir}/CMakeLists.txt" vol_cmake_contents)
          string (REGEX REPLACE "[ \t]*find_package[ \t]*\\([ \t]*HDF5[^\r\n\\)]*\\)[ \t]*[\r\n]+" "" vol_cmake_contents "${vol_cmake_contents}")
          file (WRITE "${hdf5_vol_depname_source_dir}/CMakeLists.txt" "${vol_cmake_contents}")
        endif ()
        if (EXISTS "${hdf5_vol_depname_source_dir}/src/CMakeLists.txt")
          file (READ "${hdf5_vol_depname_source_dir}/src/CMakeLists.txt" vol_cmake_contents)
          string (REGEX REPLACE "[ \t]*find_package[ \t]*\\([ \t]*HDF5[^\r\n\\)]*\\)[ \t]*[\r\n]+" "" vol_cmake_contents "${vol_cmake_contents}")
          file (WRITE "${hdf5_vol_depname_source_dir}/src/CMakeLists.txt" "${vol_cmake_contents}")
        endif ()

        # Apply any connector-specific workarounds
        apply_connector_workarounds ("${hdf5_vol_name_lower}" "${hdf5_vol_depname_source_dir}")

        add_subdirectory (${hdf5_vol_depname_source_dir} ${hdf5_vol_depname_binary_dir})

        # Get list of targets generated by build of connector
        get_generated_cmake_targets (connector_targets ${hdf5_vol_depname_source_dir})

        # Create a custom target for the connector to encompass all its
        # targets and other custom properties set by us for later use
        add_custom_target ("HDF5_VOL_${hdf5_vol_name_lower}")

        # Define and set a custom property on the VOL connector target to
        # capture all of the connector's generated targets
        define_property (
            TARGET
            PROPERTY HDF5_VOL_TARGETS
            BRIEF_DOCS "Generated targets of this connector"
            FULL_DOCS "Generated targets of this connector"
        )

        set_target_properties (
            "HDF5_VOL_${hdf5_vol_name_lower}"
            PROPERTIES
              HDF5_VOL_TARGETS "${connector_targets}"
        )

        # Define and set a custom property on the VOL connector target to
        # capture the connector's name to set for the HDF5_VOL_CONNECTOR
        # environment variable for testing
        define_property (
            TARGET
            PROPERTY HDF5_VOL_NAME
            BRIEF_DOCS "VOL connector name to use for the HDF5_VOL_CONNECTOR environment variable when testing"
            FULL_DOCS "VOL connector name to use for the HDF5_VOL_CONNECTOR environment variable when testing"
        )

        set_target_properties (
           "HDF5_VOL_${hdf5_vol_name_lower}"
            PROPERTIES
              HDF5_VOL_NAME "${HDF5_VOL_${hdf5_vol_name_upper}_NAME}"
        )

        # Define and set a custom property on the VOL connector target to
        # capture whether the connector should be tested with the parallel
        # API tests
        define_property (
            TARGET
            PROPERTY HDF5_VOL_TEST_PARALLEL
            BRIEF_DOCS "Whether the VOL connector should be tested with the parallel API tests"
            FULL_DOCS "Whether the VOL connector should be tested with the parallel API tests"
        )

        set_target_properties (
            "HDF5_VOL_${hdf5_vol_name_lower}"
            PROPERTIES
              HDF5_VOL_TEST_PARALLEL ${HDF5_VOL_${hdf5_vol_name_upper}_TEST_PARALLEL}
        )

        # Add this VOL connector's target to the list of external connector targets
        list (APPEND HDF5_EXTERNAL_VOL_TARGETS "HDF5_VOL_${hdf5_vol_name_lower}")

        # Get the list of library targets from this VOL connector
        unset (connector_lib_targets)
        foreach (connector_target ${connector_targets})
          get_target_property (target_type ${connector_target} TYPE)
          if (target_type STREQUAL "SHARED_LIBRARY" OR target_type STREQUAL "STATIC_LIBRARY")
            list (APPEND connector_lib_targets "${connector_target}")
          endif ()
        endforeach ()

        # Add all of the previous VOL connector's library targets as
        # dependencies for the current VOL connector to ensure that
        # VOL connectors get built serially in case there are dependencies
        if (DEFINED last_vol_lib_targets)
          foreach (connector_target ${connector_targets})
            add_dependencies (${connector_target} ${last_vol_lib_targets})
          endforeach ()
        endif ()

        # Use this connector's library targets as dependencies
        # for the next connector that is built
        set (last_vol_lib_targets "${connector_lib_targets}")
      endif ()
    endif ()
  endforeach ()
endif ()
```

### `CTestConfig.cmake`

```cmake
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.
#
## This file should be placed in the root directory of your project.
## Then modify the CMakeLists.txt file in the root directory of your
## project to incorporate the testing dashboard.
## # The following are required to use Dart and the CDash dashboard
##   ENABLE_TESTING()
##   INCLUDE(CTest)
set (CTEST_PROJECT_NAME "HDF5")
set (CTEST_NIGHTLY_START_TIME "18:00:00 CST")

set (CTEST_DROP_METHOD "https")
if (CTEST_DROP_SITE_INIT)
  set (CTEST_DROP_SITE "${CTEST_DROP_SITE_INIT}")
else ()
  set (CTEST_DROP_SITE "cdash.hdfgroup.org")
endif ()
if (CTEST_DROP_LOCATION_INIT)
  set (CTEST_DROP_LOCATION "${CTEST_DROP_LOCATION_INIT}")
else ()
  set (CTEST_DROP_LOCATION "/submit.php?project=HDF5")
endif ()
set (CTEST_DROP_SITE_CDASH TRUE)

set (UPDATE_TYPE git)
set (VALGRIND_COMMAND "/usr/bin/valgrind")
set (VALGRIND_COMMAND_OPTIONS "-v --tool=memcheck --leak-check=full --track-fds=yes --num-callers=50 --show-reachable=yes --track-origins=yes --malloc-fill=0xff --free-fill=0xfe")

set (CTEST_TEST_TIMEOUT 1200 CACHE STRING
    "Maximum time allowed before CTest will kill the test.")
set (DART_TESTING_TIMEOUT 1200 CACHE STRING
    "Maximum time allowed before CTest will kill the test." FORCE)

set (CTEST_SUBMIT_RETRY_DELAY 20 CACHE STRING
    "How long to wait between timed-out CTest submissions.")
```

### `HDF5Examples/C/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_C C)

#-----------------------------------------------------------------------------
# Build the C Examples
#-----------------------------------------------------------------------------
add_subdirectory (${PROJECT_SOURCE_DIR}/TUTR)
add_subdirectory (${PROJECT_SOURCE_DIR}/H5D)
add_subdirectory (${PROJECT_SOURCE_DIR}/H5G)
add_subdirectory (${PROJECT_SOURCE_DIR}/H5T)

if (${H5_LIBVER_DIR} GREATER 16)
#  add_subdirectory (${PROJECT_SOURCE_DIR}/Perf)
  if (USE_SHARED_LIBS AND H5EXAMPLE_BUILD_FILTERS AND HDF5_PROVIDES_PLUGIN_SUPPORT)
    add_subdirectory (${PROJECT_SOURCE_DIR}/H5FLT)
  endif ()
endif ()

if (${H5_LIBVER_DIR} GREATER 110)
  add_subdirectory (${PROJECT_SOURCE_DIR}/H5VDS)
endif ()

if (H5EXAMPLE_ENABLE_PARALLEL AND H5_HAVE_PARALLEL AND HDF5_PROVIDES_PARALLEL)
  add_subdirectory (${PROJECT_SOURCE_DIR}/H5PAR)
endif ()

#-- Add High Level Examples
if (H5EXAMPLE_BUILD_HL AND HDF5_PROVIDES_HL_LIB)
  add_subdirectory (HL)
endif ()
```

### `HDF5Examples/C/H5D/16/h5ex_d_alloc.c`

```c
/************************************************************

  This example shows how to set the space allocation time
  for a dataset.  The program first creates two datasets,
  one with the default allocation time (late) and one with
  early allocation time, and displays whether each has been
  allocated and their allocation size.  Next, it writes data
  to the datasets, and again displays whether each has been
  allocated and their allocation size.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_alloc.h5"
#define DATASET1 "DS1"
#define DATASET2 "DS2"
#define DIM0     4
#define DIM1     7
#define FILLVAL  99

int
main(void)
{
    hid_t file  = H5I_INVALID_HID;
    hid_t space = H5I_INVALID_HID;
    hid_t dset1 = H5I_INVALID_HID;
    hid_t dset2 = H5I_INVALID_HID;
    hid_t dcpl  = H5I_INVALID_HID;
    /* Handles */
    herr_t             status;
    H5D_space_status_t space_status;
    hsize_t            dims[2] = {DIM0, DIM1};
    hsize_t            storage_size;
    int                wdata[DIM0][DIM1]; /* Write buffer */
    hsize_t            i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, and set the chunk
     * size.
     */
    dcpl = H5Pcreate(H5P_DATASET_CREATE);

    /*
     * Set the allocation time to "early".  This way we can be sure
     * that reading from the dataset immediately after creation will
     * return the fill value.
     */
    status = H5Pset_alloc_time(dcpl, H5D_ALLOC_TIME_EARLY);

    printf("Creating datasets...\n");
    printf("%s has allocation time H5D_ALLOC_TIME_LATE\n", DATASET1);
    printf("%s has allocation time H5D_ALLOC_TIME_EARLY\n\n", DATASET2);

    /*
     * Create the dataset using the dataset creation property list.
     */
    dset1 = H5Dcreate(file, DATASET1, H5T_STD_I32LE, space, H5P_DEFAULT);
    dset2 = H5Dcreate(file, DATASET2, H5T_STD_I32LE, space, dcpl);

    /*
     * Retrieve and print space status and storage size for dset1.
     */
    status       = H5Dget_space_status(dset1, &space_status);
    storage_size = H5Dget_storage_size(dset1);
    printf("Space for %s has%sbeen allocated.\n", DATASET1,
           space_status == H5D_SPACE_STATUS_ALLOCATED ? " " : " not ");
    printf("Storage size for %s is: %ld bytes.\n", DATASET1, (long)storage_size);

    /*
     * Retrieve and print space status and storage size for dset2.
     */
    status       = H5Dget_space_status(dset2, &space_status);
    storage_size = H5Dget_storage_size(dset2);
    printf("Space for %s has%sbeen allocated.\n", DATASET2,
           space_status == H5D_SPACE_STATUS_ALLOCATED ? " " : " not ");
    printf("Storage size for %s is: %ld bytes.\n", DATASET2, (long)storage_size);

    printf("\nWriting data...\n\n");

    /*
     * Write the data to the datasets.
     */
    status = H5Dwrite(dset1, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);
    status = H5Dwrite(dset2, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Retrieve and print space status and storage size for dset1.
     */
    status       = H5Dget_space_status(dset1, &space_status);
    storage_size = H5Dget_storage_size(dset1);
    printf("Space for %s has%sbeen allocated.\n", DATASET1,
           space_status == H5D_SPACE_STATUS_ALLOCATED ? " " : " not ");
    printf("Storage size for %s is: %ld bytes.\n", DATASET1, (long)storage_size);

    /*
     * Retrieve and print space status and storage size for dset2.
     */
    status       = H5Dget_space_status(dset2, &space_status);
    storage_size = H5Dget_storage_size(dset2);
    printf("Space for %s has%sbeen allocated.\n", DATASET2,
           space_status == H5D_SPACE_STATUS_ALLOCATED ? " " : " not ");
    printf("Storage size for %s is: %ld bytes.\n", DATASET2, (long)storage_size);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset1);
    status = H5Dclose(dset2);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/16/h5ex_d_checksum.c`

```c
/************************************************************

  This example shows how to read and write data to a dataset
  using the Fletcher32 checksum filter.  The program first
  checks if the Fletcher32 filter is available, then if it
  is it writes integers to a dataset using Fletcher32, then
  closes the file.  Next, it reopens the file, reads back
  the data, checks if the filter detected an error and
  outputs the type of filter and the maximum value in the
  dataset to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_checksum.h5"
#define DATASET  "DS1"
#define DIM0     32
#define DIM1     64
#define CHUNK0   4
#define CHUNK1   8

int
main(void)
{
    hid_t file  = H5I_INVALID_HID;
    hid_t space = H5I_INVALID_HID;
    hid_t dset  = H5I_INVALID_HID;
    hid_t dcpl  = H5I_INVALID_HID;
    /* Handles */
    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_type;
    hsize_t      dims[2] = {DIM0, DIM1}, chunk[2] = {CHUNK0, CHUNK1};
    size_t       nelmts;
    unsigned int flags, filter_info;
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    int          max;
    hsize_t      i, j;

    /*
     * Check if the Fletcher32 filter is available and can be used for
     * both encoding and decoding.  Normally we do not perform error
     * checking in these examples for the sake of clarity, but in this
     * case we will make an exception because this filter is an
     * optional part of the hdf5 library.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_FLETCHER32);
    if (!avail) {
        printf("Fletcher32 filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_FLETCHER32, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("Fletcher32 filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, add the Fletcher32 filter
     * and set the chunk size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_fletcher32(dcpl);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, dcpl);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve and print the filter type.  Here we only retrieve the
     * first filter because we know that we only added one filter.
     */
    nelmts      = 0;
    filter_type = H5Pget_filter(dcpl, 0, &flags, &nelmts, NULL, 0, NULL);
    printf("Filter type is: ");
    switch (filter_type) {
        case H5Z_FILTER_DEFLATE:
            printf("H5Z_FILTER_DEFLATE\n");
            break;
        case H5Z_FILTER_SHUFFLE:
            printf("H5Z_FILTER_SHUFFLE\n");
            break;
        case H5Z_FILTER_FLETCHER32:
            printf("H5Z_FILTER_FLETCHER32\n");
            break;
        case H5Z_FILTER_SZIP:
            printf("H5Z_FILTER_SZIP\n");
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Check if the read was successful.  Normally we do not perform
     * error checking in these examples for the sake of clarity, but in
     * this case we will make an exception because this is how the
     * fletcher32 checksum filter reports data errors.
     */
    if (status < 0) {
        fprintf(stderr, "Dataset read failed!\n");
        status = H5Pclose(dcpl);
        status = H5Dclose(dset);
        status = H5Fclose(file);
        return 2;
    }

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            if (max < rdata[i][j])
                max = rdata[i][j];

    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is: %d\n", DATASET, max);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/16/h5ex_d_chunk.c`

```c
/************************************************************

  This example shows how to create a chunked dataset.  The
  program first writes integers in a hyperslab selection to
  a chunked dataset with dataspace dimensions of DIM0xDIM1
  and chunk size of CHUNK0xCHUNK1, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs it to the screen.  Finally it reads the data again
  using a different hyperslab selection, and outputs
  the result to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_chunk.h5"
#define DATASET  "DS1"
#define DIM0     6
#define DIM1     8
#define CHUNK0   4
#define CHUNK1   4

int
main(void)
{
    hid_t        file  = H5I_INVALID_HID;
    hid_t        space = H5I_INVALID_HID;
    hid_t        dset  = H5I_INVALID_HID;
    hid_t        dcpl  = H5I_INVALID_HID;
    herr_t       status;
    H5D_layout_t layout;
    hsize_t      dims[2]  = {DIM0, DIM1};
    hsize_t      chunk[2] = {CHUNK0, CHUNK1};
    hsize_t      start[2];
    hsize_t      stride[2];
    hsize_t      count[2];
    hsize_t      block[2];
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    hsize_t      i, j;

    /*
     * Initialize data to "1", to make it easier to see the selections.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = 1;

    /*
     * Print the data to the screen.
     */
    printf("Original Data:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", wdata[i][j]);
        printf("]\n");
    }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, and set the chunk
     * size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the chunked dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, dcpl);

    /*
     * Define and select the first part of the hyperslab selection.
     */
    start[0]  = 0;
    start[1]  = 0;
    stride[0] = 3;
    stride[1] = 3;
    count[0]  = 2;
    count[1]  = 3;
    block[0]  = 2;
    block[1]  = 2;
    status    = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, stride, count, block);

    /*
     * Define and select the second part of the hyperslab selection,
     * which is subtracted from the first selection by the use of
     * H5S_SELECT_NOTB
     */
    block[0] = 1;
    block[1] = 1;
    status   = H5Sselect_hyperslab(space, H5S_SELECT_NOTB, start, stride, count, block);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, space, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Retrieve the dataset creation property list, and print the
     * storage layout.
     */
    dcpl   = H5Dget_create_plist(dset);
    layout = H5Pget_layout(dcpl);
    printf("\nStorage layout for %s is: ", DATASET);
    switch (layout) {
        case H5D_COMPACT:
            printf("H5D_COMPACT\n");
            break;
        case H5D_CONTIGUOUS:
            printf("H5D_CONTIGUOUS\n");
            break;
        case H5D_CHUNKED:
            printf("H5D_CHUNKED\n");
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nData as written to disk by hyberslabs:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Initialize the read array.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            rdata[i][j] = 0;

    /*
     * Define and select the hyperslab to use for reading.
     */
    space     = H5Dget_space(dset);
    start[0]  = 0;
    start[1]  = 1;
    stride[0] = 4;
    stride[1] = 4;
    count[0]  = 2;
    count[1]  = 2;
    block[0]  = 2;
    block[1]  = 3;
    status    = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, stride, count, block);

    /*
     * Read the data using the previously defined hyperslab.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, space, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nData as read from disk by hyperslab:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/16/h5ex_d_compact.c`

```c
/************************************************************

  This example shows how to read and write data to a compact
  dataset.  The program first writes integers to a compact
  dataset with dataspace dimensions of DIM0xDIM1, then
  closes the file.  Next, it reopens the file, reads back
  the data, and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_compact.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7

int
main(void)
{
    hid_t        file  = H5I_INVALID_HID;
    hid_t        space = H5I_INVALID_HID;
    hid_t        dset  = H5I_INVALID_HID;
    hid_t        dcpl  = H5I_INVALID_HID;
    herr_t       status;
    H5D_layout_t layout;
    hsize_t      dims[2] = {DIM0, DIM1};
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    hsize_t      i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, set the layout to
     * compact.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_layout(dcpl, H5D_COMPACT);

    /*
     * Create the dataset.  We will use all default properties for this
     * example.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, dcpl);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Retrieve the dataset creation property list, and print the
     * storage layout.
     */
    dcpl   = H5Dget_create_plist(dset);
    layout = H5Pget_layout(dcpl);
    printf("Storage layout for %s is: ", DATASET);
    switch (layout) {
        case H5D_COMPACT:
            printf("H5D_COMPACT\n");
            break;
        case H5D_CONTIGUOUS:
            printf("H5D_CONTIGUOUS\n");
            break;
        case H5D_CHUNKED:
            printf("H5D_CHUNKED\n");
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/16/h5ex_d_extern.c`

```c
/************************************************************

  This example shows how to read and write data to an
  external dataset.  The program first writes integers to an
  external dataset with dataspace dimensions of DIM0xDIM1,
  then closes the file.  Next, it reopens the file, reads
  back the data, and outputs the name of the external data
  file and the data to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME      "h5ex_d_extern.h5"
#define EXTERNAL      "h5ex_d_extern.data"
#define DATASET       "DS1"
#define DIM0          4
#define DIM1          7
#define NAME_BUF_SIZE 32

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID;
    hid_t   space = H5I_INVALID_HID;
    hid_t   dset  = H5I_INVALID_HID;
    hid_t   dcpl  = H5I_INVALID_HID;
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    char    name[NAME_BUF_SIZE];
    int     wdata[DIM0][DIM1]; /* Write buffer */
    int     rdata[DIM0][DIM1]; /* Read buffer */
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, set the external
     * file.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_external(dcpl, EXTERNAL, 0, H5F_UNLIMITED);

    /*
     * Create the external dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, dcpl);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve and print the name of the external file.  Here we
     * manually set the last field in name to null, in case the name of
     * the file is longer than the buffer.
     */
    status                  = H5Pget_external(dcpl, 0, NAME_BUF_SIZE, name, NULL, NULL);
    name[NAME_BUF_SIZE - 1] = '\0';
    printf("%s is stored in file: %s\n", DATASET, name);

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/16/h5ex_d_fillval.c`

```c
/************************************************************

  This example shows how to set the fill value for a
  dataset.  The program first sets the fill value to
  FILLVAL, creates a dataset with dimensions of DIM0xDIM1,
  reads from the uninitialized dataset, and outputs the
  contents to the screen.  Next, it writes integers to the
  dataset, reads the data back, and outputs it to the
  screen.  Finally it extends the dataset, reads from it,
  and outputs the result to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_fillval.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7
#define EDIM0    6
#define EDIM1    10
#define CHUNK0   4
#define CHUNK1   4
#define FILLVAL  99

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID;
    hid_t   space = H5I_INVALID_HID;
    hid_t   dset  = H5I_INVALID_HID;
    hid_t   dcpl  = H5I_INVALID_HID;
    herr_t  status;
    hsize_t dims[2]    = {DIM0, DIM1};
    hsize_t extdims[2] = {EDIM0, EDIM1};
    hsize_t maxdims[2] = {H5S_UNLIMITED, H5S_UNLIMITED};
    hsize_t chunk[2]   = {CHUNK0, CHUNK1};
    int     wdata[DIM0][DIM1];    /* Write buffer */
    int     rdata[DIM0][DIM1];    /* Read buffer */
    int     rdata2[EDIM0][EDIM1]; /* Read buffer for extension */
    int     fillval;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace with unlimited dimensions.
     */
    space = H5Screate_simple(2, dims, maxdims);

    /*
     * Create the dataset creation property list, and set the chunk
     * size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Set the fill value for the dataset.
     */
    fillval = FILLVAL;
    status  = H5Pset_fill_value(dcpl, H5T_NATIVE_INT, &fillval);

    /*
     * Set the allocation time to "early".  This way we can be sure
     * that reading from the dataset immediately after creation will
     * return the fill value.
     */
    status = H5Pset_alloc_time(dcpl, H5D_ALLOC_TIME_EARLY);

    /*
     * Create the dataset using the dataset creation property list.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, dcpl);

    /*
     * Read values from the dataset, which has not been written to yet.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("Dataset before being written to:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Read the data back.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nDataset after being written to:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Extend the dataset.
     */
    status = H5Dset_extent(dset, extdims);

    /*
     * Read from the extended dataset.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata2[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nDataset after extension:\n");
    for (i = 0; i < extdims[0]; i++) {
        printf(" [");
        for (j = 0; j < extdims[1]; j++)
            printf(" %3d", rdata2[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/16/h5ex_d_gzip.c`

```c
/************************************************************

  This example shows how to read and write data to a dataset
  using gzip compression (also called zlib or deflate).  The
  program first checks if gzip compression is available,
  then if it is it writes integers to a dataset using gzip,
  then closes the file.  Next, it reopens the file, reads
  back the data, and outputs the type of compression and the
  maximum value in the dataset to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_gzip.h5"
#define DATASET  "DS1"
#define DIM0     32
#define DIM1     64
#define CHUNK0   4
#define CHUNK1   8

int
main(void)
{
    hid_t        file  = H5I_INVALID_HID;
    hid_t        space = H5I_INVALID_HID;
    hid_t        dset  = H5I_INVALID_HID;
    hid_t        dcpl  = H5I_INVALID_HID;
    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_type;
    hsize_t      dims[2]  = {DIM0, DIM1};
    hsize_t      chunk[2] = {CHUNK0, CHUNK1};
    size_t       nelmts;
    unsigned int flags;
    unsigned int filter_info;
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    int          max;
    hsize_t      i, j;

    /*
     * Check if gzip compression is available and can be used for both
     * compression and decompression.  Normally we do not perform error
     * checking in these examples for the sake of clarity, but in this
     * case we will make an exception because this filter is an
     * optional part of the hdf5 library.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_DEFLATE);
    if (!avail) {
        printf("gzip filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_DEFLATE, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("gzip filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, add the gzip
     * compression filter and set the chunk size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_deflate(dcpl, 9);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, dcpl);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve and print the filter type.  Here we only retrieve the
     * first filter because we know that we only added one filter.
     */
    nelmts      = 0;
    filter_type = H5Pget_filter(dcpl, 0, &flags, &nelmts, NULL, 0, NULL);
    printf("Filter type is: ");
    switch (filter_type) {
        case H5Z_FILTER_DEFLATE:
            printf("H5Z_FILTER_DEFLATE\n");
            break;
        case H5Z_FILTER_SHUFFLE:
            printf("H5Z_FILTER_SHUFFLE\n");
            break;
        case H5Z_FILTER_FLETCHER32:
            printf("H5Z_FILTER_FLETCHER32\n");
            break;
        case H5Z_FILTER_SZIP:
            printf("H5Z_FILTER_SZIP\n");
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            if (max < rdata[i][j])
                max = rdata[i][j];

    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is: %d\n", DATASET, max);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/16/h5ex_d_hyper.c`

```c
/************************************************************

  This example shows how to read and write data to a
  dataset by hyberslabs.  The program first writes integers
  in a hyperslab selection to a dataset with dataspace
  dimensions of DIM0xDIM1, then closes the file.  Next, it
  reopens the file, reads back the data, and outputs it to
  the screen.  Finally it reads the data again using a
  different hyperslab selection, and outputs the result to
  the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_hyper.h5"
#define DATASET  "DS1"
#define DIM0     6
#define DIM1     8

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID;
    hid_t   space = H5I_INVALID_HID;
    hid_t   dset  = H5I_INVALID_HID;
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    hsize_t start[2];
    hsize_t stride[2];
    hsize_t count[2];
    hsize_t block[2];
    int     wdata[DIM0][DIM1]; /* Write buffer */
    int     rdata[DIM0][DIM1]; /* Read buffer */
    hsize_t i, j;

    /*
     * Initialize data to "1", to make it easier to see the selections.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = 1;

    /*
     * Print the data to the screen.
     */
    printf("Original Data:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", wdata[i][j]);
        printf("]\n");
    }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset.  We will use all default properties for this
     * example.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);

    /*
     * Define and select the first part of the hyperslab selection.
     */
    start[0]  = 0;
    start[1]  = 0;
    stride[0] = 3;
    stride[1] = 3;
    count[0]  = 2;
    count[1]  = 3;
    block[0]  = 2;
    block[1]  = 2;
    status    = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, stride, count, block);

    /*
     * Define and select the second part of the hyperslab selection,
     * which is subtracted from the first selection by the use of
     * H5S_SELECT_NOTB
     */
    block[0] = 1;
    block[1] = 1;
    status   = H5Sselect_hyperslab(space, H5S_SELECT_NOTB, start, stride, count, block);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, space, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nData as written to disk by hyberslabs:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Initialize the read array.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            rdata[i][j] = 0;

    /*
     * Define and select the hyperslab to use for reading.
     */
    space     = H5Dget_space(dset);
    start[0]  = 0;
    start[1]  = 1;
    stride[0] = 4;
    stride[1] = 4;
    count[0]  = 2;
    count[1]  = 2;
    block[0]  = 2;
    block[1]  = 3;
    status    = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, stride, count, block);

    /*
     * Read the data using the previously defined hyperslab.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, space, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nData as read from disk by hyperslab:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/16/h5ex_d_rdwr.c`

```c
/************************************************************

  This example shows how to read and write data to a
  dataset.  The program first writes integers to a dataset
  with dataspace dimensions of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_rdwr.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID;
    hid_t   space = H5I_INVALID_HID;
    hid_t   dset  = H5I_INVALID_HID;
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    int     wdata[DIM0][DIM1]; /* Write buffer */
    int     rdata[DIM0][DIM1]; /* Read buffer */
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset.  We will use all default properties for this
     * example.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/16/h5ex_d_shuffle.c`

```c
/************************************************************

  This example shows how to read and write data to a dataset
  using the shuffle filter with gzip compression.  The
  program first checks if the shuffle and gzip filters are
  available, then if they are it writes integers to a
  dataset using shuffle+gzip, then closes the file.  Next,
  it reopens the file, reads back the data, and outputs the
  types of filters and the maximum value in the dataset to
  the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_shuffle.h5"
#define DATASET  "DS1"
#define DIM0     32
#define DIM1     64
#define CHUNK0   4
#define CHUNK1   8

int
main(void)
{
    hid_t        file  = H5I_INVALID_HID;
    hid_t        space = H5I_INVALID_HID;
    hid_t        dset  = H5I_INVALID_HID;
    hid_t        dcpl  = H5I_INVALID_HID;
    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_type;
    hsize_t      dims[2]  = {DIM0, DIM1};
    hsize_t      chunk[2] = {CHUNK0, CHUNK1};
    size_t       nelmts;
    unsigned int flags;
    unsigned int filter_info;
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    int          max, nfilters;
    int          i, j;

    /*
     * Check if gzip compression is available and can be used for both
     * compression and decompression.  Normally we do not perform error
     * checking in these examples for the sake of clarity, but in this
     * case we will make an exception because this filter is an
     * optional part of the hdf5 library.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_DEFLATE);
    if (!avail) {
        printf("gzip filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_DEFLATE, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("gzip filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Similarly, check for availability of the shuffle filter.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_SHUFFLE);
    if (!avail) {
        printf("Shuffle filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_SHUFFLE, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("Shuffle filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, add the shuffle
     * filter and the gzip compression filter and set the chunk size.
     * The order in which the filters are added here is significant -
     * we will see much greater results when the shuffle is applied
     * first.  The order in which the filters are added to the property
     * list is the order in which they will be invoked when writing
     * data.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_shuffle(dcpl);
    status = H5Pset_deflate(dcpl, 9);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, dcpl);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve the number of filters, and retrieve and print the
     * type of each.
     */
    nfilters = H5Pget_nfilters(dcpl);
    for (i = 0; i < nfilters; i++) {
        nelmts      = 0;
        filter_type = H5Pget_filter(dcpl, i, &flags, &nelmts, NULL, 0, NULL);
        printf("Filter %d: Type is: ", i);
        switch (filter_type) {
            case H5Z_FILTER_DEFLATE:
                printf("H5Z_FILTER_DEFLATE\n");
                break;
            case H5Z_FILTER_SHUFFLE:
                printf("H5Z_FILTER_SHUFFLE\n");
                break;
            case H5Z_FILTER_FLETCHER32:
                printf("H5Z_FILTER_FLETCHER32\n");
                break;
            case H5Z_FILTER_SZIP:
                printf("H5Z_FILTER_SZIP\n");
        }
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            if (max < rdata[i][j])
                max = rdata[i][j];

    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is: %d\n", DATASET, max);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/16/h5ex_d_szip.c`

```c
/************************************************************

  This example shows how to read and write data to a dataset
  using szip compression.    The program first checks if
  szip compression is available, then if it is it writes
  integers to a dataset using szip, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs the type of compression and the maximum value in
  the dataset to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_szip.h5"
#define DATASET  "DS1"
#define DIM0     32
#define DIM1     64
#define CHUNK0   4
#define CHUNK1   8

int
main(void)
{
    hid_t file  = H5I_INVALID_HID;
    hid_t space = H5I_INVALID_HID;
    hid_t dset  = H5I_INVALID_HID;
    hid_t dcpl  = H5I_INVALID_HID;

    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_type;
    hsize_t      dims[2]  = {DIM0, DIM1};
    hsize_t      chunk[2] = {CHUNK0, CHUNK1};
    size_t       nelmts;
    unsigned int flags;
    unsigned int filter_info;
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    int          max;
    hsize_t      i, j;

    /*
     * Check if szip compression is available and can be used for both
     * compression and decompression.  Normally we do not perform error
     * checking in these examples for the sake of clarity, but in this
     * case we will make an exception because this filter is an
     * optional part of the hdf5 library.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_SZIP);
    if (!avail) {
        printf("szip filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_SZIP, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("szip filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, add the szip
     * compression filter and set the chunk size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_szip(dcpl, H5_SZIP_NN_OPTION_MASK, 8);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, dcpl);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve and print the filter type.  Here we only retrieve the
     * first filter because we know that we only added one filter.
     */
    nelmts      = 0;
    filter_type = H5Pget_filter(dcpl, 0, &flags, &nelmts, NULL, 0, NULL);
    printf("Filter type is: ");
    switch (filter_type) {
        case H5Z_FILTER_DEFLATE:
            printf("H5Z_FILTER_DEFLATE\n");
            break;
        case H5Z_FILTER_SHUFFLE:
            printf("H5Z_FILTER_SHUFFLE\n");
            break;
        case H5Z_FILTER_FLETCHER32:
            printf("H5Z_FILTER_FLETCHER32\n");
            break;
        case H5Z_FILTER_SZIP:
            printf("H5Z_FILTER_SZIP\n");
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            if (max < rdata[i][j])
                max = rdata[i][j];

    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is: %d\n", DATASET, max);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/16/h5ex_d_unlimadd.c`

```c
/************************************************************

  This example shows how to create and extend an unlimited
  dataset.  The program first writes integers to a dataset
  with dataspace dimensions of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data,
  outputs it to the screen, extends the dataset, and writes
  new data to the extended portions of the dataset.  Finally
  it reopens the file again, reads back the data, and
  outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_unlimadd.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7
#define EDIM0    6
#define EDIM1    10
#define CHUNK0   4
#define CHUNK1   4

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID;
    hid_t   space = H5I_INVALID_HID;
    hid_t   dset  = H5I_INVALID_HID;
    hid_t   dcpl  = H5I_INVALID_HID;
    herr_t  status;
    hsize_t dims[2]    = {DIM0, DIM1};
    hsize_t extdims[2] = {EDIM0, EDIM1};
    hsize_t maxdims[2];
    hsize_t chunk[2] = {CHUNK0, CHUNK1};
    hsize_t start[2];
    hsize_t count[2];
    int     wdata[DIM0][DIM1];    /* Write buffer */
    int     wdata2[EDIM0][EDIM1]; /* Write buffer for extension */
    int   **rdata = NULL;         /* Read buffer */
    int     ndims;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace with unlimited dimensions.
     */
    maxdims[0] = H5S_UNLIMITED;
    maxdims[1] = H5S_UNLIMITED;
    space      = H5Screate_simple(2, dims, maxdims);

    /*
     * Create the dataset creation property list, and set the chunk
     * size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the unlimited dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, dcpl);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * In this next section we read back the data, extend the dataset,
     * and write new data to the extended portions.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDWR, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (int **)malloc(dims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("Dataset before extension:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    status = H5Sclose(space);

    /*
     * Extend the dataset.
     */
    status = H5Dextend(dset, extdims);

    /*
     * Retrieve the dataspace for the newly extended dataset.
     */
    space = H5Dget_space(dset);

    /*
     * Initialize data for writing to the extended dataset.
     */
    for (i = 0; i < EDIM0; i++)
        for (j = 0; j < EDIM1; j++)
            wdata2[i][j] = j;

    /*
     * Select the entire dataspace.
     */
    status = H5Sselect_all(space);

    /*
     * Subtract a hyperslab reflecting the original dimensions from the
     * selection.  The selection now contains only the newly extended
     * portions of the dataset.
     */
    start[0] = 0;
    start[1] = 0;
    count[0] = dims[0];
    count[1] = dims[1];
    status   = H5Sselect_hyperslab(space, H5S_SELECT_NOTB, start, NULL, count, NULL);

    /*
     * Write the data to the selected portion of the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, space, H5P_DEFAULT, wdata2[0]);

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we simply read back the data and output it to the screen.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get dataspace and allocate memory for the read buffer as before.
     */
    space    = H5Dget_space(dset);
    ndims    = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata    = (int **)malloc(dims[0] * sizeof(int *));
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nDataset after extension:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/16/h5ex_d_unlimgzip.c`

```c
/************************************************************

  This example shows how to create and extend an unlimited
  dataset with gzip compression.  The program first writes
  integers to a gzip compressed dataset with dataspace
  dimensions of DIM0xDIM1, then closes the file.  Next, it
  reopens the file, reads back the data, outputs it to the
  screen, extends the dataset, and writes new data to the
  extended portions of the dataset.  Finally it reopens the
  file again, reads back the data, and outputs it to the
  screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_unlimgzip.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7
#define EDIM0    6
#define EDIM1    10
#define CHUNK0   4
#define CHUNK1   4

int
main(void)
{
    hid_t        file  = H5I_INVALID_HID;
    hid_t        space = H5I_INVALID_HID;
    hid_t        dset  = H5I_INVALID_HID;
    hid_t        dcpl  = H5I_INVALID_HID;
    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_type;
    hsize_t      dims[2]    = {DIM0, DIM1};
    hsize_t      extdims[2] = {EDIM0, EDIM1};
    hsize_t      maxdims[2];
    hsize_t      chunk[2] = {CHUNK0, CHUNK1};
    hsize_t      start[2];
    hsize_t      count[2];
    size_t       nelmts;
    unsigned int flags, filter_info;
    int          wdata[DIM0][DIM1];    /* Write buffer */
    int          wdata2[EDIM0][EDIM1]; /* Write buffer for extension */
    int        **rdata = NULL;         /* Read buffer */
    int          ndims;
    hsize_t      i, j;

    /*
     * Check if gzip compression is available and can be used for both
     * compression and decompression.  Normally we do not perform error
     * checking in these examples for the sake of clarity, but in this
     * case we will make an exception because this filter is an
     * optional part of the hdf5 library.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_DEFLATE);
    if (!avail) {
        printf("gzip filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_DEFLATE, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("gzip filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace with unlimited dimensions.
     */
    maxdims[0] = H5S_UNLIMITED;
    maxdims[1] = H5S_UNLIMITED;
    space      = H5Screate_simple(2, dims, maxdims);

    /*
     * Create the dataset creation property list, add the gzip
     * compression filter and set the chunk size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_deflate(dcpl, 9);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the compressed unlimited dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, dcpl);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * In this next section we read back the data, extend the dataset,
     * and write new data to the extended portions.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDWR, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (int **)malloc(dims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("Dataset before extension:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    status = H5Sclose(space);

    /*
     * Extend the dataset.
     */
    status = H5Dextend(dset, extdims);

    /*
     * Retrieve the dataspace for the newly extended dataset.
     */
    space = H5Dget_space(dset);

    /*
     * Initialize data for writing to the extended dataset.
     */
    for (i = 0; i < EDIM0; i++)
        for (j = 0; j < EDIM1; j++)
            wdata2[i][j] = j;

    /*
     * Select the entire dataspace.
     */
    status = H5Sselect_all(space);

    /*
     * Subtract a hyperslab reflecting the original dimensions from the
     * selection.  The selection now contains only the newly extended
     * portions of the dataset.
     */
    start[0] = 0;
    start[1] = 0;
    count[0] = dims[0];
    count[1] = dims[1];
    status   = H5Sselect_hyperslab(space, H5S_SELECT_NOTB, start, NULL, count, NULL);

    /*
     * Write the data to the selected portion of the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, space, H5P_DEFAULT, wdata2[0]);

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we simply read back the data and output it to the screen.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve and print the filter type.  Here we only retrieve the
     * first filter because we know that we only added one filter.
     */
    nelmts      = 0;
    filter_type = H5Pget_filter(dcpl, 0, &flags, &nelmts, NULL, 0, NULL);
    printf("\nFilter type is: ");
    switch (filter_type) {
        case H5Z_FILTER_DEFLATE:
            printf("H5Z_FILTER_DEFLATE\n");
            break;
        case H5Z_FILTER_SHUFFLE:
            printf("H5Z_FILTER_SHUFFLE\n");
            break;
        case H5Z_FILTER_FLETCHER32:
            printf("H5Z_FILTER_FLETCHER32\n");
            break;
        case H5Z_FILTER_SZIP:
            printf("H5Z_FILTER_SZIP\n");
    }

    /*
     * Get dataspace and allocate memory for the read buffer as before.
     */
    space    = H5Dget_space(dset);
    ndims    = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata    = (int **)malloc(dims[0] * sizeof(int *));
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("Dataset after extension:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/16/h5ex_d_unlimmod.c`

```c
/************************************************************

  This example shows how to create and extend an unlimited
  dataset.  The program first writes integers to a dataset
  with dataspace dimensions of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data,
  outputs it to the screen, extends the dataset, and writes
  new data to the entire extended dataset.  Finally it
  reopens the file again, reads back the data, and outputs it
  to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_unlimmod.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7
#define EDIM0    6
#define EDIM1    10
#define CHUNK0   4
#define CHUNK1   4

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID;
    hid_t   space = H5I_INVALID_HID;
    hid_t   dset  = H5I_INVALID_HID;
    hid_t   dcpl  = H5I_INVALID_HID;
    herr_t  status;
    hsize_t dims[2]    = {DIM0, DIM1};
    hsize_t extdims[2] = {EDIM0, EDIM1};
    hsize_t maxdims[2];
    hsize_t chunk[2] = {CHUNK0, CHUNK1};
    int     wdata[DIM0][DIM1];    /* Write buffer */
    int     wdata2[EDIM0][EDIM1]; /* Write buffer for extension */
    int   **rdata = NULL;         /* Read buffer */
    int     ndims;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace with unlimited dimensions.
     */
    maxdims[0] = H5S_UNLIMITED;
    maxdims[1] = H5S_UNLIMITED;
    space      = H5Screate_simple(2, dims, maxdims);

    /*
     * Create the dataset creation property list, and set the chunk
     * size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the unlimited dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, dcpl);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * In this next section we read back the data, extend the dataset,
     * and write new data to the entire dataset.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDWR, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (int **)malloc(dims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("Dataset before extension:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Extend the dataset.
     */
    status = H5Dset_extent(dset, extdims);

    /*
     * Initialize data for writing to the extended dataset.
     */
    for (i = 0; i < EDIM0; i++)
        for (j = 0; j < EDIM1; j++)
            wdata2[i][j] = j;

    /*
     * Write the data to the extended dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata2[0]);

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we simply read back the data and output it to the screen.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get dataspace and allocate memory for the read buffer as before.
     */
    space    = H5Dget_space(dset);
    ndims    = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata    = (int **)malloc(dims[0] * sizeof(int *));
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nDataset after extension:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_C_H5D C)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (C_sourcefiles.cmake)

foreach (example_name ${common_examples})
  if (${H5_LIBVER_DIR} EQUAL 16 OR ${EXAMPLE_VARNAME}_USE_16_API)
    add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/16/${example_name}.c)
  else ()
    add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
  endif ()
  target_compile_options (${EXAMPLE_VARNAME}_${example_name}
      PRIVATE
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  if (H5_HAVE_PARALLEL)
    target_include_directories (${EXAMPLE_VARNAME}_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
  endif ()
  target_link_libraries (${EXAMPLE_VARNAME}_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
  if (H5EXAMPLE_BUILD_TESTING)
    add_custom_command (
        TARGET     ${EXAMPLE_VARNAME}_${example_name}
        POST_BUILD
        COMMAND    ${CMAKE_COMMAND}
        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/16/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
    )
  endif ()
endforeach ()

if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8")
  foreach (example_name ${1_8_examples})
    add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
    target_compile_options (${EXAMPLE_VARNAME}_${example_name}
        PRIVATE
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
    )
    if (H5_HAVE_PARALLEL)
      target_include_directories (${EXAMPLE_VARNAME}_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
    endif ()
    target_link_libraries (${EXAMPLE_VARNAME}_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
    if (H5EXAMPLE_BUILD_TESTING)
      add_custom_command (
          TARGET     ${EXAMPLE_VARNAME}_${example_name}
          POST_BUILD
          COMMAND    ${CMAKE_COMMAND}
          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
      )
    endif ()
  endforeach ()
endif ()

#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
#  foreach (example_name ${1_10_examples})
#    add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
#    target_compile_options(${EXAMPLE_VARNAME}_${example_name}
#        PRIVATE
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
#    )
#    if (H5_HAVE_PARALLEL)
#      target_include_directories (${EXAMPLE_VARNAME}_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
#    endif ()
#    target_link_libraries (${EXAMPLE_VARNAME}_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
#  foreach (example_name ${1_12_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#          add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
#  foreach (example_name ${1_14_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
#  foreach (example_name ${2_0_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()

if (HDF5_PROVIDES_TOOLS)
  foreach (example_name ${common_examples})
    add_custom_command (
        TARGET     ${EXAMPLE_VARNAME}_${example_name}
        POST_BUILD
        COMMAND    ${CMAKE_COMMAND}
        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/16/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
    )
  endforeach ()

  foreach (example_name ${1_8_examples})
    if (${example_name} STREQUAL "h5ex_d_nbit")
      if (HDF5_VERSION_MAJOR VERSION_EQUAL "1.8" AND HDF5_VERSION_STRING VERSION_LESS_EQUAL "1.8.22")
        add_custom_command (
            TARGET     ${EXAMPLE_VARNAME}_${example_name}
            POST_BUILD
            COMMAND    ${CMAKE_COMMAND}
            ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}22.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
        )
      elseif (HDF5_VERSION_MAJOR VERSION_EQUAL "1.10" AND HDF5_VERSION_STRING VERSION_LESS_EQUAL "1.10.7")
        add_custom_command (
            TARGET     ${EXAMPLE_VARNAME}_${example_name}
            POST_BUILD
            COMMAND    ${CMAKE_COMMAND}
            ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}07.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
        )
      else ()
        add_custom_command (
            TARGET     ${EXAMPLE_VARNAME}_${example_name}
            POST_BUILD
            COMMAND    ${CMAKE_COMMAND}
            ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
        )
      endif ()
    else ()
      if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8")
        add_custom_command (
            TARGET     ${EXAMPLE_VARNAME}_${example_name}
            POST_BUILD
            COMMAND    ${CMAKE_COMMAND}
            ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
        )
      endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
    endif ()
  endforeach ()

#  foreach (example_name ${1_10_examples})
#  endforeach ()
#  foreach (example_name ${1_12_examples})
#  endforeach ()
#  foreach (example_name ${1_14_examples})
#  endforeach ()
#  foreach (example_name ${2_0_examples})
#  endforeach ()
endif ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
    )
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${ARGN};${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-${testname} PROPERTIES
            DEPENDS ${EXAMPLE_VARNAME}_${testname}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
      endif ()
    endif ()
  endmacro ()

  foreach (example_name ${common_examples} ${1_8_examples})
    if (${example_name} STREQUAL "h5ex_d_transform")
      ADD_H5_TEST (${example_name} -n)
    else ()
      ADD_H5_TEST (${example_name})
    endif ()
  endforeach ()
endif ()
```

### `HDF5Examples/C/H5D/C_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (examples)

set (common_examples
    h5ex_d_alloc
    h5ex_d_checksum
    h5ex_d_chunk
    h5ex_d_compact
    h5ex_d_extern
    h5ex_d_fillval
    h5ex_d_hyper
    h5ex_d_rdwr
    h5ex_d_unlimadd
    h5ex_d_unlimmod
)

if (HDF5_PROVIDES_ZLIB_SUPPORT)
  set (common_examples ${common_examples}
      h5ex_d_gzip
      h5ex_d_shuffle
      h5ex_d_unlimgzip
  )
endif ()

if (HDF5_PROVIDES_SZIP_SUPPORT)
  set (common_examples ${common_examples}
      h5ex_d_szip
  )
endif ()

if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8" AND NOT ${EXAMPLE_VARNAME}_USE_16_API)
  set (1_8_examples
      h5ex_d_nbit
      h5ex_d_sofloat
      h5ex_d_soint
      h5ex_d_transform
  )
else ()
  set (1_8_examples)
endif ()
```

### `HDF5Examples/C/H5D/h5ex_d_alloc.c`

```c
/************************************************************

  This example shows how to set the space allocation time
  for a dataset.  The program first creates two datasets,
  one with the default allocation time (late) and one with
  early allocation time, and displays whether each has been
  allocated and their allocation size.  Next, it writes data
  to the datasets, and again displays whether each has been
  allocated and their allocation size.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_alloc.h5"
#define DATASET1 "DS1"
#define DATASET2 "DS2"
#define DIM0     4
#define DIM1     7
#define FILLVAL  99

int
main(void)
{
    hid_t file  = H5I_INVALID_HID;
    hid_t space = H5I_INVALID_HID;
    hid_t dset1 = H5I_INVALID_HID;
    hid_t dset2 = H5I_INVALID_HID;
    hid_t dcpl  = H5I_INVALID_HID;
    /* Handles */
    herr_t             status;
    H5D_space_status_t space_status;
    hsize_t            dims[2] = {DIM0, DIM1};
    hsize_t            storage_size;
    int                wdata[DIM0][DIM1]; /* Write buffer */
    hsize_t            i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, and set the chunk
     * size.
     */
    dcpl = H5Pcreate(H5P_DATASET_CREATE);

    /*
     * Set the allocation time to "early".  This way we can be sure
     * that reading from the dataset immediately after creation will
     * return the fill value.
     */
    status = H5Pset_alloc_time(dcpl, H5D_ALLOC_TIME_EARLY);

    printf("Creating datasets...\n");
    printf("%s has allocation time H5D_ALLOC_TIME_LATE\n", DATASET1);
    printf("%s has allocation time H5D_ALLOC_TIME_EARLY\n\n", DATASET2);

    /*
     * Create the dataset using the dataset creation property list.
     */
    dset1 = H5Dcreate(file, DATASET1, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    dset2 = H5Dcreate(file, DATASET2, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Retrieve and print space status and storage size for dset1.
     */
    status       = H5Dget_space_status(dset1, &space_status);
    storage_size = H5Dget_storage_size(dset1);
    printf("Space for %s has%sbeen allocated.\n", DATASET1,
           space_status == H5D_SPACE_STATUS_ALLOCATED ? " " : " not ");
    printf("Storage size for %s is: %ld bytes.\n", DATASET1, (long)storage_size);

    /*
     * Retrieve and print space status and storage size for dset2.
     */
    status       = H5Dget_space_status(dset2, &space_status);
    storage_size = H5Dget_storage_size(dset2);
    printf("Space for %s has%sbeen allocated.\n", DATASET2,
           space_status == H5D_SPACE_STATUS_ALLOCATED ? " " : " not ");
    printf("Storage size for %s is: %ld bytes.\n", DATASET2, (long)storage_size);

    printf("\nWriting data...\n\n");

    /*
     * Write the data to the datasets.
     */
    status = H5Dwrite(dset1, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);
    status = H5Dwrite(dset2, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Retrieve and print space status and storage size for dset1.
     */
    status       = H5Dget_space_status(dset1, &space_status);
    storage_size = H5Dget_storage_size(dset1);
    printf("Space for %s has%sbeen allocated.\n", DATASET1,
           space_status == H5D_SPACE_STATUS_ALLOCATED ? " " : " not ");
    printf("Storage size for %s is: %ld bytes.\n", DATASET1, (long)storage_size);

    /*
     * Retrieve and print space status and storage size for dset2.
     */
    status       = H5Dget_space_status(dset2, &space_status);
    storage_size = H5Dget_storage_size(dset2);
    printf("Space for %s has%sbeen allocated.\n", DATASET2,
           space_status == H5D_SPACE_STATUS_ALLOCATED ? " " : " not ");
    printf("Storage size for %s is: %ld bytes.\n", DATASET2, (long)storage_size);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset1);
    status = H5Dclose(dset2);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_checksum.c`

```c
/************************************************************

  This example shows how to read and write data to a dataset
  using the Fletcher32 checksum filter.  The program first
  checks if the Fletcher32 filter is available, then if it
  is it writes integers to a dataset using Fletcher32, then
  closes the file.  Next, it reopens the file, reads back
  the data, checks if the filter detected an error and
  outputs the type of filter and the maximum value in the
  dataset to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_checksum.h5"
#define DATASET  "DS1"
#define DIM0     32
#define DIM1     64
#define CHUNK0   4
#define CHUNK1   8

int
main(void)
{
    hid_t file  = H5I_INVALID_HID;
    hid_t space = H5I_INVALID_HID;
    hid_t dset  = H5I_INVALID_HID;
    hid_t dcpl  = H5I_INVALID_HID;
    /* Handles */
    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_type;
    hsize_t      dims[2] = {DIM0, DIM1}, chunk[2] = {CHUNK0, CHUNK1};
    size_t       nelmts;
    unsigned int flags, filter_info;
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    int          max;
    hsize_t      i, j;

    /*
     * Check if the Fletcher32 filter is available and can be used for
     * both encoding and decoding.  Normally we do not perform error
     * checking in these examples for the sake of clarity, but in this
     * case we will make an exception because this filter is an
     * optional part of the hdf5 library.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_FLETCHER32);
    if (!avail) {
        printf("Fletcher32 filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_FLETCHER32, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("Fletcher32 filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, add the Fletcher32 filter
     * and set the chunk size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_fletcher32(dcpl);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve and print the filter type.  Here we only retrieve the
     * first filter because we know that we only added one filter.
     */
    nelmts      = 0;
    filter_type = H5Pget_filter(dcpl, 0, &flags, &nelmts, NULL, 0, NULL, &filter_info);
    printf("Filter type is: ");
    switch (filter_type) {
        case H5Z_FILTER_DEFLATE:
            printf("H5Z_FILTER_DEFLATE\n");
            break;
        case H5Z_FILTER_SHUFFLE:
            printf("H5Z_FILTER_SHUFFLE\n");
            break;
        case H5Z_FILTER_FLETCHER32:
            printf("H5Z_FILTER_FLETCHER32\n");
            break;
        case H5Z_FILTER_SZIP:
            printf("H5Z_FILTER_SZIP\n");
            break;
        case H5Z_FILTER_NBIT:
            printf("H5Z_FILTER_NBIT\n");
            break;
        case H5Z_FILTER_SCALEOFFSET:
            printf("H5Z_FILTER_SCALEOFFSET\n");
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Check if the read was successful.  Normally we do not perform
     * error checking in these examples for the sake of clarity, but in
     * this case we will make an exception because this is how the
     * fletcher32 checksum filter reports data errors.
     */
    if (status < 0) {
        fprintf(stderr, "Dataset read failed!\n");
        status = H5Pclose(dcpl);
        status = H5Dclose(dset);
        status = H5Fclose(file);
        return 2;
    }

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            if (max < rdata[i][j])
                max = rdata[i][j];

    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is: %d\n", DATASET, max);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_chunk.c`

```c
/************************************************************

  This example shows how to create a chunked dataset.  The
  program first writes integers in a hyperslab selection to
  a chunked dataset with dataspace dimensions of DIM0xDIM1
  and chunk size of CHUNK0xCHUNK1, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs it to the screen.  Finally it reads the data again
  using a different hyperslab selection, and outputs
  the result to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_chunk.h5"
#define DATASET  "DS1"
#define DIM0     6
#define DIM1     8
#define CHUNK0   4
#define CHUNK1   4

int
main(void)
{
    hid_t        file  = H5I_INVALID_HID;
    hid_t        space = H5I_INVALID_HID;
    hid_t        dset  = H5I_INVALID_HID;
    hid_t        dcpl  = H5I_INVALID_HID;
    herr_t       status;
    H5D_layout_t layout;
    hsize_t      dims[2]  = {DIM0, DIM1};
    hsize_t      chunk[2] = {CHUNK0, CHUNK1};
    hsize_t      start[2];
    hsize_t      stride[2];
    hsize_t      count[2];
    hsize_t      block[2];
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    hsize_t      i, j;

    /*
     * Initialize data to "1", to make it easier to see the selections.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = 1;

    /*
     * Print the data to the screen.
     */
    printf("Original Data:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", wdata[i][j]);
        printf("]\n");
    }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, and set the chunk
     * size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the chunked dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Define and select the first part of the hyperslab selection.
     */
    start[0]  = 0;
    start[1]  = 0;
    stride[0] = 3;
    stride[1] = 3;
    count[0]  = 2;
    count[1]  = 3;
    block[0]  = 2;
    block[1]  = 2;
    status    = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, stride, count, block);

    /*
     * Define and select the second part of the hyperslab selection,
     * which is subtracted from the first selection by the use of
     * H5S_SELECT_NOTB
     */
    block[0] = 1;
    block[1] = 1;
    status   = H5Sselect_hyperslab(space, H5S_SELECT_NOTB, start, stride, count, block);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, space, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Retrieve the dataset creation property list, and print the
     * storage layout.
     */
    dcpl   = H5Dget_create_plist(dset);
    layout = H5Pget_layout(dcpl);
    printf("\nStorage layout for %s is: ", DATASET);
    switch (layout) {
        case H5D_COMPACT:
            printf("H5D_COMPACT\n");
            break;
        case H5D_CONTIGUOUS:
            printf("H5D_CONTIGUOUS\n");
            break;
        case H5D_CHUNKED:
            printf("H5D_CHUNKED\n");
            break;
#if H5_VERSION_GE(1, 10, 0) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
        case H5D_VIRTUAL:
            printf("H5D_VIRTUAL\n");
            break;
#endif
        case H5D_LAYOUT_ERROR:
        case H5D_NLAYOUTS:
            printf("H5D_LAYOUT_ERROR\n");
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nData as written to disk by hyberslabs:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Initialize the read array.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            rdata[i][j] = 0;

    /*
     * Define and select the hyperslab to use for reading.
     */
    space     = H5Dget_space(dset);
    start[0]  = 0;
    start[1]  = 1;
    stride[0] = 4;
    stride[1] = 4;
    count[0]  = 2;
    count[1]  = 2;
    block[0]  = 2;
    block[1]  = 3;
    status    = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, stride, count, block);

    /*
     * Read the data using the previously defined hyperslab.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, space, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nData as read from disk by hyperslab:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_compact.c`

```c
/************************************************************

  This example shows how to read and write data to a compact
  dataset.  The program first writes integers to a compact
  dataset with dataspace dimensions of DIM0xDIM1, then
  closes the file.  Next, it reopens the file, reads back
  the data, and outputs it to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_compact.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7

int
main(void)
{
    hid_t        file  = H5I_INVALID_HID;
    hid_t        space = H5I_INVALID_HID;
    hid_t        dset  = H5I_INVALID_HID;
    hid_t        dcpl  = H5I_INVALID_HID;
    herr_t       status;
    H5D_layout_t layout;
    hsize_t      dims[2] = {DIM0, DIM1};
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    hsize_t      i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, set the layout to
     * compact.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_layout(dcpl, H5D_COMPACT);

    /*
     * Create the dataset.  We will use all default properties for this
     * example.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Retrieve the dataset creation property list, and print the
     * storage layout.
     */
    dcpl   = H5Dget_create_plist(dset);
    layout = H5Pget_layout(dcpl);
    printf("Storage layout for %s is: ", DATASET);
    switch (layout) {
        case H5D_COMPACT:
            printf("H5D_COMPACT\n");
            break;
        case H5D_CONTIGUOUS:
            printf("H5D_CONTIGUOUS\n");
            break;
        case H5D_CHUNKED:
            printf("H5D_CHUNKED\n");
            break;
#if H5_VERSION_GE(1, 10, 0) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
        case H5D_VIRTUAL:
            printf("H5D_VIRTUAL\n");
            break;
#endif
        case H5D_LAYOUT_ERROR:
        case H5D_NLAYOUTS:
            printf("H5D_LAYOUT_ERROR\n");
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_extern.c`

```c
/************************************************************

  This example shows how to read and write data to an
  external dataset.  The program first writes integers to an
  external dataset with dataspace dimensions of DIM0xDIM1,
  then closes the file.  Next, it reopens the file, reads
  back the data, and outputs the name of the external data
  file and the data to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME      "h5ex_d_extern.h5"
#define EXTERNAL      "h5ex_d_extern.data"
#define DATASET       "DS1"
#define DIM0          4
#define DIM1          7
#define NAME_BUF_SIZE 32

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID;
    hid_t   space = H5I_INVALID_HID;
    hid_t   dset  = H5I_INVALID_HID;
    hid_t   dcpl  = H5I_INVALID_HID;
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    char    name[NAME_BUF_SIZE];
    int     wdata[DIM0][DIM1]; /* Write buffer */
    int     rdata[DIM0][DIM1]; /* Read buffer */
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, set the external
     * file.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_external(dcpl, EXTERNAL, 0, H5F_UNLIMITED);

    /*
     * Create the external dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve and print the name of the external file.  Here we
     * manually set the last field in name to null, in case the name of
     * the file is longer than the buffer.
     */
    status                  = H5Pget_external(dcpl, 0, NAME_BUF_SIZE, name, NULL, NULL);
    name[NAME_BUF_SIZE - 1] = '\0';
    printf("%s is stored in file: %s\n", DATASET, name);

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_fillval.c`

```c
/************************************************************

  This example shows how to set the fill value for a
  dataset.  The program first sets the fill value to
  FILLVAL, creates a dataset with dimensions of DIM0xDIM1,
  reads from the uninitialized dataset, and outputs the
  contents to the screen.  Next, it writes integers to the
  dataset, reads the data back, and outputs it to the
  screen.  Finally it extends the dataset, reads from it,
  and outputs the result to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_fillval.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7
#define EDIM0    6
#define EDIM1    10
#define CHUNK0   4
#define CHUNK1   4
#define FILLVAL  99

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID;
    hid_t   space = H5I_INVALID_HID;
    hid_t   dset  = H5I_INVALID_HID;
    hid_t   dcpl  = H5I_INVALID_HID;
    herr_t  status;
    hsize_t dims[2]    = {DIM0, DIM1};
    hsize_t extdims[2] = {EDIM0, EDIM1};
    hsize_t maxdims[2] = {H5S_UNLIMITED, H5S_UNLIMITED};
    hsize_t chunk[2]   = {CHUNK0, CHUNK1};
    int     wdata[DIM0][DIM1];    /* Write buffer */
    int     rdata[DIM0][DIM1];    /* Read buffer */
    int     rdata2[EDIM0][EDIM1]; /* Read buffer for extension */
    int     fillval;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace with unlimited dimensions.
     */
    space = H5Screate_simple(2, dims, maxdims);

    /*
     * Create the dataset creation property list, and set the chunk
     * size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Set the fill value for the dataset.
     */
    fillval = FILLVAL;
    status  = H5Pset_fill_value(dcpl, H5T_NATIVE_INT, &fillval);

    /*
     * Set the allocation time to "early".  This way we can be sure
     * that reading from the dataset immediately after creation will
     * return the fill value.
     */
    status = H5Pset_alloc_time(dcpl, H5D_ALLOC_TIME_EARLY);

    /*
     * Create the dataset using the dataset creation property list.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Read values from the dataset, which has not been written to yet.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("Dataset before being written to:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Read the data back.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nDataset after being written to:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Extend the dataset.
     */
    status = H5Dset_extent(dset, extdims);

    /*
     * Read from the extended dataset.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata2[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nDataset after extension:\n");
    for (i = 0; i < extdims[0]; i++) {
        printf(" [");
        for (j = 0; j < extdims[1]; j++)
            printf(" %3d", rdata2[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_gzip.c`

```c
/************************************************************

  This example shows how to read and write data to a dataset
  using gzip compression (also called zlib or deflate).  The
  program first checks if gzip compression is available,
  then if it is it writes integers to a dataset using gzip,
  then closes the file.  Next, it reopens the file, reads
  back the data, and outputs the type of compression and the
  maximum value in the dataset to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_gzip.h5"
#define DATASET  "DS1"
#define DIM0     32
#define DIM1     64
#define CHUNK0   4
#define CHUNK1   8

int
main(void)
{
    hid_t        file  = H5I_INVALID_HID;
    hid_t        space = H5I_INVALID_HID;
    hid_t        dset  = H5I_INVALID_HID;
    hid_t        dcpl  = H5I_INVALID_HID;
    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_type;
    hsize_t      dims[2]  = {DIM0, DIM1};
    hsize_t      chunk[2] = {CHUNK0, CHUNK1};
    size_t       nelmts;
    unsigned int flags;
    unsigned int filter_info;
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    int          max;
    hsize_t      i, j;

    /*
     * Check if gzip compression is available and can be used for both
     * compression and decompression.  Normally we do not perform error
     * checking in these examples for the sake of clarity, but in this
     * case we will make an exception because this filter is an
     * optional part of the hdf5 library.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_DEFLATE);
    if (!avail) {
        printf("gzip filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_DEFLATE, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("gzip filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, add the gzip
     * compression filter and set the chunk size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_deflate(dcpl, 9);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve and print the filter type.  Here we only retrieve the
     * first filter because we know that we only added one filter.
     */
    nelmts      = 0;
    filter_type = H5Pget_filter(dcpl, 0, &flags, &nelmts, NULL, 0, NULL, &filter_info);
    printf("Filter type is: ");
    switch (filter_type) {
        case H5Z_FILTER_DEFLATE:
            printf("H5Z_FILTER_DEFLATE\n");
            break;
        case H5Z_FILTER_SHUFFLE:
            printf("H5Z_FILTER_SHUFFLE\n");
            break;
        case H5Z_FILTER_FLETCHER32:
            printf("H5Z_FILTER_FLETCHER32\n");
            break;
        case H5Z_FILTER_SZIP:
            printf("H5Z_FILTER_SZIP\n");
            break;
        case H5Z_FILTER_NBIT:
            printf("H5Z_FILTER_NBIT\n");
            break;
        case H5Z_FILTER_SCALEOFFSET:
            printf("H5Z_FILTER_SCALEOFFSET\n");
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            if (max < rdata[i][j])
                max = rdata[i][j];

    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is: %d\n", DATASET, max);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_hyper.c`

```c
/************************************************************

  This example shows how to read and write data to a
  dataset by hyberslabs.  The program first writes integers
  in a hyperslab selection to a dataset with dataspace
  dimensions of DIM0xDIM1, then closes the file.  Next, it
  reopens the file, reads back the data, and outputs it to
  the screen.  Finally it reads the data again using a
  different hyperslab selection, and outputs the result to
  the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_hyper.h5"
#define DATASET  "DS1"
#define DIM0     6
#define DIM1     8

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID;
    hid_t   space = H5I_INVALID_HID;
    hid_t   dset  = H5I_INVALID_HID;
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    hsize_t start[2];
    hsize_t stride[2];
    hsize_t count[2];
    hsize_t block[2];
    int     wdata[DIM0][DIM1]; /* Write buffer */
    int     rdata[DIM0][DIM1]; /* Read buffer */
    hsize_t i, j;

    /*
     * Initialize data to "1", to make it easier to see the selections.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = 1;

    /*
     * Print the data to the screen.
     */
    printf("Original Data:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", wdata[i][j]);
        printf("]\n");
    }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset.  We will use all default properties for this
     * example.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Define and select the first part of the hyperslab selection.
     */
    start[0]  = 0;
    start[1]  = 0;
    stride[0] = 3;
    stride[1] = 3;
    count[0]  = 2;
    count[1]  = 3;
    block[0]  = 2;
    block[1]  = 2;
    status    = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, stride, count, block);

    /*
     * Define and select the second part of the hyperslab selection,
     * which is subtracted from the first selection by the use of
     * H5S_SELECT_NOTB
     */
    block[0] = 1;
    block[1] = 1;
    status   = H5Sselect_hyperslab(space, H5S_SELECT_NOTB, start, stride, count, block);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, space, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nData as written to disk by hyberslabs:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Initialize the read array.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            rdata[i][j] = 0;

    /*
     * Define and select the hyperslab to use for reading.
     */
    space     = H5Dget_space(dset);
    start[0]  = 0;
    start[1]  = 1;
    stride[0] = 4;
    stride[1] = 4;
    count[0]  = 2;
    count[1]  = 2;
    block[0]  = 2;
    block[1]  = 3;
    status    = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, stride, count, block);

    /*
     * Read the data using the previously defined hyperslab.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, space, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nData as read from disk by hyperslab:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_nbit.c`

```c
/************************************************************

  This example shows how to read and write data to a dataset
  using the N-Bit filter.  The program first checks if the
  N-Bit filter is available, then if it is it writes
  integers to a dataset using N-Bit, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs the type of filter and the maximum value in the
  dataset to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_nbit.h5"
#define DATASET  "DS1"
#define DIM0     32
#define DIM1     64
#define CHUNK0   4
#define CHUNK1   8

int
main(void)
{
    hid_t file  = H5I_INVALID_HID;
    hid_t space = H5I_INVALID_HID;
    hid_t dset  = H5I_INVALID_HID;
    hid_t dtype = H5I_INVALID_HID;
    hid_t dcpl  = H5I_INVALID_HID;
    /* Handles */
    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_type;
    hsize_t      dims[2]  = {DIM0, DIM1};
    hsize_t      chunk[2] = {CHUNK0, CHUNK1};
    size_t       nelmts;
    unsigned int flags;
    unsigned int filter_info;
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    int          max;
    hsize_t      i, j;

    /*
     * Check if N-Bit compression is available and can be used for both
     * compression and decompression.  Normally we do not perform error
     * checking in these examples for the sake of clarity, but in this
     * case we will make an exception because this filter is an
     * optional part of the hdf5 library.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_NBIT);
    if (!avail) {
        printf("N-Bit filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_NBIT, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("N-Bit filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the datatype to use with the N-Bit filter.  It has an
     * uncompressed size of 32 bits, but will have a size of 16 bits
     * after being packed by the N-Bit filter.
     */
    dtype  = H5Tcopy(H5T_STD_I32LE);
    status = H5Tset_precision(dtype, 16);
    status = H5Tset_offset(dtype, 5);

    /*
     * Create the dataset creation property list, add the N-Bit filter
     * and set the chunk size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_nbit(dcpl);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the dataset.
     */
    dset = H5Dcreate(file, DATASET, dtype, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Tclose(dtype);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve and print the filter type.  Here we only retrieve the
     * first filter because we know that we only added one filter.
     */
    nelmts      = 0;
    filter_type = H5Pget_filter(dcpl, 0, &flags, &nelmts, NULL, 0, NULL, &filter_info);
    printf("Filter type is: ");
    switch (filter_type) {
        case H5Z_FILTER_DEFLATE:
            printf("H5Z_FILTER_DEFLATE\n");
            break;
        case H5Z_FILTER_SHUFFLE:
            printf("H5Z_FILTER_SHUFFLE\n");
            break;
        case H5Z_FILTER_FLETCHER32:
            printf("H5Z_FILTER_FLETCHER32\n");
            break;
        case H5Z_FILTER_SZIP:
            printf("H5Z_FILTER_SZIP\n");
            break;
        case H5Z_FILTER_NBIT:
            printf("H5Z_FILTER_NBIT\n");
            break;
        case H5Z_FILTER_SCALEOFFSET:
            printf("H5Z_FILTER_SCALEOFFSET\n");
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            if (max < rdata[i][j])
                max = rdata[i][j];

    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is: %d\n", DATASET, max);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_rdwr.c`

```c
/************************************************************

  This example shows how to read and write data to a
  dataset.  The program first writes integers to a dataset
  with dataspace dimensions of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_rdwr.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID;
    hid_t   space = H5I_INVALID_HID;
    hid_t   dset  = H5I_INVALID_HID;
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    int     wdata[DIM0][DIM1]; /* Write buffer */
    int     rdata[DIM0][DIM1]; /* Read buffer */
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset.  We will use all default properties for this
     * example.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_shuffle.c`

```c
/************************************************************

  This example shows how to read and write data to a dataset
  using the shuffle filter with gzip compression.  The
  program first checks if the shuffle and gzip filters are
  available, then if they are it writes integers to a
  dataset using shuffle+gzip, then closes the file.  Next,
  it reopens the file, reads back the data, and outputs the
  types of filters and the maximum value in the dataset to
  the screen.

  This file is intended for use with HDF5 Library version 1.8

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_shuffle.h5"
#define DATASET  "DS1"
#define DIM0     32
#define DIM1     64
#define CHUNK0   4
#define CHUNK1   8

int
main(void)
{
    hid_t        file  = H5I_INVALID_HID;
    hid_t        space = H5I_INVALID_HID;
    hid_t        dset  = H5I_INVALID_HID;
    hid_t        dcpl  = H5I_INVALID_HID;
    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_type;
    hsize_t      dims[2]  = {DIM0, DIM1};
    hsize_t      chunk[2] = {CHUNK0, CHUNK1};
    size_t       nelmts;
    unsigned int flags;
    unsigned int filter_info;
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    int          max, nfilters;
    int          i, j;

    /*
     * Check if gzip compression is available and can be used for both
     * compression and decompression.  Normally we do not perform error
     * checking in these examples for the sake of clarity, but in this
     * case we will make an exception because this filter is an
     * optional part of the hdf5 library.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_DEFLATE);
    if (!avail) {
        printf("gzip filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_DEFLATE, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("gzip filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Similarly, check for availability of the shuffle filter.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_SHUFFLE);
    if (!avail) {
        printf("Shuffle filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_SHUFFLE, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("Shuffle filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, add the shuffle
     * filter and the gzip compression filter and set the chunk size.
     * The order in which the filters are added here is significant -
     * we will see much greater results when the shuffle is applied
     * first.  The order in which the filters are added to the property
     * list is the order in which they will be invoked when writing
     * data.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_shuffle(dcpl);
    status = H5Pset_deflate(dcpl, 9);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve the number of filters, and retrieve and print the
     * type of each.
     */
    nfilters = H5Pget_nfilters(dcpl);
    for (i = 0; i < nfilters; i++) {
        nelmts      = 0;
        filter_type = H5Pget_filter(dcpl, i, &flags, &nelmts, NULL, 0, NULL, &filter_info);
        printf("Filter %d: Type is: ", i);
        switch (filter_type) {
            case H5Z_FILTER_DEFLATE:
                printf("H5Z_FILTER_DEFLATE\n");
                break;
            case H5Z_FILTER_SHUFFLE:
                printf("H5Z_FILTER_SHUFFLE\n");
                break;
            case H5Z_FILTER_FLETCHER32:
                printf("H5Z_FILTER_FLETCHER32\n");
                break;
            case H5Z_FILTER_SZIP:
                printf("H5Z_FILTER_SZIP\n");
                break;
            case H5Z_FILTER_NBIT:
                printf("H5Z_FILTER_NBIT\n");
                break;
            case H5Z_FILTER_SCALEOFFSET:
                printf("H5Z_FILTER_SCALEOFFSET\n");
        }
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            if (max < rdata[i][j])
                max = rdata[i][j];

    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is: %d\n", DATASET, max);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_sofloat.c`

```c
/************************************************************

  This example shows how to read and write data to a dataset
  using the Scale-Offset filter.  The program first checks
  if the Scale-Offset filter is available, then if it is it
  writes floating point numbers to a dataset using
  Scale-Offset, then closes the file Next, it reopens the
  file, reads back the data, and outputs the type of filter
  and the maximum value in the dataset to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_sofloat.h5"
#define DATASET  "DS1"
#define DIM0     32
#define DIM1     64
#define CHUNK0   4
#define CHUNK1   8

int
main(void)
{
    hid_t file  = H5I_INVALID_HID;
    hid_t space = H5I_INVALID_HID;
    hid_t dset  = H5I_INVALID_HID;
    hid_t dcpl  = H5I_INVALID_HID;
    /* Handles */
    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_type;
    hsize_t      dims[2]  = {DIM0, DIM1};
    hsize_t      chunk[2] = {CHUNK0, CHUNK1};
    size_t       nelmts;
    unsigned int flags;
    unsigned int filter_info;
    double       wdata[DIM0][DIM1]; /* Write buffer */
    double       rdata[DIM0][DIM1]; /* Read buffer */
    double       max, min;
    hsize_t      i, j;

    /*
     * Check if Scale-Offset compression is available and can be used
     * for both compression and decompression.  Normally we do not
     * perform error checking in these examples for the sake of
     * clarity, but in this case we will make an exception because this
     * filter is an optional part of the hdf5 library.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_SCALEOFFSET);
    if (!avail) {
        printf("Scale-Offset filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_SCALEOFFSET, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("Scale-Offset filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = (double)(i + 1) / (j + 0.3) + j;

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = wdata[0][0];
    min = wdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            if (max < wdata[i][j])
                max = wdata[i][j];
            if (min > wdata[i][j])
                min = wdata[i][j];
        }

    /*
     * Print the maximum value.
     */
    printf("Maximum value in write buffer is: %f\n", max);
    printf("Minimum value in write buffer is: %f\n", min);

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, add the Scale-Offset
     * filter and set the chunk size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_scaleoffset(dcpl, H5Z_SO_FLOAT_DSCALE, 2);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_IEEE_F64LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_DOUBLE, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve and print the filter type.  Here we only retrieve the
     * first filter because we know that we only added one filter.
     */
    nelmts      = 0;
    filter_type = H5Pget_filter(dcpl, 0, &flags, &nelmts, NULL, 0, NULL, &filter_info);
    printf("Filter type is: ");
    switch (filter_type) {
        case H5Z_FILTER_DEFLATE:
            printf("H5Z_FILTER_DEFLATE\n");
            break;
        case H5Z_FILTER_SHUFFLE:
            printf("H5Z_FILTER_SHUFFLE\n");
            break;
        case H5Z_FILTER_FLETCHER32:
            printf("H5Z_FILTER_FLETCHER32\n");
            break;
        case H5Z_FILTER_SZIP:
            printf("H5Z_FILTER_SZIP\n");
            break;
        case H5Z_FILTER_NBIT:
            printf("H5Z_FILTER_NBIT\n");
            break;
        case H5Z_FILTER_SCALEOFFSET:
            printf("H5Z_FILTER_SCALEOFFSET\n");
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_DOUBLE, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    min = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            if (max < rdata[i][j])
                max = rdata[i][j];
            if (min > rdata[i][j])
                min = rdata[i][j];
        }

    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is: %f\n", DATASET, max);
    printf("Minimum value in %s is: %f\n", DATASET, min);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_soint.c`

```c
/************************************************************

  This example shows how to read and write data to a dataset
  using the Scale-Offset filter.  The program first checks
  if the Scale-Offset filter is available, then if it is it
  writes integers to a dataset using Scale-Offset, then
  closes the file Next, it reopens the file, reads back the
  data, and outputs the type of filter and the maximum value
  in the dataset to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_soint.h5"
#define DATASET  "DS1"
#define DIM0     32
#define DIM1     64
#define CHUNK0   4
#define CHUNK1   8

int
main(void)
{
    hid_t file  = H5I_INVALID_HID;
    hid_t space = H5I_INVALID_HID;
    hid_t dset  = H5I_INVALID_HID;
    hid_t dcpl  = H5I_INVALID_HID;
    /* Handles */
    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_type;
    hsize_t      dims[2]  = {DIM0, DIM1};
    hsize_t      chunk[2] = {CHUNK0, CHUNK1};
    size_t       nelmts;
    unsigned int flags;
    unsigned int filter_info;
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    int          max;
    hsize_t      i, j;

    /*
     * Check if Scale-Offset compression is available and can be used
     * for both compression and decompression.  Normally we do not
     * perform error checking in these examples for the sake of
     * clarity, but in this case we will make an exception because this
     * filter is an optional part of the hdf5 library.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_SCALEOFFSET);
    if (!avail) {
        printf("Scale-Offset filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_SCALEOFFSET, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("Scale-Offset filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, add the Scale-Offset
     * filter and set the chunk size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_scaleoffset(dcpl, H5Z_SO_INT, H5Z_SO_INT_MINBITS_DEFAULT);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve and print the filter type.  Here we only retrieve the
     * first filter because we know that we only added one filter.
     */
    nelmts      = 0;
    filter_type = H5Pget_filter(dcpl, 0, &flags, &nelmts, NULL, 0, NULL, &filter_info);
    printf("Filter type is: ");
    switch (filter_type) {
        case H5Z_FILTER_DEFLATE:
            printf("H5Z_FILTER_DEFLATE\n");
            break;
        case H5Z_FILTER_SHUFFLE:
            printf("H5Z_FILTER_SHUFFLE\n");
            break;
        case H5Z_FILTER_FLETCHER32:
            printf("H5Z_FILTER_FLETCHER32\n");
            break;
        case H5Z_FILTER_SZIP:
            printf("H5Z_FILTER_SZIP\n");
            break;
        case H5Z_FILTER_NBIT:
            printf("H5Z_FILTER_NBIT\n");
            break;
        case H5Z_FILTER_SCALEOFFSET:
            printf("H5Z_FILTER_SCALEOFFSET\n");
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            if (max < rdata[i][j])
                max = rdata[i][j];

    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is: %d\n", DATASET, max);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_szip.c`

```c
/************************************************************

  This example shows how to read and write data to a dataset
  using szip compression.    The program first checks if
  szip compression is available, then if it is it writes
  integers to a dataset using szip, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs the type of compression and the maximum value in
  the dataset to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_szip.h5"
#define DATASET  "DS1"
#define DIM0     32
#define DIM1     64
#define CHUNK0   4
#define CHUNK1   8

int
main(void)
{
    hid_t file  = H5I_INVALID_HID;
    hid_t space = H5I_INVALID_HID;
    hid_t dset  = H5I_INVALID_HID;
    hid_t dcpl  = H5I_INVALID_HID;

    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_type;
    hsize_t      dims[2]  = {DIM0, DIM1};
    hsize_t      chunk[2] = {CHUNK0, CHUNK1};
    size_t       nelmts;
    unsigned int flags;
    unsigned int filter_info;
    int          wdata[DIM0][DIM1]; /* Write buffer */
    int          rdata[DIM0][DIM1]; /* Read buffer */
    int          max;
    hsize_t      i, j;

    /*
     * Check if szip compression is available and can be used for both
     * compression and decompression.  Normally we do not perform error
     * checking in these examples for the sake of clarity, but in this
     * case we will make an exception because this filter is an
     * optional part of the hdf5 library.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_SZIP);
    if (!avail) {
        printf("szip filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_SZIP, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("szip filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset creation property list, add the szip
     * compression filter and set the chunk size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_szip(dcpl, H5_SZIP_NN_OPTION_MASK, 8);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve and print the filter type.  Here we only retrieve the
     * first filter because we know that we only added one filter.
     */
    nelmts      = 0;
    filter_type = H5Pget_filter(dcpl, 0, &flags, &nelmts, NULL, 0, NULL, &filter_info);
    printf("Filter type is: ");
    switch (filter_type) {
        case H5Z_FILTER_DEFLATE:
            printf("H5Z_FILTER_DEFLATE\n");
            break;
        case H5Z_FILTER_SHUFFLE:
            printf("H5Z_FILTER_SHUFFLE\n");
            break;
        case H5Z_FILTER_FLETCHER32:
            printf("H5Z_FILTER_FLETCHER32\n");
            break;
        case H5Z_FILTER_SZIP:
            printf("H5Z_FILTER_SZIP\n");
            break;
        case H5Z_FILTER_NBIT:
            printf("H5Z_FILTER_NBIT\n");
            break;
        case H5Z_FILTER_SCALEOFFSET:
            printf("H5Z_FILTER_SCALEOFFSET\n");
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            if (max < rdata[i][j])
                max = rdata[i][j];

    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is: %d\n", DATASET, max);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_transform.c`

```c
/************************************************************

  This example shows how to read and write data to a dataset
  using a data transform expression.  The program first
  writes integers to a dataset using the transform
  expression TRANSFORM, then closes the file.  Next, it
  reopens the file, reads back the data without a transform,
  and outputs the data to the screen.  Finally it reads the
  data using the transform expression RTRANSFORM and outputs
  the results to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME   "h5ex_d_transform.h5"
#define DATASET    "DS1"
#define DIM0       4
#define DIM1       7
#define TRANSFORM  "x+1"
#define RTRANSFORM "x-1"

int
main(void)
{
    hid_t file  = H5I_INVALID_HID;
    hid_t space = H5I_INVALID_HID;
    hid_t dset  = H5I_INVALID_HID;
    hid_t dxpl  = H5I_INVALID_HID;
    /* Handles */
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    int     wdata[DIM0][DIM1]; /* Write buffer */
    int     rdata[DIM0][DIM1]; /* Read buffer */
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Output the data to the screen.
     */
    printf("Original Data:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", wdata[i][j]);
        printf("]\n");
    }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset transfer property list and define the
     * transform expression.
     */
    dxpl   = H5Pcreate(H5P_DATASET_XFER);
    status = H5Pset_data_transform(dxpl, TRANSFORM);

    /*
     * Create the dataset using the default properties.  Unfortunately
     * we must save as a native type or the transform operation will
     * fail.
     */
    dset = H5Dcreate(file, DATASET, H5T_NATIVE_INT, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Write the data to the dataset using the dataset transfer
     * property list.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, dxpl, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dxpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nData as written with transform \"%s\":\n", TRANSFORM);
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Create the dataset transfer property list and define the
     * transform expression.
     */
    dxpl   = H5Pcreate(H5P_DATASET_XFER);
    status = H5Pset_data_transform(dxpl, RTRANSFORM);

    /*
     * Read the data using the dataset transfer property list.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, dxpl, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nData as written with transform \"%s\" and read with transform \"%s\":\n", TRANSFORM,
           RTRANSFORM);
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dxpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_unlimadd.c`

```c
/************************************************************

  This example shows how to create and extend an unlimited
  dataset.  The program first writes integers to a dataset
  with dataspace dimensions of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data,
  outputs it to the screen, extends the dataset, and writes
  new data to the extended portions of the dataset.  Finally
  it reopens the file again, reads back the data, and
  outputs it to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_unlimadd.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7
#define EDIM0    6
#define EDIM1    10
#define CHUNK0   4
#define CHUNK1   4

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID;
    hid_t   space = H5I_INVALID_HID;
    hid_t   dset  = H5I_INVALID_HID;
    hid_t   dcpl  = H5I_INVALID_HID;
    herr_t  status;
    hsize_t dims[2]    = {DIM0, DIM1};
    hsize_t extdims[2] = {EDIM0, EDIM1};
    hsize_t maxdims[2];
    hsize_t chunk[2] = {CHUNK0, CHUNK1};
    hsize_t start[2];
    hsize_t count[2];
    int     wdata[DIM0][DIM1];    /* Write buffer */
    int     wdata2[EDIM0][EDIM1]; /* Write buffer for extension */
    int   **rdata = NULL;         /* Read buffer */
    int     ndims;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace with unlimited dimensions.
     */
    maxdims[0] = H5S_UNLIMITED;
    maxdims[1] = H5S_UNLIMITED;
    space      = H5Screate_simple(2, dims, maxdims);

    /*
     * Create the dataset creation property list, and set the chunk
     * size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the unlimited dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * In this next section we read back the data, extend the dataset,
     * and write new data to the extended portions.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDWR, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (int **)malloc(dims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("Dataset before extension:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    status = H5Sclose(space);

    /*
     * Extend the dataset.
     */
    status = H5Dset_extent(dset, extdims);

    /*
     * Retrieve the dataspace for the newly extended dataset.
     */
    space = H5Dget_space(dset);

    /*
     * Initialize data for writing to the extended dataset.
     */
    for (i = 0; i < EDIM0; i++)
        for (j = 0; j < EDIM1; j++)
            wdata2[i][j] = j;

    /*
     * Select the entire dataspace.
     */
    status = H5Sselect_all(space);

    /*
     * Subtract a hyperslab reflecting the original dimensions from the
     * selection.  The selection now contains only the newly extended
     * portions of the dataset.
     */
    start[0] = 0;
    start[1] = 0;
    count[0] = dims[0];
    count[1] = dims[1];
    status   = H5Sselect_hyperslab(space, H5S_SELECT_NOTB, start, NULL, count, NULL);

    /*
     * Write the data to the selected portion of the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, space, H5P_DEFAULT, wdata2[0]);

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we simply read back the data and output it to the screen.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for the read buffer as before.
     */
    space    = H5Dget_space(dset);
    ndims    = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata    = (int **)malloc(dims[0] * sizeof(int *));
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nDataset after extension:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_unlimgzip.c`

```c
/************************************************************

  This example shows how to create and extend an unlimited
  dataset with gzip compression.  The program first writes
  integers to a gzip compressed dataset with dataspace
  dimensions of DIM0xDIM1, then closes the file.  Next, it
  reopens the file, reads back the data, outputs it to the
  screen, extends the dataset, and writes new data to the
  extended portions of the dataset.  Finally it reopens the
  file again, reads back the data, and outputs it to the
  screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_unlimgzip.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7
#define EDIM0    6
#define EDIM1    10
#define CHUNK0   4
#define CHUNK1   4

int
main(void)
{
    hid_t        file  = H5I_INVALID_HID;
    hid_t        space = H5I_INVALID_HID;
    hid_t        dset  = H5I_INVALID_HID;
    hid_t        dcpl  = H5I_INVALID_HID;
    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_type;
    hsize_t      dims[2]    = {DIM0, DIM1};
    hsize_t      extdims[2] = {EDIM0, EDIM1};
    hsize_t      maxdims[2];
    hsize_t      chunk[2] = {CHUNK0, CHUNK1};
    hsize_t      start[2];
    hsize_t      count[2];
    size_t       nelmts;
    unsigned int flags, filter_info;
    int          wdata[DIM0][DIM1];    /* Write buffer */
    int          wdata2[EDIM0][EDIM1]; /* Write buffer for extension */
    int        **rdata = NULL;         /* Read buffer */
    int          ndims;
    hsize_t      i, j;

    /*
     * Check if gzip compression is available and can be used for both
     * compression and decompression.  Normally we do not perform error
     * checking in these examples for the sake of clarity, but in this
     * case we will make an exception because this filter is an
     * optional part of the hdf5 library.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_DEFLATE);
    if (!avail) {
        printf("gzip filter not available.\n");
        return 1;
    }
    status = H5Zget_filter_info(H5Z_FILTER_DEFLATE, &filter_info);
    if (!(filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED) ||
        !(filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED)) {
        printf("gzip filter not available for encoding and decoding.\n");
        return 1;
    }

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace with unlimited dimensions.
     */
    maxdims[0] = H5S_UNLIMITED;
    maxdims[1] = H5S_UNLIMITED;
    space      = H5Screate_simple(2, dims, maxdims);

    /*
     * Create the dataset creation property list, add the gzip
     * compression filter and set the chunk size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_deflate(dcpl, 9);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the compressed unlimited dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * In this next section we read back the data, extend the dataset,
     * and write new data to the extended portions.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDWR, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (int **)malloc(dims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("Dataset before extension:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    status = H5Sclose(space);

    /*
     * Extend the dataset.
     */
    status = H5Dset_extent(dset, extdims);

    /*
     * Retrieve the dataspace for the newly extended dataset.
     */
    space = H5Dget_space(dset);

    /*
     * Initialize data for writing to the extended dataset.
     */
    for (i = 0; i < EDIM0; i++)
        for (j = 0; j < EDIM1; j++)
            wdata2[i][j] = j;

    /*
     * Select the entire dataspace.
     */
    status = H5Sselect_all(space);

    /*
     * Subtract a hyperslab reflecting the original dimensions from the
     * selection.  The selection now contains only the newly extended
     * portions of the dataset.
     */
    start[0] = 0;
    start[1] = 0;
    count[0] = dims[0];
    count[1] = dims[1];
    status   = H5Sselect_hyperslab(space, H5S_SELECT_NOTB, start, NULL, count, NULL);

    /*
     * Write the data to the selected portion of the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, space, H5P_DEFAULT, wdata2[0]);

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we simply read back the data and output it to the screen.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Retrieve dataset creation property list.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Retrieve and print the filter type.  Here we only retrieve the
     * first filter because we know that we only added one filter.
     */
    nelmts      = 0;
    filter_type = H5Pget_filter(dcpl, 0, &flags, &nelmts, NULL, 0, NULL, &filter_info);
    printf("\nFilter type is: ");
    switch (filter_type) {
        case H5Z_FILTER_DEFLATE:
            printf("H5Z_FILTER_DEFLATE\n");
            break;
        case H5Z_FILTER_SHUFFLE:
            printf("H5Z_FILTER_SHUFFLE\n");
            break;
        case H5Z_FILTER_FLETCHER32:
            printf("H5Z_FILTER_FLETCHER32\n");
            break;
        case H5Z_FILTER_SZIP:
            printf("H5Z_FILTER_SZIP\n");
            break;
        case H5Z_FILTER_NBIT:
            printf("H5Z_FILTER_NBIT\n");
            break;
        case H5Z_FILTER_SCALEOFFSET:
            printf("H5Z_FILTER_SCALEOFFSET\n");
    }

    /*
     * Get dataspace and allocate memory for the read buffer as before.
     */
    space    = H5Dget_space(dset);
    ndims    = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata    = (int **)malloc(dims[0] * sizeof(int *));
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("Dataset after extension:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/h5ex_d_unlimmod.c`

```c
/************************************************************

  This example shows how to create and extend an unlimited
  dataset.  The program first writes integers to a dataset
  with dataspace dimensions of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data,
  outputs it to the screen, extends the dataset, and writes
  new data to the entire extended dataset.  Finally it
  reopens the file again, reads back the data, and outputs it
  to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_d_unlimmod.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7
#define EDIM0    6
#define EDIM1    10
#define CHUNK0   4
#define CHUNK1   4

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID;
    hid_t   space = H5I_INVALID_HID;
    hid_t   dset  = H5I_INVALID_HID;
    hid_t   dcpl  = H5I_INVALID_HID;
    herr_t  status;
    hsize_t dims[2]    = {DIM0, DIM1};
    hsize_t extdims[2] = {EDIM0, EDIM1};
    hsize_t maxdims[2];
    hsize_t chunk[2] = {CHUNK0, CHUNK1};
    int     wdata[DIM0][DIM1];    /* Write buffer */
    int     wdata2[EDIM0][EDIM1]; /* Write buffer for extension */
    int   **rdata = NULL;         /* Read buffer */
    int     ndims;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace with unlimited dimensions.
     */
    maxdims[0] = H5S_UNLIMITED;
    maxdims[1] = H5S_UNLIMITED;
    space      = H5Screate_simple(2, dims, maxdims);

    /*
     * Create the dataset creation property list, and set the chunk
     * size.
     */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_chunk(dcpl, 2, chunk);

    /*
     * Create the unlimited dataset.
     */
    dset = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);

    /*
     * Write the data to the dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * In this next section we read back the data, extend the dataset,
     * and write new data to the entire dataset.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDWR, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (int **)malloc(dims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("Dataset before extension:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Extend the dataset.
     */
    status = H5Dset_extent(dset, extdims);

    /*
     * Initialize data for writing to the extended dataset.
     */
    for (i = 0; i < EDIM0; i++)
        for (j = 0; j < EDIM1; j++)
            wdata2[i][j] = j;

    /*
     * Write the data to the extended dataset.
     */
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata2[0]);

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we simply read back the data and output it to the screen.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for the read buffer as before.
     */
    space    = H5Dget_space(dset);
    ndims    = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata    = (int **)malloc(dims[0] * sizeof(int *));
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("\nDataset after extension:\n");
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5D/test-pc.sh`

```bash
#! /bin/sh
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.

# This file is for use of h5cc created with the CMake process.
# Environment variable, HDF5_HOME is expected to be set.
# $1 is the path name of the source directory.
# $2 is the path name of the build directory.
# $3 is the current path name.

top_srcdir=$1
top_builddir=$2
currentpath=$3
verbose=yes
nerrors=0

echo "Current build directory: $top_builddir/$currentpath"

# HDF5 compile commands, assuming they are in your $PATH.
H5CC=$HDF5_HOME/bin/h5cc
LD_LIBRARY_PATH=$HDF5_HOME/lib
export LD_LIBRARY_PATH

if ! test -f $H5CC; then
    echo "Set paths for H5CC and LD_LIBRARY_PATH in test.sh"
    echo "Set environment variable HDF5_HOME to the hdf5 install dir"
    echo "h5cc was not found at $H5CC"
    exit $EXIT_FAILURE
fi

H5DUMP=`echo $H5CC | sed -e 's/\/[^/]*$/\/h5dump/'`;
H5_LIBVER=$($H5CC -showconfig | grep -i "HDF5 Version:" | sed 's/^.* //g' | sed 's/[-].*//g')
H5_APIVER=$($H5CC -showconfig | grep -i "Default API mapping:" | sed 's/^.* //g' | sed 's/v//g' | sed 's/1/1_/')

H5_MAJORVER=$(echo $H5_LIBVER | cut -f1 -d'.'  | sed -E 's/\./_/g')
H5_MINORVER=$(echo $H5_LIBVER | cut -f2 -d'.'  | sed -E 's/\./_/g')
H5_RELEASEVER=$(echo $H5_LIBVER | cut -f3 -d'.'  | sed -E 's/\./_/g')
H5_LIBVER_DIR=$H5_MAJORVER$H5_MINORVER

# Shell commands used in Makefiles
RM="rm -rf"
DIFF="diff -c"
CMP="cmp -s"
GREP='grep'
CP="cp -p"  # Use -p to preserve mode,ownership,timestamps
DIRNAME='dirname'
LS='ls'
AWK='awk'

# setup plugin path
ENVCMD="env HDF5_PLUGIN_PATH=$LD_LIBRARY_PATH/plugin"

TESTDIR=$top_builddir/$currentpath


case `echo "testing\c"; echo 1,2,3`,`echo -n testing; echo 1,2,3` in
  *c*,-n*) ECHO_N= ECHO_C='
' ;;
  *c*,*  ) ECHO_N=-n ECHO_C= ;;
  *)       ECHO_N= ECHO_C='\c' ;;
esac
ECHO_N="echo $ECHO_N"


exout() {
    cd $TESTDIR
    "$@"
}

dumpout() {
    cd $TESTDIR
    $H5DUMP "$@"
}

compileout() {
    cd $TESTDIR
    $H5CC "$@"
}

# compare current version, required version.
# returns if cur_ver < req_ver is true.
version_compare() {
  version_lt=0
  if [ ! "$(printf '%s\n' "$1" "$2" | sort -V | head -n1)" = "$2" ]; then
          version_lt=1
  fi
}


topics="alloc checksum chunk compact extern fillval gzip hyper \
rdwr shuffle szip unlimadd unlimgzip unlimmod"
topics18=""

version_compare "$H5_LIBVER" "1.8.0"
# check if HDF5 version is < 1.8.0
if [ "$version_lt" = 1 ]; then
    dir16="/16"
else
    dir16=""
    topics18="nbit sofloat soint transform"
fi

return_val=0

#Remove external data file from h5ex_d_extern
rm -f h5ex_d_extern.data

for topic in $topics
do
    compileout $top_srcdir/$currentpath/$dir16/h5ex_d_$topic.c -o h5ex_d_$topic
done

for topic in $topics
do
    fname=h5ex_d_$topic
    $ECHO_N "Testing C/H5D/$fname...$ECHO_C"
    exout ./$fname >tmp.test
    status=$?
    if test $status -eq 1
    then
        echo "  Unsupported feature"
        status=0
    else
        cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/16/$fname.tst
        status=$?
        if test $status -ne 0
        then
            echo "  FAILED!"
        else
          dumpout $fname.h5 >tmp.test
          rm -f $TESTDIR/$fname.h5
          cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/16/$fname.ddl
          status=$?
          if test $status -ne 0
          then
              echo "  FAILED!"
          else
              echo "  Passed"
          fi
        fi
        return_val=`expr $status + $return_val`
    fi
done

#######Non-standard tests#######
USE_ALT=""
### Set default tfiles directory for tests
nbitdir="18"
version_compare "$H5_LIBVER" "1.8.23"
# check if HDF5 version is < 1.8.23
if [ "$version_lt" = 1 ]; then
    USE_ALT="22"
else
# check if HDF5 version is >= 1.10.0 and < 1.10.8
  version_compare "$H5_LIBVER" "1.10.0"
  if [ "$version_lt" = 0 ]; then
    version_compare "$H5_LIBVER" "1.10.8"
    if [ "$version_lt" = 1 ]; then
      USE_ALT="07"
      nbitdir="110"
    fi
  fi
fi

for topic in $topics18
do
    compileout $top_srcdir/$currentpath/h5ex_d_$topic.c -o h5ex_d_$topic
done

for topic in $topics18
do
    fname=h5ex_d_$topic
    $ECHO_N "Testing C/H5D/$fname...$ECHO_C"
    exout ./$fname >tmp.test
    status=$?
    if test $status -eq 1
    then
        echo "  Unsupported feature"
        status=0
    else
        if [ "$fname" = "h5ex_d_nbit" ]; then
            tdir=$nbitdir
            if [ "$USE_ALT" = "" ]; then
                ### set USE_ALT=07 if not set above
                USE_ALT="07"
            fi
        else
            tdir=18
            ### unset USE_ALT for the other topics
            USE_ALT=""
        fi
        cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.tst
        status=$?
        if test $status -ne 0
        then
            echo "  FAILED!"
        else
          if [ "$fname" = "h5ex_d_transform" ]; then
              targ="-n"
          else
              targ=""
          fi
          dumpout $targ $fname.h5 >tmp.test
          rm -f $TESTDIR/$fname.h5
          cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/$tdir/$fname$USE_ALT.ddl
          status=$?
          if test $status -ne 0
          then
              echo "  FAILED!"
          else
              echo "  Passed"
          fi
        fi
        return_val=`expr $status + $return_val`
    fi
done


#Remove external data file from h5ex_d_extern
rm -f $TESTDIR/h5ex_d_extern.data
rm -f $TESTDIR/tmp.test
echo "$return_val tests failed in C/H5D/"
exit $return_val
```

### `HDF5Examples/C/H5FLT/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_C_H5FLT C)

set (dyn_examples)

option (ENABLE_BLOSC "Enable Library Building for blosc plugin" ON)
if (ENABLE_BLOSC)
  if (WIN32)
    if (NOT CMAKE_C_COMPILER_ID MATCHES "[Cc]lang" AND MSVC_VERSION GREATER 1600)
      set (BLOSC_AVAILABLE 1)
      set (dyn_examples ${dyn_examples} h5ex_d_blosc)
    else ()
      set (BLOSC_AVAILABLE 0)
    endif ()
  elseif (APPLE)
    if (NOT CMAKE_C_COMPILER_ID STREQUAL "Intel")
      set (BLOSC_AVAILABLE 1)
      set (dyn_examples ${dyn_examples} h5ex_d_blosc)
    endif ()
  else ()
    set (BLOSC_AVAILABLE 0)
  endif ()
else ()
  set (BLOSC_AVAILABLE 0)
endif ()

option (ENABLE_BLOSC2 "Enable Library Building for blosc2 plugin" ON)
if (ENABLE_BLOSC2)
  set (BLOSC_AVAILABLE 1)
  set (dyn_examples ${dyn_examples} h5ex_d_blosc2)
else ()
  set (BLOSC_AVAILABLE 0)
endif ()

option (ENABLE_BITGROOM "Enable Library Building for bitgroom plugin" ON)
if (ENABLE_BITGROOM)
  set (BITGROOM_AVAILABLE 1)
  set (dyn_examples ${dyn_examples} h5ex_d_bitgroom)
else ()
  set (BITGROOM_AVAILABLE 0)
endif ()

option (ENABLE_BITROUND "Enable Library Building for bitround plugin" ON)
if (ENABLE_BITROUND)
  set (BITROUND_AVAILABLE 1)
  set (dyn_examples ${dyn_examples} h5ex_d_granularbr)
else ()
  set (BITROUND_AVAILABLE 0)
endif ()

option (ENABLE_BSHUF "Enable Library Building for bshuf plugin" ON)
if (ENABLE_BSHUF)
  if (NOT CMAKE_C_COMPILER_ID STREQUAL "Intel")
    set (BSHUF_AVAILABLE 1)
    set (dyn_examples ${dyn_examples} h5ex_d_bshuf)
  else ()
    set (BSHUF_AVAILABLE 0)
  endif ()
else ()
  set (BSHUF_AVAILABLE 0)
endif ()

option (ENABLE_BZIP2 "Enable Library Building for bzip2 plugin" ON)
if (ENABLE_BZIP2)
  set (BZIP2_AVAILABLE 1)
  set (dyn_examples ${dyn_examples} h5ex_d_bzip2)
else ()
  set (BZIP2_AVAILABLE 0)
endif ()

option (ENABLE_FPZIP "Enable Library Building for fpzip plugin" OFF)
if (ENABLE_FPZIP)
  set (FPZIP_AVAILABLE 1)
  set (dyn_examples ${dyn_examples} h5ex_d_fpzip)
else ()
  set (FPZIP_AVAILABLE 0)
endif ()
option (ENABLE_JPEG "Enable Library Building for jpeg plugin" ON)
if (ENABLE_JPEG)
  if (NOT WIN32)
    set (JPEG_AVAILABLE 1)
    set (dyn_examples ${dyn_examples} h5ex_d_jpeg)
  else ()
    set (JPEG_AVAILABLE 0)
  endif ()
else ()
  set (JPEG_AVAILABLE 0)
endif ()

option (ENABLE_LZ4 "Enable Library Building for lz4 plugin" ON)
if (ENABLE_MAFISC)
  set (LZ4_AVAILABLE 1)
  set (dyn_examples ${dyn_examples} h5ex_d_lz4)
else ()
  set (LZ4_AVAILABLE 0)
endif ()

option (ENABLE_LZF "Enable Library Building for lzf plugin" ON)
if (ENABLE_LZF)
  set (LZF_AVAILABLE 1)
  set (dyn_examples ${dyn_examples} h5ex_d_lzf)
else ()
  set (LZF_AVAILABLE 0)
endif ()

option (ENABLE_MAFISC "Enable Library Building for mafisc plugin" OFF)
if (ENABLE_MAFISC)
  set (MAFISC_AVAILABLE 1)
  set (dyn_examples ${dyn_examples} h5ex_d_mafisc)
else ()
  set (MAFISC_AVAILABLE 0)
endif ()
option (ENABLE_SZ "Enable Library Building for sz plugin" OFF)
if (ENABLE_SZ)
  if (WIN32 AND MSVC_VERSION GREATER 1900)
    if (CMAKE_C_COMPILER_ID MATCHES "[Cc]lang" OR CMAKE_C_COMPILER_ID STREQUAL "Intel")
      set (SZ_AVAILABLE 1)
      set (dyn_examples ${dyn_examples} h5ex_d_sz)
    else ()
      set (SZ_AVAILABLE 0)
    endif ()
  elseif (NOT WIN32)
    if(CMAKE_C_COMPILER_ID MATCHES "[Cc]lang" AND NOT CMAKE_C_COMPILER_ID MATCHES "Apple[Cc]lang")
      set (SZ_AVAILABLE 1)
      set (dyn_examples ${dyn_examples} h5ex_d_sz)
    else ()
      set (SZ_AVAILABLE 0)
    endif ()
  else ()
    set (SZ_AVAILABLE 0)
  endif ()
else ()
  set (SZ_AVAILABLE 0)
endif ()

option (ENABLE_ZFP "Enable Library Building for zfp plugin" OFF)
if (ENABLE_ZFP)
  set (ZFP_AVAILABLE 1)
  set (dyn_examples ${dyn_examples} h5ex_d_zfp)
else ()
  set (ZFP_AVAILABLE 0)
endif ()

option (ENABLE_ZSTD "Enable Library Building for zstd plugin" ON)
if (ENABLE_ZSTD)
  if (WIN32)
    if (CMAKE_C_COMPILER_ID MATCHES "[Cc]lang")
      set (ZSTD_AVAILABLE 1)
      set (dyn_examples ${dyn_examples} h5ex_d_zstd)
    else ()
      set (ZSTD_AVAILABLE 0)
    endif ()
  elseif (APPLE)
    if (NOT CMAKE_C_COMPILER_ID STREQUAL "Intel")
      set (ZSTD_AVAILABLE 1)
      set (dyn_examples ${dyn_examples} h5ex_d_zstd)
    else ()
      set (ZSTD_AVAILABLE 0)
    endif ()
  else ()
    set (ZSTD_AVAILABLE 1)
    set (dyn_examples ${dyn_examples} h5ex_d_zstd)
  endif ()
else ()
  set (ZSTD_AVAILABLE 0)
endif ()

set (LIB_TYPE STATIC)
if (BUILD_SHARED_LIBS)
  set (LIB_TYPE SHARED)
endif ()

#run-time loadable library examples
foreach (example ${dyn_examples})
  add_executable (${EXAMPLE_VARNAME}_${example} ${PROJECT_SOURCE_DIR}/${example}.c)
  TARGET_C_PROPERTIES (${EXAMPLE_VARNAME}_${example} ${LIB_TYPE})
  target_compile_options (${EXAMPLE_VARNAME}_${example}
      PRIVATE
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  if (H5_HAVE_PARALLEL)
    target_include_directories (${EXAMPLE_VARNAME}_${example} PUBLIC ${MPI_C_INCLUDE_DIRS})
  endif ()
  target_link_libraries (${EXAMPLE_VARNAME}_${example} PRIVATE ${H5EXAMPLE_HDF5_LINK_LIBS})
  if (NOT WIN32)
    target_link_libraries (${EXAMPLE_VARNAME}_${example} PRIVATE dl)
  endif ()
endforeach ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove
            ${testname}.out
            ${testname}.out.err
            ${testname}.ddl.out
            ${testname}.ddl.out.err
            ${testname}.h5
    )
    if (NOT "${last_test}" STREQUAL "")
      set_tests_properties (${EXAMPLE_VARNAME}_${testname}-clearall PROPERTIES DEPENDS ${last_test})
    endif ()
    set (last_test "${EXAMPLE_VARNAME}_${testname}-clearall")
    if (DISABLE_H5PL_ENCODER)
      add_test (
          NAME ${EXAMPLE_VARNAME}_${testname}-ERR
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>"
              -D "TEST_ARGS:STRING=${ARGN}"
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=1"
              -D "TEST_MASK_ERROR=true"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_ENV_VAR=HDF5_PLUGIN_PATH"
              -D "TEST_ENV_VALUE=${H5EXAMPLE_HDF5_PLUGIN_PATH}"
              -D "TEST_SKIP_COMPARE=1"
              -D "TEST_ERRREF=1"
              -D "GREP_ERRREF=Filter present but encoding disabled"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_${testname}-ERR PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      add_test (
          NAME ${EXAMPLE_VARNAME}_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -E copy_if_different
              "${PROJECT_SOURCE_DIR}/tfiles/${testname}.h5" "${PROJECT_BINARY_DIR}/${testname}.h5"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_${testname}-ERR)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>"
              -D "TEST_ARGS:STRING=${ARGN}"
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_ENV_VAR=HDF5_PLUGIN_PATH"
              -D "TEST_ENV_VALUE=${H5EXAMPLE_HDF5_PLUGIN_PATH}"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
    endif ()
    set (last_test "${EXAMPLE_VARNAME}_${testname}")
    if (HDF5_PROVIDES_TOOLS)
      if ("${ARGN}" STREQUAL "FILTERALL")
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=--enable-error-stack;-p;${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.ddl.out"
                -D "TEST_FILTER:STRING=PARAMS {[ -0-9]*}"
                -D "TEST_FILTER_REPLACE:STRING=PARAMS { XXXX }"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_ENV_VAR=HDF5_PLUGIN_PATH"
                -D "TEST_ENV_VALUE=${H5EXAMPLE_HDF5_PLUGIN_PATH}"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
      elseif ("${ARGN}" STREQUAL "FILTERHEADER")
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=--enable-error-stack;-pH;${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.ddl.out"
                -D "TEST_FILTER:STRING=PARAMS {[ -0-9]*}"
                -D "TEST_FILTER_REPLACE:STRING=PARAMS { XXXX }"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_ENV_VAR=HDF5_PLUGIN_PATH"
                -D "TEST_ENV_VALUE=${H5EXAMPLE_HDF5_PLUGIN_PATH}"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
      else ()
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=--enable-error-stack;-p;${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.ddl.out"
                -D "TEST_FILTER=PARAMS { ([0-9]) [-]?[0-9]+ ([0-9] [0-9] [0-9] [0-9] [0-9] [0-9]) }\n"
                -D "TEST_FILTER_REPLACE=PARAMS { \\\\1 XXXX \\\\2 }\n"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_ENV_VAR=HDF5_PLUGIN_PATH"
                -D "TEST_ENV_VALUE=${H5EXAMPLE_HDF5_PLUGIN_PATH}"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
      endif ()
      set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_${testname}
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      set (last_test "${EXAMPLE_VARNAME}_H5DUMP-${testname}")
    endif ()
  endmacro ()

  # --------------------------------------------------------------------
  # Copy all the HDF5 files from the source directory into the test directory
  # --------------------------------------------------------------------
  foreach (h5_file ${dyn_examples})
    HDFTEST_COPY_FILE ("${PROJECT_SOURCE_DIR}/tfiles/${h5_file}.tst" "${PROJECT_BINARY_DIR}/${h5_file}.tst" "example_files")
    if (WIN32 AND MSVC_VERSION LESS 1900 AND ${h5_file} MATCHES "h5ex_d_zfp")
      HDFTEST_COPY_FILE ("${PROJECT_SOURCE_DIR}/tfiles/h5ex_d_zfp.wddl" "${PROJECT_BINARY_DIR}/h5ex_d_zfp.ddl" "example_files")
    else ()
      HDFTEST_COPY_FILE ("${PROJECT_SOURCE_DIR}/tfiles/${h5_file}.ddl" "${PROJECT_BINARY_DIR}/${h5_file}.ddl" "example_files")
    endif()
  endforeach ()
  foreach (h5_file ${LIST_HDF5_TEST_FILES} ${LIST_OTHER_TEST_FILES})
    HDFTEST_COPY_FILE ("${PROJECT_SOURCE_DIR}/tfiles/${h5_file}" "${PROJECT_BINARY_DIR}/${h5_file}" "example_files")
  endforeach ()
  add_custom_target (${EXAMPLE_VARNAME}_example_files ALL COMMENT "Copying files needed by example tests" DEPENDS ${example_files_list})

  foreach (h5_file ${dyn_examples})
    if (NOT HDF5_ENABLE_USING_MEMCHECKER)
      if (${h5_file} MATCHES "h5ex_d_zfp")
        ## special filter
        ADD_H5_TEST (h5ex_d_zfp FILTERALL)
      elseif (${h5_file} MATCHES "h5ex_d_granularbr")
        ADD_H5_TEST (h5ex_d_granularbr FILTERHEADER)
      else ()
        ADD_H5_TEST (${h5_file})
      endif ()
    endif ()
  endforeach ()

endif ()
```

### `HDF5Examples/C/H5FLT/h5ex_d_bitgroom.c`

```c
/************************************************************

  This example shows how to write data and read it from a dataset
  using BitGroom quantization.
  The BitGroom filter is not available in HDF5.
  The example uses a new feature available in HDF5 version 1.8.11
  to discover, load and register filters at run time.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME            "h5ex_d_bitgroom.h5"
#define DATASET             "DS1"
#define DIM0                32
#define DIM1                64
#define CHUNK0              4
#define CHUNK1              8
#define H5Z_FILTER_BITGROOM 32022

int
main(void)
{
    hid_t              file_id  = H5I_INVALID_HID;
    hid_t              space_id = H5I_INVALID_HID;
    hid_t              dset_id  = H5I_INVALID_HID;
    hid_t              dcpl_id  = H5I_INVALID_HID;
    herr_t             status;
    htri_t             avail;
    H5Z_filter_t       filter_id = 0;
    char               filter_name[80];
    hsize_t            dims[2] = {DIM0, DIM1}, chunk[2] = {CHUNK0, CHUNK1};
    size_t             nelmts = 5; /* number of elements in cd_values */
    unsigned int       flags;
    unsigned           filter_config;
    const unsigned int cd_values[5] = {
        3, 4, 0, 0, 0}; /* BitGroom argument ordering is
                           NSD,sizeof(data),has_mss_val,mss_val_byt_1to4[,mss_val_byt_5to8] */
    unsigned int values_out[5] = {99, 99, 99, 99, 99};
    float        wdata[DIM0][DIM1]; /* Write buffer */
    float        rdata[DIM0][DIM1]; /* Read buffer */
    float        max;
    hsize_t      i, j;
    int          ret_value = 1;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = (float)(i * j) - (float)(j);

    /*
     * Create a new file using the default properties.
     */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space_id = H5Screate_simple(2, dims, NULL);
    if (space_id < 0)
        goto done;

    /*
     * Create the dataset creation property list, add the BitGroom
     * quantization filter and set the chunk size.
     */
    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);
    if (dcpl_id < 0)
        goto done;

    /* 20200929: csz change this to H5Z_FLAG_OPTIONAL, so that can_apply() can reject filter for
       integers without causing program to exit()? */
    status = H5Pset_filter(dcpl_id, H5Z_FILTER_BITGROOM, H5Z_FLAG_MANDATORY, nelmts, cd_values);
    if (status < 0)
        goto done;

    /*
     * Check that filter is registered with the library now.
     * If it is registered, retrieve filter's configuration.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_BITGROOM);
    if (avail) {
        status = H5Zget_filter_info(H5Z_FILTER_BITGROOM, &filter_config);
        if ((filter_config & H5Z_FILTER_CONFIG_ENCODE_ENABLED) &&
            (filter_config & H5Z_FILTER_CONFIG_DECODE_ENABLED))
            printf("BitGroom filter is available for quantization and decoding.\n");
    }
    else {
        printf("H5Zfilter_avail - not found.\n");
        goto done;
    }
    status = H5Pset_chunk(dcpl_id, 2, chunk);
    if (status < 0)
        printf("failed to set chunk.\n");

    /*
     * Create the dataset.
     */
    printf("....Create dataset ................\n");
    dset_id = H5Dcreate(file_id, DATASET, H5T_IEEE_F32LE, space_id, H5P_DEFAULT, dcpl_id, H5P_DEFAULT);
    if (dset_id < 0) {
        printf("failed to create dataset.\n");
        goto done;
    }

    /*
     * Write the data to the dataset.
     */
    printf("....Writing BitGroom-quantized data ................\n");
    status = H5Dwrite(dset_id, H5T_NATIVE_FLOAT, H5S_ALL, H5S_ALL, H5P_DEFAULT, (void *)wdata);
    if (status < 0)
        printf("failed to write data.\n");

    /*
     * Close and release resources.
     */
    H5Dclose(dset_id);
    dset_id = -1;
    H5Pclose(dcpl_id);
    dcpl_id = -1;
    H5Sclose(space_id);
    space_id = -1;
    H5Fclose(file_id);
    file_id = -1;
    status  = H5close();
    if (status < 0) {
        printf("\nFAILED to close library\n");
        goto done;
    }

    printf("....Close the file and reopen for reading ........\n");
    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    dset_id = H5Dopen(file_id, DATASET, H5P_DEFAULT);
    if (dset_id < 0)
        goto done;

    /*
     * Retrieve dataset creation property list.
     */
    dcpl_id = H5Dget_create_plist(dset_id);
    if (dcpl_id < 0)
        goto done;

    /*
     * Retrieve and print the filter id, quantization level and filter's name for BitGroom.
     */
    filter_id = H5Pget_filter2(dcpl_id, (unsigned)0, &flags, &nelmts, values_out, sizeof(filter_name),
                               filter_name, NULL);
    printf("Filter info is available from the dataset creation property\n");
    printf("   Filter identifier is ");
    switch (filter_id) {
        case H5Z_FILTER_BITGROOM:
            printf("%d\n", filter_id);
            printf("   Number of parameters is %lu with the values %u, %u, %u, %u, %u\n", nelmts,
                   values_out[0], values_out[1], values_out[2], values_out[3], values_out[4]);
            printf("   To find more about the filter check %s\n", filter_name);
            break;
        default:
            printf("Not expected filter\n");
            break;
    }

    /*
     * Read the data using the default properties.
     */
    printf("....Reading BitGroom-quantized data ................\n");
    status = H5Dread(dset_id, H5T_NATIVE_FLOAT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);
    if (status < 0)
        printf("failed to read data.\n");

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            /*printf("%f \n", rdata[i][j]); */
            if (max < rdata[i][j])
                max = rdata[i][j];
        }
    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is %g\n", DATASET, max);
    /*
     * Check that filter is registered with the library now.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_BITGROOM);
    if (avail)
        printf("BitGroom filter is available now since H5Dread triggered loading of the filter.\n");

    ret_value = 0;

done:
    /*
     * Close and release resources.
     */
    if (dcpl_id >= 0)
        H5Pclose(dcpl_id);
    if (dset_id >= 0)
        H5Dclose(dset_id);
    if (space_id >= 0)
        H5Sclose(space_id);
    if (file_id >= 0)
        H5Fclose(file_id);

    return ret_value;
}
```

### `HDF5Examples/C/H5FLT/h5ex_d_blosc.c`

```c
/************************************************************

  This example shows how to write data and read it from a dataset
  using blosc compression.
  blosc filter is not available in HDF5.
  The example uses a new feature available in HDF5 version 1.8.11
  to discover, load and register filters at run time.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME         "h5ex_d_blosc.h5"
#define DATASET          "DS1"
#define DIM0             32
#define DIM1             64
#define CHUNK0           4
#define CHUNK1           8
#define H5Z_FILTER_BLOSC 32001

int
main(void)
{
    hid_t              file_id  = H5I_INVALID_HID;
    hid_t              space_id = H5I_INVALID_HID;
    hid_t              dset_id  = H5I_INVALID_HID;
    hid_t              dcpl_id  = H5I_INVALID_HID;
    herr_t             status;
    htri_t             avail;
    H5Z_filter_t       filter_id = 0;
    char               filter_name[128];
    hsize_t            dims[2] = {DIM0, DIM1}, chunk[2] = {CHUNK0, CHUNK1};
    size_t             nelmts = 7; /* number of elements in cd_values */
    unsigned int       flags;
    unsigned           filter_config;
    const unsigned int cd_values[7]  = {0, 0, 0, 0, 4, 1, 2}; /* blosc parameters */
    unsigned int       values_out[7] = {99, 99, 99, 99, 99, 99, 99};
    int                wdata[DIM0][DIM1]; /* Write buffer */
    int                rdata[DIM0][DIM1]; /* Read buffer */
    int                max;
    hsize_t            i, j;
    int                ret_value = 1;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space_id = H5Screate_simple(2, dims, NULL);
    if (space_id < 0)
        goto done;

    /*
     * Create the dataset creation property list, add the gzip
     * compression filter and set the chunk size.
     */
    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);
    if (dcpl_id < 0)
        goto done;

    status = H5Pset_filter(dcpl_id, H5Z_FILTER_BLOSC, H5Z_FLAG_OPTIONAL, nelmts, cd_values);
    if (status < 0)
        goto done;

    /*
     * Check that filter is registered with the library now.
     * If it is registered, retrieve filter's configuration.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_BLOSC);
    if (avail) {
        status = H5Zget_filter_info(H5Z_FILTER_BLOSC, &filter_config);
        if ((filter_config & H5Z_FILTER_CONFIG_ENCODE_ENABLED) &&
            (filter_config & H5Z_FILTER_CONFIG_DECODE_ENABLED))
            printf("blosc filter is available for encoding and decoding.\n");
    }
    else {
        printf("H5Zfilter_avail - not found.\n");
        goto done;
    }
    status = H5Pset_chunk(dcpl_id, 2, chunk);
    if (status < 0)
        printf("failed to set chunk.\n");

    /*
     * Create the dataset.
     */
    printf("....Create dataset ................\n");
    dset_id = H5Dcreate(file_id, DATASET, H5T_STD_I32LE, space_id, H5P_DEFAULT, dcpl_id, H5P_DEFAULT);
    if (dset_id < 0) {
        printf("failed to create dataset.\n");
        goto done;
    }

    /*
     * Write the data to the dataset.
     */
    printf("....Writing blosc compressed data ................\n");
    status = H5Dwrite(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);
    if (status < 0)
        printf("failed to write data.\n");

    /*
     * Close and release resources.
     */
    H5Dclose(dset_id);
    dset_id = -1;
    H5Pclose(dcpl_id);
    dcpl_id = -1;
    H5Sclose(space_id);
    space_id = -1;
    H5Fclose(file_id);
    file_id = -1;
    status  = H5close();
    if (status < 0) {
        printf("\nFAILED to close library\n");
        goto done;
    }

    printf("....Close the file and reopen for reading ........\n");
    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    dset_id = H5Dopen(file_id, DATASET, H5P_DEFAULT);
    if (dset_id < 0)
        goto done;

    /*
     * Retrieve dataset creation property list.
     */
    dcpl_id = H5Dget_create_plist(dset_id);
    if (dcpl_id < 0)
        goto done;

    /*
     * Retrieve and print the filter id, compression level and filter's name for blosc.
     */
    filter_id = H5Pget_filter2(dcpl_id, (unsigned)0, &flags, &nelmts, values_out, sizeof(filter_name),
                               filter_name, NULL);
    printf("Filter info is available from the dataset creation property\n");
    printf("   Filter identifier is ");
    switch (filter_id) {
        case H5Z_FILTER_BLOSC:
            printf("%d\n", filter_id);
            printf("   Number of parameters is %d with the value %u %u %u\n", nelmts, values_out[4],
                   values_out[5], values_out[6]);
            printf("   To find more about the filter check %s\n", filter_name);
            break;
        default:
            printf("Not expected filter\n");
            break;
    }

    /*
     * Read the data using the default properties.
     */
    printf("....Reading blosc compressed data ................\n");
    status = H5Dread(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);
    if (status < 0)
        printf("failed to read data.\n");

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            /*printf("%d \n", rdata[i][j]); */
            if (max < rdata[i][j])
                max = rdata[i][j];
        }
    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is %d\n", DATASET, max);
    /*
     * Check that filter is registered with the library now.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_BLOSC);
    if (avail)
        printf("blosc filter is available now since H5Dread triggered loading of the filter.\n");

    ret_value = 0;

done:
    /*
     * Close and release resources.
     */
    if (dcpl_id >= 0)
        H5Pclose(dcpl_id);
    if (dset_id >= 0)
        H5Dclose(dset_id);
    if (space_id >= 0)
        H5Sclose(space_id);
    if (file_id >= 0)
        H5Fclose(file_id);

    return ret_value;
}
```

### `HDF5Examples/C/H5FLT/h5ex_d_blosc2.c`

```c
/************************************************************

  This example shows how to write data and read it from a dataset
  using blosc2 compression.
  blosc2 filter is not available in HDF5.
  The example uses a new feature available in HDF5 version 1.8.11
  to discover, load and register filters at run time.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME          "h5ex_d_blosc2.h5"
#define DATASET           "DS1"
#define DIM0              32
#define DIM1              64
#define CHUNK0            4
#define CHUNK1            8
#define H5Z_FILTER_BLOSC2 32026

int
main(void)
{
    hid_t              file_id  = H5I_INVALID_HID;
    hid_t              space_id = H5I_INVALID_HID;
    hid_t              dset_id  = H5I_INVALID_HID;
    hid_t              dcpl_id  = H5I_INVALID_HID;
    herr_t             status;
    htri_t             avail;
    H5Z_filter_t       filter_id = 0;
    char               filter_name[128];
    hsize_t            dims[2] = {DIM0, DIM1}, chunk[2] = {CHUNK0, CHUNK1};
    size_t             nelmts = 10; /* number of elements in cd_values */
    unsigned int       flags;
    unsigned           filter_config;
    const unsigned int cd_values[10]  = {0, 0, 0, 0, 4, 1, 2, 2, 4, 8}; /* blosc parameters */
    unsigned int       values_out[10] = {99, 99, 99, 99, 99, 99, 99, 99, 99, 99};
    int                wdata[DIM0][DIM1]; /* Write buffer */
    int                rdata[DIM0][DIM1]; /* Read buffer */
    int                max;
    hsize_t            i, j;
    int                ret_value = 1;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space_id = H5Screate_simple(2, dims, NULL);
    if (space_id < 0)
        goto done;

    /*
     * Create the dataset creation property list, add the gzip
     * compression filter and set the chunk size.
     */
    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);
    if (dcpl_id < 0)
        goto done;

    status = H5Pset_filter(dcpl_id, H5Z_FILTER_BLOSC2, H5Z_FLAG_OPTIONAL, nelmts, cd_values);
    if (status < 0)
        goto done;

    /*
     * Check that filter is registered with the library now.
     * If it is registered, retrieve filter's configuration.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_BLOSC2);
    if (avail) {
        status = H5Zget_filter_info(H5Z_FILTER_BLOSC2, &filter_config);
        if ((filter_config & H5Z_FILTER_CONFIG_ENCODE_ENABLED) &&
            (filter_config & H5Z_FILTER_CONFIG_DECODE_ENABLED))
            printf("blosc2 filter is available for encoding and decoding.\n");
    }
    else {
        printf("H5Zfilter_avail - not found.\n");
        goto done;
    }
    status = H5Pset_chunk(dcpl_id, 2, chunk);
    if (status < 0)
        printf("failed to set chunk.\n");

    /*
     * Create the dataset.
     */
    printf("....Create dataset ................\n");
    dset_id = H5Dcreate(file_id, DATASET, H5T_STD_I32LE, space_id, H5P_DEFAULT, dcpl_id, H5P_DEFAULT);
    if (dset_id < 0) {
        printf("failed to create dataset.\n");
        goto done;
    }

    /*
     * Write the data to the dataset.
     */
    printf("....Writing blosc2 compressed data ................\n");
    status = H5Dwrite(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);
    if (status < 0)
        printf("failed to write data.\n");

    /*
     * Close and release resources.
     */
    H5Dclose(dset_id);
    dset_id = -1;
    H5Pclose(dcpl_id);
    dcpl_id = -1;
    H5Sclose(space_id);
    space_id = -1;
    H5Fclose(file_id);
    file_id = -1;
    status  = H5close();
    if (status < 0) {
        printf("\nFAILED to close library\n");
        goto done;
    }

    printf("....Close the file and reopen for reading ........\n");
    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    dset_id = H5Dopen(file_id, DATASET, H5P_DEFAULT);
    if (dset_id < 0)
        goto done;

    /*
     * Retrieve dataset creation property list.
     */
    dcpl_id = H5Dget_create_plist(dset_id);
    if (dcpl_id < 0)
        goto done;

    /*
     * Retrieve and print the filter id, compression level and filter's name for blosc2.
     */
    filter_id = H5Pget_filter2(dcpl_id, (unsigned)0, &flags, &nelmts, values_out, sizeof(filter_name),
                               filter_name, NULL);
    printf("Filter info is available from the dataset creation property\n");
    printf("   Filter identifier is ");
    switch (filter_id) {
        case H5Z_FILTER_BLOSC2:
            printf("%d\n", filter_id);
            printf("   Number of parameters is %d with the value %u %u %u\n", nelmts, values_out[4],
                   values_out[5], values_out[6]);
            printf("   To find more about the filter check %s\n", filter_name);
            break;
        default:
            printf("Not expected filter\n");
            break;
    }

    /*
     * Read the data using the default properties.
     */
    printf("....Reading blosc2 compressed data ................\n");
    status = H5Dread(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);
    if (status < 0)
        printf("failed to read data.\n");

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            /*printf("%d \n", rdata[i][j]); */
            if (max < rdata[i][j])
                max = rdata[i][j];
        }
    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is %d\n", DATASET, max);
    /*
     * Check that filter is registered with the library now.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_BLOSC2);
    if (avail)
        printf("blosc2 filter is available now since H5Dread triggered loading of the filter.\n");

    ret_value = 0;

done:
    /*
     * Close and release resources.
     */
    if (dcpl_id >= 0)
        H5Pclose(dcpl_id);
    if (dset_id >= 0)
        H5Dclose(dset_id);
    if (space_id >= 0)
        H5Sclose(space_id);
    if (file_id >= 0)
        H5Fclose(file_id);

    return ret_value;
}
```

### `HDF5Examples/C/H5FLT/h5ex_d_bshuf.c`

```c
/************************************************************

  This example shows how to write data and read it from a dataset
  using bshuf filter.
  bshuf filter is not available in HDF5.
  The example uses a new feature available in HDF5 version 1.8.11
  to discover, load and register filters at run time.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME         "h5ex_d_bshuf.h5"
#define DATASET          "DS1"
#define DIM0             32
#define DIM1             64
#define CHUNK0           4
#define CHUNK1           8
#define H5Z_FILTER_BSHUF 32008

int
main(void)
{
    hid_t              file_id  = H5I_INVALID_HID;
    hid_t              space_id = H5I_INVALID_HID;
    hid_t              dset_id  = H5I_INVALID_HID;
    hid_t              dcpl_id  = H5I_INVALID_HID;
    herr_t             status;
    htri_t             avail;
    H5Z_filter_t       filter_id = 0;
    char               filter_name[128];
    hsize_t            dims[2] = {DIM0, DIM1}, chunk[2] = {CHUNK0, CHUNK1};
    size_t             nelmts = 3; /* number of elements in cd_values */
    unsigned int       flags;
    unsigned           filter_config;
    const unsigned int cd_values[3]  = {0, 0, 0};
    unsigned int       values_out[3] = {99, 99, 99};
    int                wdata[DIM0][DIM1]; /* Write buffer */
    int                rdata[DIM0][DIM1]; /* Read buffer */
    int                max;
    hsize_t            i, j;
    int                ret_value = 1;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space_id = H5Screate_simple(2, dims, NULL);
    if (space_id < 0)
        goto done;

    /*
     * Create the dataset creation property list, add the gzip
     * compression filter and set the chunk size.
     */
    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);
    if (dcpl_id < 0)
        goto done;

    status = H5Pset_filter(dcpl_id, H5Z_FILTER_BSHUF, H5Z_FLAG_MANDATORY, nelmts, cd_values);
    if (status < 0)
        goto done;

    /*
     * Check that filter is registered with the library now.
     * If it is registered, retrieve filter's configuration.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_BSHUF);
    if (avail) {
        status = H5Zget_filter_info(H5Z_FILTER_BSHUF, &filter_config);
        if ((filter_config & H5Z_FILTER_CONFIG_ENCODE_ENABLED) &&
            (filter_config & H5Z_FILTER_CONFIG_DECODE_ENABLED))
            printf("bshuf filter is available for encoding and decoding.\n");
    }
    else {
        printf("H5Zfilter_avail - not found.\n");
        goto done;
    }
    status = H5Pset_chunk(dcpl_id, 2, chunk);
    if (status < 0)
        printf("failed to set chunk.\n");

    /*
     * Create the dataset.
     */
    printf("....Create dataset ................\n");
    dset_id = H5Dcreate(file_id, DATASET, H5T_STD_I32LE, space_id, H5P_DEFAULT, dcpl_id, H5P_DEFAULT);
    if (dset_id < 0) {
        printf("failed to create dataset.\n");
        goto done;
    }

    /*
     * Write the data to the dataset.
     */
    printf("....Writing bshuf filtered data ................\n");
    status = H5Dwrite(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);
    if (status < 0)
        printf("failed to write data.\n");

    /*
     * Close and release resources.
     */
    H5Dclose(dset_id);
    dset_id = -1;
    H5Pclose(dcpl_id);
    dcpl_id = -1;
    H5Sclose(space_id);
    space_id = -1;
    H5Fclose(file_id);
    file_id = -1;
    status  = H5close();
    if (status < 0) {
        printf("\nFAILED to close library\n");
        goto done;
    }

    printf("....Close the file and reopen for reading ........\n");
    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    dset_id = H5Dopen(file_id, DATASET, H5P_DEFAULT);
    if (dset_id < 0)
        goto done;

    /*
     * Retrieve dataset creation property list.
     */
    dcpl_id = H5Dget_create_plist(dset_id);
    if (dcpl_id < 0)
        goto done;

    /*
     * Retrieve and print the filter id, compression level and filter's name for bshuf.
     */
    filter_id = H5Pget_filter2(dcpl_id, (unsigned)0, &flags, &nelmts, values_out, sizeof(filter_name),
                               filter_name, NULL);
    printf("Filter info is available from the dataset creation property\n");
    printf("   Filter identifier is ");
    switch (filter_id) {
        case H5Z_FILTER_BSHUF:
            printf("%d\n", filter_id);
            printf("   Number of parameters is %d with the value %u\n", nelmts, values_out[0]);
            printf("   To find more about the filter check %s\n", filter_name);
            break;
        default:
            printf("Not expected filter\n");
            break;
    }

    /*
     * Read the data using the default properties.
     */
    printf("....Reading bshuf filtered data ................\n");
    status = H5Dread(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);
    if (status < 0)
        printf("failed to read data.\n");

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            /*printf("%d \n", rdata[i][j]); */
            if (max < rdata[i][j])
                max = rdata[i][j];
        }
    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is %d\n", DATASET, max);
    /*
     * Check that filter is registered with the library now.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_BSHUF);
    if (avail)
        printf("bshuf filter is available now since H5Dread triggered loading of the filter.\n");

    ret_value = 0;

done:
    /*
     * Close and release resources.
     */
    if (dcpl_id >= 0)
        H5Pclose(dcpl_id);
    if (dset_id >= 0)
        H5Dclose(dset_id);
    if (space_id >= 0)
        H5Sclose(space_id);
    if (file_id >= 0)
        H5Fclose(file_id);

    return ret_value;
}
```

### `HDF5Examples/C/H5FLT/h5ex_d_bzip2.c`

```c
/************************************************************

  This example shows how to write data and read it from a dataset
  using bzip2 compression.
  bzip2 filter is not available in HDF5.
  The example uses a new feature available in HDF5 version 1.8.11
  to discover, load and register filters at run time.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME         "h5ex_d_bzip2.h5"
#define DATASET          "DS1"
#define DIM0             32
#define DIM1             64
#define CHUNK0           4
#define CHUNK1           8
#define H5Z_FILTER_BZIP2 307

int
main(void)
{
    hid_t              file_id  = H5I_INVALID_HID;
    hid_t              space_id = H5I_INVALID_HID;
    hid_t              dset_id  = H5I_INVALID_HID;
    hid_t              dcpl_id  = H5I_INVALID_HID;
    herr_t             status;
    htri_t             avail;
    H5Z_filter_t       filter_id = 0;
    char               filter_name[128];
    hsize_t            dims[2] = {DIM0, DIM1}, chunk[2] = {CHUNK0, CHUNK1};
    size_t             nelmts = 1; /* number of elements in cd_values */
    unsigned int       flags;
    unsigned           filter_config;
    const unsigned int cd_values[1]  = {2}; /* bzip2 default level is 2 */
    unsigned int       values_out[1] = {99};
    int                wdata[DIM0][DIM1]; /* Write buffer */
    int                rdata[DIM0][DIM1]; /* Read buffer */
    int                max;
    hsize_t            i, j;
    int                ret_value = 1;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space_id = H5Screate_simple(2, dims, NULL);
    if (space_id < 0)
        goto done;

    /*
     * Create the dataset creation property list, add the gzip
     * compression filter and set the chunk size.
     */
    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);
    if (dcpl_id < 0)
        goto done;

    status = H5Pset_filter(dcpl_id, H5Z_FILTER_BZIP2, H5Z_FLAG_MANDATORY, nelmts, cd_values);
    if (status < 0)
        goto done;

    /*
     * Check that filter is registered with the library now.
     * If it is registered, retrieve filter's configuration.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_BZIP2);
    if (avail) {
        status = H5Zget_filter_info(H5Z_FILTER_BZIP2, &filter_config);
        if ((filter_config & H5Z_FILTER_CONFIG_ENCODE_ENABLED) &&
            (filter_config & H5Z_FILTER_CONFIG_DECODE_ENABLED))
            printf("bzip2 filter is available for encoding and decoding.\n");
    }
    else {
        printf("H5Zfilter_avail - not found.\n");
        goto done;
    }
    status = H5Pset_chunk(dcpl_id, 2, chunk);
    if (status < 0)
        printf("failed to set chunk.\n");

    /*
     * Create the dataset.
     */
    printf("....Create dataset ................\n");
    dset_id = H5Dcreate(file_id, DATASET, H5T_STD_I32LE, space_id, H5P_DEFAULT, dcpl_id, H5P_DEFAULT);
    if (dset_id < 0) {
        printf("failed to create dataset.\n");
        goto done;
    }

    /*
     * Write the data to the dataset.
     */
    printf("....Writing bzip2 compressed data ................\n");
    status = H5Dwrite(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);
    if (status < 0)
        printf("failed to write data.\n");

    /*
     * Close and release resources.
     */
    H5Dclose(dset_id);
    dset_id = -1;
    H5Pclose(dcpl_id);
    dcpl_id = -1;
    H5Sclose(space_id);
    space_id = -1;
    H5Fclose(file_id);
    file_id = -1;
    status  = H5close();
    if (status < 0) {
        printf("\nFAILED to close library\n");
        goto done;
    }

    printf("....Close the file and reopen for reading ........\n");
    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    dset_id = H5Dopen(file_id, DATASET, H5P_DEFAULT);
    if (dset_id < 0)
        goto done;

    /*
     * Retrieve dataset creation property list.
     */
    dcpl_id = H5Dget_create_plist(dset_id);
    if (dcpl_id < 0)
        goto done;

    /*
     * Retrieve and print the filter id, compression level and filter's name for bzip2.
     */
    filter_id = H5Pget_filter2(dcpl_id, (unsigned)0, &flags, &nelmts, values_out, sizeof(filter_name),
                               filter_name, NULL);
    printf("Filter info is available from the dataset creation property\n");
    printf("   Filter identifier is ");
    switch (filter_id) {
        case H5Z_FILTER_BZIP2:
            printf("%d\n", filter_id);
            printf("   Number of parameters is %ld with the value %u\n", nelmts, values_out[0]);
            printf("   To find more about the filter check %s\n", filter_name);
            break;
        default:
            printf("Not expected filter\n");
            break;
    }

    /*
     * Read the data using the default properties.
     */
    printf("....Reading bzip2 compressed data ................\n");
    status = H5Dread(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);
    if (status < 0)
        printf("failed to read data.\n");

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            /*printf("%d \n", rdata[i][j]); */
            if (max < rdata[i][j])
                max = rdata[i][j];
        }
    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is %d\n", DATASET, max);
    /*
     * Check that filter is registered with the library now.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_BZIP2);
    if (avail)
        printf("bzip2 filter is available now since H5Dread triggered loading of the filter.\n");

    ret_value = 0;

done:
    /*
     * Close and release resources.
     */
    if (dcpl_id >= 0)
        H5Pclose(dcpl_id);
    if (dset_id >= 0)
        H5Dclose(dset_id);
    if (space_id >= 0)
        H5Sclose(space_id);
    if (file_id >= 0)
        H5Fclose(file_id);

    return ret_value;
}
```

### `HDF5Examples/C/H5FLT/h5ex_d_granularbr.c`

```c
/************************************************************

  This example shows how to write data and read it from a dataset
  using Granular BitRound quantization.
  The Granular BitRound filter is not available in HDF5.
  The example uses a new feature available in HDF5 version 1.8.11
  to discover, load and register filters at run time.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME              "h5ex_d_granularbr.h5"
#define DATASET               "DS1"
#define DIM0                  32
#define DIM1                  64
#define CHUNK0                4
#define CHUNK1                8
#define H5Z_FILTER_GRANULARBR 32023

int
main(void)
{
    hid_t              file_id  = H5I_INVALID_HID;
    hid_t              space_id = H5I_INVALID_HID;
    hid_t              dset_id  = H5I_INVALID_HID;
    hid_t              dcpl_id  = H5I_INVALID_HID;
    herr_t             status;
    htri_t             avail;
    H5Z_filter_t       filter_id = 0;
    char               filter_name[80];
    hsize_t            dims[2] = {DIM0, DIM1}, chunk[2] = {CHUNK0, CHUNK1};
    size_t             nelmts = 5; /* number of elements in cd_values */
    unsigned int       flags;
    unsigned           filter_config;
    const unsigned int cd_values[5] = {
        3, 4, 0, 0, 0}; /* Granular BitRound argument ordering is
                           NSD,sizeof(data),has_mss_val,mss_val_byt_1to4[,mss_val_byt_5to8] */
    unsigned int values_out[5] = {99, 99, 99, 99, 99};
    float        wdata[DIM0][DIM1]; /* Write buffer */
    float        rdata[DIM0][DIM1]; /* Read buffer */
    float        max;
    hsize_t      i, j;
    int          ret_value = 1;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space_id = H5Screate_simple(2, dims, NULL);
    if (space_id < 0)
        goto done;

    /*
     * Create the dataset creation property list, add the Granular BitRound
     * quantization filter and set the chunk size.
     */
    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);
    if (dcpl_id < 0)
        goto done;

    /* 20200929: csz change this to H5Z_FLAG_OPTIONAL, so that can_apply() can reject filter for
       integers without causing program to exit()? */
    status = H5Pset_filter(dcpl_id, H5Z_FILTER_GRANULARBR, H5Z_FLAG_MANDATORY, nelmts, cd_values);
    if (status < 0)
        goto done;

    /*
     * Check that filter is registered with the library now.
     * If it is registered, retrieve filter's configuration.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_GRANULARBR);
    if (avail) {
        status = H5Zget_filter_info(H5Z_FILTER_GRANULARBR, &filter_config);
        if ((filter_config & H5Z_FILTER_CONFIG_ENCODE_ENABLED) &&
            (filter_config & H5Z_FILTER_CONFIG_DECODE_ENABLED))
            printf("Granular BitRound filter is available for quantization and decoding.\n");
    }
    else {
        printf("H5Zfilter_avail - not found.\n");
        goto done;
    }
    status = H5Pset_chunk(dcpl_id, 2, chunk);
    if (status < 0)
        printf("failed to set chunk.\n");

    /*
     * Create the dataset.
     */
    printf("....Create dataset ................\n");
    dset_id = H5Dcreate(file_id, DATASET, H5T_IEEE_F32LE, space_id, H5P_DEFAULT, dcpl_id, H5P_DEFAULT);
    if (dset_id < 0) {
        printf("failed to create dataset.\n");
        goto done;
    }

    /*
     * Write the data to the dataset.
     */
    printf("....Writing Granular BitRound-quantized data ................\n");
    status = H5Dwrite(dset_id, H5T_NATIVE_FLOAT, H5S_ALL, H5S_ALL, H5P_DEFAULT, (void *)wdata);
    if (status < 0)
        printf("failed to write data.\n");

    /*
     * Close and release resources.
     */
    H5Dclose(dset_id);
    dset_id = -1;
    H5Pclose(dcpl_id);
    dcpl_id = -1;
    H5Sclose(space_id);
    space_id = -1;
    H5Fclose(file_id);
    file_id = -1;
    status  = H5close();
    if (status < 0) {
        printf("\nFAILED to close library\n");
        goto done;
    }

    printf("....Close the file and reopen for reading ........\n");
    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    dset_id = H5Dopen(file_id, DATASET, H5P_DEFAULT);
    if (dset_id < 0)
        goto done;

    /*
     * Retrieve dataset creation property list.
     */
    dcpl_id = H5Dget_create_plist(dset_id);
    if (dcpl_id < 0)
        goto done;

    /*
     * Retrieve and print the filter id, quantization level and filter's name for Granular BitRound.
     */
    filter_id = H5Pget_filter2(dcpl_id, (unsigned)0, &flags, &nelmts, values_out, sizeof(filter_name),
                               filter_name, NULL);
    printf("Filter info is available from the dataset creation property\n");
    printf("   Filter identifier is ");
    switch (filter_id) {
        case H5Z_FILTER_GRANULARBR:
            printf("%d\n", filter_id);
            printf("   Number of parameters is %lu with the values %u, %u, %u, %u, %u\n", nelmts,
                   values_out[0], values_out[1], values_out[2], values_out[3], values_out[4]);
            printf("   To find more about the filter check %s\n", filter_name);
            break;
        default:
            printf("Not expected filter\n");
            break;
    }

    /*
     * Read the data using the default properties.
     */
    printf("....Reading Granular BitRound-quantized data ................\n");
    status = H5Dread(dset_id, H5T_NATIVE_FLOAT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);
    if (status < 0)
        printf("failed to read data.\n");

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            /*printf("%d \n", rdata[i][j]); */
            if (max < rdata[i][j])
                max = rdata[i][j];
        }
    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is %g\n", DATASET, max);
    /*
     * Check that filter is registered with the library now.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_GRANULARBR);
    if (avail)
        printf("Granular BitRound filter is available now since H5Dread triggered loading of the filter.\n");

    ret_value = 0;

done:
    /*
     * Close and release resources.
     */
    if (dcpl_id >= 0)
        H5Pclose(dcpl_id);
    if (dset_id >= 0)
        H5Dclose(dset_id);
    if (space_id >= 0)
        H5Sclose(space_id);
    if (file_id >= 0)
        H5Fclose(file_id);

    return ret_value;
}
```

### `HDF5Examples/C/H5FLT/h5ex_d_jpeg.c`

```c
/************************************************************

  This example shows how to write data and read it from a dataset
  using jpeg compression.
  jpeg filter is not available in HDF5.
  The example uses a new feature available in HDF5 version 1.8.11
  to discover, load and register filters at run time.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME        "h5ex_d_jpeg.h5"
#define DATASET         "DS1"
#define DIM0            512
#define DIM1            1024
#define NUM_IMAGES      10
#define JPEG_QUALITY    100
#define CHUNK0          1
#define CHUNK1          DIM0
#define CHUNK2          DIM1
#define H5Z_FILTER_JPEG 32019

int
main(void)
{
    hid_t        file_id  = H5I_INVALID_HID;
    hid_t        space_id = H5I_INVALID_HID;
    hid_t        dset_id  = H5I_INVALID_HID;
    hid_t        dcpl_id  = H5I_INVALID_HID;
    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_id = 0;
    char         filter_name[128];
    hsize_t      dims[3]  = {NUM_IMAGES, DIM0, DIM1};
    hsize_t      chunk[3] = {CHUNK0, CHUNK1, CHUNK2};
    size_t       nelmts   = 4; /* number of elements in cd_values */
    unsigned int flags;
    unsigned     filter_config;
    size_t       data_size = DIM0 * DIM1 * NUM_IMAGES;

    /* JPEG filter requires 4 parameters */
    /* JPEG quality factor (1-100) */
    /* Number of columns */
    /* Number of rows */
    /* Color mode (0=Mono, 1=RGB) */
    const unsigned int cd_values[4]  = {JPEG_QUALITY, DIM0, DIM1, 0}; /* jpeg default level is 2 */
    unsigned int       values_out[4] = {99, 99, 99, 99};
    unsigned char     *wdata; /* Write buffer */
    unsigned char     *rdata; /* Read buffer */
    int                num_diff = 0;
    hsize_t            i;
    int                ret_value = 1;

    wdata = (unsigned char *)malloc(sizeof(unsigned char) * data_size);
    rdata = (unsigned char *)malloc(sizeof(unsigned char) * data_size);

    for (i = 0; i < data_size; i++) {
        wdata[i] = i;
    }

    /*
     * Create a new file using the default properties.
     */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space_id = H5Screate_simple(3, dims, NULL);
    if (space_id < 0)
        goto done;

    /*
     * Create the dataset creation property list, add the gzip
     * compression filter and set the chunk size.
     */
    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);
    if (dcpl_id < 0)
        goto done;

    status = H5Pset_filter(dcpl_id, H5Z_FILTER_JPEG, H5Z_FLAG_MANDATORY, nelmts, cd_values);
    if (status < 0)
        goto done;

    /*
     * Check that filter is registered with the library now.
     * If it is registered, retrieve filter's configuration.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_JPEG);
    if (avail) {
        status = H5Zget_filter_info(H5Z_FILTER_JPEG, &filter_config);
        if ((filter_config & H5Z_FILTER_CONFIG_ENCODE_ENABLED) &&
            (filter_config & H5Z_FILTER_CONFIG_DECODE_ENABLED))
            printf("jpeg filter is available for encoding and decoding.\n");
    }
    else {
        printf("H5Zfilter_avail - not found.\n");
        goto done;
    }
    status = H5Pset_chunk(dcpl_id, 3, chunk);
    if (status < 0)
        printf("failed to set chunk.\n");

    /*
     * Create the dataset.
     */
    printf("....Create dataset ................\n");
    dset_id = H5Dcreate(file_id, DATASET, H5T_NATIVE_UINT8, space_id, H5P_DEFAULT, dcpl_id, H5P_DEFAULT);
    if (dset_id < 0) {
        printf("failed to create dataset.\n");
        goto done;
    }

    /*
     * Write the data to the dataset.
     */
    printf("....Writing jpeg compressed data ................\n");
    status = H5Dwrite(dset_id, H5T_NATIVE_UINT8, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);
    if (status < 0)
        printf("failed to write data.\n");

    /*
     * Close and release resources.
     */
    H5Dclose(dset_id);
    dset_id = -1;
    H5Pclose(dcpl_id);
    dcpl_id = -1;
    H5Sclose(space_id);
    space_id = -1;
    H5Fclose(file_id);
    file_id = -1;
    status  = H5close();
    if (status < 0) {
        printf("\nFAILED to close library\n");
        goto done;
    }

    printf("....Close the file and reopen for reading ........\n");
    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    dset_id = H5Dopen(file_id, DATASET, H5P_DEFAULT);
    if (dset_id < 0)
        goto done;

    /*
     * Retrieve dataset creation property list.
     */
    dcpl_id = H5Dget_create_plist(dset_id);
    if (dcpl_id < 0)
        goto done;

    /*
     * Retrieve and print the filter id, compression level and filter's name for jpeg.
     */
    filter_id = H5Pget_filter2(dcpl_id, (unsigned)0, &flags, &nelmts, values_out, sizeof(filter_name),
                               filter_name, NULL);
    printf("Filter info is available from the dataset creation property\n");
    printf("   Filter identifier is ");
    switch (filter_id) {
        case H5Z_FILTER_JPEG:
            printf("%d\n", filter_id);
            printf("   Number of parameters is %d with the value %u\n", nelmts, values_out[0]);
            printf("   To find more about the filter check %s\n", filter_name);
            break;
        default:
            printf("Not expected filter\n");
            break;
    }

    /*
     * Read the data using the default properties.
     */
    printf("....Reading jpeg compressed data ................\n");
    status = H5Dread(dset_id, H5T_NATIVE_UINT8, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);
    if (status < 0)
        printf("failed to read data.\n");

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    for (i = 0; i < data_size; i++) {
        /*printf("%d \n", rdata[i]); */
        if (rdata[i] != wdata[i])
            num_diff++;
    }
    /*
     * Print the number of differences.
     */
    printf("JPEG quality=%d, percent of differing array elements=%f\n", JPEG_QUALITY,
           100. * (double)num_diff / data_size);
    /*
     * Check that filter is registered with the library now.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_JPEG);
    if (avail)
        printf("jpeg filter is available now since H5Dread triggered loading of the filter.\n");

    ret_value = 0;

done:
    free(rdata);
    free(wdata);
    /*
     * Close and release resources.
     */
    if (dcpl_id >= 0)
        H5Pclose(dcpl_id);
    if (dset_id >= 0)
        H5Dclose(dset_id);
    if (space_id >= 0)
        H5Sclose(space_id);
    if (file_id >= 0)
        H5Fclose(file_id);

    return ret_value;
}
```

### `HDF5Examples/C/H5FLT/h5ex_d_lz4.c`

```c
/************************************************************

  This example shows how to write data and read it from a dataset
  using lz4 compression.
  lz4 filter is not available in HDF5.
  The example uses a new feature available in HDF5 version 1.8.11
  to discover, load and register filters at run time.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME       "h5ex_d_lz4.h5"
#define DATASET        "DS1"
#define DIM0           32
#define DIM1           64
#define CHUNK0         4
#define CHUNK1         8
#define H5Z_FILTER_LZ4 32004

int
main(void)
{
    hid_t              file_id  = H5I_INVALID_HID;
    hid_t              space_id = H5I_INVALID_HID;
    hid_t              dset_id  = H5I_INVALID_HID;
    hid_t              dcpl_id  = H5I_INVALID_HID;
    herr_t             status;
    htri_t             avail;
    H5Z_filter_t       filter_id = 0;
    char               filter_name[128];
    hsize_t            dims[2] = {DIM0, DIM1}, chunk[2] = {CHUNK0, CHUNK1};
    size_t             nelmts = 1; /* number of elements in cd_values */
    unsigned int       flags;
    unsigned           filter_config;
    const unsigned int cd_values[1]  = {CHUNK0 * CHUNK1 *
                                        sizeof(int)}; /* lz4 default is 1,073,741,824 bytes */
    unsigned int       values_out[1] = {99};
    int                wdata[DIM0][DIM1]; /* Write buffer */
    int                rdata[DIM0][DIM1]; /* Read buffer */
    int                max;
    hsize_t            i, j;
    int                ret_value = 1;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space_id = H5Screate_simple(2, dims, NULL);
    if (space_id < 0)
        goto done;

    /*
     * Create the dataset creation property list, add the gzip
     * compression filter and set the chunk size.
     */
    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);
    if (dcpl_id < 0)
        goto done;

    status = H5Pset_filter(dcpl_id, H5Z_FILTER_LZ4, H5Z_FLAG_MANDATORY, nelmts, cd_values);
    if (status < 0)
        goto done;

    /*
     * Check that filter is registered with the library now.
     * If it is registered, retrieve filter's configuration.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_LZ4);
    if (avail) {
        status = H5Zget_filter_info(H5Z_FILTER_LZ4, &filter_config);
        if ((filter_config & H5Z_FILTER_CONFIG_ENCODE_ENABLED) &&
            (filter_config & H5Z_FILTER_CONFIG_DECODE_ENABLED))
            printf("lz4 filter is available for encoding and decoding.\n");
    }
    else {
        printf("H5Zfilter_avail - not found.\n");
        goto done;
    }
    status = H5Pset_chunk(dcpl_id, 2, chunk);
    if (status < 0)
        printf("failed to set chunk.\n");

    /*
     * Create the dataset.
     */
    printf("....Create dataset ................\n");
    dset_id = H5Dcreate(file_id, DATASET, H5T_STD_I32LE, space_id, H5P_DEFAULT, dcpl_id, H5P_DEFAULT);
    if (dset_id < 0) {
        printf("failed to create dataset.\n");
        goto done;
    }

    /*
     * Write the data to the dataset.
     */
    printf("....Writing lz4 compressed data ................\n");
    status = H5Dwrite(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);
    if (status < 0)
        printf("failed to write data.\n");

    /*
     * Close and release resources.
     */
    H5Dclose(dset_id);
    dset_id = -1;
    H5Pclose(dcpl_id);
    dcpl_id = -1;
    H5Sclose(space_id);
    space_id = -1;
    H5Fclose(file_id);
    file_id = -1;
    status  = H5close();
    if (status < 0) {
        printf("\nFAILED to close library\n");
        goto done;
    }

    printf("....Close the file and reopen for reading ........\n");
    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    dset_id = H5Dopen(file_id, DATASET, H5P_DEFAULT);
    if (dset_id < 0)
        goto done;

    /*
     * Retrieve dataset creation property list.
     */
    dcpl_id = H5Dget_create_plist(dset_id);
    if (dcpl_id < 0)
        goto done;

    /*
     * Retrieve and print the filter id, compression level and filter's name for lz4.
     */
    filter_id = H5Pget_filter2(dcpl_id, (unsigned)0, &flags, &nelmts, values_out, sizeof(filter_name),
                               filter_name, NULL);
    printf("Filter info is available from the dataset creation property\n");
    printf("   Filter identifier is ");
    switch (filter_id) {
        case H5Z_FILTER_LZ4:
            printf("%d\n", filter_id);
            printf("   Number of parameters is %ld with the value %u\n", nelmts, values_out[0]);
            printf("   To find more about the filter check %s\n", filter_name);
            break;
        default:
            printf("Not expected filter\n");
            break;
    }

    /*
     * Read the data using the default properties.
     */
    printf("....Reading lz4 compressed data ................\n");
    status = H5Dread(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);
    if (status < 0)
        printf("failed to read data.\n");

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            /*printf("%d \n", rdata[i][j]); */
            if (max < rdata[i][j])
                max = rdata[i][j];
        }
    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is %d\n", DATASET, max);
    /*
     * Check that filter is registered with the library now.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_LZ4);
    if (avail)
        printf("lz4 filter is available now since H5Dread triggered loading of the filter.\n");

    ret_value = 0;

done:
    /*
     * Close and release resources.
     */
    if (dcpl_id >= 0)
        H5Pclose(dcpl_id);
    if (dset_id >= 0)
        H5Dclose(dset_id);
    if (space_id >= 0)
        H5Sclose(space_id);
    if (file_id >= 0)
        H5Fclose(file_id);

    return ret_value;
}
```

### `HDF5Examples/C/H5FLT/h5ex_d_lzf.c`

```c
/************************************************************

  This example shows how to write data and read it from a dataset
  using lzf compression.
  lzf filter is not available in HDF5.
  The example uses a new feature available in HDF5 version 1.8.11
  to discover, load and register filters at run time.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME       "h5ex_d_lzf.h5"
#define DATASET        "DS1"
#define DIM0           32
#define DIM1           64
#define CHUNK0         4
#define CHUNK1         8
#define H5Z_FILTER_LZF 32000

int
main(void)
{
    hid_t              file_id  = H5I_INVALID_HID;
    hid_t              space_id = H5I_INVALID_HID;
    hid_t              dset_id  = H5I_INVALID_HID;
    hid_t              dcpl_id  = H5I_INVALID_HID;
    herr_t             status;
    htri_t             avail;
    H5Z_filter_t       filter_id = 0;
    char               filter_name[128];
    hsize_t            dims[2] = {DIM0, DIM1}, chunk[2] = {CHUNK0, CHUNK1};
    size_t             nelmts = 3; /* number of elements in cd_values */
    unsigned int       flags;
    unsigned           filter_config;
    const unsigned int cd_values[3]  = {0, 0, 0};
    unsigned int       values_out[3] = {99, 99, 99};
    int                wdata[DIM0][DIM1]; /* Write buffer */
    int                rdata[DIM0][DIM1]; /* Read buffer */
    int                max;
    hsize_t            i, j;
    int                ret_value = 1;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space_id = H5Screate_simple(2, dims, NULL);
    if (space_id < 0)
        goto done;

    /*
     * Create the dataset creation property list, add the gzip
     * compression filter and set the chunk size.
     */
    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);
    if (dcpl_id < 0)
        goto done;

    status = H5Pset_filter(dcpl_id, H5Z_FILTER_LZF, H5Z_FLAG_OPTIONAL, 0, NULL);
    if (status < 0)
        goto done;

    /*
     * Check that filter is registered with the library now.
     * If it is registered, retrieve filter's configuration.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_LZF);
    if (avail) {
        status = H5Zget_filter_info(H5Z_FILTER_LZF, &filter_config);
        if ((filter_config & H5Z_FILTER_CONFIG_ENCODE_ENABLED) &&
            (filter_config & H5Z_FILTER_CONFIG_DECODE_ENABLED))
            printf("lzf filter is available for encoding and decoding.\n");
    }
    else {
        printf("H5Zfilter_avail - not found.\n");
        goto done;
    }
    status = H5Pset_chunk(dcpl_id, 2, chunk);
    if (status < 0)
        printf("failed to set chunk.\n");

    /*
     * Create the dataset.
     */
    printf("....Create dataset ................\n");
    dset_id = H5Dcreate(file_id, DATASET, H5T_STD_I32LE, space_id, H5P_DEFAULT, dcpl_id, H5P_DEFAULT);
    if (dset_id < 0) {
        printf("failed to create dataset.\n");
        goto done;
    }

    /*
     * Write the data to the dataset.
     */
    printf("....Writing lzf compressed data ................\n");
    status = H5Dwrite(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);
    if (status < 0)
        printf("failed to write data.\n");

    /*
     * Close and release resources.
     */
    H5Dclose(dset_id);
    dset_id = -1;
    H5Pclose(dcpl_id);
    dcpl_id = -1;
    H5Sclose(space_id);
    space_id = -1;
    H5Fclose(file_id);
    file_id = -1;
    status  = H5close();
    if (status < 0) {
        printf("\nFAILED to close library\n");
        goto done;
    }

    printf("....Close the file and reopen for reading ........\n");
    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    dset_id = H5Dopen(file_id, DATASET, H5P_DEFAULT);
    if (dset_id < 0)
        goto done;

    /*
     * Retrieve dataset creation property list.
     */
    dcpl_id = H5Dget_create_plist(dset_id);
    if (dcpl_id < 0)
        goto done;

    /*
     * Retrieve and print the filter id, compression level and filter's name for lzf.
     */
    filter_id = H5Pget_filter2(dcpl_id, (unsigned)0, &flags, &nelmts, values_out, sizeof(filter_name),
                               filter_name, NULL);
    printf("Filter info is available from the dataset creation property\n");
    printf("   Filter identifier is ");
    switch (filter_id) {
        case H5Z_FILTER_LZF:
            printf("%d\n", filter_id);
            printf("   Number of parameters is %d with the value %u\n", nelmts, values_out[0]);
            printf("   To find more about the filter check %s\n", filter_name);
            break;
        default:
            printf("Not expected filter\n");
            break;
    }

    /*
     * Read the data using the default properties.
     */
    printf("....Reading lzf compressed data ................\n");
    status = H5Dread(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);
    if (status < 0)
        printf("failed to read data.\n");

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            /*printf("%d \n", rdata[i][j]); */
            if (max < rdata[i][j])
                max = rdata[i][j];
        }
    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is %d\n", DATASET, max);
    /*
     * Check that filter is registered with the library now.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_LZF);
    if (avail)
        printf("lzf filter is available now since H5Dread triggered loading of the filter.\n");

    ret_value = 0;

done:
    /*
     * Close and release resources.
     */
    if (dcpl_id >= 0)
        H5Pclose(dcpl_id);
    if (dset_id >= 0)
        H5Dclose(dset_id);
    if (space_id >= 0)
        H5Sclose(space_id);
    if (file_id >= 0)
        H5Fclose(file_id);

    return ret_value;
}
```

### `HDF5Examples/C/H5FLT/h5ex_d_zfp.c`

```c
/************************************************************

  This example shows how to write data and read it from a dataset
  using zfp compression.
  zfp filter is not available in HDF5.
  The example uses a new feature available in HDF5 version 1.8.11
  to discover, load and register filters at run time.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME       "h5ex_d_zfp.h5"
#define DATASET        "DS1"
#define DIM0           32
#define DIM1           64
#define CHUNK0         4
#define CHUNK1         8
#define H5Z_FILTER_ZFP 32013

int
main(void)
{
    hid_t              file_id  = H5I_INVALID_HID;
    hid_t              space_id = H5I_INVALID_HID;
    hid_t              dset_id  = H5I_INVALID_HID;
    hid_t              dcpl_id  = H5I_INVALID_HID;
    herr_t             status;
    htri_t             avail;
    H5Z_filter_t       filter_id = 0;
    char               filter_name[128];
    hsize_t            dims[2] = {DIM0, DIM1}, chunk[2] = {CHUNK0, CHUNK1};
    size_t             nelmts = 3; /* number of elements in cd_values */
    unsigned int       flags;
    unsigned           filter_config;
    const unsigned int cd_values[10]  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
    unsigned int       values_out[10] = {99, 99, 99, 99, 99, 99, 99, 99, 99, 99};
    float              wdata[DIM0][DIM1]; /* Write buffer */
    float              rdata[DIM0][DIM1]; /* Read buffer */
    float              max;
    hsize_t            i, j;
    int                ret_value = 1;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = (float)(i * j) - (float)(j);

    /*
     * Create a new file using the default properties.
     */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space_id = H5Screate_simple(2, dims, NULL);
    if (space_id < 0)
        goto done;

    /*
     * Create the dataset creation property list, add the gzip
     * compression filter and set the chunk size.
     */
    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);
    if (dcpl_id < 0)
        goto done;

    status = H5Pset_filter(dcpl_id, H5Z_FILTER_ZFP, H5Z_FLAG_OPTIONAL, 0, NULL);
    if (status < 0)
        goto done;

    /*
     * Check that filter is registered with the library now.
     * If it is registered, retrieve filter's configuration.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_ZFP);
    if (avail) {
        status = H5Zget_filter_info(H5Z_FILTER_ZFP, &filter_config);
        if ((filter_config & H5Z_FILTER_CONFIG_ENCODE_ENABLED) &&
            (filter_config & H5Z_FILTER_CONFIG_DECODE_ENABLED))
            printf("zfp filter is available for encoding and decoding.\n");
    }
    else {
        printf("H5Zfilter_avail - not found.\n");
        goto done;
    }
    status = H5Pset_chunk(dcpl_id, 2, chunk);
    if (status < 0)
        printf("failed to set chunk.\n");

    /*
     * Create the dataset.
     */
    printf("....Create dataset ................\n");
    dset_id = H5Dcreate(file_id, DATASET, H5T_IEEE_F32LE, space_id, H5P_DEFAULT, dcpl_id, H5P_DEFAULT);
    if (dset_id < 0) {
        printf("failed to create dataset.\n");
        goto done;
    }

    /*
     * Write the data to the dataset.
     */
    printf("....Writing zfp compressed data ................\n");
    status = H5Dwrite(dset_id, H5T_IEEE_F32LE, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);
    if (status < 0)
        printf("failed to write data.\n");

    /*
     * Close and release resources.
     */
    H5Dclose(dset_id);
    dset_id = -1;
    H5Pclose(dcpl_id);
    dcpl_id = -1;
    H5Sclose(space_id);
    space_id = -1;
    H5Fclose(file_id);
    file_id = -1;
    status  = H5close();
    if (status < 0) {
        printf("\nFAILED to close library\n");
        goto done;
    }

    printf("....Close the file and reopen for reading ........\n");
    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    dset_id = H5Dopen(file_id, DATASET, H5P_DEFAULT);
    if (dset_id < 0)
        goto done;

    /*
     * Retrieve dataset creation property list.
     */
    dcpl_id = H5Dget_create_plist(dset_id);
    if (dcpl_id < 0)
        goto done;

    /*
     * Retrieve and print the filter id, compression level and filter's name for zfp.
     */
    filter_id = H5Pget_filter2(dcpl_id, (unsigned)0, &flags, &nelmts, values_out, sizeof(filter_name),
                               filter_name, NULL);
    printf("Filter info is available from the dataset creation property\n");
    printf("   Filter identifier is ");
    switch (filter_id) {
        case H5Z_FILTER_ZFP:
            printf("%d\n", filter_id);
            printf("   Number of parameters is %d with the value %u\n", nelmts, values_out[0]);
            printf("   To find more about the filter check %s\n", filter_name);
            break;
        default:
            printf("Not expected filter\n");
            break;
    }

    /*
     * Read the data using the default properties.
     */
    printf("....Reading zfp compressed data ................\n");
    status = H5Dread(dset_id, H5T_IEEE_F32LE, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);
    if (status < 0)
        printf("failed to read data.\n");

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    max = rdata[0][0];
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            /*printf("%f \n", rdata[i][j]); */
            if (max < rdata[i][j])
                max = rdata[i][j];
        }
    /*
     * Print the maximum value.
     */
    printf("Maximum value in %s is %6.4f\n", DATASET, max);
    /*
     * Check that filter is registered with the library now.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_ZFP);
    if (avail)
        printf("zfp filter is available now since H5Dread triggered loading of the filter.\n");

    ret_value = 0;

done:
    /*
     * Close and release resources.
     */
    if (dcpl_id >= 0)
        H5Pclose(dcpl_id);
    if (dset_id >= 0)
        H5Dclose(dset_id);
    if (space_id >= 0)
        H5Sclose(space_id);
    if (file_id >= 0)
        H5Fclose(file_id);

    return ret_value;
}
```

### `HDF5Examples/C/H5FLT/h5ex_d_zstd.c`

```c
/************************************************************

  This example shows how to write data and read it from a dataset
  using zstd compression.
  zstd filter is not available in HDF5.
  The example uses a new feature available in HDF5 version 1.8.11
  to discover, load and register filters at run time.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME        "h5ex_d_zstd.h5"
#define DATASET         "DS1"
#define DIM0            512
#define DIM1            1024
#define NUM_IMAGES      2
#define CHUNK0          1
#define CHUNK1          DIM0
#define CHUNK2          DIM1
#define H5Z_FILTER_ZSTD 32015

int
main(void)
{
    hid_t        file_id  = H5I_INVALID_HID;
    hid_t        space_id = H5I_INVALID_HID;
    hid_t        dset_id  = H5I_INVALID_HID;
    hid_t        dcpl_id  = H5I_INVALID_HID;
    herr_t       status;
    htri_t       avail;
    H5Z_filter_t filter_id = 0;
    char         filter_name[128];
    hsize_t      dims[3] = {NUM_IMAGES, DIM0, DIM1}, chunk[3] = {CHUNK0, CHUNK1, CHUNK2};
    size_t       nelmts = 1; /* number of elements in cd_values */
    unsigned int flags;
    unsigned     filter_config;
    size_t       data_size = DIM0 * DIM1 * NUM_IMAGES;

    /* ZSTD filter optionally uses 1 parameter */
    /* Number of columns */
    /* Number of rows */
    const unsigned int cd_values[1]  = {0}; /* zstd default level is 3 */
    unsigned int       values_out[1] = {99};
    unsigned char     *wdata, /* Write buffer */
        *rdata;               /* Read buffer */
    int     num_diff = 0;
    hsize_t i;
    int     ret_value = 1;

    wdata = (unsigned char *)malloc(sizeof(unsigned char) * data_size);
    rdata = (unsigned char *)malloc(sizeof(unsigned char) * data_size);

    for (i = 0; i < data_size; i++) {
        wdata[i] = i;
    }

    /*
     * Create a new file using the default properties.
     */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space_id = H5Screate_simple(3, dims, NULL);
    if (space_id < 0)
        goto done;

    /*
     * Create the dataset creation property list, add the ZSTD
     * compression filter and set the chunk size.
     */
    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);
    if (dcpl_id < 0)
        goto done;

    status = H5Pset_filter(dcpl_id, H5Z_FILTER_ZSTD, H5Z_FLAG_MANDATORY, nelmts, cd_values);
    if (status < 0)
        goto done;

    /*
     * Check that filter is registered with the library now.
     * If it is registered, retrieve filter's configuration.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_ZSTD);
    if (avail) {
        status = H5Zget_filter_info(H5Z_FILTER_ZSTD, &filter_config);
        if ((filter_config & H5Z_FILTER_CONFIG_ENCODE_ENABLED) &&
            (filter_config & H5Z_FILTER_CONFIG_DECODE_ENABLED))
            printf("zstd filter is available for encoding and decoding.\n");
    }
    else {
        printf("H5Zfilter_avail - not found.\n");
        goto done;
    }
    status = H5Pset_chunk(dcpl_id, 3, chunk);
    if (status < 0)
        printf("failed to set chunk.\n");

    /*
     * Create the dataset.
     */
    printf("....Create dataset ................\n");
    dset_id = H5Dcreate(file_id, DATASET, H5T_NATIVE_UINT8, space_id, H5P_DEFAULT, dcpl_id, H5P_DEFAULT);
    if (dset_id < 0) {
        printf("failed to create dataset.\n");
        goto done;
    }

    /*
     * Write the data to the dataset.
     */
    printf("....Writing zstd compressed data ................\n");
    status = H5Dwrite(dset_id, H5T_NATIVE_UINT8, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);
    if (status < 0)
        printf("failed to write data.\n");

    /*
     * Close and release resources.
     */
    H5Dclose(dset_id);
    dset_id = -1;
    H5Pclose(dcpl_id);
    dcpl_id = -1;
    H5Sclose(space_id);
    space_id = -1;
    H5Fclose(file_id);
    file_id = -1;
    status  = H5close();
    if (status < 0) {
        printf("\nFAILED to close library\n");
        goto done;
    }

    printf("....Close the file and reopen for reading ........\n");
    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    if (file_id < 0)
        goto done;

    dset_id = H5Dopen(file_id, DATASET, H5P_DEFAULT);
    if (dset_id < 0)
        goto done;

    /*
     * Retrieve dataset creation property list.
     */
    dcpl_id = H5Dget_create_plist(dset_id);
    if (dcpl_id < 0)
        goto done;

    /*
     * Retrieve and print the filter id, compression level and filter's name for zstd.
     */
    filter_id = H5Pget_filter2(dcpl_id, (unsigned)0, &flags, &nelmts, values_out, sizeof(filter_name),
                               filter_name, NULL);
    printf("Filter info is available from the dataset creation property\n");
    printf("   Filter identifier is ");
    switch (filter_id) {
        case H5Z_FILTER_ZSTD:
            printf("%d\n", filter_id);
            printf("   Number of parameters is %d with the value %u\n", nelmts, values_out[0]);
            printf("   To find more about the filter check %s\n", filter_name);
            break;
        default:
            printf("Not expected filter\n");
            break;
    }

    /*
     * Read the data using the default properties.
     */
    printf("....Reading zstd compressed data ................\n");
    status = H5Dread(dset_id, H5T_NATIVE_UINT8, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);
    if (status < 0)
        printf("failed to read data.\n");

    /*
     * Find the maximum value in the dataset, to verify that it was
     * read correctly.
     */
    for (i = 0; i < data_size; i++) {
        /*printf("%d \n", rdata[i]); */
        if (rdata[i] != wdata[i])
            num_diff++;
    }
    /*
     * Print the number of differences.
     */
    printf("ZSTD number of differing array elements=%d\n", num_diff);
    /*
     * Check that filter is registered with the library now.
     */
    avail = H5Zfilter_avail(H5Z_FILTER_ZSTD);
    if (avail)
        printf("zstd filter is available now since H5Dread triggered loading of the filter.\n");

    ret_value = 0;

done:
    free(rdata);
    free(wdata);
    /*
     * Close and release resources.
     */
    if (dcpl_id >= 0)
        H5Pclose(dcpl_id);
    if (dset_id >= 0)
        H5Dclose(dset_id);
    if (space_id >= 0)
        H5Sclose(space_id);
    if (file_id >= 0)
        H5Fclose(file_id);

    return ret_value;
}
```

### `HDF5Examples/C/H5G/16/h5ex_g_create.c`

```c
/************************************************************

  This example shows how to create, open, and close a group.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"

#define FILENAME "h5ex_g_create.h5"

int
main(void)
{
    hid_t  file, group; /* Handles */
    herr_t status;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create a group named "G1" in the file.
     */
    group = H5Gcreate(file, "/G1", 0);

    /*
     * Close the group.  The handle "group" can no longer be used.
     */
    status = H5Gclose(group);

    /*
     * Re-open the group, obtaining a new handle.
     */
    group = H5Gopen(file, "/G1");

    /*
     * Close and release resources.
     */
    status = H5Gclose(group);
    status = H5Fclose(file);
}
```

### `HDF5Examples/C/H5G/16/h5ex_g_iterate.c`

```c
/************************************************************

  This example shows how to iterate over group members using
  H5Giterate.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>

#define FILENAME "h5ex_g_iterate.h5"

/*
 * Operator function to be called by H5Giterate.
 */
herr_t op_func(hid_t loc_id, const char *name, void *operator_data);

int
main(void)
{
    hid_t  file; /* Handle */
    herr_t status;

    /*
     * Open file.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);

    /*
     * Begin iteration.
     */
    printf("Objects in root group:\n");
    status = H5Giterate(file, "/", NULL, op_func, NULL);

    /*
     * Close and release resources.
     */
    status = H5Fclose(file);

    return 0;
}

/************************************************************

  Operator function.  Prints the name and type of the object
  being examined.

 ************************************************************/
herr_t
op_func(hid_t loc_id, const char *name, void *operator_data)
{
    herr_t     status;
    H5G_stat_t statbuf;

    /*
     * Get type of the object and display its name and type.
     * The name of the object is passed to this function by
     * the Library.
     */
    status = H5Gget_objinfo(loc_id, name, 0, &statbuf);
    switch (statbuf.type) {
        case H5G_GROUP:
            printf("  Group: %s\n", name);
            break;
        case H5G_DATASET:
            printf("  Dataset: %s\n", name);
            break;
        case H5G_TYPE:
            printf("  Datatype: %s\n", name);
            break;
        default:
            printf("  Unknown: %s\n", name);
    }

    return 0;
}
```

### `HDF5Examples/C/H5G/16/h5ex_g_traverse.c`

```c
/************************************************************

  This example shows a way to recursively traverse the file
  using H5Giterate.  The method shown here guarantees that
  the recursion will not enter an infinite loop, but does
  not prevent objects from being visited more than once.
  The program prints the directory structure of the file
  specified in FILE.  The default file used by this example
  implements the structure described in the User's Guide,
  chapter 4, figure 26.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>

#define FILENAME "h5ex_g_traverse.h5"

/*
 * Define operator data structure type for H5Giterate callback.
 * During recursive iteration, these structures will form a
 * linked list that can be searched for duplicate groups,
 * preventing infinite recursion.
 */
struct opdata {
    unsigned       recurs;     /* recursion level.  0=root */
    struct opdata *prev;       /* pointer to previous opdata */
    unsigned long  groupno[2]; /* unique group number */
};

/*
 * Operator function to be called by H5Giterate.
 */
herr_t op_func(hid_t loc_id, const char *name, void *operator_data);

/*
 * Function to check for duplicate groups in a path.
 */
int group_check(struct opdata *od, unsigned long target_groupno[2]);

int
main(void)
{
    hid_t         file; /* Handle */
    herr_t        status;
    H5G_stat_t    statbuf;
    struct opdata od;

    /*
     * Open file and initialize the operator data structure.
     */
    file          = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    status        = H5Gget_objinfo(file, "/", 0, &statbuf);
    od.recurs     = 0;
    od.prev       = NULL;
    od.groupno[0] = statbuf.objno[0];
    od.groupno[1] = statbuf.objno[1];

    /*
     * Print the root group and formatting, begin iteration.
     */
    printf("/ {\n");
    status = H5Giterate(file, "/", NULL, op_func, (void *)&od);
    printf("}\n");

    /*
     * Close and release resources.
     */
    status = H5Fclose(file);

    return 0;
}

/************************************************************

  Operator function.  This function prints the name and type
  of the object passed to it.  If the object is a group, it
  is first checked against other groups in its path using
  the group_check function, then if it is not a duplicate,
  H5Giterate is called for that group.  This guarantees that
  the program will not enter infinite recursion due to a
  circular path in the file.

 ************************************************************/
herr_t
op_func(hid_t loc_id, const char *name, void *operator_data)
{
    herr_t         status, return_val = 0;
    H5G_stat_t     statbuf;
    struct opdata *od = (struct opdata *)operator_data;
    /* Type conversion */
    unsigned spaces = 2 * (od->recurs + 1);
    /* Number of whitespaces to prepend
       to output */

    /*
     * Get type of the object and display its name and type.
     * The name of the object is passed to this function by
     * the Library.
     */
    status = H5Gget_objinfo(loc_id, name, 0, &statbuf);
    printf("%*s", spaces, ""); /* Format output */
    switch (statbuf.type) {
        case H5G_GROUP:
            printf("Group: %s {\n", name);

            /*
             * Check group objno against linked list of operator
             * data structures.  Only necessary if there is more
             * than 1 link to the group.
             */
            if ((statbuf.nlink > 1) && group_check(od, statbuf.objno)) {
                printf("%*s  Warning: Loop detected!\n", spaces, "");
            }
            else {

                /*
                 * Initialize new operator data structure and
                 * begin recursive iteration on the discovered
                 * group.  The new opdata structure is given a
                 * pointer to the current one.
                 */
                struct opdata nextod;
                nextod.recurs     = od->recurs + 1;
                nextod.prev       = od;
                nextod.groupno[0] = statbuf.objno[0];
                nextod.groupno[1] = statbuf.objno[1];
                return_val        = H5Giterate(loc_id, name, NULL, op_func, (void *)&nextod);
            }
            printf("%*s}\n", spaces, "");
            break;
        case H5G_DATASET:
            printf("Dataset: %s\n", name);
            break;
        case H5G_TYPE:
            printf("Datatype: %s\n", name);
            break;
        default:
            printf("Unknown: %s\n", name);
    }

    return return_val;
}

/************************************************************

  This function recursively searches the linked list of
  opdata structures for one whose groupno field matches
  target_groupno.  Returns 1 if a match is found, and 0
  otherwise.

 ************************************************************/
int
group_check(struct opdata *od, unsigned long target_groupno[2])
{
    if ((od->groupno[0] == target_groupno[0]) && (od->groupno[1] == target_groupno[1]))
        return 1; /* Group numbers match */
    else if (!od->recurs)
        return 0; /* Root group reached with no matches */
    else
        return group_check(od->prev, target_groupno);
    /* Recursively examine the next node */
}
```

### `HDF5Examples/C/H5G/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_C_H5G C)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (C_sourcefiles.cmake)

foreach (example_name ${common_examples})
  if (${H5_LIBVER_DIR} EQUAL 16 OR ${EXAMPLE_VARNAME}_USE_16_API)
    add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/16/${example_name}.c)
  else ()
    add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
  endif ()
  target_compile_options (${EXAMPLE_VARNAME}_${example_name}
      PRIVATE
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  if (H5_HAVE_PARALLEL)
    target_include_directories (${EXAMPLE_VARNAME}_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
  endif ()
  target_link_libraries (${EXAMPLE_VARNAME}_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
  if (H5EXAMPLE_BUILD_TESTING)
    if (NOT ${example_name} STREQUAL "h5ex_g_create")
      add_custom_command (
          TARGET     ${EXAMPLE_VARNAME}_${example_name}
          POST_BUILD
          COMMAND    ${CMAKE_COMMAND}
          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/16/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
      )
    endif ()
  endif ()
endforeach ()

if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8" AND NOT ${EXAMPLE_VARNAME}_USE_16_API)
  foreach (example_name ${1_8_examples})
    add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
    target_compile_options(${EXAMPLE_VARNAME}_${example_name}
        PRIVATE
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
    )
    if (H5_HAVE_PARALLEL)
      target_include_directories (${EXAMPLE_VARNAME}_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
    endif ()
    target_link_libraries (${EXAMPLE_VARNAME}_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
    if (H5EXAMPLE_BUILD_TESTING)
      add_custom_command (
          TARGET     ${EXAMPLE_VARNAME}_${example_name}
          POST_BUILD
          COMMAND    ${CMAKE_COMMAND}
          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
      )
    endif ()
  endforeach ()
endif ()

#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
#  foreach (example_name ${1_10_examples})
#    add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
#    target_compile_options(${EXAMPLE_VARNAME}_${example_name}
#        PRIVATE
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
#    )
#    if (H5_HAVE_PARALLEL)
#      target_include_directories (${EXAMPLE_VARNAME}_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
#    endif ()
#    target_link_libraries (${EXAMPLE_VARNAME}_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
#  foreach (example_name ${1_12_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#          add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
#  foreach (example_name ${1_14_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
#  foreach (example_name ${2_0_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()

if (HDF5_PROVIDES_TOOLS)
  foreach (example_name ${common_examples})
    if (${example_name} STREQUAL "h5ex_g_create")
      add_custom_command (
          TARGET     ${EXAMPLE_VARNAME}_${example_name}
          POST_BUILD
          COMMAND    ${CMAKE_COMMAND}
          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/16/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      )
    endif ()
  endforeach ()

  if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8" AND NOT ${${EXAMPLE_VARNAME}_USE_16_API})
    add_custom_command (
        TARGET     ${EXAMPLE_VARNAME}_h5ex_g_compact
        POST_BUILD
        COMMAND    ${CMAKE_COMMAND}
        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/h5ex_g_compact1.ddl ${PROJECT_BINARY_DIR}/h5ex_g_compact1.ddl
    )
    add_custom_command (
        TARGET     ${EXAMPLE_VARNAME}_h5ex_g_compact
        POST_BUILD
        COMMAND    ${CMAKE_COMMAND}
        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/h5ex_g_compact2.ddl ${PROJECT_BINARY_DIR}/h5ex_g_compact2.ddl
    )
  endif ()

#  foreach (example_name ${1_8_examples})
    #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8")
    #  add_custom_command (
    #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
    #      POST_BUILD
    #      COMMAND    ${CMAKE_COMMAND}
    #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
    #  )
    #endif ()
    #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
    #  add_custom_command (
    #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
    #      POST_BUILD
    #      COMMAND    ${CMAKE_COMMAND}
    #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
    #  )
    #endif ()
    #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
    #  add_custom_command (
    #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
    #      POST_BUILD
    #      COMMAND    ${CMAKE_COMMAND}
    #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
    #  )
    #endif ()
    #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
    #  add_custom_command (
    #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
    #      POST_BUILD
    #      COMMAND    ${CMAKE_COMMAND}
    #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
    #  )
    #endif ()
    #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
    #  add_custom_command (
    #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
    #      POST_BUILD
    #      COMMAND    ${CMAKE_COMMAND}
    #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
    #  )
    #endif ()
#  endforeach ()

#  foreach (example_name ${1_10_examples})
#  endforeach ()
#  foreach (example_name ${1_12_examples})
#  endforeach ()
#  foreach (example_name ${1_14_examples})
#  endforeach ()
#  foreach (example_name ${2_0_examples})
#  endforeach ()
endif ()

if (H5EXAMPLE_BUILD_TESTING)
  set (exfiles
      h5ex_g_iterate
      h5ex_g_traverse
  )
  if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8" AND NOT ${EXAMPLE_VARNAME}_USE_16_API)
    set (exfiles ${exfiles}
        h5ex_g_visit
    )
  endif ()
  foreach (example ${exfiles})
    add_custom_command (
        TARGET     ${EXAMPLE_VARNAME}_${example}
        POST_BUILD
        COMMAND    ${CMAKE_COMMAND}
        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/${example}.h5 ${PROJECT_BINARY_DIR}/${example}.h5
    )
  endforeach ()

  macro (ADD_DUMP_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
    )
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_SKIP_COMPARE=TRUE"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${ARGN};${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-${testname} PROPERTIES
            DEPENDS ${EXAMPLE_VARNAME}_${testname}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
    endif ()
    endif ()
  endmacro ()

  macro (ADD_H5_DUMP_TEST testname)
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${ARGN};${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-${testname} PROPERTIES
            DEPENDS ${EXAMPLE_VARNAME}_${testname}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
      endif ()
    endif ()
  endmacro ()

  macro (ADD_H5_DUMP2_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove
            ${testname}1.h5
            ${testname}2.h5
    )
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-${testname}1
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${testname}1.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}1.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}1.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-${testname}1 PROPERTIES
            DEPENDS ${EXAMPLE_VARNAME}_${testname}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-${testname}2
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${testname}2.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}2.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}2.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-${testname}2 PROPERTIES
            DEPENDS ${EXAMPLE_VARNAME}_H5DUMP-${testname}1
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
      endif ()
    endif ()
  endmacro ()

  macro (ADD_H5_CMP_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.out.tmp
    )
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>"
              -D "TEST_ARGS:STRING=${ARGN}"
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
    endif ()
  endmacro ()

  ADD_DUMP_TEST (h5ex_g_create)
  ADD_H5_CMP_TEST (h5ex_g_iterate)
  ADD_H5_CMP_TEST (h5ex_g_traverse)
  if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8" AND NOT ${EXAMPLE_VARNAME}_USE_16_API)
    ADD_H5_DUMP2_TEST (h5ex_g_compact)
    ADD_H5_CMP_TEST (h5ex_g_corder)
    ADD_H5_CMP_TEST (h5ex_g_phase)
    ADD_H5_CMP_TEST (h5ex_g_intermediate)
    ADD_H5_CMP_TEST (h5ex_g_visit)
  endif ()

endif ()
```

### `HDF5Examples/C/H5G/C_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (examples)

set (common_examples
    h5ex_g_create
    h5ex_g_iterate
    h5ex_g_traverse
  )

if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8" AND NOT ${EXAMPLE_VARNAME}_USE_16_API)
  set (1_8_examples
      h5ex_g_compact
      h5ex_g_corder
      h5ex_g_phase
      h5ex_g_intermediate
      h5ex_g_visit
  )
else ()
  set (1_8_examples)
endif ()
```

### `HDF5Examples/C/H5G/h5ex_g_compact.c`

```c
/************************************************************

  This example shows how to create "compact-or-indexed"
  format groups, new to 1.8.  This example also illustrates
  the space savings of compact groups by creating 2 files
  which are identical except for the group format, and
  displaying the file size of each.  Both files have one
  empty group in the root group.

  This file is intended for use with HDF5 Library version 1.8

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>

#define FILENAME1 "h5ex_g_compact1.h5"
#define FILENAME2 "h5ex_g_compact2.h5"
#define GROUP     "G1"

int
main(void)
{
    hid_t      file  = H5I_INVALID_HID;
    hid_t      group = H5I_INVALID_HID;
    hid_t      fapl  = H5I_INVALID_HID;
    herr_t     status;
    H5G_info_t ginfo;
    hsize_t    size;

    /*
     * Set file access property list to use the earliest file format.
     * This will force the library to create original format groups.
     */
    fapl   = H5Pcreate(H5P_FILE_ACCESS);
    status = H5Pset_libver_bounds(fapl, H5F_LIBVER_EARLIEST, H5F_LIBVER_LATEST);

    /*
     * Create file 1.  This file will use original format groups.
     */
    file  = H5Fcreate(FILENAME1, H5F_ACC_TRUNC, H5P_DEFAULT, fapl);
    group = H5Gcreate(file, GROUP, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Obtain the group info and print the group storage type.
     */
    status = H5Gget_info(group, &ginfo);
    printf("Group storage type for %s is: ", FILENAME1);
    switch (ginfo.storage_type) {
        case H5G_STORAGE_TYPE_COMPACT:
            printf("H5G_STORAGE_TYPE_COMPACT\n"); /* New compact format */
            break;
        case H5G_STORAGE_TYPE_DENSE:
            printf("H5G_STORAGE_TYPE_DENSE\n"); /* New dense (indexed) format */
            break;
        case H5G_STORAGE_TYPE_SYMBOL_TABLE:
            printf("H5G_STORAGE_TYPE_SYMBOL_TABLE\n"); /* Original format */
            break;
        case H5G_STORAGE_TYPE_UNKNOWN:
            printf("H5G_STORAGE_TYPE_UNKNOWN\n"); /* Unknown format */
    }

    /*
     * Close and re-open file.  Needed to get the correct file size.
     */
    status = H5Gclose(group);
    status = H5Fclose(file);
    file   = H5Fopen(FILENAME1, H5F_ACC_RDONLY, H5P_DEFAULT);

    /*
     * Obtain and print the file size.
     */
    status = H5Fget_filesize(file, &size);
    printf("File size for %s is: %d bytes\n\n", FILENAME1, (int)size);

    /*
     * Close FILE1.
     */
    status = H5Fclose(file);

    /*
     * Now use the default file access property list to allow the latest file
     * format. This will allow the library to create new compact format groups.
     * Since HDF5 2.0, the default is to use the 1.8 file format as the low
     * bound, which includes compact groups.
     */

    /*
     * Create file 2 using the default access property list.
     */
    file  = H5Fcreate(FILENAME2, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    group = H5Gcreate(file, GROUP, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Obtain the group info and print the group storage type.
     */
    status = H5Gget_info(group, &ginfo);
    printf("Group storage type for %s is: ", FILENAME2);
    switch (ginfo.storage_type) {
        case H5G_STORAGE_TYPE_COMPACT:
            printf("H5G_STORAGE_TYPE_COMPACT\n"); /* New compact format */
            break;
        case H5G_STORAGE_TYPE_DENSE:
            printf("H5G_STORAGE_TYPE_DENSE\n"); /* New dense (indexed) format */
            break;
        case H5G_STORAGE_TYPE_SYMBOL_TABLE:
            printf("H5G_STORAGE_TYPE_SYMBOL_TABLE\n"); /* Original format */
            break;
        case H5G_STORAGE_TYPE_UNKNOWN:
            printf("H5G_STORAGE_TYPE_UNKNOWN\n"); /* Unknown format */
    }

    /*
     * Close and re-open file.  Needed to get the correct file size.
     */
    status = H5Gclose(group);
    status = H5Fclose(file);
    file   = H5Fopen(FILENAME2, H5F_ACC_RDONLY, fapl);

    /*
     * Obtain and print the file size.
     */
    status = H5Fget_filesize(file, &size);
    printf("File size for %s is: %d bytes\n", FILENAME2, (int)size);
    printf("\n");

    /*
     * Close and release resources.
     */
    status = H5Pclose(fapl);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5G/h5ex_g_corder.c`

```c
/************************************************************

  This example shows how to track links in a group by
  creation order.  The program creates a series of groups,
  then reads back their names: first in alphabetical order,
  then in creation order.

  This file is intended for use with HDF5 Library version 1.8

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_g_corder.h5"

int
main(void)
{
    hid_t      file     = H5I_INVALID_HID;
    hid_t      group    = H5I_INVALID_HID;
    hid_t      subgroup = H5I_INVALID_HID;
    hid_t      gcpl     = H5I_INVALID_HID;
    herr_t     status;
    H5G_info_t ginfo;
    ssize_t    size;        /* Size of name */
    hsize_t    i;           /* Index */
    char      *name = NULL; /* Output buffer */

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create group creation property list and enable link creation
     * order tracking.  Attempting to track by creation order in a
     * group that does not have this property set will result in an
     * error.
     */
    gcpl   = H5Pcreate(H5P_GROUP_CREATE);
    status = H5Pset_link_creation_order(gcpl, H5P_CRT_ORDER_TRACKED | H5P_CRT_ORDER_INDEXED);

    /*
     * Create primary group using the property list.
     */
    group = H5Gcreate(file, "index_group", H5P_DEFAULT, gcpl, H5P_DEFAULT);

    /*
     * Create subgroups in the primary group.  These will be tracked
     * by creation order.  Note that these groups do not have to have
     * the creation order tracking property set.
     */
    subgroup = H5Gcreate(group, "H", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status   = H5Gclose(subgroup);
    subgroup = H5Gcreate(group, "D", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status   = H5Gclose(subgroup);
    subgroup = H5Gcreate(group, "F", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status   = H5Gclose(subgroup);
    subgroup = H5Gcreate(group, "5", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status   = H5Gclose(subgroup);

    /*
     * Get group info.
     */
    status = H5Gget_info(group, &ginfo);

    /*
     * Traverse links in the primary group using alphabetical indices
     * (H5_INDEX_NAME).
     */
    printf("Traversing group using alphabetical indices:\n\n");
    for (i = 0; i < ginfo.nlinks; i++) {

        /*
         * Get size of name, add 1 for null terminator.
         */
        size = 1 + H5Lget_name_by_idx(group, ".", H5_INDEX_NAME, H5_ITER_INC, i, NULL, 0, H5P_DEFAULT);

        /*
         * Allocate storage for name.
         */
        name = (char *)malloc(size);

        /*
         * Retrieve name, print it, and free the previously allocated
         * space.
         */
        size = H5Lget_name_by_idx(group, ".", H5_INDEX_NAME, H5_ITER_INC, i, name, (size_t)size, H5P_DEFAULT);
        printf("Index %d: %s\n", (int)i, name);
        free(name);
    }

    /*
     * Traverse links in the primary group by creation order
     * (H5_INDEX_CRT_ORDER).
     */
    printf("\nTraversing group using creation order indices:\n\n");
    for (i = 0; i < ginfo.nlinks; i++) {

        /*
         * Get size of name, add 1 for null terminator.
         */
        size = 1 + H5Lget_name_by_idx(group, ".", H5_INDEX_CRT_ORDER, H5_ITER_INC, i, NULL, 0, H5P_DEFAULT);

        /*
         * Allocate storage for name.
         */
        name = (char *)malloc(size);

        /*
         * Retrieve name, print it, and free the previously allocated
         * space.
         */
        size = H5Lget_name_by_idx(group, ".", H5_INDEX_CRT_ORDER, H5_ITER_INC, i, name, (size_t)size,
                                  H5P_DEFAULT);
        printf("Index %d: %s\n", (int)i, name);
        free(name);
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(gcpl);
    status = H5Gclose(group);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5G/h5ex_g_create.c`

```c
/************************************************************

  This example shows how to create, open, and close a group.

  This file is intended for use with HDF5 Library version 1.8

 ************************************************************/

#include "hdf5.h"

#define FILENAME "h5ex_g_create.h5"

int
main(void)
{
    hid_t  file  = H5I_INVALID_HID;
    hid_t  group = H5I_INVALID_HID;
    herr_t status;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create a group named "G1" in the file.
     */
    group = H5Gcreate(file, "/G1", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Close the group.  The handle "group" can no longer be used.
     */
    status = H5Gclose(group);

    /*
     * Re-open the group, obtaining a new handle.
     */
    group = H5Gopen(file, "/G1", H5P_DEFAULT);

    /*
     * Close and release resources.
     */
    status = H5Gclose(group);
    status = H5Fclose(file);
}
```

### `HDF5Examples/C/H5G/h5ex_g_intermediate.c`

```c
/************************************************************

  This example shows how to create intermediate groups with
  a single call to H5Gcreate.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"

#define FILENAME "h5ex_g_intermediate.h5"

/*
 * Operator function to be called by H5Ovisit.
 */
herr_t op_func(hid_t loc_id, const char *name, const H5O_info_t *info, void *operator_data);

int
main(void)
{
    hid_t  file  = H5I_INVALID_HID;
    hid_t  group = H5I_INVALID_HID;
    hid_t  gcpl  = H5I_INVALID_HID;
    herr_t status;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create group creation property list and set it to allow creation
     * of intermediate groups.
     */
    gcpl   = H5Pcreate(H5P_LINK_CREATE);
    status = H5Pset_create_intermediate_group(gcpl, 1);

    /*
     * Create the group /G1/G2/G3.  Note that /G1 and /G1/G2 do not
     * exist yet.  This call would cause an error if we did not use the
     * previously created property list.
     */
    group = H5Gcreate(file, "/G1/G2/G3", gcpl, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Print all the objects in the files to show that intermediate
     * groups have been created.  See h5ex_g_visit for more information
     * on how to use H5Ovisit.
     */
    printf("Objects in the file:\n");
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Ovisit(file, H5_INDEX_NAME, H5_ITER_NATIVE, op_func, NULL, H5O_INFO_ALL);
#else
    status = H5Ovisit(file, H5_INDEX_NAME, H5_ITER_NATIVE, op_func, NULL);
#endif

    /*
     * Close and release resources.
     */
    status = H5Pclose(gcpl);
    status = H5Gclose(group);
    status = H5Fclose(file);

    return 0;
}

/************************************************************

  Operator function for H5Ovisit.  This function prints the
  name and type of the object passed to it.

 ************************************************************/
herr_t
op_func(hid_t loc_id, const char *name, const H5O_info_t *info, void *operator_data)
{
    printf("/"); /* Print root group in object path */

    /*
     * Check if the current object is the root group, and if not print
     * the full path name and type.
     */
    if (name[0] == '.') /* Root group, do not print '.' */
        printf("  (Group)\n");
    else
        switch (info->type) {
            case H5O_TYPE_GROUP:
                printf("%s  (Group)\n", name);
                break;
            case H5O_TYPE_DATASET:
                printf("%s  (Dataset)\n", name);
                break;
            case H5O_TYPE_NAMED_DATATYPE:
                printf("%s  (Datatype)\n", name);
                break;
            default:
                printf("%s  (Unknown)\n", name);
        }

    return 0;
}
```

### `HDF5Examples/C/H5G/h5ex_g_iterate.c`

```c
/************************************************************

  This example shows how to iterate over group members using
  H5Literate.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>

#define FILENAME "h5ex_g_iterate.h5"

/*
 * Operator function to be called by H5Literate.
 */
herr_t op_func(hid_t loc_id, const char *name, const H5L_info_t *info, void *operator_data);

int
main(void)
{
    hid_t  file; /* Handle */
    herr_t status;

    /*
     * Open file.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);

    /*
     * Begin iteration.
     */
    printf("Objects in root group:\n");
    status = H5Literate(file, H5_INDEX_NAME, H5_ITER_NATIVE, NULL, op_func, NULL);

    /*
     * Close and release resources.
     */
    status = H5Fclose(file);

    return 0;
}

/************************************************************

  Operator function.  Prints the name and type of the object
  being examined.

 ************************************************************/
herr_t
op_func(hid_t loc_id, const char *name, const H5L_info_t *info, void *operator_data)
{
    herr_t     status;
    H5O_info_t infobuf;

    /*
     * Get type of the object and display its name and type.
     * The name of the object is passed to this function by
     * the Library.
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Oget_info_by_name(loc_id, name, &infobuf, H5O_INFO_ALL, H5P_DEFAULT);
#else
    status = H5Oget_info_by_name(loc_id, name, &infobuf, H5P_DEFAULT);
#endif
    switch (infobuf.type) {
        case H5O_TYPE_GROUP:
            printf("  Group: %s\n", name);
            break;
        case H5O_TYPE_DATASET:
            printf("  Dataset: %s\n", name);
            break;
        case H5O_TYPE_NAMED_DATATYPE:
            printf("  Datatype: %s\n", name);
            break;
        default:
            printf("  Unknown: %s\n", name);
    }

    return 0;
}
```

### `HDF5Examples/C/H5G/h5ex_g_phase.c`

```c
/************************************************************

  This example shows how to set the conditions for
  conversion between compact and dense (indexed) groups.

  This file is intended for use with HDF5 Library version 1.8

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>

#define FILENAME    "h5ex_g_phase.h5"
#define MAX_GROUPS  7
#define MAX_COMPACT 5
#define MIN_DENSE   3

int
main(void)
{
    hid_t      file, group, subgroup, fapl, gcpl; /* Handles */
    herr_t     status;
    H5G_info_t ginfo;
    char       name[3] = "G0"; /* Name of subgroup */
    unsigned   i;

    /*
     * Set file access property list to allow the latest file format.
     * This will allow the library to create new format groups.
     */
    fapl   = H5Pcreate(H5P_FILE_ACCESS);
    status = H5Pset_libver_bounds(fapl, H5F_LIBVER_LATEST, H5F_LIBVER_LATEST);

    /*
     * Create group access property list and set the phase change
     * conditions.  In this example we lowered the conversion threshold
     * to simplify the output, though this may not be optimal.
     */
    gcpl   = H5Pcreate(H5P_GROUP_CREATE);
    status = H5Pset_link_phase_change(gcpl, MAX_COMPACT, MIN_DENSE);

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, fapl);

    /*
     * Create primary group.
     */
    group = H5Gcreate(file, name, H5P_DEFAULT, gcpl, H5P_DEFAULT);

    /*
     * Add subgroups to "group" one at a time, print the storage type
     * for "group" after each subgroup is created.
     */
    for (i = 1; i <= MAX_GROUPS; i++) {

        /*
         * Define the subgroup name and create the subgroup.
         */
        name[1]  = ((char)i) + '0'; /* G1, G2, G3 etc. */
        subgroup = H5Gcreate(group, name, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
        status   = H5Gclose(subgroup);

        /*
         * Obtain the group info and print the group storage type
         */
        status = H5Gget_info(group, &ginfo);
        printf("%d Group%s: Storage type is ", (int)ginfo.nlinks, ginfo.nlinks == 1 ? " " : "s");
        switch (ginfo.storage_type) {
            case H5G_STORAGE_TYPE_COMPACT:
                printf("H5G_STORAGE_TYPE_COMPACT\n"); /* New compact format */
                break;
            case H5G_STORAGE_TYPE_DENSE:
                printf("H5G_STORAGE_TYPE_DENSE\n"); /* New dense (indexed) format */
                break;
            case H5G_STORAGE_TYPE_SYMBOL_TABLE:
                printf("H5G_STORAGE_TYPE_SYMBOL_TABLE\n"); /* Original format */
                break;
            case H5G_STORAGE_TYPE_UNKNOWN:
                printf("H5G_STORAGE_TYPE_UNKNOWN\n"); /* Unknown format */
        }
    }

    printf("\n");

    /*
     * Delete subgroups one at a time, print the storage type for
     * "group" after each subgroup is deleted.
     */
    for (i = MAX_GROUPS; i >= 1; i--) {

        /*
         * Define the subgroup name and delete the subgroup.
         */
        name[1] = ((char)i) + '0'; /* G1, G2, G3 etc. */
        status  = H5Ldelete(group, name, H5P_DEFAULT);

        /*
         * Obtain the group info and print the group storage type
         */
        status = H5Gget_info(group, &ginfo);
        printf("%d Group%s: Storage type is ", (int)ginfo.nlinks, ginfo.nlinks == 1 ? " " : "s");
        switch (ginfo.storage_type) {
            case H5G_STORAGE_TYPE_COMPACT:
                printf("H5G_STORAGE_TYPE_COMPACT\n"); /* New compact format */
                break;
            case H5G_STORAGE_TYPE_DENSE:
                printf("H5G_STORAGE_TYPE_DENSE\n"); /* New dense (indexed) format */
                break;
            case H5G_STORAGE_TYPE_SYMBOL_TABLE:
                printf("H5G_STORAGE_TYPE_SYMBOL_TABLE\n"); /* Original format */
                break;
            case H5G_STORAGE_TYPE_UNKNOWN:
                printf("H5G_STORAGE_TYPE_UNKNOWN\n"); /* Unknown format */
        }
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(fapl);
    status = H5Pclose(gcpl);
    status = H5Gclose(group);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5G/h5ex_g_traverse.c`

```c
/************************************************************

  This example shows a way to recursively traverse the file
  using H5Literate.  The method shown here guarantees that
  the recursion will not enter an infinite loop, but does
  not prevent objects from being visited more than once.
  The program prints the directory structure of the file
  specified in FILE.  The default file used by this example
  implements the structure described in the User's Guide,
  chapter 4, figure 26.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.
 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <string.h>

#define FILENAME "h5ex_g_traverse.h5"

/*
 * Define operator data structure type for H5Literate callback.
 * During recursive iteration, these structures will form a
 * linked list that can be searched for duplicate groups,
 * preventing infinite recursion.
 */
struct opdata {
    unsigned       recurs; /* Recursion level.  0=root */
    struct opdata *prev;   /* Pointer to previous opdata */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    H5O_token_t token; /* Group token */
#else
    haddr_t addr; /* Group address */
#endif
};

/*
 * Operator function to be called by H5Literate.
 */
herr_t op_func(hid_t loc_id, const char *name, const H5L_info_t *info, void *operator_data);

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
/* Function to check for duplicate groups in a path. */
int group_check(hid_t loc_id, struct opdata *od, H5O_token_t target_token);

#else
/* Function to check for duplicate groups in a path. */
int group_check(struct opdata *od, haddr_t target_addr);

#endif

int
main(void)
{
    hid_t         file; /* Handle */
    herr_t        status;
    H5O_info_t    infobuf;
    struct opdata od;

    /*
     * Open file and initialize the operator data structure.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Oget_info(file, &infobuf, H5O_INFO_ALL);
    memcpy(&od.token, &infobuf.token, sizeof(H5O_token_t));
#else
    status  = H5Oget_info(file, &infobuf);
    od.addr = infobuf.addr;
#endif
    od.recurs = 0;
    od.prev   = NULL;

    /*
     * Print the root group and formatting, begin iteration.
     */
    printf("/ {\n");
    status = H5Literate(file, H5_INDEX_NAME, H5_ITER_NATIVE, NULL, op_func, (void *)&od);
    printf("}\n");

    /*
     * Close and release resources.
     */
    status = H5Fclose(file);

    return 0;
}

/************************************************************

  Operator function.  This function prints the name and type
  of the object passed to it.  If the object is a group, it
  is first checked against other groups in its path using
  the group_check function, then if it is not a duplicate,
  H5Literate is called for that group.  This guarantees that
  the program will not enter infinite recursion due to a
  circular path in the file.

 ************************************************************/
herr_t
op_func(hid_t loc_id, const char *name, const H5L_info_t *info, void *operator_data)
{
    herr_t         status;
    herr_t         return_val = 0;
    struct opdata *od         = (struct opdata *)operator_data; /* Type conversion */
    unsigned       spaces     = 2 * (od->recurs + 1); /* Number of whitespaces to prepend to output */

    /*
     * Get type of the object and display its name and type.
     * The name of the object is passed to this function by
     * the Library.
     */
    H5O_info_t infobuf;
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Oget_info_by_name(loc_id, name, &infobuf, H5O_INFO_ALL, H5P_DEFAULT);
#else
    status = H5Oget_info_by_name(loc_id, name, &infobuf, H5P_DEFAULT);
#endif
    printf("%*s", spaces, ""); /* Format output */
    switch (infobuf.type) {
        case H5O_TYPE_GROUP:
            printf("Group: %s {\n", name);

            /*
             * Check group address/token against linked list of operator
             * data structures.  We will always run the check, as the
             * reference count cannot be relied upon if there are
             * symbolic links, and H5Oget_info_by_name always follows
             * symbolic links.  Alternatively we could use H5Lget_info
             * and never recurse on groups discovered by symbolic
             * links, however it could still fail if an object's
             * reference count was manually manipulated with
             * H5Odecr_refcount.
             */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
            if (group_check(loc_id, od, infobuf.token)) {
#else
            if (group_check(od, infobuf.addr)) {
#endif
                printf("%*s  Warning: Loop detected!\n", spaces, "");
            }
            else {
                if (od->recurs + 1 > 7)
                    return -1;

                /*
                 * Initialize new operator data structure and
                 * begin recursive iteration on the discovered
                 * group.  The new opdata structure is given a
                 * pointer to the current one.
                 */
                struct opdata nextod;
                nextod.recurs = od->recurs + 1;
                nextod.prev   = od;
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
                memcpy(&nextod.token, &infobuf.token, sizeof(H5O_token_t));
#else
                nextod.addr = infobuf.addr;
#endif
                return_val = H5Literate_by_name(loc_id, name, H5_INDEX_NAME, H5_ITER_NATIVE, NULL, op_func,
                                                (void *)&nextod, H5P_DEFAULT);
            }
            printf("%*s}\n", spaces, "");
            break;
        case H5O_TYPE_DATASET:
            printf("Dataset: %s\n", name);
            break;
        case H5O_TYPE_NAMED_DATATYPE:
            printf("Datatype: %s\n", name);
            break;
        default:
            printf("Unknown: %s\n", name);
    }

    return return_val;
}

/************************************************************

  This function recursively searches the linked list of
  opdata structures for one whose address/token matches
  target_addr/token.  Returns 1 if a match is found, and 0
  otherwise.

 ************************************************************/
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
int
group_check(hid_t loc_id, struct opdata *od, H5O_token_t target_token)
{
    int token_cmp;

    if (H5Otoken_cmp(loc_id, &od->token, &target_token, &token_cmp) < 0) {
        return 0;
    }
    if (!token_cmp) {
        return 1; /* Tokens match */
    }
    else if (!od->recurs)
        return 0; /* Root group reached with no matches */
    else {        /* Recursively examine the next node */
        return group_check(loc_id, od->prev, target_token);
    }
}
#else
int
group_check(struct opdata *od, haddr_t target_addr)
{
    if (od->addr == target_addr)
        return 1; /* Addresses match */
    else if (!od->recurs)
        return 0; /* Root group reached with no matches */
    else {        /* Recursively examine the next node */
        return group_check(od->prev, target_addr);
    }
}
#endif
```

### `HDF5Examples/C/H5G/h5ex_g_visit.c`

```c
/************************************************************

  This example shows how to recursively traverse a file
  using H5Ovisit and H5Lvisit.  The program prints all of
  the objects in the file specified in FILE, then prints all
  of the links in that file.  The default file used by this
  example implements the structure described in the User's
  Guide, chapter 4, figure 26.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>

#define FILENAME "h5ex_g_visit.h5"

/*
 * Operator function to be called by H5Ovisit.
 */
herr_t op_func(hid_t loc_id, const char *name, const H5O_info_t *info, void *operator_data);

/*
 * Operator function to be called by H5Lvisit.
 */
herr_t op_func_L(hid_t loc_id, const char *name, const H5L_info_t *info, void *operator_data);

int
main(void)
{
    hid_t  file = H5I_INVALID_HID;
    herr_t status;

    /*
     * Open file
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);

    /*
     * Begin iteration using H5Ovisit
     */
    printf("Objects in the file:\n");
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Ovisit(file, H5_INDEX_NAME, H5_ITER_NATIVE, op_func, NULL, H5O_INFO_ALL);
#else
    status = H5Ovisit(file, H5_INDEX_NAME, H5_ITER_NATIVE, op_func, NULL);
#endif

    /*
     * Repeat the same process using H5Lvisit
     */
    printf("\nLinks in the file:\n");
    status = H5Lvisit(file, H5_INDEX_NAME, H5_ITER_NATIVE, op_func_L, NULL);

    /*
     * Close and release resources.
     */
    status = H5Fclose(file);

    return 0;
}

/************************************************************

  Operator function for H5Ovisit.  This function prints the
  name and type of the object passed to it.

 ************************************************************/
herr_t
op_func(hid_t loc_id, const char *name, const H5O_info_t *info, void *operator_data)
{
    printf("/"); /* Print root group in object path */

    /*
     * Check if the current object is the root group, and if not print
     * the full path name and type.
     */
    if (name[0] == '.') /* Root group, do not print '.' */
        printf("  (Group)\n");
    else
        switch (info->type) {
            case H5O_TYPE_GROUP:
                printf("%s  (Group)\n", name);
                break;
            case H5O_TYPE_DATASET:
                printf("%s  (Dataset)\n", name);
                break;
            case H5O_TYPE_NAMED_DATATYPE:
                printf("%s  (Datatype)\n", name);
                break;
            default:
                printf("%s  (Unknown)\n", name);
        }

    return 0;
}

/************************************************************

  Operator function for H5Lvisit.  This function simply
  retrieves the info for the object the current link points
  to, and calls the operator function for H5Ovisit.

 ************************************************************/
herr_t
op_func_L(hid_t loc_id, const char *name, const H5L_info_t *info, void *operator_data)
{
    herr_t     status;
    H5O_info_t infobuf;

    /*
     * Get type of the object and display its name and type.
     * The name of the object is passed to this function by
     * the Library.
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Oget_info_by_name(loc_id, name, &infobuf, H5O_INFO_ALL, H5P_DEFAULT);
#else
    status = H5Oget_info_by_name(loc_id, name, &infobuf, H5P_DEFAULT);
#endif
    return op_func(loc_id, name, &infobuf, operator_data);
}
```

### `HDF5Examples/C/H5G/test-pc.sh`

```bash
#! /bin/sh
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.

# This file is for use of h5cc created with the CMake process.
# Environment variable, HDF5_HOME is expected to be set.
# $1 is the path name of the source directory.
# $2 is the path name of the build directory.
# $3 is the current path name.

top_srcdir=$1
top_builddir=$2
currentpath=$3
verbose=yes
nerrors=0

echo "Current build directory: $top_builddir/$currentpath"

# HDF5 compile commands, assuming they are in your $PATH.
H5CC=$HDF5_HOME/bin/h5cc
LD_LIBRARY_PATH=$HDF5_HOME/lib
export LD_LIBRARY_PATH

if ! test -f $H5CC; then
    echo "Set paths for H5CC and LD_LIBRARY_PATH in test.sh"
    echo "Set environment variable HDF5_HOME to the hdf5 install dir"
    echo "h5cc was not found at $H5CC"
    exit $EXIT_FAILURE
fi

H5DUMP=`echo $H5CC | sed -e 's/\/[^/]*$/\/h5dump/'`;
H5_LIBVER=$($H5CC -showconfig | grep -i "HDF5 Version:" | sed 's/^.* //g' | sed 's/[-].*//g')
H5_APIVER=$($H5CC -showconfig | grep -i "Default API mapping:" | sed 's/^.* //g' | sed 's/v//g' | sed 's/1/1_/')

H5_MAJORVER=$(echo $H5_LIBVER | cut -f1 -d'.'  | sed -E 's/\./_/g')
H5_MINORVER=$(echo $H5_LIBVER | cut -f2 -d'.'  | sed -E 's/\./_/g')
H5_RELEASEVER=$(echo $H5_LIBVER | cut -f3 -d'.'  | sed -E 's/\./_/g')
H5_LIBVER_DIR=$H5_MAJORVER$H5_MINORVER

# Shell commands used in Makefiles
RM="rm -rf"
DIFF="diff -c"
CMP="cmp -s"
GREP='grep'
CP="cp -p"  # Use -p to preserve mode,ownership,timestamps
DIRNAME='dirname'
LS='ls'
AWK='awk'

# setup plugin path
ENVCMD="env HDF5_PLUGIN_PATH=$LD_LIBRARY_PATH/plugin"

TESTDIR=$top_builddir/$currentpath


case `echo "testing\c"; echo 1,2,3`,`echo -n testing; echo 1,2,3` in
  *c*,-n*) ECHO_N= ECHO_C='
' ;;
  *c*,*  ) ECHO_N=-n ECHO_C= ;;
  *)       ECHO_N= ECHO_C='\c' ;;
esac
ECHO_N="echo $ECHO_N"


exout() {
    cd $TESTDIR
    "$@"
}

dumpout() {
    cd $TESTDIR
    $H5DUMP "$@"
}

compileout() {
    cd $TESTDIR
    $H5CC "$@"
}

return_val=0

compileout $top_srcdir/$currentpath/h5ex_g_create.c -o h5ex_g_create

$ECHO_N "Testing C/H5G/h5ex_g_create...$ECHO_C"
exout ./h5ex_g_create
dumpout h5ex_g_create.h5 >tmp.test
rm -f $TESTDIR/h5ex_g_create.h5
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/16/h5ex_g_create.ddl
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
    echo "  Passed"
fi
return_val=`expr $status + $return_val`

compileout $top_srcdir/$currentpath/h5ex_g_iterate.c -o h5ex_g_iterate

$ECHO_N "Testing C/H5G/h5ex_g_iterate...$ECHO_C"
if test -f $TESTDIR/h5ex_g_iterate.h5
then
    exout ./h5ex_g_iterate >tmp.test
else
    cp $top_srcdir/$currentpath/h5ex_g_iterate.h5 $TESTDIR/h5ex_g_iterate.h5
    exout ./h5ex_g_iterate >tmp.test
    rm  -f $TESTDIR/h5ex_g_iterate.h5
fi
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/16/h5ex_g_iterate.tst
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
    echo "  Passed"
fi
return_val=`expr $status + $return_val`

compileout $top_srcdir/$currentpath/h5ex_g_traverse.c -o h5ex_g_traverse

$ECHO_N "Testing C/H5G/h5ex_g_traverse...$ECHO_C"
if test -f $TESTDIR/h5ex_g_traverse.h5
then
    exout ./h5ex_g_traverse >tmp.test
else
    cp $top_srcdir/$currentpath/h5ex_g_traverse.h5 $TESTDIR/h5ex_g_traverse.h5
    exout ./h5ex_g_traverse >tmp.test
    rm  -f $TESTDIR/h5ex_g_traverse.h5
fi
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/16/h5ex_g_traverse.tst
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
    echo "  Passed"
fi
return_val=`expr $status + $return_val`

compileout $top_srcdir/$currentpath/h5ex_g_visit.c -o h5ex_g_visit

$ECHO_N "Testing C/H5G/h5ex_g_visit...$ECHO_C"
if test -f $TESTDIR/h5ex_g_visit.h5
then
    exout ./h5ex_g_visit >tmp.test
else
    cp $top_srcdir/$currentpath/h5ex_g_visit.h5 $TESTDIR/h5ex_g_visit.h5
    exout ./h5ex_g_visit >tmp.test
    rm  -f $TESTDIR/h5ex_g_visit.h5
fi
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/h5ex_g_visit.tst
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
    echo "  Passed"
fi
return_val=`expr $status + $return_val`

compileout $top_srcdir/$currentpath/h5ex_g_compact.c -o h5ex_g_compact

$ECHO_N "Testing C/H5G/h5ex_g_compact...$ECHO_C"
exout ./h5ex_g_compact >tmp.test
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/h5ex_g_compact.tst
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
  dumpout h5ex_g_compact1.h5 >tmp.test
  cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/h5ex_g_compact1.ddl
  status=$?
  if test $status -ne 0
  then
      echo "  FAILED!"
  else
    dumpout h5ex_g_compact2.h5 >tmp.test
    cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/h5ex_g_compact2.ddl
    status=$?
    if test $status -ne 0
    then
        echo "  FAILED!"
    else
        echo "  Passed"
    fi
  fi
fi
return_val=`expr $status + $return_val`
rm -f $TESTDIR/h5ex_g_compact1.h5
rm -f $TESTDIR/h5ex_g_compact2.h5

compileout $top_srcdir/$currentpath/h5ex_g_phase.c -o h5ex_g_phase

$ECHO_N "Testing C/H5G/h5ex_g_phase...$ECHO_C"
exout ./h5ex_g_phase >tmp.test
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/h5ex_g_phase.tst
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
    echo "  Passed"
fi
return_val=`expr $status + $return_val`
rm -f $TESTDIR/h5ex_g_phase.h5

compileout $top_srcdir/$currentpath/h5ex_g_corder.c -o h5ex_g_corder

$ECHO_N "Testing C/H5G/h5ex_g_corder...$ECHO_C"
exout ./h5ex_g_corder >tmp.test
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/h5ex_g_corder.tst
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
    echo "  Passed"
fi
return_val=`expr $status + $return_val`
rm -f $TESTDIR/h5ex_g_corder.h5

compileout $top_srcdir/$currentpath/h5ex_g_intermediate.c -o h5ex_g_intermediate

$ECHO_N "Testing C/H5G/h5ex_g_intermediate...$ECHO_C"
exout ./h5ex_g_intermediate >tmp.test
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/h5ex_g_intermediate.tst
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
    echo "  Passed"
fi
return_val=`expr $status + $return_val`
rm -f $TESTDIR/h5ex_g_intermediate.h5


rm -f $TESTDIR/tmp.test
echo "$return_val tests failed in C/H5G/"
exit $return_val
```

### `HDF5Examples/C/H5PAR/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (H5PAR_C C)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (C_sourcefiles.cmake)

foreach (example_name ${examples})
  add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
  target_compile_options(${EXAMPLE_VARNAME}_${example_name}
      PRIVATE
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  target_include_directories (${EXAMPLE_VARNAME}_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
  target_link_libraries (${EXAMPLE_VARNAME}_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
endforeach ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_GREP_TEST testname mumprocs)
    add_test (
        NAME ${EXAMPLE_VARNAME}_${testname}-clearall
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${testname}.h5
    )
    if (last_test)
      set_tests_properties (${EXAMPLE_VARNAME}_${testname}-clearall PROPERTIES DEPENDS ${last_test})
    endif ()
    add_test (NAME MPI_TEST_${EXAMPLE_VARNAME}_${testname} COMMAND "${CMAKE_COMMAND}"
        -D "TEST_PROGRAM=${MPIEXEC_EXECUTABLE};${MPIEXEC_NUMPROC_FLAG};${mumprocs};${MPIEXEC_PREFLAGS};$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>;${MPIEXEC_POSTFLAGS}"
        -D "TEST_ARGS:STRING="
        -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
        -D "TEST_EXPECT=0"
        -D "TEST_SKIP_COMPARE=TRUE"
        -D "TEST_OUTPUT=${testname}.out"
        -D "TEST_GREP_COMPARE=TRUE"
        -D "TEST_REFERENCE:STRING=PHDF5 example finished with no errors"
        -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
        -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
    )
    set_tests_properties (MPI_TEST_${EXAMPLE_VARNAME}_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall)
    set (last_test "MPI_TEST_${EXAMPLE_VARNAME}_${testname}")
  endmacro ()

  # Ensure that 24 is a multiple of the number of processes.
  # The number 24 corresponds to SPACE1_DIM1 and SPACE1_DIM2 defined in ph5example.c
  math(EXPR NUMPROCS "24 / ((24 + ${MPIEXEC_MAX_NUMPROCS} - 1) / ${MPIEXEC_MAX_NUMPROCS})")

  foreach (example_name ${examples})
    if (${example_name} STREQUAL "ph5_hyperslab_by_col")
      ADD_GREP_TEST (${example_name} 2)
    elseif (${example_name} STREQUAL "ph5_hyperslab_by_chunk" OR ${example_name} STREQUAL "ph5_hyperslab_by_pattern")
      ADD_GREP_TEST (${example_name} 4)
    else ()
      ADD_GREP_TEST (${example_name} ${NUMPROCS})
    endif ()
  endforeach ()

endif ()
```

### `HDF5Examples/C/H5PAR/C_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (examples
  ph5_filtered_writes
  ph5_filtered_writes_no_sel
  ph5_dataset
  ph5_file_create
  ph5_hyperslab_by_row
  ph5_hyperslab_by_col
  ph5_hyperslab_by_pattern
  ph5_hyperslab_by_chunk
)
if (${HDF5_PROVIDES_SUBFILING_VFD})
    list (APPEND examples ph5_subfiling)
endif ()
```

### `HDF5Examples/C/H5PAR/ph5_dataset.c`

```c
/*
 *  This example writes data to the HDF5 file.
 *  Number of processes is assumed to be 1 or multiples of 2 (up to 8)
 */

#include "hdf5.h"
#include "stdlib.h"

#define H5FILE_NAME "SDS.h5"
#define DATASETNAME "IntArray"
#define NX          8 /* dataset dimensions */
#define NY          5
#define RANK        2

int
main(int argc, char **argv)
{
    /*
     * HDF5 APIs definitions
     */
    hid_t   file_id, dset_id;   /* file and dataset identifiers */
    hid_t   filespace;          /* file and memory dataspace identifiers */
    hsize_t dimsf[] = {NX, NY}; /* dataset dimensions */
    int    *data;               /* pointer to data buffer to write */
    hid_t   plist_id;           /* property list identifier */
    int     i;
    herr_t  status;

    /*
     * MPI variables
     */
    int      mpi_size, mpi_rank;
    MPI_Comm comm = MPI_COMM_WORLD;
    MPI_Info info = MPI_INFO_NULL;

    /*
     * Initialize MPI
     */
    MPI_Init(&argc, &argv);
    MPI_Comm_size(comm, &mpi_size);
    MPI_Comm_rank(comm, &mpi_rank);

    /*
     * Initialize data buffer
     */
    data = (int *)malloc(sizeof(int) * dimsf[0] * dimsf[1]);
    for (i = 0; i < dimsf[0] * dimsf[1]; i++) {
        data[i] = i;
    }
    /*
     * Set up file access property list with parallel I/O access
     */
    plist_id = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_fapl_mpio(plist_id, comm, info);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata reads on FAPL to perform metadata reads
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_all_coll_metadata_ops(plist_id, true);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata writes on FAPL to perform metadata writes
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_coll_metadata_write(plist_id, true);

    /*
     * Create a new file collectively and release property list identifier.
     */
    file_id = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, plist_id);
    H5Pclose(plist_id);

    /*
     * Create the dataspace for the dataset.
     */
    filespace = H5Screate_simple(RANK, dimsf, NULL);

    /*
     * Create the dataset with default properties and close filespace.
     */
    dset_id =
        H5Dcreate(file_id, DATASETNAME, H5T_NATIVE_INT, filespace, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    /*
     * Create property list for collective dataset write.
     */
    plist_id = H5Pcreate(H5P_DATASET_XFER);
    H5Pset_dxpl_mpio(plist_id, H5FD_MPIO_COLLECTIVE);

    /*
     * To write dataset independently use
     *
     * H5Pset_dxpl_mpio(plist_id, H5FD_MPIO_INDEPENDENT);
     */

    status = H5Dwrite(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, plist_id, data);
    free(data);

    /*
     * Close/release resources.
     */
    H5Dclose(dset_id);
    H5Sclose(filespace);
    H5Pclose(plist_id);
    H5Fclose(file_id);

    if (mpi_rank == 0)
        printf("PHDF5 example finished with no errors\n");

    MPI_Finalize();

    return 0;
}
```

### `HDF5Examples/C/H5PAR/ph5_file_create.c`

```c
/*
 *  This example creates an HDF5 file.
 */

#include "hdf5.h"

#define H5FILE_NAME "SDS_row.h5"

int
main(int argc, char **argv)
{
    /*
     * HDF5 APIs definitions
     */
    hid_t  file_id;  /* file and dataset identifiers */
    hid_t  plist_id; /* property list identifier( access template) */
    herr_t status;

    /*
     * MPI variables
     */
    int      mpi_size, mpi_rank;
    MPI_Comm comm = MPI_COMM_WORLD;
    MPI_Info info = MPI_INFO_NULL;

    /*
     * Initialize MPI
     */
    MPI_Init(&argc, &argv);
    MPI_Comm_size(comm, &mpi_size);
    MPI_Comm_rank(comm, &mpi_rank);

    /*
     * Set up file access property list with parallel I/O access
     */
    plist_id = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_fapl_mpio(plist_id, comm, info);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata reads on FAPL to perform metadata reads
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_all_coll_metadata_ops(plist_id, true);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata writes on FAPL to perform metadata writes
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_coll_metadata_write(plist_id, true);

    /*
     * Create a new file collectively.
     */
    file_id = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, plist_id);

    /*
     * Close property list.
     */
    H5Pclose(plist_id);

    /*
     * Close the file.
     */
    H5Fclose(file_id);

    if (mpi_rank == 0)
        printf("PHDF5 example finished with no errors\n");

    MPI_Finalize();

    return 0;
}
```

### `HDF5Examples/C/H5PAR/ph5_filtered_writes.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 * Example of using the parallel HDF5 library to write to datasets
 * with filters applied to them.
 *
 * If the HDF5_NOCLEANUP environment variable is set, the file that
 * this example creates will not be removed as the example finishes.
 *
 * The need of requirement of parallel file prefix is that in general
 * the current working directory in which compiling is done, is not suitable
 * for parallel I/O and there is no standard pathname for parallel file
 * systems. In some cases, the parallel file name may even need some
 * parallel file type prefix such as: "pfs:/GF/...".  Therefore, this
 * example parses the HDF5_PARAPREFIX environment variable for a prefix,
 * if one is needed.
 */

#include <stdbool.h>
#include <stdlib.h>

#include "hdf5.h"

#if defined(H5_HAVE_PARALLEL) && defined(H5_HAVE_PARALLEL_FILTERED_WRITES)

#define EXAMPLE_FILE       "ph5_filtered_writes.h5"
#define EXAMPLE_DSET1_NAME "DSET1"
#define EXAMPLE_DSET2_NAME "DSET2"

#define EXAMPLE_DSET_DIMS           2
#define EXAMPLE_DSET_CHUNK_DIM_SIZE 10

/* Dataset datatype */
#define HDF5_DATATYPE H5T_NATIVE_INT
typedef int C_DATATYPE;

#ifndef PATH_MAX
#define PATH_MAX 512
#endif

/* Global variables */
int mpi_rank, mpi_size;

/*
 * Routine to set an HDF5 filter on the given DCPL
 */
static void
set_filter(hid_t dcpl_id)
{
    htri_t filter_avail;

    /*
     * Check if 'deflate' filter is available
     */
    filter_avail = H5Zfilter_avail(H5Z_FILTER_DEFLATE);
    if (filter_avail < 0)
        return;
    else if (filter_avail) {
        /*
         * Set 'deflate' filter with reasonable
         * compression level on DCPL
         */
        H5Pset_deflate(dcpl_id, 6);
    }
    else {
        /*
         * Set Fletcher32 checksum filter on DCPL
         * since it is always available in HDF5
         */
        H5Pset_fletcher32(dcpl_id);
    }
}

/*
 * Routine to fill a data buffer with data. Assumes
 * dimension rank is 2 and data is stored contiguous.
 */
void
fill_databuf(hsize_t start[], hsize_t count[], hsize_t stride[], C_DATATYPE *data)
{
    C_DATATYPE *dataptr = data;
    hsize_t     i, j;

    /* Use MPI rank value for data */
    for (i = 0; i < count[0]; i++) {
        for (j = 0; j < count[1]; j++) {
            *dataptr++ = mpi_rank;
        }
    }
}

/* Cleanup created file */
static void
cleanup(char *filename)
{
    bool do_cleanup = getenv(HDF5_NOCLEANUP) ? false : true;

    if (do_cleanup)
        MPI_File_delete(filename, MPI_INFO_NULL);
}

/*
 * Routine to write to a dataset in a fashion
 * where no chunks in the dataset are written
 * to by more than 1 MPI rank. This will
 * generally give the best performance as the
 * MPI ranks will need the least amount of
 * inter-process communication.
 */
static void
write_dataset_no_overlap(hid_t file_id, hid_t dxpl_id)
{
    C_DATATYPE data[EXAMPLE_DSET_CHUNK_DIM_SIZE][4 * EXAMPLE_DSET_CHUNK_DIM_SIZE];
    hsize_t    dataset_dims[EXAMPLE_DSET_DIMS];
    hsize_t    chunk_dims[EXAMPLE_DSET_DIMS];
    hsize_t    start[EXAMPLE_DSET_DIMS];
    hsize_t    stride[EXAMPLE_DSET_DIMS];
    hsize_t    count[EXAMPLE_DSET_DIMS];
    hid_t      dset_id        = H5I_INVALID_HID;
    hid_t      dcpl_id        = H5I_INVALID_HID;
    hid_t      file_dataspace = H5I_INVALID_HID;

    /*
     * ------------------------------------
     * Setup Dataset Creation Property List
     * ------------------------------------
     */

    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);

    /*
     * REQUIRED: Dataset chunking must be enabled to
     *           apply a data filter to the dataset.
     *           Chunks in the dataset are of size
     *           EXAMPLE_DSET_CHUNK_DIM_SIZE x EXAMPLE_DSET_CHUNK_DIM_SIZE.
     */
    chunk_dims[0] = EXAMPLE_DSET_CHUNK_DIM_SIZE;
    chunk_dims[1] = EXAMPLE_DSET_CHUNK_DIM_SIZE;
    H5Pset_chunk(dcpl_id, EXAMPLE_DSET_DIMS, chunk_dims);

    /* Set filter to be applied to created datasets */
    set_filter(dcpl_id);

    /*
     * ------------------------------------
     * Define the dimensions of the dataset
     * and create it
     * ------------------------------------
     */

    /*
     * Create a dataset composed of 4 chunks
     * per MPI rank. The first dataset dimension
     * scales according to the number of MPI ranks.
     * The second dataset dimension stays fixed
     * according to the chunk size.
     */
    dataset_dims[0] = EXAMPLE_DSET_CHUNK_DIM_SIZE * mpi_size;
    dataset_dims[1] = 4 * EXAMPLE_DSET_CHUNK_DIM_SIZE;

    file_dataspace = H5Screate_simple(EXAMPLE_DSET_DIMS, dataset_dims, NULL);

    /* Create the dataset */
    dset_id = H5Dcreate2(file_id, EXAMPLE_DSET1_NAME, HDF5_DATATYPE, file_dataspace, H5P_DEFAULT, dcpl_id,
                         H5P_DEFAULT);

    /*
     * ------------------------------------
     * Setup selection in the dataset for
     * each MPI rank
     * ------------------------------------
     */

    /*
     * Each MPI rank's selection covers a
     * single chunk in the first dataset
     * dimension. Each MPI rank's selection
     * covers 4 chunks in the second dataset
     * dimension. This leads to each MPI rank
     * writing to 4 chunks of the dataset.
     */
    start[0]  = mpi_rank * EXAMPLE_DSET_CHUNK_DIM_SIZE;
    start[1]  = 0;
    stride[0] = 1;
    stride[1] = 1;
    count[0]  = EXAMPLE_DSET_CHUNK_DIM_SIZE;
    count[1]  = 4 * EXAMPLE_DSET_CHUNK_DIM_SIZE;

    H5Sselect_hyperslab(file_dataspace, H5S_SELECT_SET, start, stride, count, NULL);

    /*
     * --------------------------------------
     * Fill data buffer with MPI rank's rank
     * value to make it easy to see which
     * part of the dataset each rank wrote to
     * --------------------------------------
     */

    fill_databuf(start, count, stride, &data[0][0]);

    /*
     * ---------------------------------
     * Write to the dataset collectively
     * ---------------------------------
     */

    H5Dwrite(dset_id, HDF5_DATATYPE, H5S_BLOCK, file_dataspace, dxpl_id, data);

    /*
     * --------------
     * Close HDF5 IDs
     * --------------
     */

    H5Sclose(file_dataspace);
    H5Pclose(dcpl_id);
    H5Dclose(dset_id);
}

/*
 * Routine to write to a dataset in a fashion
 * where every chunk in the dataset is written
 * to by every MPI rank. This will generally
 * give the worst performance as the MPI ranks
 * will need the most amount of inter-process
 * communication.
 */
static void
write_dataset_overlap(hid_t file_id, hid_t dxpl_id)
{
    C_DATATYPE *data = NULL;
    hsize_t     dataset_dims[EXAMPLE_DSET_DIMS];
    hsize_t     chunk_dims[EXAMPLE_DSET_DIMS];
    hsize_t     start[EXAMPLE_DSET_DIMS];
    hsize_t     stride[EXAMPLE_DSET_DIMS];
    hsize_t     count[EXAMPLE_DSET_DIMS];
    hid_t       dset_id        = H5I_INVALID_HID;
    hid_t       dcpl_id        = H5I_INVALID_HID;
    hid_t       file_dataspace = H5I_INVALID_HID;

    /*
     * ------------------------------------
     * Setup Dataset Creation Property List
     * ------------------------------------
     */

    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);

    /*
     * REQUIRED: Dataset chunking must be enabled to
     *           apply a data filter to the dataset.
     *           Chunks in the dataset are of size
     *           mpi_size x EXAMPLE_DSET_CHUNK_DIM_SIZE.
     */
    chunk_dims[0] = mpi_size;
    chunk_dims[1] = EXAMPLE_DSET_CHUNK_DIM_SIZE;
    H5Pset_chunk(dcpl_id, EXAMPLE_DSET_DIMS, chunk_dims);

    /* Set filter to be applied to created datasets */
    set_filter(dcpl_id);

    /*
     * ------------------------------------
     * Define the dimensions of the dataset
     * and create it
     * ------------------------------------
     */

    /*
     * Create a dataset composed of N chunks,
     * where N is the number of MPI ranks. The
     * first dataset dimension scales according
     * to the number of MPI ranks. The second
     * dataset dimension stays fixed according
     * to the chunk size.
     */
    dataset_dims[0] = mpi_size * chunk_dims[0];
    dataset_dims[1] = EXAMPLE_DSET_CHUNK_DIM_SIZE;

    file_dataspace = H5Screate_simple(EXAMPLE_DSET_DIMS, dataset_dims, NULL);

    /* Create the dataset */
    dset_id = H5Dcreate2(file_id, EXAMPLE_DSET2_NAME, HDF5_DATATYPE, file_dataspace, H5P_DEFAULT, dcpl_id,
                         H5P_DEFAULT);

    /*
     * ------------------------------------
     * Setup selection in the dataset for
     * each MPI rank
     * ------------------------------------
     */

    /*
     * Each MPI rank's selection covers
     * part of every chunk in the first
     * dimension. Each MPI rank's selection
     * covers all of every chunk in the
     * second dimension. This leads to
     * each MPI rank writing an equal
     * amount of data to every chunk
     * in the dataset.
     */
    start[0]  = mpi_rank;
    start[1]  = 0;
    stride[0] = chunk_dims[0];
    stride[1] = 1;
    count[0]  = mpi_size;
    count[1]  = EXAMPLE_DSET_CHUNK_DIM_SIZE;

    H5Sselect_hyperslab(file_dataspace, H5S_SELECT_SET, start, stride, count, NULL);

    /*
     * --------------------------------------
     * Fill data buffer with MPI rank's rank
     * value to make it easy to see which
     * part of the dataset each rank wrote to
     * --------------------------------------
     */

    data = malloc(mpi_size * EXAMPLE_DSET_CHUNK_DIM_SIZE * sizeof(C_DATATYPE));

    fill_databuf(start, count, stride, data);

    /*
     * ---------------------------------
     * Write to the dataset collectively
     * ---------------------------------
     */

    H5Dwrite(dset_id, HDF5_DATATYPE, H5S_BLOCK, file_dataspace, dxpl_id, data);

    free(data);

    /*
     * --------------
     * Close HDF5 IDs
     * --------------
     */

    H5Sclose(file_dataspace);
    H5Pclose(dcpl_id);
    H5Dclose(dset_id);
}

int
main(int argc, char **argv)
{
    MPI_Comm comm       = MPI_COMM_WORLD;
    MPI_Info info       = MPI_INFO_NULL;
    hid_t    file_id    = H5I_INVALID_HID;
    hid_t    fapl_id    = H5I_INVALID_HID;
    hid_t    dxpl_id    = H5I_INVALID_HID;
    char    *par_prefix = NULL;
    char     filename[PATH_MAX];

    MPI_Init(&argc, &argv);
    MPI_Comm_size(comm, &mpi_size);
    MPI_Comm_rank(comm, &mpi_rank);

    /*
     * ----------------------------------
     * Start parallel access to HDF5 file
     * ----------------------------------
     */

    /* Setup File Access Property List with parallel I/O access */
    fapl_id = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_fapl_mpio(fapl_id, comm, info);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata reads on FAPL to perform metadata reads
     *           collectively, which usually allows filtered datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_all_coll_metadata_ops(fapl_id, true);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata writes on FAPL to perform metadata writes
     *           collectively, which usually allows filtered datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_coll_metadata_write(fapl_id, true);

    /*
     * OPTIONAL: Set the latest file format version for HDF5 in
     *           order to gain access to different dataset chunk
     *           index types and better data encoding methods.
     *           While not strictly necessary, this is generally
     *           recommended.
     */
    H5Pset_libver_bounds(fapl_id, H5F_LIBVER_LATEST, H5F_LIBVER_LATEST);

    /* Parse any parallel prefix and create filename */
    par_prefix = getenv("HDF5_PARAPREFIX");

    snprintf(filename, PATH_MAX, "%s%s%s", par_prefix ? par_prefix : "", par_prefix ? "/" : "", EXAMPLE_FILE);

    /* Create HDF5 file */
    file_id = H5Fcreate(filename, H5F_ACC_TRUNC, H5P_DEFAULT, fapl_id);

    /*
     * --------------------------------------
     * Setup Dataset Transfer Property List
     * with collective I/O
     * --------------------------------------
     */

    dxpl_id = H5Pcreate(H5P_DATASET_XFER);

    /*
     * REQUIRED: Setup collective I/O for the dataset
     *           write operations. Parallel writes to
     *           filtered datasets MUST be collective,
     *           even if some ranks have no data to
     *           contribute to the write operation.
     *
     *           Refer to the 'ph5_filtered_writes_no_sel'
     *           example to see how to setup a dataset
     *           write when one or more MPI ranks have
     *           no data to contribute to the write
     *           operation.
     */
    H5Pset_dxpl_mpio(dxpl_id, H5FD_MPIO_COLLECTIVE);

    /*
     * --------------------------------
     * Create and write to each dataset
     * --------------------------------
     */

    /*
     * Write to a dataset in a fashion where no
     * chunks in the dataset are written to by
     * more than 1 MPI rank. This will generally
     * give the best performance as the MPI ranks
     * will need the least amount of inter-process
     * communication.
     */
    write_dataset_no_overlap(file_id, dxpl_id);

    /*
     * Write to a dataset in a fashion where
     * every chunk in the dataset is written
     * to by every MPI rank. This will generally
     * give the worst performance as the MPI ranks
     * will need the most amount of inter-process
     * communication.
     */
    write_dataset_overlap(file_id, dxpl_id);

    /*
     * ------------------
     * Close all HDF5 IDs
     * ------------------
     */

    H5Pclose(dxpl_id);
    H5Pclose(fapl_id);
    H5Fclose(file_id);

    printf("PHDF5 example finished with no errors\n");

    /*
     * ------------------------------------
     * Cleanup created HDF5 file and finish
     * ------------------------------------
     */

    cleanup(filename);

    MPI_Finalize();

    return 0;
}

#else

int
main(void)
{
    printf("HDF5 not configured with parallel support or parallel filtered writes are disabled!\n");
    return 0;
}

#endif
```

### `HDF5Examples/C/H5PAR/ph5_filtered_writes_no_sel.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 * Example of using the parallel HDF5 library to collectively write to
 * datasets with filters applied to them when one or MPI ranks do not
 * have data to contribute to the dataset.
 *
 * If the HDF5_NOCLEANUP environment variable is set, the file that
 * this example creates will not be removed as the example finishes.
 *
 * The need of requirement of parallel file prefix is that in general
 * the current working directory in which compiling is done, is not suitable
 * for parallel I/O and there is no standard pathname for parallel file
 * systems. In some cases, the parallel file name may even need some
 * parallel file type prefix such as: "pfs:/GF/...".  Therefore, this
 * example parses the HDF5_PARAPREFIX environment variable for a prefix,
 * if one is needed.
 */

#include <stdbool.h>
#include <stdlib.h>

#include "hdf5.h"

#if defined(H5_HAVE_PARALLEL) && defined(H5_HAVE_PARALLEL_FILTERED_WRITES)

#define EXAMPLE_FILE      "ph5_filtered_writes_no_sel.h5"
#define EXAMPLE_DSET_NAME "DSET"

#define EXAMPLE_DSET_DIMS           2
#define EXAMPLE_DSET_CHUNK_DIM_SIZE 10

/* Dataset datatype */
#define HDF5_DATATYPE H5T_NATIVE_INT
typedef int C_DATATYPE;

#ifndef PATH_MAX
#define PATH_MAX 512
#endif

/* Global variables */
int mpi_rank, mpi_size;

/*
 * Routine to set an HDF5 filter on the given DCPL
 */
static void
set_filter(hid_t dcpl_id)
{
    htri_t filter_avail;

    /*
     * Check if 'deflate' filter is available
     */
    filter_avail = H5Zfilter_avail(H5Z_FILTER_DEFLATE);
    if (filter_avail < 0)
        return;
    else if (filter_avail) {
        /*
         * Set 'deflate' filter with reasonable
         * compression level on DCPL
         */
        H5Pset_deflate(dcpl_id, 6);
    }
    else {
        /*
         * Set Fletcher32 checksum filter on DCPL
         * since it is always available in HDF5
         */
        H5Pset_fletcher32(dcpl_id);
    }
}

/*
 * Routine to fill a data buffer with data. Assumes
 * dimension rank is 2 and data is stored contiguous.
 */
void
fill_databuf(hsize_t start[], hsize_t count[], hsize_t stride[], C_DATATYPE *data)
{
    C_DATATYPE *dataptr = data;
    hsize_t     i, j;

    /* Use MPI rank value for data */
    for (i = 0; i < count[0]; i++) {
        for (j = 0; j < count[1]; j++) {
            *dataptr++ = mpi_rank;
        }
    }
}

/* Cleanup created file */
static void
cleanup(char *filename)
{
    bool do_cleanup = getenv(HDF5_NOCLEANUP) ? false : true;

    if (do_cleanup)
        MPI_File_delete(filename, MPI_INFO_NULL);
}

/*
 * Routine to write to a dataset in a fashion
 * where no chunks in the dataset are written
 * to by more than 1 MPI rank. This will
 * generally give the best performance as the
 * MPI ranks will need the least amount of
 * inter-process communication.
 */
static void
write_dataset_some_no_sel(hid_t file_id, hid_t dxpl_id)
{
    C_DATATYPE data[EXAMPLE_DSET_CHUNK_DIM_SIZE][4 * EXAMPLE_DSET_CHUNK_DIM_SIZE];
    hsize_t    dataset_dims[EXAMPLE_DSET_DIMS];
    hsize_t    chunk_dims[EXAMPLE_DSET_DIMS];
    hsize_t    start[EXAMPLE_DSET_DIMS];
    hsize_t    stride[EXAMPLE_DSET_DIMS];
    hsize_t    count[EXAMPLE_DSET_DIMS];
    bool       no_selection;
    hid_t      dset_id        = H5I_INVALID_HID;
    hid_t      dcpl_id        = H5I_INVALID_HID;
    hid_t      file_dataspace = H5I_INVALID_HID;

    /*
     * ------------------------------------
     * Setup Dataset Creation Property List
     * ------------------------------------
     */

    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);

    /*
     * REQUIRED: Dataset chunking must be enabled to
     *           apply a data filter to the dataset.
     *           Chunks in the dataset are of size
     *           EXAMPLE_DSET_CHUNK_DIM_SIZE x EXAMPLE_DSET_CHUNK_DIM_SIZE.
     */
    chunk_dims[0] = EXAMPLE_DSET_CHUNK_DIM_SIZE;
    chunk_dims[1] = EXAMPLE_DSET_CHUNK_DIM_SIZE;
    H5Pset_chunk(dcpl_id, EXAMPLE_DSET_DIMS, chunk_dims);

    /* Set filter to be applied to created datasets */
    set_filter(dcpl_id);

    /*
     * ------------------------------------
     * Define the dimensions of the dataset
     * and create it
     * ------------------------------------
     */

    /*
     * Create a dataset composed of 4 chunks
     * per MPI rank. The first dataset dimension
     * scales according to the number of MPI ranks.
     * The second dataset dimension stays fixed
     * according to the chunk size.
     */
    dataset_dims[0] = EXAMPLE_DSET_CHUNK_DIM_SIZE * mpi_size;
    dataset_dims[1] = 4 * EXAMPLE_DSET_CHUNK_DIM_SIZE;

    file_dataspace = H5Screate_simple(EXAMPLE_DSET_DIMS, dataset_dims, NULL);

    /* Create the dataset */
    dset_id = H5Dcreate2(file_id, EXAMPLE_DSET_NAME, HDF5_DATATYPE, file_dataspace, H5P_DEFAULT, dcpl_id,
                         H5P_DEFAULT);

    /*
     * ------------------------------------
     * Setup selection in the dataset for
     * each MPI rank
     * ------------------------------------
     */

    /*
     * Odd rank value MPI ranks do not
     * contribute any data to the dataset.
     */
    no_selection = (mpi_rank % 2) == 1;

    if (no_selection) {
        /*
         * MPI ranks not contributing data to
         * the dataset should call H5Sselect_none
         * on the file dataspace that will be
         * passed to H5Dwrite.
         */
        H5Sselect_none(file_dataspace);
    }
    else {
        /*
         * Even MPI ranks contribute data to
         * the dataset. Each MPI rank's selection
         * covers a single chunk in the first dataset
         * dimension. Each MPI rank's selection
         * covers 4 chunks in the second dataset
         * dimension. This leads to each contributing
         * MPI rank writing to 4 chunks of the dataset.
         */
        start[0]  = mpi_rank * EXAMPLE_DSET_CHUNK_DIM_SIZE;
        start[1]  = 0;
        stride[0] = 1;
        stride[1] = 1;
        count[0]  = EXAMPLE_DSET_CHUNK_DIM_SIZE;
        count[1]  = 4 * EXAMPLE_DSET_CHUNK_DIM_SIZE;

        H5Sselect_hyperslab(file_dataspace, H5S_SELECT_SET, start, stride, count, NULL);

        /*
         * --------------------------------------
         * Fill data buffer with MPI rank's rank
         * value to make it easy to see which
         * part of the dataset each rank wrote to
         * --------------------------------------
         */

        fill_databuf(start, count, stride, &data[0][0]);
    }

    /*
     * ---------------------------------
     * Write to the dataset collectively
     * ---------------------------------
     */

    H5Dwrite(dset_id, HDF5_DATATYPE, no_selection ? H5S_ALL : H5S_BLOCK, file_dataspace, dxpl_id, data);

    /*
     * --------------
     * Close HDF5 IDs
     * --------------
     */

    H5Sclose(file_dataspace);
    H5Pclose(dcpl_id);
    H5Dclose(dset_id);
}

int
main(int argc, char **argv)
{
    MPI_Comm comm       = MPI_COMM_WORLD;
    MPI_Info info       = MPI_INFO_NULL;
    hid_t    file_id    = H5I_INVALID_HID;
    hid_t    fapl_id    = H5I_INVALID_HID;
    hid_t    dxpl_id    = H5I_INVALID_HID;
    char    *par_prefix = NULL;
    char     filename[PATH_MAX];

    MPI_Init(&argc, &argv);
    MPI_Comm_size(comm, &mpi_size);
    MPI_Comm_rank(comm, &mpi_rank);

    /*
     * ----------------------------------
     * Start parallel access to HDF5 file
     * ----------------------------------
     */

    /* Setup File Access Property List with parallel I/O access */
    fapl_id = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_fapl_mpio(fapl_id, comm, info);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata reads on FAPL to perform metadata reads
     *           collectively, which usually allows filtered datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_all_coll_metadata_ops(fapl_id, true);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata writes on FAPL to perform metadata writes
     *           collectively, which usually allows filtered datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_coll_metadata_write(fapl_id, true);

    /*
     * OPTIONAL: Set the latest file format version for HDF5 in
     *           order to gain access to different dataset chunk
     *           index types and better data encoding methods.
     *           While not strictly necessary, this is generally
     *           recommended.
     */
    H5Pset_libver_bounds(fapl_id, H5F_LIBVER_LATEST, H5F_LIBVER_LATEST);

    /* Parse any parallel prefix and create filename */
    par_prefix = getenv("HDF5_PARAPREFIX");

    snprintf(filename, PATH_MAX, "%s%s%s", par_prefix ? par_prefix : "", par_prefix ? "/" : "", EXAMPLE_FILE);

    /* Create HDF5 file */
    file_id = H5Fcreate(filename, H5F_ACC_TRUNC, H5P_DEFAULT, fapl_id);

    /*
     * --------------------------------------
     * Setup Dataset Transfer Property List
     * with collective I/O
     * --------------------------------------
     */

    dxpl_id = H5Pcreate(H5P_DATASET_XFER);

    /*
     * REQUIRED: Setup collective I/O for the dataset
     *           write operations. Parallel writes to
     *           filtered datasets MUST be collective,
     *           even if some ranks have no data to
     *           contribute to the write operation.
     */
    H5Pset_dxpl_mpio(dxpl_id, H5FD_MPIO_COLLECTIVE);

    /*
     * --------------------------------
     * Create and write to the dataset
     * --------------------------------
     */

    /*
     * Write to a dataset in a fashion where no
     * chunks in the dataset are written to by
     * more than 1 MPI rank and some MPI ranks
     * have nothing to contribute to the dataset.
     * In this case, the MPI ranks that have no
     * data to contribute must still participate
     * in the collective H5Dwrite call, but should
     * call H5Sselect_none on the file dataspace
     * passed to the H5Dwrite call.
     */
    write_dataset_some_no_sel(file_id, dxpl_id);

    /*
     * ------------------
     * Close all HDF5 IDs
     * ------------------
     */

    H5Pclose(dxpl_id);
    H5Pclose(fapl_id);
    H5Fclose(file_id);

    printf("PHDF5 example finished with no errors\n");

    /*
     * ------------------------------------
     * Cleanup created HDF5 file and finish
     * ------------------------------------
     */

    cleanup(filename);

    MPI_Finalize();

    return 0;
}

#else

int
main(void)
{
    printf("HDF5 not configured with parallel support or parallel filtered writes are disabled!\n");
    return 0;
}

#endif
```

### `HDF5Examples/C/H5PAR/ph5_hyperslab_by_chunk.c`

```c
/*
 *  This example writes dataset sing chunking. Each process writes
 *  exactly one chunk.
 *             - |
 *             * V
 *  Number of processes is assumed to be 4.
 */

#include "hdf5.h"
#include "stdlib.h"

#define H5FILE_NAME "SDS_chnk.h5"
#define DATASETNAME "IntArray"
#define NX          8 /* dataset dimensions */
#define NY          4
#define CH_NX       4 /* chunk dimensions */
#define CH_NY       2
#define RANK        2

int
main(int argc, char **argv)
{
    /*
     * HDF5 APIs definitions
     */
    hid_t   file_id, dset_id;    /* file and dataset identifiers */
    hid_t   filespace, memspace; /* file and memory dataspace identifiers */
    hsize_t dimsf[2];            /* dataset dimensions */
    hsize_t chunk_dims[2];       /* chunk dimensions */
    int    *data;                /* pointer to data buffer to write */
    hsize_t count[2];            /* hyperslab selection parameters */
    hsize_t stride[2];
    hsize_t block[2];
    hsize_t offset[2];
    hid_t   plist_id; /* property list identifier */
    int     i;
    herr_t  status;

    /*
     * MPI variables
     */
    int      mpi_size, mpi_rank;
    MPI_Comm comm = MPI_COMM_WORLD;
    MPI_Info info = MPI_INFO_NULL;

    /*
     * Initialize MPI
     */
    MPI_Init(&argc, &argv);
    MPI_Comm_size(comm, &mpi_size);
    MPI_Comm_rank(comm, &mpi_rank);
    /*
     * Exit if number of processes is not 4.
     */
    if (mpi_size != 4) {
        printf("This example to set up to use only 4 processes \n");
        printf("Quitting...\n");
        return 0;
    }

    /*
     * Set up file access property list with parallel I/O access
     */
    plist_id = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_fapl_mpio(plist_id, comm, info);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata reads on FAPL to perform metadata reads
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_all_coll_metadata_ops(plist_id, true);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata writes on FAPL to perform metadata writes
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_coll_metadata_write(plist_id, true);

    /*
     * Create a new file collectively and release property list identifier.
     */
    file_id = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, plist_id);
    H5Pclose(plist_id);

    /*
     * Create the dataspace for the dataset.
     */
    dimsf[0]      = NX;
    dimsf[1]      = NY;
    chunk_dims[0] = CH_NX;
    chunk_dims[1] = CH_NY;
    filespace     = H5Screate_simple(RANK, dimsf, NULL);
    memspace      = H5Screate_simple(RANK, chunk_dims, NULL);

    /*
     * Create chunked dataset.
     */
    plist_id = H5Pcreate(H5P_DATASET_CREATE);
    H5Pset_chunk(plist_id, RANK, chunk_dims);
    dset_id = H5Dcreate(file_id, DATASETNAME, H5T_NATIVE_INT, filespace, H5P_DEFAULT, plist_id, H5P_DEFAULT);
    H5Pclose(plist_id);
    H5Sclose(filespace);

    /*
     * Each process defines dataset in memory and writes it to the hyperslab
     * in the file.
     */
    count[0]  = 1;
    count[1]  = 1;
    stride[0] = 1;
    stride[1] = 1;
    block[0]  = chunk_dims[0];
    block[1]  = chunk_dims[1];
    if (mpi_rank == 0) {
        offset[0] = 0;
        offset[1] = 0;
    }
    if (mpi_rank == 1) {
        offset[0] = 0;
        offset[1] = chunk_dims[1];
    }
    if (mpi_rank == 2) {
        offset[0] = chunk_dims[0];
        offset[1] = 0;
    }
    if (mpi_rank == 3) {
        offset[0] = chunk_dims[0];
        offset[1] = chunk_dims[1];
    }

    /*
     * Select hyperslab in the file.
     */
    filespace = H5Dget_space(dset_id);
    status    = H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, stride, count, block);

    /*
     * Initialize data buffer
     */
    data = (int *)malloc(sizeof(int) * chunk_dims[0] * chunk_dims[1]);
    for (i = 0; i < (int)chunk_dims[0] * chunk_dims[1]; i++) {
        data[i] = mpi_rank + 1;
    }

    /*
     * Create property list for collective dataset write.
     */
    plist_id = H5Pcreate(H5P_DATASET_XFER);
    H5Pset_dxpl_mpio(plist_id, H5FD_MPIO_COLLECTIVE);

    status = H5Dwrite(dset_id, H5T_NATIVE_INT, memspace, filespace, plist_id, data);
    free(data);

    /*
     * Close/release resources.
     */
    H5Dclose(dset_id);
    H5Sclose(filespace);
    H5Sclose(memspace);
    H5Pclose(plist_id);
    H5Fclose(file_id);

    if (mpi_rank == 0)
        printf("PHDF5 example finished with no errors\n");

    MPI_Finalize();

    return 0;
}
```

### `HDF5Examples/C/H5PAR/ph5_hyperslab_by_col.c`

```c
/*
 *  This example writes data to the HDF5 file by columns.
 *  Number of processes is assumed to be 2.
 */

#include "hdf5.h"
#include "stdlib.h"

#define H5FILE_NAME "SDS_col.h5"
#define DATASETNAME "IntArray"
#define NX          8 /* dataset dimensions */
#define NY          6
#define RANK        2

int
main(int argc, char **argv)
{
    /*
     * HDF5 APIs definitions
     */
    hid_t   file_id, dset_id;    /* file and dataset identifiers */
    hid_t   filespace, memspace; /* file and memory dataspace identifiers */
    hsize_t dimsf[2];            /* dataset dimensions */
    hsize_t dimsm[2];            /* dataset dimensions */
    int    *data;                /* pointer to data buffer to write */
    hsize_t count[2];            /* hyperslab selection parameters */
    hsize_t stride[2];
    hsize_t block[2];
    hsize_t offset[2];
    hid_t   plist_id; /* property list identifier */
    int     i, j, k;
    herr_t  status;

    /*
     * MPI variables
     */
    int      mpi_size, mpi_rank;
    MPI_Comm comm = MPI_COMM_WORLD;
    MPI_Info info = MPI_INFO_NULL;

    /*
     * Initialize MPI
     */
    MPI_Init(&argc, &argv);
    MPI_Comm_size(comm, &mpi_size);
    MPI_Comm_rank(comm, &mpi_rank);
    /*
     * Exit if number of processes is not 2
     */
    if (mpi_size != 2) {
        printf("This example to set up to use only 2 processes \n");
        printf("Quitting...\n");
        return 0;
    }

    /*
     * Set up file access property list with parallel I/O access
     */
    plist_id = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_fapl_mpio(plist_id, comm, info);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata reads on FAPL to perform metadata reads
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_all_coll_metadata_ops(plist_id, true);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata writes on FAPL to perform metadata writes
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_coll_metadata_write(plist_id, true);

    /*
     * Create a new file collectively and release property list identifier.
     */
    file_id = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, plist_id);
    H5Pclose(plist_id);

    /*
     * Create the dataspace for the dataset.
     */
    dimsf[0]  = NX;
    dimsf[1]  = NY;
    dimsm[0]  = NX;
    dimsm[1]  = NY / 2;
    filespace = H5Screate_simple(RANK, dimsf, NULL);
    memspace  = H5Screate_simple(RANK, dimsm, NULL);

    /*
     * Create the dataset with default properties and close filespace.
     */
    dset_id =
        H5Dcreate(file_id, DATASETNAME, H5T_NATIVE_INT, filespace, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    H5Sclose(filespace);

    /*
     * Each process defines dataset in memory and writes it to the hyperslab
     * in the file.
     */
    count[0]  = 1;
    count[1]  = dimsm[1];
    offset[0] = 0;
    offset[1] = mpi_rank;
    stride[0] = 1;
    stride[1] = 2;
    block[0]  = dimsf[0];
    block[1]  = 1;

    /*
     * Select hyperslab in the file.
     */
    filespace = H5Dget_space(dset_id);
    H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, stride, count, block);

    /*
     * Initialize data buffer
     */
    data = (int *)malloc(sizeof(int) * (size_t)dimsm[0] * (size_t)dimsm[1]);
    for (i = 0; i < dimsm[0] * dimsm[1]; i = i + dimsm[1]) {
        k = 1;
        for (j = 0; j < dimsm[1]; j++) {
            data[i + j] = (mpi_rank + 1) * k;
            k           = k * 10;
        }
    }

    /*
     * Create property list for collective dataset write.
     */
    plist_id = H5Pcreate(H5P_DATASET_XFER);
    H5Pset_dxpl_mpio(plist_id, H5FD_MPIO_COLLECTIVE);

    status = H5Dwrite(dset_id, H5T_NATIVE_INT, memspace, filespace, plist_id, data);
    free(data);

    /*
     * Close/release resources.
     */
    H5Dclose(dset_id);
    H5Sclose(filespace);
    H5Sclose(memspace);
    H5Pclose(plist_id);
    H5Fclose(file_id);

    if (mpi_rank == 0)
        printf("PHDF5 example finished with no errors\n");

    MPI_Finalize();

    return 0;
}
```

### `HDF5Examples/C/H5PAR/ph5_hyperslab_by_pattern.c`

```c
/*
 *  This example writes data to the HDF5 file following some pattern
 *             - | - | ......
 *             * V * V ......
 *             - | - | ......
 *             * V * V ......
 *             ..............
 *  Number of processes is assumed to be 4.
 */

#include "hdf5.h"
#include "stdlib.h"

#define H5FILE_NAME "SDS_pat.h5"
#define DATASETNAME "IntArray"
#define NX          8 /* dataset dimensions */
#define NY          4
#define RANK        2
#define RANK1       1

int
main(int argc, char **argv)
{
    /*
     * HDF5 APIs definitions
     */
    hid_t   file_id, dset_id;    /* file and dataset identifiers */
    hid_t   filespace, memspace; /* file and memory dataspace identifiers */
    hsize_t dimsf[2];            /* dataset dimensions */
    hsize_t dimsm[1];            /* dataset dimensions */
    int    *data;                /* pointer to data buffer to write */
    hsize_t count[2];            /* hyperslab selection parameters */
    hsize_t stride[2];
    hsize_t offset[2];
    hid_t   plist_id; /* property list identifier */
    int     i;
    herr_t  status;

    /*
     * MPI variables
     */
    int      mpi_size, mpi_rank;
    MPI_Comm comm = MPI_COMM_WORLD;
    MPI_Info info = MPI_INFO_NULL;

    /*
     * Initialize MPI
     */
    MPI_Init(&argc, &argv);
    MPI_Comm_size(comm, &mpi_size);
    MPI_Comm_rank(comm, &mpi_rank);
    /*
     * Exit if number of processes is not 4.
     */
    if (mpi_size != 4) {
        printf("This example to set up to use only 4 processes \n");
        printf("Quitting...\n");
        return 0;
    }

    /*
     * Set up file access property list with parallel I/O access
     */
    plist_id = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_fapl_mpio(plist_id, comm, info);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata reads on FAPL to perform metadata reads
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_all_coll_metadata_ops(plist_id, true);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata writes on FAPL to perform metadata writes
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_coll_metadata_write(plist_id, true);

    /*
     * Create a new file collectively and release property list identifier.
     */
    file_id = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, plist_id);
    H5Pclose(plist_id);

    /*
     * Create the dataspace for the dataset.
     */
    dimsf[0]  = NX;
    dimsf[1]  = NY;
    dimsm[0]  = NX;
    filespace = H5Screate_simple(RANK, dimsf, NULL);
    memspace  = H5Screate_simple(RANK1, dimsm, NULL);

    /*
     * Create the dataset with default properties and close filespace.
     */
    dset_id =
        H5Dcreate(file_id, DATASETNAME, H5T_NATIVE_INT, filespace, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    H5Sclose(filespace);

    /*
     * Each process defines dataset in memory and writes it to the hyperslab
     * in the file.
     */
    count[0]  = 4;
    count[1]  = 2;
    stride[0] = 2;
    stride[1] = 2;
    if (mpi_rank == 0) {
        offset[0] = 0;
        offset[1] = 0;
    }
    if (mpi_rank == 1) {
        offset[0] = 1;
        offset[1] = 0;
    }
    if (mpi_rank == 2) {
        offset[0] = 0;
        offset[1] = 1;
    }
    if (mpi_rank == 3) {
        offset[0] = 1;
        offset[1] = 1;
    }

    /*
     * Select hyperslab in the file.
     */
    filespace = H5Dget_space(dset_id);
    status    = H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, stride, count, NULL);

    /*
     * Initialize data buffer
     */
    data = (int *)malloc(sizeof(int) * dimsm[0]);
    for (i = 0; i < (int)dimsm[0]; i++) {
        data[i] = mpi_rank + 1;
    }

    /*
     * Create property list for collective dataset write.
     */
    plist_id = H5Pcreate(H5P_DATASET_XFER);
    H5Pset_dxpl_mpio(plist_id, H5FD_MPIO_COLLECTIVE);

    status = H5Dwrite(dset_id, H5T_NATIVE_INT, memspace, filespace, plist_id, data);
    free(data);

    /*
     * Close/release resources.
     */
    H5Dclose(dset_id);
    H5Sclose(filespace);
    H5Sclose(memspace);
    H5Pclose(plist_id);
    H5Fclose(file_id);

    if (mpi_rank == 0)
        printf("PHDF5 example finished with no errors\n");

    MPI_Finalize();

    return 0;
}
```

### `HDF5Examples/C/H5PAR/ph5_hyperslab_by_row.c`

```c
/*
 *  This example writes data to the HDF5 file by rows.
 *  Number of processes is assumed to be 1 or multiples of 2 (up to 8)
 */

#include "hdf5.h"
#include "stdlib.h"

#define H5FILE_NAME "SDS_row.h5"
#define DATASETNAME "IntArray"
#define NX          8 /* dataset dimensions */
#define NY          5
#define RANK        2

int
main(int argc, char **argv)
{
    /*
     * HDF5 APIs definitions
     */
    hid_t   file_id, dset_id;    /* file and dataset identifiers */
    hid_t   filespace, memspace; /* file and memory dataspace identifiers */
    hsize_t dimsf[2];            /* dataset dimensions */
    int    *data;                /* pointer to data buffer to write */
    hsize_t count[2];            /* hyperslab selection parameters */
    hsize_t offset[2];
    hid_t   plist_id; /* property list identifier */
    int     i;
    herr_t  status;

    /*
     * MPI variables
     */
    int      mpi_size, mpi_rank;
    MPI_Comm comm = MPI_COMM_WORLD;
    MPI_Info info = MPI_INFO_NULL;

    /*
     * Initialize MPI
     */
    MPI_Init(&argc, &argv);
    MPI_Comm_size(comm, &mpi_size);
    MPI_Comm_rank(comm, &mpi_rank);

    /*
     * Set up file access property list with parallel I/O access
     */
    plist_id = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_fapl_mpio(plist_id, comm, info);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata reads on FAPL to perform metadata reads
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_all_coll_metadata_ops(plist_id, true);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata writes on FAPL to perform metadata writes
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_coll_metadata_write(plist_id, true);

    /*
     * Create a new file collectively and release property list identifier.
     */
    file_id = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, plist_id);
    H5Pclose(plist_id);

    /*
     * Create the dataspace for the dataset.
     */
    dimsf[0]  = NX;
    dimsf[1]  = NY;
    filespace = H5Screate_simple(RANK, dimsf, NULL);

    /*
     * Create the dataset with default properties and close filespace.
     */
    dset_id =
        H5Dcreate(file_id, DATASETNAME, H5T_NATIVE_INT, filespace, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    H5Sclose(filespace);

    /*
     * Each process defines dataset in memory and writes it to the hyperslab
     * in the file.
     */
    count[0]  = dimsf[0] / mpi_size;
    count[1]  = dimsf[1];
    offset[0] = mpi_rank * count[0];
    offset[1] = 0;
    memspace  = H5Screate_simple(RANK, count, NULL);

    /*
     * Select hyperslab in the file.
     */
    filespace = H5Dget_space(dset_id);
    H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, count, NULL);

    /*
     * Initialize data buffer
     */
    data = (int *)malloc(sizeof(int) * count[0] * count[1]);
    for (i = 0; i < count[0] * count[1]; i++) {
        data[i] = mpi_rank + 10;
    }

    /*
     * Create property list for collective dataset write.
     */
    plist_id = H5Pcreate(H5P_DATASET_XFER);
    H5Pset_dxpl_mpio(plist_id, H5FD_MPIO_COLLECTIVE);

    status = H5Dwrite(dset_id, H5T_NATIVE_INT, memspace, filespace, plist_id, data);
    free(data);

    /*
     * Close/release resources.
     */
    H5Dclose(dset_id);
    H5Sclose(filespace);
    H5Sclose(memspace);
    H5Pclose(plist_id);
    H5Fclose(file_id);

    if (mpi_rank == 0)
        printf("PHDF5 example finished with no errors\n");

    MPI_Finalize();

    return 0;
}
```

### `HDF5Examples/C/H5PAR/ph5_subfiling.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 * Example of using HDF5's Subfiling VFD to write to an
 * HDF5 file that is striped across multiple subfiles
 *
 * If the HDF5_NOCLEANUP environment variable is set, the
 * files that this example creates will not be removed as
 * the example finishes.
 *
 * In general, the current working directory in which compiling
 * is done, is not suitable for parallel I/O and there is no
 * standard pathname for parallel file systems. In some cases,
 * the parallel file name may even need some parallel file type
 * prefix such as: "pfs:/GF/...".  Therefore, this example parses
 * the HDF5_PARAPREFIX environment variable for a prefix, if one
 * is needed.
 */

#include <stdbool.h>
#include <stdlib.h>

#include "hdf5.h"

#if defined(H5_HAVE_PARALLEL) && defined(H5_HAVE_SUBFILING_VFD)

#define EXAMPLE_FILE  "h5_subfiling_default_example.h5"
#define EXAMPLE_FILE2 "h5_subfiling_custom_example.h5"
#define EXAMPLE_FILE3 "h5_subfiling_precreate_example.h5"

#define EXAMPLE_DSET_NAME "DSET"
#define EXAMPLE_DSET_DIMS 2

/* Have each MPI rank write 16MiB of data */
#define EXAMPLE_DSET_NY 4194304

/* Dataset datatype */
#define EXAMPLE_DSET_DATATYPE H5T_NATIVE_INT
typedef int EXAMPLE_DSET_C_DATATYPE;

/* Cleanup created files */
static void
cleanup(char *filename, hid_t fapl_id)
{
    bool do_cleanup = getenv(HDF5_NOCLEANUP) ? false : true;

    if (do_cleanup)
        H5Fdelete(filename, fapl_id);
}

/*
 * An example of using the HDF5 Subfiling VFD with
 * its default settings of 1 subfile per node, with
 * a stripe size of 32MiB
 */
static void
subfiling_write_default(hid_t fapl_id, int mpi_size, int mpi_rank)
{
    EXAMPLE_DSET_C_DATATYPE *data;
    hsize_t                  dset_dims[EXAMPLE_DSET_DIMS];
    hsize_t                  start[EXAMPLE_DSET_DIMS];
    hsize_t                  count[EXAMPLE_DSET_DIMS];
    hid_t                    file_id;
    hid_t                    subfiling_fapl;
    hid_t                    dset_id;
    hid_t                    filespace;
    char                     filename[512];
    char                    *par_prefix;

    /*
     * Make a copy of the FAPL so we don't disturb
     * it for the other examples
     */
    subfiling_fapl = H5Pcopy(fapl_id);

    /*
     * Set Subfiling VFD on FAPL using default settings
     * (use IOC VFD, 1 IOC per node, 32MiB stripe size)
     *
     * Note that all of Subfiling's configuration settings
     * can be adjusted with environment variables as well
     * in this case.
     */
    H5Pset_fapl_subfiling(subfiling_fapl, NULL);

    /*
     * OPTIONAL: Set alignment of objects in HDF5 file to
     *           be equal to the Subfiling stripe size.
     *           Choosing a Subfiling stripe size and HDF5
     *           object alignment value that are some
     *           multiple of the disk block size can
     *           generally help performance by ensuring
     *           that I/O is well-aligned and doesn't
     *           excessively cross stripe boundaries.
     *
     *           Note that this option can substantially
     *           increase the size of the resulting HDF5
     *           files, so it is a good idea to keep an eye
     *           on this.
     */
    H5Pset_alignment(subfiling_fapl, 0, 33554432); /* Align to default 32MiB stripe size */

    /* Parse any parallel prefix and create filename */
    par_prefix = getenv("HDF5_PARAPREFIX");

    snprintf(filename, sizeof(filename), "%s%s%s", par_prefix ? par_prefix : "", par_prefix ? "/" : "",
             EXAMPLE_FILE);

    /*
     * Create a new file collectively
     */
    file_id = H5Fcreate(filename, H5F_ACC_TRUNC, H5P_DEFAULT, subfiling_fapl);

    /*
     * Create the dataspace for the dataset. The first
     * dimension varies with the number of MPI ranks
     * while the second dimension is fixed.
     */
    dset_dims[0] = mpi_size;
    dset_dims[1] = EXAMPLE_DSET_NY;
    filespace    = H5Screate_simple(EXAMPLE_DSET_DIMS, dset_dims, NULL);

    /*
     * Create the dataset with default properties
     */
    dset_id = H5Dcreate2(file_id, EXAMPLE_DSET_NAME, EXAMPLE_DSET_DATATYPE, filespace, H5P_DEFAULT,
                         H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Each MPI rank writes from a contiguous memory
     * region to the hyperslab in the file
     */
    start[0] = mpi_rank;
    start[1] = 0;
    count[0] = 1;
    count[1] = dset_dims[1];
    H5Sselect_hyperslab(filespace, H5S_SELECT_SET, start, NULL, count, NULL);

    /*
     * Initialize data buffer
     */
    data = malloc(count[0] * count[1] * sizeof(EXAMPLE_DSET_C_DATATYPE));
    for (size_t i = 0; i < count[0] * count[1]; i++) {
        data[i] = mpi_rank + i;
    }

    /*
     * Write to dataset
     */
    H5Dwrite(dset_id, EXAMPLE_DSET_DATATYPE, H5S_BLOCK, filespace, H5P_DEFAULT, data);

    /*
     * Close/release resources.
     */

    free(data);

    H5Dclose(dset_id);
    H5Sclose(filespace);
    H5Fclose(file_id);

    cleanup(EXAMPLE_FILE, subfiling_fapl);

    H5Pclose(subfiling_fapl);
}

/*
 * An example of using the HDF5 Subfiling VFD with
 * custom settings
 */
static void
subfiling_write_custom(hid_t fapl_id, int mpi_size, int mpi_rank)
{
    EXAMPLE_DSET_C_DATATYPE *data;
    H5FD_subfiling_config_t  subf_config;
    H5FD_ioc_config_t        ioc_config;
    hsize_t                  dset_dims[EXAMPLE_DSET_DIMS];
    hsize_t                  start[EXAMPLE_DSET_DIMS];
    hsize_t                  count[EXAMPLE_DSET_DIMS];
    hid_t                    file_id;
    hid_t                    subfiling_fapl;
    hid_t                    dset_id;
    hid_t                    filespace;
    char                     filename[512];
    char                    *par_prefix;

    /*
     * Make a copy of the FAPL so we don't disturb
     * it for the other examples
     */
    subfiling_fapl = H5Pcopy(fapl_id);

    /*
     * Get a default Subfiling and IOC configuration
     */
    H5Pget_fapl_subfiling(subfiling_fapl, &subf_config);
    H5Pget_fapl_ioc(subfiling_fapl, &ioc_config);

    /*
     * Set Subfiling configuration to use a 1MiB
     * stripe size and the SELECT_IOC_EVERY_NTH_RANK
     * selection method. By default, without a setting
     * in the H5FD_SUBFILING_IOC_SELECTION_CRITERIA
     * environment variable, this will use every MPI
     * rank as an I/O concentrator.
     */
    subf_config.shared_cfg.stripe_size   = 1048576;
    subf_config.shared_cfg.ioc_selection = SELECT_IOC_EVERY_NTH_RANK;

    /*
     * Set IOC configuration to use 2 worker threads
     * per IOC instead of the default setting and
     * update IOC configuration with new subfiling
     * configuration.
     */
    ioc_config.thread_pool_size = 2;

    /*
     * Set our new configuration on the IOC
     * FAPL used for Subfiling
     */
    H5Pset_fapl_ioc(subf_config.ioc_fapl_id, &ioc_config);

    /*
     * Finally, set our new Subfiling configuration
     * on the original FAPL
     */
    H5Pset_fapl_subfiling(subfiling_fapl, &subf_config);

    /*
     * OPTIONAL: Set alignment of objects in HDF5 file to
     *           be equal to the Subfiling stripe size.
     *           Choosing a Subfiling stripe size and HDF5
     *           object alignment value that are some
     *           multiple of the disk block size can
     *           generally help performance by ensuring
     *           that I/O is well-aligned and doesn't
     *           excessively cross stripe boundaries.
     *
     *           Note that this option can substantially
     *           increase the size of the resulting HDF5
     *           files, so it is a good idea to keep an eye
     *           on this.
     */
    H5Pset_alignment(subfiling_fapl, 0, 1048576); /* Align to custom 1MiB stripe size */

    /* Parse any parallel prefix and create filename */
    par_prefix = getenv("HDF5_PARAPREFIX");

    snprintf(filename, sizeof(filename), "%s%s%s", par_prefix ? par_prefix : "", par_prefix ? "/" : "",
             EXAMPLE_FILE2);

    /*
     * Create a new file collectively
     */
    file_id = H5Fcreate(filename, H5F_ACC_TRUNC, H5P_DEFAULT, subfiling_fapl);

    /*
     * Create the dataspace for the dataset. The first
     * dimension varies with the number of MPI ranks
     * while the second dimension is fixed.
     */
    dset_dims[0] = mpi_size;
    dset_dims[1] = EXAMPLE_DSET_NY;
    filespace    = H5Screate_simple(EXAMPLE_DSET_DIMS, dset_dims, NULL);

    /*
     * Create the dataset with default properties
     */
    dset_id = H5Dcreate2(file_id, EXAMPLE_DSET_NAME, EXAMPLE_DSET_DATATYPE, filespace, H5P_DEFAULT,
                         H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Each MPI rank writes from a contiguous memory
     * region to the hyperslab in the file
     */
    start[0] = mpi_rank;
    start[1] = 0;
    count[0] = 1;
    count[1] = dset_dims[1];
    H5Sselect_hyperslab(filespace, H5S_SELECT_SET, start, NULL, count, NULL);

    /*
     * Initialize data buffer
     */
    data = malloc(count[0] * count[1] * sizeof(EXAMPLE_DSET_C_DATATYPE));
    for (size_t i = 0; i < count[0] * count[1]; i++) {
        data[i] = mpi_rank + i;
    }

    /*
     * Write to dataset
     */
    H5Dwrite(dset_id, EXAMPLE_DSET_DATATYPE, H5S_BLOCK, filespace, H5P_DEFAULT, data);

    /*
     * Close/release resources.
     */

    free(data);

    H5Dclose(dset_id);
    H5Sclose(filespace);
    H5Fclose(file_id);

    cleanup(EXAMPLE_FILE2, subfiling_fapl);

    H5Pclose(subf_config.ioc_fapl_id);
    H5Pclose(subfiling_fapl);
}

/*
 * An example of pre-creating an HDF5 file on MPI rank
 * 0 when using the HDF5 Subfiling VFD. In this case,
 * the subfiling stripe count must be set so that rank
 * 0 knows how many subfiles to pre-create.
 */
static void
subfiling_write_precreate(hid_t fapl_id, int mpi_size, int mpi_rank)
{
    EXAMPLE_DSET_C_DATATYPE *data;
    H5FD_subfiling_config_t  subf_config;
    hsize_t                  dset_dims[EXAMPLE_DSET_DIMS];
    hsize_t                  start[EXAMPLE_DSET_DIMS];
    hsize_t                  count[EXAMPLE_DSET_DIMS];
    hid_t                    file_id;
    hid_t                    subfiling_fapl;
    hid_t                    dset_id;
    hid_t                    filespace;
    char                     filename[512];
    char                    *par_prefix;

    /*
     * Make a copy of the FAPL so we don't disturb
     * it for the other examples
     */
    subfiling_fapl = H5Pcopy(fapl_id);

    /*
     * Get a default Subfiling and IOC configuration
     */
    H5Pget_fapl_subfiling(subfiling_fapl, &subf_config);

    /*
     * Set the Subfiling stripe count so that rank
     * 0 knows how many subfiles the logical HDF5
     * file should consist of. In this case, use
     * 5 subfiles with a default stripe size of
     * 32MiB.
     */
    subf_config.shared_cfg.stripe_count = 5;

    /*
     * OPTIONAL: Set alignment of objects in HDF5 file to
     *           be equal to the Subfiling stripe size.
     *           Choosing a Subfiling stripe size and HDF5
     *           object alignment value that are some
     *           multiple of the disk block size can
     *           generally help performance by ensuring
     *           that I/O is well-aligned and doesn't
     *           excessively cross stripe boundaries.
     *
     *           Note that this option can substantially
     *           increase the size of the resulting HDF5
     *           files, so it is a good idea to keep an eye
     *           on this.
     */
    H5Pset_alignment(subfiling_fapl, 0, 1048576); /* Align to custom 1MiB stripe size */

    /* Parse any parallel prefix and create filename */
    par_prefix = getenv("HDF5_PARAPREFIX");

    snprintf(filename, sizeof(filename), "%s%s%s", par_prefix ? par_prefix : "", par_prefix ? "/" : "",
             EXAMPLE_FILE3);

    /* Set dataset dimensionality */
    dset_dims[0] = mpi_size;
    dset_dims[1] = EXAMPLE_DSET_NY;

    if (mpi_rank == 0) {
        /*
         * Make sure only this rank opens the file
         */
        H5Pset_mpi_params(subfiling_fapl, MPI_COMM_SELF, MPI_INFO_NULL);

        /*
         * Set the Subfiling VFD on our FAPL using
         * our custom configuration
         */
        H5Pset_fapl_subfiling(subfiling_fapl, &subf_config);

        /*
         * Create a new file on rank 0
         */
        file_id = H5Fcreate(filename, H5F_ACC_TRUNC, H5P_DEFAULT, subfiling_fapl);

        /*
         * Create the dataspace for the dataset. The first
         * dimension varies with the number of MPI ranks
         * while the second dimension is fixed.
         */
        filespace = H5Screate_simple(EXAMPLE_DSET_DIMS, dset_dims, NULL);

        /*
         * Create the dataset with default properties
         */
        dset_id = H5Dcreate2(file_id, EXAMPLE_DSET_NAME, EXAMPLE_DSET_DATATYPE, filespace, H5P_DEFAULT,
                             H5P_DEFAULT, H5P_DEFAULT);

        /*
         * Initialize data buffer
         */
        data = malloc(dset_dims[0] * dset_dims[1] * sizeof(EXAMPLE_DSET_C_DATATYPE));
        for (size_t i = 0; i < dset_dims[0] * dset_dims[1]; i++) {
            data[i] = i;
        }

        /*
         * Rank 0 writes to the whole dataset
         */
        H5Dwrite(dset_id, EXAMPLE_DSET_DATATYPE, H5S_BLOCK, filespace, H5P_DEFAULT, data);

        /*
         * Close/release resources.
         */

        free(data);

        H5Dclose(dset_id);
        H5Sclose(filespace);
        H5Fclose(file_id);
    }

    MPI_Barrier(MPI_COMM_WORLD);

    /*
     * Use all MPI ranks to re-open the file and
     * read back the dataset that was created
     */
    H5Pset_mpi_params(subfiling_fapl, MPI_COMM_WORLD, MPI_INFO_NULL);

    /*
     * Use the same subfiling configuration as rank 0
     * used to create the file
     */
    H5Pset_fapl_subfiling(subfiling_fapl, &subf_config);

    /*
     * Re-open the file on all ranks
     */
    file_id = H5Fopen(filename, H5F_ACC_RDONLY, subfiling_fapl);

    /*
     * Open the dataset that was created
     */
    dset_id = H5Dopen2(file_id, EXAMPLE_DSET_NAME, H5P_DEFAULT);

    /*
     * Initialize data buffer
     */
    data = malloc(dset_dims[0] * dset_dims[1] * sizeof(EXAMPLE_DSET_C_DATATYPE));

    /*
     * Read the dataset on all ranks
     */
    H5Dread(dset_id, EXAMPLE_DSET_DATATYPE, H5S_BLOCK, H5S_ALL, H5P_DEFAULT, data);

    /*
     * Close/release resources.
     */

    free(data);

    H5Dclose(dset_id);
    H5Fclose(file_id);

    cleanup(EXAMPLE_FILE3, subfiling_fapl);

    H5Pclose(subf_config.ioc_fapl_id);
    H5Pclose(subfiling_fapl);
}

int
main(int argc, char **argv)
{
    MPI_Comm comm = MPI_COMM_WORLD;
    MPI_Info info = MPI_INFO_NULL;
    hid_t    fapl_id;
    int      mpi_size;
    int      mpi_rank;
    int      mpi_thread_required = MPI_THREAD_MULTIPLE;
    int      mpi_thread_provided = 0;

    /* HDF5 Subfiling VFD requires MPI_Init_thread with MPI_THREAD_MULTIPLE */
    MPI_Init_thread(&argc, &argv, mpi_thread_required, &mpi_thread_provided);
    if (mpi_thread_provided < mpi_thread_required) {
        printf("MPI_THREAD_MULTIPLE not supported\n");
        MPI_Abort(comm, -1);
    }

    MPI_Comm_size(comm, &mpi_size);
    MPI_Comm_rank(comm, &mpi_rank);

    /*
     * Set up File Access Property List with MPI
     * parameters for the Subfiling VFD to use
     */
    fapl_id = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_mpi_params(fapl_id, comm, info);

    /* Use Subfiling VFD with default settings */
    subfiling_write_default(fapl_id, mpi_size, mpi_rank);

    /* Use Subfiling VFD with custom settings */
    subfiling_write_custom(fapl_id, mpi_size, mpi_rank);

    /*
     * Use Subfiling VFD to precreate the HDF5
     * file on MPI rank 0
     */
    subfiling_write_precreate(fapl_id, mpi_size, mpi_rank);

    H5Pclose(fapl_id);

    if (mpi_rank == 0)
        printf("PHDF5 example finished with no errors\n");

    MPI_Finalize();

    return 0;
}

#else

/* dummy program since HDF5 is not parallel-enabled */
int
main(void)
{
    printf(
        "Example program cannot run - HDF5 must be built with parallel support and Subfiling VFD support\n");
    return 0;
}

#endif /* H5_HAVE_PARALLEL && H5_HAVE_SUBFILING_VFD */
```

### `HDF5Examples/C/H5PAR/ph5example.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 * Example of using the parallel HDF5 library to access datasets.
 * Last revised: April 24, 2001.
 *
 * This program contains two parts.  In the first part, the mpi processes
 * collectively create a new parallel HDF5 file and create two fixed
 * dimension datasets in it.  Then each process writes a hyperslab into
 * each dataset in an independent mode.  All processes collectively
 * close the datasets and the file.
 * In the second part, the processes collectively open the created file
 * and the two datasets in it.  Then each process reads a hyperslab from
 * each dataset in an independent mode and prints them out.
 * All processes collectively close the datasets and the file.
 *
 * The need of requirement of parallel file prefix is that in general
 * the current working directory in which compiling is done, is not suitable
 * for parallel I/O and there is no standard pathname for parallel file
 * systems.  In some cases, the parallel file name may even needs some
 * parallel file type prefix such as: "pfs:/GF/...".  Therefore, this
 * example requires an explicit parallel file prefix.  See the usage
 * for more detail.
 */

#include <assert.h>
#include "hdf5.h"
#include <string.h>
#include <stdlib.h>

#ifdef H5_HAVE_PARALLEL
/* Temporary source code */
#define FAIL -1
/* temporary code end */

/* Define some handy debugging shorthands, routines, ... */
/* debugging tools */
#define MESG(x)                                                                                              \
    do {                                                                                                     \
        if (verbose)                                                                                         \
            printf("%s\n", x);                                                                               \
    } while (0)

#define MPI_BANNER(mesg)                                                                                     \
    do {                                                                                                     \
        printf("--------------------------------\n");                                                        \
        printf("Proc %d: ", mpi_rank);                                                                       \
        printf("*** %s\n", mesg);                                                                            \
        printf("--------------------------------\n");                                                        \
    } while (0)

#define SYNC(comm)                                                                                           \
    do {                                                                                                     \
        MPI_BANNER("doing a SYNC");                                                                          \
        MPI_Barrier(comm);                                                                                   \
        MPI_BANNER("SYNC DONE");                                                                             \
    } while (0)
/* End of Define some handy debugging shorthands, routines, ... */

/* Constants definitions */
/* 24 is a multiple of 2, 3, 4, 6, 8, 12.  Neat for parallel tests. */
#define SPACE1_DIM1  24
#define SPACE1_DIM2  24
#define SPACE1_RANK  2
#define DATASETNAME1 "Data1"
#define DATASETNAME2 "Data2"
#define DATASETNAME3 "Data3"
/* hyperslab layout styles */
#define BYROW 1 /* divide into slabs of rows */
#define BYCOL 2 /* divide into blocks of columns */

#define PARAPREFIX "HDF5_PARAPREFIX" /* file prefix environment variable name */

/* dataset data type.  Int's can be easily octo dumped. */
typedef int DATATYPE;

/* global variables */
int nerrors = 0; /* errors count */
#ifndef PATH_MAX
#define PATH_MAX 512
#endif /* !PATH_MAX */
char testfiles[2][PATH_MAX];

int mpi_size, mpi_rank; /* mpi variables */

/* option flags */
int verbose   = 0; /* verbose, default as no. */
int doread    = 1; /* read test */
int dowrite   = 1; /* write test */
int docleanup = 1; /* cleanup */

/* Prototypes */
void slab_set(hsize_t start[], hsize_t count[], hsize_t stride[], int mode);
void dataset_fill(hsize_t start[], hsize_t count[], hsize_t stride[], DATATYPE *dataset);
void dataset_print(hsize_t start[], hsize_t count[], hsize_t stride[], DATATYPE *dataset);
int  dataset_vrfy(hsize_t start[], hsize_t count[], hsize_t stride[], DATATYPE *dataset, DATATYPE *original);
void phdf5writeInd(char *filename);
void phdf5readInd(char *filename);
void phdf5writeAll(char *filename);
void phdf5readAll(char *filename);
void test_split_comm_access(char filenames[][PATH_MAX]);
int  parse_options(int argc, char **argv);
void usage(void);
int  mkfilenames(char *prefix);
void cleanup(void);

/*
 * Setup the dimensions of the hyperslab.
 * Two modes--by rows or by columns.
 * Assume dimension rank is 2.
 */
void
slab_set(hsize_t start[], hsize_t count[], hsize_t stride[], int mode)
{
    switch (mode) {
        case BYROW:
            /* Each process takes a slabs of rows. */
            stride[0] = 1;
            stride[1] = 1;
            count[0]  = SPACE1_DIM1 / mpi_size;
            count[1]  = SPACE1_DIM2;
            start[0]  = mpi_rank * count[0];
            start[1]  = 0;
            break;
        case BYCOL:
            /* Each process takes a block of columns. */
            stride[0] = 1;
            stride[1] = 1;
            count[0]  = SPACE1_DIM1;
            count[1]  = SPACE1_DIM2 / mpi_size;
            start[0]  = 0;
            start[1]  = mpi_rank * count[1];
            break;
        default:
            /* Unknown mode.  Set it to cover the whole dataset. */
            printf("unknown slab_set mode (%d)\n", mode);
            stride[0] = 1;
            stride[1] = 1;
            count[0]  = SPACE1_DIM1;
            count[1]  = SPACE1_DIM2;
            start[0]  = 0;
            start[1]  = 0;
            break;
    }
}

/*
 * Fill the dataset with trivial data for testing.
 * Assume dimension rank is 2 and data is stored contiguous.
 */
void
dataset_fill(hsize_t start[], hsize_t count[], hsize_t stride[], DATATYPE *dataset)
{
    DATATYPE *dataptr = dataset;
    hsize_t   i, j;

    /* put some trivial data in the data_array */
    for (i = 0; i < count[0]; i++) {
        for (j = 0; j < count[1]; j++) {
            *dataptr++ = (i * stride[0] + start[0]) * 100 + (j * stride[1] + start[1] + 1);
        }
    }
}

/*
 * Print the content of the dataset.
 */
void
dataset_print(hsize_t start[], hsize_t count[], hsize_t stride[], DATATYPE *dataset)
{
    DATATYPE *dataptr = dataset;
    hsize_t   i, j;

    /* print the slab read */
    for (i = 0; i < count[0]; i++) {
        printf("Row %lu: ", (unsigned long)(i * stride[0] + start[0]));
        for (j = 0; j < count[1]; j++) {
            printf("%03d ", *dataptr++);
        }
        printf("\n");
    }
}

/*
 * Print the content of the dataset.
 */
int
dataset_vrfy(hsize_t start[], hsize_t count[], hsize_t stride[], DATATYPE *dataset, DATATYPE *original)
{
#define MAX_ERR_REPORT 10 /* Maximum number of errors reported */

    hsize_t i, j;
    int     nerr;

    /* print it if verbose */
    if (verbose)
        dataset_print(start, count, stride, dataset);

    nerr = 0;
    for (i = 0; i < count[0]; i++) {
        for (j = 0; j < count[1]; j++) {
            if (*dataset++ != *original++) {
                nerr++;
                if (nerr <= MAX_ERR_REPORT) {
                    printf("Dataset Verify failed at [%lu][%lu](row %lu, col %lu): expect %d, got %d\n",
                           (unsigned long)i, (unsigned long)j, (unsigned long)(i * stride[0] + start[0]),
                           (unsigned long)(j * stride[1] + start[1]), *(dataset - 1), *(original - 1));
                }
            }
        }
    }
    if (nerr > MAX_ERR_REPORT)
        printf("[more errors ...]\n");
    if (nerr)
        printf("%d errors found in dataset_vrfy\n", nerr);
    return (nerr);
}

/*
 * Example of using the parallel HDF5 library to create two datasets
 * in one HDF5 files with parallel MPIO access support.
 * The Datasets are of sizes (number-of-mpi-processes x DIM1) x DIM2.
 * Each process controls only a slab of size DIM1 x DIM2 within each
 * dataset.
 */

void
phdf5writeInd(char *filename)
{
    hid_t    fid1;                                            /* HDF5 file IDs */
    hid_t    acc_tpl1;                                        /* File access templates */
    hid_t    sid1;                                            /* Dataspace ID */
    hid_t    file_dataspace;                                  /* File dataspace ID */
    hid_t    mem_dataspace;                                   /* memory dataspace ID */
    hid_t    dataset1, dataset2;                              /* Dataset ID */
    hsize_t  dims1[SPACE1_RANK] = {SPACE1_DIM1, SPACE1_DIM2}; /* dataspace dim sizes */
    DATATYPE data_array1[SPACE1_DIM1][SPACE1_DIM2];           /* data buffer */

    hsize_t start[SPACE1_RANK];                      /* for hyperslab setting */
    hsize_t count[SPACE1_RANK], stride[SPACE1_RANK]; /* for hyperslab setting */

    herr_t ret; /* Generic return value */

    MPI_Comm comm = MPI_COMM_WORLD;
    MPI_Info info = MPI_INFO_NULL;

    if (verbose)
        printf("Independent write test on file %s\n", filename);

    /* -------------------
     * START AN HDF5 FILE
     * -------------------*/
    /* setup file access template with parallel IO access. */
    acc_tpl1 = H5Pcreate(H5P_FILE_ACCESS);
    assert(acc_tpl1 != FAIL);
    MESG("H5Pcreate access succeed");
    /* set Parallel access with communicator */
    ret = H5Pset_fapl_mpio(acc_tpl1, comm, info);
    assert(ret != FAIL);
    MESG("H5Pset_fapl_mpio succeed");

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata reads on FAPL to perform metadata reads
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_all_coll_metadata_ops(acc_tpl1, true);

    /*
     * OPTIONAL: It is generally recommended to set collective
     *           metadata writes on FAPL to perform metadata writes
     *           collectively, which usually allows datasets
     *           to perform better at scale, although it is not
     *           strictly necessary.
     */
    H5Pset_coll_metadata_write(acc_tpl1, true);

    /* create the file collectively */
    fid1 = H5Fcreate(filename, H5F_ACC_TRUNC, H5P_DEFAULT, acc_tpl1);
    assert(fid1 != FAIL);
    MESG("H5Fcreate succeed");

    /* Release file-access template */
    ret = H5Pclose(acc_tpl1);
    assert(ret != FAIL);

    /* --------------------------
     * Define the dimensions of the overall datasets
     * and the slabs local to the MPI process.
     * ------------------------- */
    /* setup dimensionality object */
    sid1 = H5Screate_simple(SPACE1_RANK, dims1, NULL);
    assert(sid1 != FAIL);
    MESG("H5Screate_simple succeed");

    /* create a dataset collectively */
    dataset1 = H5Dcreate2(fid1, DATASETNAME1, H5T_NATIVE_INT, sid1, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    assert(dataset1 != FAIL);
    MESG("H5Dcreate2 succeed");

    /* create another dataset collectively */
    dataset2 = H5Dcreate2(fid1, DATASETNAME2, H5T_NATIVE_INT, sid1, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    assert(dataset2 != FAIL);
    MESG("H5Dcreate2 succeed");

    /* set up dimensions of the slab this process accesses */
    start[0]  = mpi_rank * SPACE1_DIM1 / mpi_size;
    start[1]  = 0;
    count[0]  = SPACE1_DIM1 / mpi_size;
    count[1]  = SPACE1_DIM2;
    stride[0] = 1;
    stride[1] = 1;
    if (verbose)
        printf("start[]=(%lu,%lu), count[]=(%lu,%lu), total datapoints=%lu\n", (unsigned long)start[0],
               (unsigned long)start[1], (unsigned long)count[0], (unsigned long)count[1],
               (unsigned long)(count[0] * count[1]));

    /* put some trivial data in the data_array */
    dataset_fill(start, count, stride, &data_array1[0][0]);
    MESG("data_array initialized");

    /* create a file dataspace independently */
    file_dataspace = H5Dget_space(dataset1);
    assert(file_dataspace != FAIL);
    MESG("H5Dget_space succeed");
    ret = H5Sselect_hyperslab(file_dataspace, H5S_SELECT_SET, start, stride, count, NULL);
    assert(ret != FAIL);
    MESG("H5Sset_hyperslab succeed");

    /* create a memory dataspace independently */
    mem_dataspace = H5Screate_simple(SPACE1_RANK, count, NULL);
    assert(mem_dataspace != FAIL);

    /* write data independently */
    ret = H5Dwrite(dataset1, H5T_NATIVE_INT, mem_dataspace, file_dataspace, H5P_DEFAULT, data_array1);
    assert(ret != FAIL);
    MESG("H5Dwrite succeed");

    /* write data independently */
    ret = H5Dwrite(dataset2, H5T_NATIVE_INT, mem_dataspace, file_dataspace, H5P_DEFAULT, data_array1);
    assert(ret != FAIL);
    MESG("H5Dwrite succeed");

    /* release dataspace ID */
    H5Sclose(file_dataspace);

    /* close dataset collectively */
    ret = H5Dclose(dataset1);
    assert(ret != FAIL);
    MESG("H5Dclose1 succeed");
    ret = H5Dclose(dataset2);
    assert(ret != FAIL);
    MESG("H5Dclose2 succeed");

    /* release all IDs created */
    H5Sclose(sid1);

    /* close the file collectively */
    H5Fclose(fid1);
}

/* Example of using the parallel HDF5 library to read a dataset */
void
phdf5readInd(char *filename)
{
    hid_t    fid1;                                   /* HDF5 file IDs */
    hid_t    acc_tpl1;                               /* File access templates */
    hid_t    file_dataspace;                         /* File dataspace ID */
    hid_t    mem_dataspace;                          /* memory dataspace ID */
    hid_t    dataset1, dataset2;                     /* Dataset ID */
    DATATYPE data_array1[SPACE1_DIM1][SPACE1_DIM2];  /* data buffer */
    DATATYPE data_origin1[SPACE1_DIM1][SPACE1_DIM2]; /* expected data buffer */

    hsize_t start[SPACE1_RANK];                      /* for hyperslab setting */
    hsize_t count[SPACE1_RANK], stride[SPACE1_RANK]; /* for hyperslab setting */

    herr_t ret; /* Generic return value */

    MPI_Comm comm = MPI_COMM_WORLD;
    MPI_Info info = MPI_INFO_NULL;

    if (verbose)
        printf("Independent read test on file %s\n", filename);

    /* setup file access template */
    acc_tpl1 = H5Pcreate(H5P_FILE_ACCESS);
    assert(acc_tpl1 != FAIL);
    /* set Parallel access with communicator */
    ret = H5Pset_fapl_mpio(acc_tpl1, comm, info);
    assert(ret != FAIL);

    /* open the file collectively */
    fid1 = H5Fopen(filename, H5F_ACC_RDWR, acc_tpl1);
    assert(fid1 != FAIL);

    /* Release file-access template */
    ret = H5Pclose(acc_tpl1);
    assert(ret != FAIL);

    /* open the dataset1 collectively */
    dataset1 = H5Dopen2(fid1, DATASETNAME1, H5P_DEFAULT);
    assert(dataset1 != FAIL);

    /* open another dataset collectively */
    dataset2 = H5Dopen2(fid1, DATASETNAME1, H5P_DEFAULT);
    assert(dataset2 != FAIL);

    /* set up dimensions of the slab this process accesses */
    start[0]  = mpi_rank * SPACE1_DIM1 / mpi_size;
    start[1]  = 0;
    count[0]  = SPACE1_DIM1 / mpi_size;
    count[1]  = SPACE1_DIM2;
    stride[0] = 1;
    stride[1] = 1;
    if (verbose)
        printf("start[]=(%lu,%lu), count[]=(%lu,%lu), total datapoints=%lu\n", (unsigned long)start[0],
               (unsigned long)start[1], (unsigned long)count[0], (unsigned long)count[1],
               (unsigned long)(count[0] * count[1]));

    /* create a file dataspace independently */
    file_dataspace = H5Dget_space(dataset1);
    assert(file_dataspace != FAIL);
    ret = H5Sselect_hyperslab(file_dataspace, H5S_SELECT_SET, start, stride, count, NULL);
    assert(ret != FAIL);

    /* create a memory dataspace independently */
    mem_dataspace = H5Screate_simple(SPACE1_RANK, count, NULL);
    assert(mem_dataspace != FAIL);

    /* fill dataset with test data */
    dataset_fill(start, count, stride, &data_origin1[0][0]);

    /* read data independently */
    ret = H5Dread(dataset1, H5T_NATIVE_INT, mem_dataspace, file_dataspace, H5P_DEFAULT, data_array1);
    assert(ret != FAIL);

    /* verify the read data with original expected data */
    ret = dataset_vrfy(start, count, stride, &data_array1[0][0], &data_origin1[0][0]);
    assert(ret != FAIL);

    /* read data independently */
    ret = H5Dread(dataset2, H5T_NATIVE_INT, mem_dataspace, file_dataspace, H5P_DEFAULT, data_array1);
    assert(ret != FAIL);

    /* verify the read data with original expected data */
    ret = dataset_vrfy(start, count, stride, &data_array1[0][0], &data_origin1[0][0]);
    assert(ret == 0);

    /* close dataset collectively */
    ret = H5Dclose(dataset1);
    assert(ret != FAIL);
    ret = H5Dclose(dataset2);
    assert(ret != FAIL);

    /* release all IDs created */
    H5Sclose(file_dataspace);

    /* close the file collectively */
    H5Fclose(fid1);
}

/*
 * Example of using the parallel HDF5 library to create two datasets
 * in one HDF5 file with collective parallel access support.
 * The Datasets are of sizes (number-of-mpi-processes x DIM1) x DIM2.
 * Each process controls only a slab of size DIM1 x DIM2 within each
 * dataset. [Note: not so yet.  Datasets are of sizes DIM1xDIM2 and
 * each process controls a hyperslab within.]
 */

void
phdf5writeAll(char *filename)
{
    hid_t    fid1;                                            /* HDF5 file IDs */
    hid_t    acc_tpl1;                                        /* File access templates */
    hid_t    xfer_plist;                                      /* Dataset transfer properties list */
    hid_t    sid1;                                            /* Dataspace ID */
    hid_t    file_dataspace;                                  /* File dataspace ID */
    hid_t    mem_dataspace;                                   /* memory dataspace ID */
    hid_t    dataset1, dataset2;                              /* Dataset ID */
    hsize_t  dims1[SPACE1_RANK] = {SPACE1_DIM1, SPACE1_DIM2}; /* dataspace dim sizes */
    DATATYPE data_array1[SPACE1_DIM1][SPACE1_DIM2];           /* data buffer */

    hsize_t start[SPACE1_RANK];                      /* for hyperslab setting */
    hsize_t count[SPACE1_RANK], stride[SPACE1_RANK]; /* for hyperslab setting */

    herr_t ret; /* Generic return value */

    MPI_Comm comm = MPI_COMM_WORLD;
    MPI_Info info = MPI_INFO_NULL;

    if (verbose)
        printf("Collective write test on file %s\n", filename);

    /* -------------------
     * START AN HDF5 FILE
     * -------------------*/
    /* setup file access template with parallel IO access. */
    acc_tpl1 = H5Pcreate(H5P_FILE_ACCESS);
    assert(acc_tpl1 != FAIL);
    MESG("H5Pcreate access succeed");
    /* set Parallel access with communicator */
    ret = H5Pset_fapl_mpio(acc_tpl1, comm, info);
    assert(ret != FAIL);
    MESG("H5Pset_fapl_mpio succeed");

    /* create the file collectively */
    fid1 = H5Fcreate(filename, H5F_ACC_TRUNC, H5P_DEFAULT, acc_tpl1);
    assert(fid1 != FAIL);
    MESG("H5Fcreate succeed");

    /* Release file-access template */
    ret = H5Pclose(acc_tpl1);
    assert(ret != FAIL);

    /* --------------------------
     * Define the dimensions of the overall datasets
     * and create the dataset
     * ------------------------- */
    /* setup dimensionality object */
    sid1 = H5Screate_simple(SPACE1_RANK, dims1, NULL);
    assert(sid1 != FAIL);
    MESG("H5Screate_simple succeed");

    /* create a dataset collectively */
    dataset1 = H5Dcreate2(fid1, DATASETNAME1, H5T_NATIVE_INT, sid1, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    assert(dataset1 != FAIL);
    MESG("H5Dcreate2 succeed");

    /* create another dataset collectively */
    dataset2 = H5Dcreate2(fid1, DATASETNAME2, H5T_NATIVE_INT, sid1, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    assert(dataset2 != FAIL);
    MESG("H5Dcreate2 2 succeed");

    /*
     * Set up dimensions of the slab this process accesses.
     */

    /* Dataset1: each process takes a block of rows. */
    slab_set(start, count, stride, BYROW);
    if (verbose)
        printf("start[]=(%lu,%lu), count[]=(%lu,%lu), total datapoints=%lu\n", (unsigned long)start[0],
               (unsigned long)start[1], (unsigned long)count[0], (unsigned long)count[1],
               (unsigned long)(count[0] * count[1]));

    /* create a file dataspace independently */
    file_dataspace = H5Dget_space(dataset1);
    assert(file_dataspace != FAIL);
    MESG("H5Dget_space succeed");
    ret = H5Sselect_hyperslab(file_dataspace, H5S_SELECT_SET, start, stride, count, NULL);
    assert(ret != FAIL);
    MESG("H5Sset_hyperslab succeed");

    /* create a memory dataspace independently */
    mem_dataspace = H5Screate_simple(SPACE1_RANK, count, NULL);
    assert(mem_dataspace != FAIL);

    /* fill the local slab with some trivial data */
    dataset_fill(start, count, stride, &data_array1[0][0]);
    MESG("data_array initialized");
    if (verbose) {
        MESG("data_array created");
        dataset_print(start, count, stride, &data_array1[0][0]);
    }

    /* set up the collective transfer properties list */
    xfer_plist = H5Pcreate(H5P_DATASET_XFER);
    assert(xfer_plist != FAIL);
    ret = H5Pset_dxpl_mpio(xfer_plist, H5FD_MPIO_COLLECTIVE);
    assert(ret != FAIL);
    MESG("H5Pcreate xfer succeed");

    /* write data collectively */
    ret = H5Dwrite(dataset1, H5T_NATIVE_INT, mem_dataspace, file_dataspace, xfer_plist, data_array1);
    assert(ret != FAIL);
    MESG("H5Dwrite succeed");

    /* release all temporary handles. */
    /* Could have used them for dataset2 but it is cleaner */
    /* to create them again.*/
    H5Sclose(file_dataspace);
    H5Sclose(mem_dataspace);
    H5Pclose(xfer_plist);

    /* Dataset2: each process takes a block of columns. */
    slab_set(start, count, stride, BYCOL);
    if (verbose)
        printf("start[]=(%lu,%lu), count[]=(%lu,%lu), total datapoints=%lu\n", (unsigned long)start[0],
               (unsigned long)start[1], (unsigned long)count[0], (unsigned long)count[1],
               (unsigned long)(count[0] * count[1]));

    /* put some trivial data in the data_array */
    dataset_fill(start, count, stride, &data_array1[0][0]);
    MESG("data_array initialized");
    if (verbose) {
        MESG("data_array created");
        dataset_print(start, count, stride, &data_array1[0][0]);
    }

    /* create a file dataspace independently */
    file_dataspace = H5Dget_space(dataset1);
    assert(file_dataspace != FAIL);
    MESG("H5Dget_space succeed");
    ret = H5Sselect_hyperslab(file_dataspace, H5S_SELECT_SET, start, stride, count, NULL);
    assert(ret != FAIL);
    MESG("H5Sset_hyperslab succeed");

    /* create a memory dataspace independently */
    mem_dataspace = H5Screate_simple(SPACE1_RANK, count, NULL);
    assert(mem_dataspace != FAIL);

    /* fill the local slab with some trivial data */
    dataset_fill(start, count, stride, &data_array1[0][0]);
    MESG("data_array initialized");
    if (verbose) {
        MESG("data_array created");
        dataset_print(start, count, stride, &data_array1[0][0]);
    }

    /* set up the collective transfer properties list */
    xfer_plist = H5Pcreate(H5P_DATASET_XFER);
    assert(xfer_plist != FAIL);
    ret = H5Pset_dxpl_mpio(xfer_plist, H5FD_MPIO_COLLECTIVE);
    assert(ret != FAIL);
    MESG("H5Pcreate xfer succeed");

    /* write data independently */
    ret = H5Dwrite(dataset2, H5T_NATIVE_INT, mem_dataspace, file_dataspace, xfer_plist, data_array1);
    assert(ret != FAIL);
    MESG("H5Dwrite succeed");

    /* release all temporary handles. */
    H5Sclose(file_dataspace);
    H5Sclose(mem_dataspace);
    H5Pclose(xfer_plist);

    /*
     * All writes completed.  Close datasets collectively
     */
    ret = H5Dclose(dataset1);
    assert(ret != FAIL);
    MESG("H5Dclose1 succeed");
    ret = H5Dclose(dataset2);
    assert(ret != FAIL);
    MESG("H5Dclose2 succeed");

    /* release all IDs created */
    H5Sclose(sid1);

    /* close the file collectively */
    H5Fclose(fid1);
}

/*
 * Example of using the parallel HDF5 library to read two datasets
 * in one HDF5 file with collective parallel access support.
 * The Datasets are of sizes (number-of-mpi-processes x DIM1) x DIM2.
 * Each process controls only a slab of size DIM1 x DIM2 within each
 * dataset. [Note: not so yet.  Datasets are of sizes DIM1xDIM2 and
 * each process controls a hyperslab within.]
 */

void
phdf5readAll(char *filename)
{
    hid_t    fid1;                                   /* HDF5 file IDs */
    hid_t    acc_tpl1;                               /* File access templates */
    hid_t    xfer_plist;                             /* Dataset transfer properties list */
    hid_t    file_dataspace;                         /* File dataspace ID */
    hid_t    mem_dataspace;                          /* memory dataspace ID */
    hid_t    dataset1, dataset2;                     /* Dataset ID */
    DATATYPE data_array1[SPACE1_DIM1][SPACE1_DIM2];  /* data buffer */
    DATATYPE data_origin1[SPACE1_DIM1][SPACE1_DIM2]; /* expected data buffer */

    hsize_t start[SPACE1_RANK];                      /* for hyperslab setting */
    hsize_t count[SPACE1_RANK], stride[SPACE1_RANK]; /* for hyperslab setting */

    herr_t ret; /* Generic return value */

    MPI_Comm comm = MPI_COMM_WORLD;
    MPI_Info info = MPI_INFO_NULL;

    if (verbose)
        printf("Collective read test on file %s\n", filename);

    /* -------------------
     * OPEN AN HDF5 FILE
     * -------------------*/
    /* setup file access template with parallel IO access. */
    acc_tpl1 = H5Pcreate(H5P_FILE_ACCESS);
    assert(acc_tpl1 != FAIL);
    MESG("H5Pcreate access succeed");
    /* set Parallel access with communicator */
    ret = H5Pset_fapl_mpio(acc_tpl1, comm, info);
    assert(ret != FAIL);
    MESG("H5Pset_fapl_mpio succeed");

    /* open the file collectively */
    fid1 = H5Fopen(filename, H5F_ACC_RDWR, acc_tpl1);
    assert(fid1 != FAIL);
    MESG("H5Fopen succeed");

    /* Release file-access template */
    ret = H5Pclose(acc_tpl1);
    assert(ret != FAIL);

    /* --------------------------
     * Open the datasets in it
     * ------------------------- */
    /* open the dataset1 collectively */
    dataset1 = H5Dopen2(fid1, DATASETNAME1, H5P_DEFAULT);
    assert(dataset1 != FAIL);
    MESG("H5Dopen2 succeed");

    /* open another dataset collectively */
    dataset2 = H5Dopen2(fid1, DATASETNAME1, H5P_DEFAULT);
    assert(dataset2 != FAIL);
    MESG("H5Dopen2 2 succeed");

    /*
     * Set up dimensions of the slab this process accesses.
     */

    /* Dataset1: each process takes a block of columns. */
    slab_set(start, count, stride, BYCOL);
    if (verbose)
        printf("start[]=(%lu,%lu), count[]=(%lu,%lu), total datapoints=%lu\n", (unsigned long)start[0],
               (unsigned long)start[1], (unsigned long)count[0], (unsigned long)count[1],
               (unsigned long)(count[0] * count[1]));

    /* create a file dataspace independently */
    file_dataspace = H5Dget_space(dataset1);
    assert(file_dataspace != FAIL);
    MESG("H5Dget_space succeed");
    ret = H5Sselect_hyperslab(file_dataspace, H5S_SELECT_SET, start, stride, count, NULL);
    assert(ret != FAIL);
    MESG("H5Sset_hyperslab succeed");

    /* create a memory dataspace independently */
    mem_dataspace = H5Screate_simple(SPACE1_RANK, count, NULL);
    assert(mem_dataspace != FAIL);

    /* fill dataset with test data */
    dataset_fill(start, count, stride, &data_origin1[0][0]);
    MESG("data_array initialized");
    if (verbose) {
        MESG("data_array created");
        dataset_print(start, count, stride, &data_array1[0][0]);
    }

    /* set up the collective transfer properties list */
    xfer_plist = H5Pcreate(H5P_DATASET_XFER);
    assert(xfer_plist != FAIL);
    ret = H5Pset_dxpl_mpio(xfer_plist, H5FD_MPIO_COLLECTIVE);
    assert(ret != FAIL);
    MESG("H5Pcreate xfer succeed");

    /* read data collectively */
    ret = H5Dread(dataset1, H5T_NATIVE_INT, mem_dataspace, file_dataspace, xfer_plist, data_array1);
    assert(ret != FAIL);
    MESG("H5Dread succeed");

    /* verify the read data with original expected data */
    ret = dataset_vrfy(start, count, stride, &data_array1[0][0], &data_origin1[0][0]);
    assert(ret != FAIL);

    /* release all temporary handles. */
    /* Could have used them for dataset2 but it is cleaner */
    /* to create them again.*/
    H5Sclose(file_dataspace);
    H5Sclose(mem_dataspace);
    H5Pclose(xfer_plist);

    /* Dataset2: each process takes a block of rows. */
    slab_set(start, count, stride, BYROW);
    if (verbose)
        printf("start[]=(%lu,%lu), count[]=(%lu,%lu), total datapoints=%lu\n", (unsigned long)start[0],
               (unsigned long)start[1], (unsigned long)count[0], (unsigned long)count[1],
               (unsigned long)(count[0] * count[1]));

    /* create a file dataspace independently */
    file_dataspace = H5Dget_space(dataset1);
    assert(file_dataspace != FAIL);
    MESG("H5Dget_space succeed");
    ret = H5Sselect_hyperslab(file_dataspace, H5S_SELECT_SET, start, stride, count, NULL);
    assert(ret != FAIL);
    MESG("H5Sset_hyperslab succeed");

    /* create a memory dataspace independently */
    mem_dataspace = H5Screate_simple(SPACE1_RANK, count, NULL);
    assert(mem_dataspace != FAIL);

    /* fill dataset with test data */
    dataset_fill(start, count, stride, &data_origin1[0][0]);
    MESG("data_array initialized");
    if (verbose) {
        MESG("data_array created");
        dataset_print(start, count, stride, &data_array1[0][0]);
    }

    /* set up the collective transfer properties list */
    xfer_plist = H5Pcreate(H5P_DATASET_XFER);
    assert(xfer_plist != FAIL);
    ret = H5Pset_dxpl_mpio(xfer_plist, H5FD_MPIO_COLLECTIVE);
    assert(ret != FAIL);
    MESG("H5Pcreate xfer succeed");

    /* read data independently */
    ret = H5Dread(dataset2, H5T_NATIVE_INT, mem_dataspace, file_dataspace, xfer_plist, data_array1);
    assert(ret != FAIL);
    MESG("H5Dread succeed");

    /* verify the read data with original expected data */
    ret = dataset_vrfy(start, count, stride, &data_array1[0][0], &data_origin1[0][0]);
    assert(ret != FAIL);

    /* release all temporary handles. */
    H5Sclose(file_dataspace);
    H5Sclose(mem_dataspace);
    H5Pclose(xfer_plist);

    /*
     * All reads completed.  Close datasets collectively
     */
    ret = H5Dclose(dataset1);
    assert(ret != FAIL);
    MESG("H5Dclose1 succeed");
    ret = H5Dclose(dataset2);
    assert(ret != FAIL);
    MESG("H5Dclose2 succeed");

    /* close the file collectively */
    H5Fclose(fid1);
}

/*
 * test file access by communicator besides COMM_WORLD.
 * Split COMM_WORLD into two, one (even_comm) contains the original
 * processes of even ranks.  The other (odd_comm) contains the original
 * processes of odd ranks.  Processes in even_comm creates a file, then
 * cloose it, using even_comm.  Processes in old_comm just do a barrier
 * using odd_comm.  Then they all do a barrier using COMM_WORLD.
 * If the file creation and cloose does not do correct collective action
 * according to the communicator argument, the processes will freeze up
 * sooner or later due to barrier mixed up.
 */
void
test_split_comm_access(char filenames[][PATH_MAX])
{
    MPI_Comm comm;
    MPI_Info info = MPI_INFO_NULL;
    int      color, mrc;
    int      newrank, newprocs;
    hid_t    fid;     /* file IDs */
    hid_t    acc_tpl; /* File access properties */
    herr_t   ret;     /* generic return value */

    if (verbose)
        printf("Independent write test on file %s %s\n", filenames[0], filenames[1]);

    color = mpi_rank % 2;
    mrc   = MPI_Comm_split(MPI_COMM_WORLD, color, mpi_rank, &comm);
    assert(mrc == MPI_SUCCESS);
    MPI_Comm_size(comm, &newprocs);
    MPI_Comm_rank(comm, &newrank);

    if (color) {
        /* odd-rank processes */
        mrc = MPI_Barrier(comm);
        assert(mrc == MPI_SUCCESS);
    }
    else {
        /* even-rank processes */
        /* setup file access template */
        acc_tpl = H5Pcreate(H5P_FILE_ACCESS);
        assert(acc_tpl != FAIL);

        /* set Parallel access with communicator */
        ret = H5Pset_fapl_mpio(acc_tpl, comm, info);
        assert(ret != FAIL);

        /* create the file collectively */
        fid = H5Fcreate(filenames[color], H5F_ACC_TRUNC, H5P_DEFAULT, acc_tpl);
        assert(fid != FAIL);
        MESG("H5Fcreate succeed");

        /* Release file-access template */
        ret = H5Pclose(acc_tpl);
        assert(ret != FAIL);

        ret = H5Fclose(fid);
        assert(ret != FAIL);
    }
    if (mpi_rank == 0) {
        mrc = MPI_File_delete(filenames[color], info);
        assert(mrc == MPI_SUCCESS);
    }
    MPI_Comm_free(&comm);
}

/*
 * Show command usage
 */
void
usage(void)
{
    printf("Usage: testphdf5 [-f <prefix>] [-r] [-w] [-v]\n");
    printf("\t-f\tfile prefix for parallel test files.\n");
    printf("\t  \t e.g. pfs:/PFS/myname\n");
    printf("\t  \tcan be set via $" PARAPREFIX ".\n");
    printf("\t  \tDefault is current directory.\n");
    printf("\t-c\tno cleanup\n");
    printf("\t-r\tno read\n");
    printf("\t-w\tno write\n");
    printf("\t-v\tverbose on\n");
    printf("\tdefault do write then read\n");
    printf("\n");
}

/*
 * compose the test filename with the prefix supplied.
 * return code: 0 if no error
 *              1 otherwise.
 */
int
mkfilenames(char *prefix)
{
    int    i, n;
    size_t strsize;

    /* filename will be prefix/ParaEgN.h5 where N is 0 to 9. */
    /* So, string must be big enough to hold the prefix, / and 10 more chars */
    /* and the terminating null. */
    strsize = strlen(prefix) + 12;
    if (strsize > PATH_MAX) {
        printf("File prefix too long;  Use a short path name.\n");
        return (1);
    }
    n = sizeof(testfiles) / sizeof(testfiles[0]);
    if (n > 9) {
        printf("Warning: Too many entries in testfiles. "
               "Need to adjust the code to accommodate the large size.\n");
    }
    for (i = 0; i < n; i++) {
        snprintf(testfiles[i], PATH_MAX, "%s/ParaEg%d.h5", prefix, i);
    }
    return (0);
}

/*
 * parse the command line options
 */
int
parse_options(int argc, char **argv)
{
    int i, n;

    /* initialize testfiles to nulls */
    n = sizeof(testfiles) / sizeof(testfiles[0]);
    for (i = 0; i < n; i++) {
        testfiles[i][0] = '\0';
    }

    while (--argc) {
        if (**(++argv) != '-') {
            break;
        }
        else {
            switch (*(*argv + 1)) {
                case 'f':
                    ++argv;
                    if (--argc < 1) {
                        usage();
                        nerrors++;
                        return (1);
                    }
                    if (mkfilenames(*argv)) {
                        nerrors++;
                        return (1);
                    }
                    break;
                case 'c':
                    docleanup = 0; /* no cleanup */
                    break;
                case 'r':
                    doread = 0;
                    break;
                case 'w':
                    dowrite = 0;
                    break;
                case 'v':
                    verbose = 1;
                    break;
                default:
                    usage();
                    nerrors++;
                    return (1);
            }
        }
    }

    /* check the file prefix */
    if (testfiles[0][0] == '\0') {
        /* try get it from environment variable HDF5_PARAPREFIX */
        char *env;
        char *env_default = "."; /* default to current directory */
        if ((env = getenv(PARAPREFIX)) == NULL) {
            env = env_default;
        }
        mkfilenames(env);
    }
    return (0);
}

/*
 * cleanup test files created
 */
void
cleanup(void)
{
    int i, n;

    n = sizeof(testfiles) / sizeof(testfiles[0]);
    for (i = 0; i < n; i++) {
        MPI_File_delete(testfiles[i], MPI_INFO_NULL);
    }
}

/* Main Program */
int
main(int argc, char **argv)
{
    int  mpi_namelen;
    char mpi_name[MPI_MAX_PROCESSOR_NAME];
    int  i, n;

    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);
    MPI_Get_processor_name(mpi_name, &mpi_namelen);
    /* Make sure datasets can be divided into equal chunks by the processes */
    if ((SPACE1_DIM1 % mpi_size) || (SPACE1_DIM2 % mpi_size)) {
        printf("DIM1(%d) and DIM2(%d) must be multiples of processes (%d)\n", SPACE1_DIM1, SPACE1_DIM2,
               mpi_size);
        nerrors++;
        goto finish;
    }

    if (parse_options(argc, argv) != 0)
        goto finish;

    /* show test file names */
    if (mpi_rank == 0) {
        n = sizeof(testfiles) / sizeof(testfiles[0]);
        printf("Parallel test files are:\n");
        for (i = 0; i < n; i++) {
            printf("   %s\n", testfiles[i]);
        }
    }

    if (dowrite) {
        MPI_BANNER("testing PHDF5 dataset using split communicators...");
        test_split_comm_access(testfiles);
        MPI_BANNER("testing PHDF5 dataset independent write...");
        phdf5writeInd(testfiles[0]);
        MPI_BANNER("testing PHDF5 dataset collective write...");
        phdf5writeAll(testfiles[1]);
    }
    if (doread) {
        MPI_BANNER("testing PHDF5 dataset independent read...");
        phdf5readInd(testfiles[0]);
        MPI_BANNER("testing PHDF5 dataset collective read...");
        phdf5readAll(testfiles[1]);
    }

    if (!(dowrite || doread)) {
        usage();
        nerrors++;
    }

finish:
    if (mpi_rank == 0) { /* only process 0 reports */
        if (nerrors)
            printf("***PHDF5 example detected %d errors***\n", nerrors);
        else {
            printf("=====================================\n");
            printf("PHDF5 example finished with no errors\n");
            printf("=====================================\n");
        }
    }
    if (docleanup)
        cleanup();
    MPI_Finalize();

    return (nerrors);
}

#else  /* H5_HAVE_PARALLEL */
/* dummy program since H5_HAVE_PARALLE is not configured in */
int
main(void)
{
    printf("No PHDF5 example because parallel is not configured in\n");
    return (0);
}
#endif /* H5_HAVE_PARALLEL */
```

### `HDF5Examples/C/H5T/16/h5ex_t_array.c`

```c
/************************************************************

  This example shows how to read and write array datatypes
  to a dataset.  The program first writes integers arrays of
  dimension ADIM0xADIM1 to a dataset with a dataspace of
  DIM0, then closes the  file.  Next, it reopens the file,
  reads back the data, and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_array.h5"
#define DATASET  "DS1"
#define DIM0     4
#define ADIM0    3
#define ADIM1    5

int
main(void)
{
    hid_t file, filetype, memtype, space, dset;
    /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0}, adims[2] = {ADIM0, ADIM1};
    int     wdata[DIM0][ADIM0][ADIM1], /* Write buffer */
        ***rdata,                      /* Read buffer */
        ndims, i, j, k;

    /*
     * Initialize data.  i is the element in the dataspace, j and k the
     * elements within the array datatype.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < ADIM0; j++)
            for (k = 0; k < ADIM1; k++)
                wdata[i][j][k] = i * j - j * k + i * k;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create array datatypes for file and memory.
     */
    filetype = H5Tarray_create(H5T_STD_I64LE, 2, adims, NULL);
    memtype  = H5Tarray_create(H5T_NATIVE_INT, 2, adims, NULL);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the array data to it.
     */
    dset   = H5Dcreate(file, DATASET, filetype, space, H5P_DEFAULT);
    status = H5Dwrite(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0][0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset and array have the same name and rank, but can have
     * any size.  Therefore we must allocate a new array to read in
     * data using malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get the datatype and its dimensions.
     */
    filetype = H5Dget_type(dset);
    ndims    = H5Tget_array_dims(filetype, adims, NULL);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * three dimensional dataset when the array datatype is included so
     * the dynamic allocation must be done in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to two-dimensional arrays (the
     * elements of the dataset.
     */
    rdata = (int ***)malloc(dims[0] * sizeof(int **));

    /*
     * Allocate two dimensional array of pointers to rows in the data
     * elements.
     */
    rdata[0] = (int **)malloc(dims[0] * adims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0][0] = (int *)malloc(dims[0] * adims[0] * adims[1] * sizeof(int));

    /*
     * Set the members of the pointer arrays allocated above to point
     * to the correct locations in their respective arrays.
     */
    for (i = 0; i < dims[0]; i++) {
        rdata[i] = rdata[0] + i * adims[0];
        for (j = 0; j < adims[0]; j++)
            rdata[i][j] = rdata[0][0] + (adims[0] * adims[1] * i) + (adims[1] * j);
    }

    /*
     * Create the memory datatype.
     */
    memtype = H5Tarray_create(H5T_NATIVE_INT, 2, adims, NULL);

    /*
     * Read the data.
     */
    status = H5Dread(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0][0]);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%d]:\n", DATASET, i);
        for (j = 0; j < adims[0]; j++) {
            printf(" [");
            for (k = 0; k < adims[1]; k++)
                printf(" %3d", rdata[i][j][k]);
            printf("]\n");
        }
        printf("\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0][0]);
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_arrayatt.c`

```c
/************************************************************

  This example shows how to read and write array datatypes
  to an attribute.  The program first writes integers arrays
  of dimension ADIM0xADIM1 to an attribute with a dataspace
  of DIM0, then closes the  file.  Next, it reopens the
  file, reads back the data, and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_arrayatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4
#define ADIM0     3
#define ADIM1     5

int
main(void)
{
    hid_t file, filetype, memtype, space, dset, attr;
    /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0}, adims[2] = {ADIM0, ADIM1};
    int     wdata[DIM0][ADIM0][ADIM1], /* Write buffer */
        ***rdata,                      /* Read buffer */
        ndims, i, j, k;

    /*
     * Initialize data.  i is the element in the dataspace, j and k the
     * elements within the array datatype.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < ADIM0; j++)
            for (k = 0; k < ADIM1; k++)
                wdata[i][j][k] = i * j - j * k + i * k;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create array datatypes for file and memory.
     */
    filetype = H5Tarray_create(H5T_STD_I64LE, 2, adims, NULL);
    memtype  = H5Tarray_create(H5T_NATIVE_INT, 2, adims, NULL);

    /*
     * Create dataset with a scalar dataspace.
     */
    space  = H5Screate(H5S_SCALAR);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the array data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, filetype, space, H5P_DEFAULT);
    status = H5Awrite(attr, memtype, wdata[0][0]);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute and array have the same name and rank, but can
     * have any size.  Therefore we must allocate a new array to read
     * in data using malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);
    attr = H5Aopen_name(dset, ATTRIBUTE);

    /*
     * Get the datatype and its dimensions.
     */
    filetype = H5Aget_type(attr);
    ndims    = H5Tget_array_dims(filetype, adims, NULL);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * three dimensional attribute when the array datatype is included
     * so the dynamic allocation must be done in steps.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to two-dimensional arrays (the
     * elements of the attribute.
     */
    rdata = (int ***)malloc(dims[0] * sizeof(int **));

    /*
     * Allocate two dimensional array of pointers to rows in the data
     * elements.
     */
    rdata[0] = (int **)malloc(dims[0] * adims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0][0] = (int *)malloc(dims[0] * adims[0] * adims[1] * sizeof(int));

    /*
     * Set the members of the pointer arrays allocated above to point
     * to the correct locations in their respective arrays.
     */
    for (i = 0; i < dims[0]; i++) {
        rdata[i] = rdata[0] + i * adims[0];
        for (j = 0; j < adims[0]; j++)
            rdata[i][j] = rdata[0][0] + (adims[0] * adims[1] * i) + (adims[1] * j);
    }

    /*
     * Create the memory datatype.
     */
    memtype = H5Tarray_create(H5T_NATIVE_INT, 2, adims, NULL);

    /*
     * Read the data.
     */
    status = H5Aread(attr, memtype, rdata[0][0]);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%d]:\n", ATTRIBUTE, i);
        for (j = 0; j < adims[0]; j++) {
            printf(" [");
            for (k = 0; k < adims[1]; k++)
                printf(" %3d", rdata[i][j][k]);
            printf("]\n");
        }
        printf("\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0][0]);
    free(rdata[0]);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_bit.c`

```c
/************************************************************

  This example shows how to read and write bitfield
  datatypes to a dataset.  The program first writes bit
  fields to a dataset with a dataspace of DIM0xDIM1, then
  closes the file.  Next, it reopens the file, reads back
  the data, and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_bit.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7

int
main(void)
{
    hid_t         file, space, dset; /* Handles */
    herr_t        status;
    hsize_t       dims[2] = {DIM0, DIM1};
    unsigned char wdata[DIM0][DIM1], /* Write buffer */
        **rdata;                     /* Read buffer */
    int ndims, A, B, C, D, i, j;

    /*
     * Initialize data.  We will manually pack 4 2-bit integers into
     * each unsigned char data element.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            wdata[i][j] = 0;
            wdata[i][j] |= (i * j - j) & 0x03;    /* Field "A" */
            wdata[i][j] |= (i & 0x03) << 2;       /* Field "B" */
            wdata[i][j] |= (j & 0x03) << 4;       /* Field "C" */
            wdata[i][j] |= ((i + j) & 0x03) << 6; /* Field "D" */
        }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset and write the bitfield data to it.
     */
    dset   = H5Dcreate(file, DATASET, H5T_STD_B8BE, space, H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_NATIVE_B8, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (unsigned char **)malloc(dims[0] * sizeof(unsigned char *));

    /*
     * Allocate space for bitfield data.
     */
    rdata[0] = (unsigned char *)malloc(dims[0] * dims[1] * sizeof(unsigned char));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Dread(dset, H5T_NATIVE_B8, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++) {
            A = rdata[i][j] & 0x03;        /* Retrieve field "A" */
            B = (rdata[i][j] >> 2) & 0x03; /* Retrieve field "B" */
            C = (rdata[i][j] >> 4) & 0x03; /* Retrieve field "C" */
            D = (rdata[i][j] >> 6) & 0x03; /* Retrieve field "D" */
            printf(" {%d, %d, %d, %d}", A, B, C, D);
        }
        printf(" ]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_bitatt.c`

```c
/************************************************************

  This example shows how to read and write bitfield
  datatypes to an attribute.  The program first writes bit
  fields to an attribute with a dataspace of DIM0xDIM1, then
  closes the file.  Next, it reopens the file, reads back
  the data, and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_bitatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4
#define DIM1      7

int
main(void)
{
    hid_t         file, space, dset, attr; /* Handles */
    herr_t        status;
    hsize_t       dims[2] = {DIM0, DIM1};
    unsigned char wdata[DIM0][DIM1], /* Write buffer */
        **rdata;                     /* Read buffer */
    int ndims, A, B, C, D, i, j;

    /*
     * Initialize data.  We will manually pack 4 2-bit integers into
     * each unsigned char data element.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            wdata[i][j] = 0;
            wdata[i][j] |= (i * j - j) & 0x03;    /* Field "A" */
            wdata[i][j] |= (i & 0x03) << 2;       /* Field "B" */
            wdata[i][j] |= (j & 0x03) << 4;       /* Field "C" */
            wdata[i][j] |= ((i + j) & 0x03) << 6; /* Field "D" */
        }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataset with a scalar dataspace.
     */
    space  = H5Screate(H5S_SCALAR);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the attribute and write the bitfield data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, H5T_STD_B8BE, space, H5P_DEFAULT);
    status = H5Awrite(attr, H5T_NATIVE_B8, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);
    attr = H5Aopen_name(dset, ATTRIBUTE);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional attribute so the dynamic allocation must be done
     * in steps.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (unsigned char **)malloc(dims[0] * sizeof(unsigned char *));

    /*
     * Allocate space for bitfield data.
     */
    rdata[0] = (unsigned char *)malloc(dims[0] * dims[1] * sizeof(unsigned char));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Aread(attr, H5T_NATIVE_B8, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", ATTRIBUTE);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++) {
            A = rdata[i][j] & 0x03;        /* Retrieve field "A" */
            B = (rdata[i][j] >> 2) & 0x03; /* Retrieve field "B" */
            C = (rdata[i][j] >> 4) & 0x03; /* Retrieve field "C" */
            D = (rdata[i][j] >> 6) & 0x03; /* Retrieve field "D" */
            printf(" {%d, %d, %d, %d}", A, B, C, D);
        }
        printf(" ]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_cmpd.c`

```c
/************************************************************

  This example shows how to read and write compound
  datatypes to a dataset.  The program first writes
  compound structures to a dataset with a dataspace of DIM0,
  then closes the file.  Next, it reopens the file, reads
  back the data, and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_cmpd.h5"
#define DATASET  "DS1"
#define DIM0     4

typedef struct {
    int    serial_no;
    char  *location;
    double temperature;
    double pressure;
} sensor_t; /* Compound type */

int
main(void)
{
    hid_t file, filetype, memtype, strtype, space, dset;
    /* Handles */
    herr_t   status;
    hsize_t  dims[1] = {DIM0};
    sensor_t wdata[DIM0], /* Write buffer */
        *rdata;           /* Read buffer */
    int ndims, i;

    /*
     * Initialize data.
     */
    wdata[0].serial_no   = 1153;
    wdata[0].location    = "Exterior (static)";
    wdata[0].temperature = 53.23;
    wdata[0].pressure    = 24.57;
    wdata[1].serial_no   = 1184;
    wdata[1].location    = "Intake";
    wdata[1].temperature = 55.12;
    wdata[1].pressure    = 22.95;
    wdata[2].serial_no   = 1027;
    wdata[2].location    = "Intake manifold";
    wdata[2].temperature = 103.55;
    wdata[2].pressure    = 31.23;
    wdata[3].serial_no   = 1313;
    wdata[3].location    = "Exhaust manifold";
    wdata[3].temperature = 1252.89;
    wdata[3].pressure    = 84.11;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the compound datatype for memory.
     */
    memtype = H5Tcreate(H5T_COMPOUND, sizeof(sensor_t));
    status  = H5Tinsert(memtype, "Serial number", HOFFSET(sensor_t, serial_no), H5T_NATIVE_INT);
    status  = H5Tinsert(memtype, "Location", HOFFSET(sensor_t, location), strtype);
    status  = H5Tinsert(memtype, "Temperature (F)", HOFFSET(sensor_t, temperature), H5T_NATIVE_DOUBLE);
    status  = H5Tinsert(memtype, "Pressure (inHg)", HOFFSET(sensor_t, pressure), H5T_NATIVE_DOUBLE);

    /*
     * Create the compound datatype for the file.  Because the standard
     * types we are using for the file may have different sizes than
     * the corresponding native types, we must manually calculate the
     * offset of each member.
     */
    filetype = H5Tcreate(H5T_COMPOUND, 8 + sizeof(hvl_t) + 8 + 8);
    status   = H5Tinsert(filetype, "Serial number", 0, H5T_STD_I64BE);
    status   = H5Tinsert(filetype, "Location", 8, strtype);
    status   = H5Tinsert(filetype, "Temperature (F)", 8 + sizeof(hvl_t), H5T_IEEE_F64BE);
    status   = H5Tinsert(filetype, "Pressure (inHg)", 8 + sizeof(hvl_t) + 8, H5T_IEEE_F64BE);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the compound data to it.
     */
    dset   = H5Dcreate(file, DATASET, filetype, space, H5P_DEFAULT);
    status = H5Dwrite(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().  For simplicity, we do not rebuild memtype.
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (sensor_t *)malloc(dims[0] * sizeof(sensor_t));

    /*
     * Read the data.
     */
    status = H5Dread(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%d]:\n", DATASET, i);
        printf("Serial number   : %d\n", rdata[i].serial_no);
        printf("Location        : %s\n", rdata[i].location);
        printf("Temperature (F) : %f\n", rdata[i].temperature);
        printf("Pressure (inHg) : %f\n\n", rdata[i].pressure);
    }

    /*
     * Close and release resources.  H5Dvlen_reclaim will automatically
     * traverse the structure and free any vlen data (strings in this
     * case).
     */
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, rdata);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(memtype);
    status = H5Tclose(strtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_cmpdatt.c`

```c
/************************************************************

  This example shows how to read and write compound
  datatypes to an attribute.  The program first writes
  compound structures to an attribute with a dataspace of
  DIM0, then closes the file.  Next, it reopens the file,
  reads back the data, and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_cmpdatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4

typedef struct {
    int    serial_no;
    char  *location;
    double temperature;
    double pressure;
} sensor_t; /* Compound type */

int
main(void)
{
    hid_t file, filetype, memtype, strtype, space, dset, attr;
    /* Handles */
    herr_t   status;
    hsize_t  dims[1] = {DIM0};
    sensor_t wdata[DIM0], /* Write buffer */
        *rdata;           /* Read buffer */
    int ndims, i;

    /*
     * Initialize data.
     */
    wdata[0].serial_no   = 1153;
    wdata[0].location    = "Exterior (static)";
    wdata[0].temperature = 53.23;
    wdata[0].pressure    = 24.57;
    wdata[1].serial_no   = 1184;
    wdata[1].location    = "Intake";
    wdata[1].temperature = 55.12;
    wdata[1].pressure    = 22.95;
    wdata[2].serial_no   = 1027;
    wdata[2].location    = "Intake manifold";
    wdata[2].temperature = 103.55;
    wdata[2].pressure    = 31.23;
    wdata[3].serial_no   = 1313;
    wdata[3].location    = "Exhaust manifold";
    wdata[3].temperature = 1252.89;
    wdata[3].pressure    = 84.11;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the compound datatype for memory.
     */
    memtype = H5Tcreate(H5T_COMPOUND, sizeof(sensor_t));
    status  = H5Tinsert(memtype, "Serial number", HOFFSET(sensor_t, serial_no), H5T_NATIVE_INT);
    status  = H5Tinsert(memtype, "Location", HOFFSET(sensor_t, location), strtype);
    status  = H5Tinsert(memtype, "Temperature (F)", HOFFSET(sensor_t, temperature), H5T_NATIVE_DOUBLE);
    status  = H5Tinsert(memtype, "Pressure (inHg)", HOFFSET(sensor_t, pressure), H5T_NATIVE_DOUBLE);

    /*
     * Create the compound datatype for the file.  Because the standard
     * types we are using for the file may have different sizes than
     * the corresponding native types, we must manually calculate the
     * offset of each member.
     */
    filetype = H5Tcreate(H5T_COMPOUND, 8 + sizeof(hvl_t) + 8 + 8);
    status   = H5Tinsert(filetype, "Serial number", 0, H5T_STD_I64BE);
    status   = H5Tinsert(filetype, "Location", 8, strtype);
    status   = H5Tinsert(filetype, "Temperature (F)", 8 + sizeof(hvl_t), H5T_IEEE_F64BE);
    status   = H5Tinsert(filetype, "Pressure (inHg)", 8 + sizeof(hvl_t) + 8, H5T_IEEE_F64BE);

    /*
     * Create dataset with a scalar dataspace.
     */
    space  = H5Screate(H5S_SCALAR);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the compound data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, filetype, space, H5P_DEFAULT);
    status = H5Awrite(attr, memtype, wdata);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().  For simplicity, we do not rebuild memtype.
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);
    attr = H5Aopen_name(dset, ATTRIBUTE);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (sensor_t *)malloc(dims[0] * sizeof(sensor_t));

    /*
     * Read the data.
     */
    status = H5Aread(attr, memtype, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%d]:\n", ATTRIBUTE, i);
        printf("Serial number   : %d\n", rdata[i].serial_no);
        printf("Location        : %s\n", rdata[i].location);
        printf("Temperature (F) : %f\n", rdata[i].temperature);
        printf("Pressure (inHg) : %f\n\n", rdata[i].pressure);
    }

    /*
     * Close and release resources.  H5Dvlen_reclaim will automatically
     * traverse the structure and free any vlen data (strings in this
     * case).
     */
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, rdata);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(memtype);
    status = H5Tclose(strtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_commit.c`

```c
/************************************************************

  This example shows how to commit a named datatype to a
  file, and read back that datatype.  The program first
  defines a compound datatype, commits it to a file, then
  closes the file.  Next, it reopens the file, opens the
  datatype, and outputs the names of its fields to the
  screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_commit.h5"
#define DATATYPE "Sensor_Type"

int
main(void)
{
    hid_t file, filetype, strtype;
    /* Handles */
    herr_t      status;
    H5T_class_t typeclass;
    char       *name;
    int         nmembs, i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the compound datatype.  Because the standard types we are
     * using may have different sizes than the corresponding native
     * types, we must manually calculate the offset of each member.
     */
    filetype = H5Tcreate(H5T_COMPOUND, 8 + sizeof(char *) + 8 + 8);
    status   = H5Tinsert(filetype, "Serial number", 0, H5T_STD_I64BE);
    status   = H5Tinsert(filetype, "Location", 8, strtype);
    status   = H5Tinsert(filetype, "Temperature (F)", 8 + sizeof(char *), H5T_IEEE_F64BE);
    status   = H5Tinsert(filetype, "Pressure (inHg)", 8 + sizeof(char *) + 8, H5T_IEEE_F64BE);

    /*
     * Commit the compound datatype to the file, creating a named
     * datatype.
     */
    status = H5Tcommit(file, DATATYPE, filetype);

    /*
     * Close and release resources.
     */
    status = H5Tclose(filetype);
    status = H5Tclose(strtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);

    /*
     * Open the named datatype.
     */
    filetype = H5Topen(file, DATATYPE);

    /*
     * Output the data to the screen.
     */
    printf("Named datatype: %s:\n", DATATYPE);
    /*
     * Get datatype class.  If it isn't compound, we won't print
     * anything.
     */
    typeclass = H5Tget_class(filetype);
    if (typeclass == H5T_COMPOUND) {
        printf("   Class: H5T_COMPOUND\n");
        nmembs = H5Tget_nmembers(filetype);
        /*
         * Iterate over compound datatype members.
         */
        for (i = 0; i < nmembs; i++) {
            /*
             * Get the member name and print it.  Note that
             * H5Tget_member_name allocates space for the string in
             * name, so we must free() it after use.
             */
            name = H5Tget_member_name(filetype, i);
            printf("   %s\n", name);
            free(name);
        }
    }

    /*
     * Close and release resources.
     */
    status = H5Tclose(filetype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_convert.c`

```c
/************************************************************

  This example shows how to convert between different
  datatypes in memory.  The program converts DIM0 elements
  of compound type sourcetype to desttype, then outputs the
  converted data to the screen.  A background buffer is used
  to fill in the elements of desttype that are not in
  sourcetype.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define DIM0 4

typedef struct {
    double temperature;
    double pressure;
} reading_t; /* Source type */

typedef struct {
    int    serial_no;
    char  *location;
    double temperature;
    double pressure;
} sensor_t; /* Destination type */

int
main(void)
{
    hid_t sourcetype, desttype, strtype, space;
    /* Handles */
    herr_t     status;
    hsize_t    dims[1] = {DIM0};
    reading_t *reading; /* Conversion buffer */
    sensor_t  *sensor,  /* Conversion buffer */
        bkgrd[DIM0];    /* Background buffer */
    int i;

    /*
     * Allocate memory for conversion buffer.  We will allocate space
     * for it to hold DIM0 elements of the destination type, as the
     * type conversion is performed in place.  Of course, if the
     * destination type were smaller than the source type, we would
     * allocate space to hold DIM0 elements of the source type.
     */
    reading = (reading_t *)malloc(DIM0 * sizeof(sensor_t));

    /*
     * Assign the allocated space to a pointer of the destination type,
     * to allow the buffer to be accessed correctly after the
     * conversion has taken place.
     */
    sensor = (sensor_t *)reading;

    /*
     * Initialize data.
     */
    bkgrd[0].serial_no   = 1153;
    bkgrd[0].location    = "Exterior (static)";
    bkgrd[0].temperature = 53.23;
    bkgrd[0].pressure    = 24.57;
    bkgrd[1].serial_no   = 1184;
    bkgrd[1].location    = "Intake";
    bkgrd[1].temperature = 55.12;
    bkgrd[1].pressure    = 22.95;
    bkgrd[2].serial_no   = 1027;
    bkgrd[2].location    = "Intake manifold";
    bkgrd[2].temperature = 103.55;
    bkgrd[2].pressure    = 31.23;
    bkgrd[3].serial_no   = 1313;
    bkgrd[3].location    = "Exhaust manifold";
    bkgrd[3].temperature = 1252.89;
    bkgrd[3].pressure    = 84.11;

    reading[0].temperature = 54.84;
    reading[0].pressure    = 24.76;
    reading[1].temperature = 56.63;
    reading[1].pressure    = 23.10;
    reading[2].temperature = 102.69;
    reading[2].pressure    = 30.97;
    reading[3].temperature = 1238.27;
    reading[3].pressure    = 82.15;

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the compound datatype for memory.
     */
    sourcetype = H5Tcreate(H5T_COMPOUND, sizeof(reading_t));
    status     = H5Tinsert(sourcetype, "Temperature (F)", HOFFSET(reading_t, temperature), H5T_NATIVE_DOUBLE);
    status     = H5Tinsert(sourcetype, "Pressure (inHg)", HOFFSET(reading_t, pressure), H5T_NATIVE_DOUBLE);

    desttype = H5Tcreate(H5T_COMPOUND, sizeof(sensor_t));
    status   = H5Tinsert(desttype, "Serial number", HOFFSET(sensor_t, serial_no), H5T_NATIVE_INT);
    status   = H5Tinsert(desttype, "Location", HOFFSET(sensor_t, location), strtype);
    status   = H5Tinsert(desttype, "Temperature (F)", HOFFSET(sensor_t, temperature), H5T_NATIVE_DOUBLE);
    status   = H5Tinsert(desttype, "Pressure (inHg)", HOFFSET(sensor_t, pressure), H5T_NATIVE_DOUBLE);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Convert the buffer in reading from sourcetype to desttype.
     * After this conversion we will use sensor to access the buffer,
     * as the buffer now matches its type.
     */
    status = H5Tconvert(sourcetype, desttype, DIM0, reading, bkgrd, H5P_DEFAULT);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < DIM0; i++) {
        printf("sensor[%d]:\n", i);
        printf("Serial number   : %d\n", sensor[i].serial_no);
        printf("Location        : %s\n", sensor[i].location);
        printf("Temperature (F) : %f\n", sensor[i].temperature);
        printf("Pressure (inHg) : %f\n\n", sensor[i].pressure);
    }

    /*
     * Close and release resources.  In this case H5Tconvert preserves
     * the memory locations of the variable-length strings in
     * "location", so we do not need to free those strings as they were
     * initialized as string constants.
     */
    free(sensor);
    status = H5Sclose(space);
    status = H5Tclose(sourcetype);
    status = H5Tclose(desttype);
    status = H5Tclose(strtype);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_cpxcmpd.c`

```c
/************************************************************

  This example shows how to read and write a complex
  compound datatype to a dataset.  The program first writes
  complex compound structures to a dataset with a dataspace
  of DIM0, then closes the file.  Next, it reopens the file,
  reads back selected fields in the structure, and outputs
  them to the screen.

  Unlike the other datatype examples, in this example we
  save to the file using native datatypes to simplify the
  type definitions here.  To save using standard types you
  must manually calculate the sizes and offsets of compound
  types as shown in h5ex_t_cmpd.c, and convert enumerated
  values as shown in h5ex_t_enum.c.

  The datatype defined here consists of a compound
  containing a variable-length list of compound types, as
  well as a variable-length string, enumeration, double
  array, object reference and region reference.  The nested
  compound type contains an int, variable-length string and
  two doubles.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_cpxcmpd.h5"
#define DATASET  "DS1"
#define DIM0     2

typedef struct {
    int    serial_no;
    char  *location;
    double temperature;
    double pressure;
} sensor_t; /* Nested compound type */

typedef enum { RED, GREEN, BLUE } color_t; /* Enumerated type */

typedef struct {
    hvl_t           sensors;
    char           *name;
    color_t         color;
    double          location[3];
    hobj_ref_t      group;
    hdset_reg_ref_t surveyed_areas;
} vehicle_t; /* Main compound type */

typedef struct {
    hvl_t sensors;
    char *name;
} rvehicle_t; /* Read type */

int
main(void)
{
    hid_t file, vehicletype, colortype, sensortype, sensorstype, loctype, strtype, rvehicletype, rsensortype,
        rsensorstype, space, dset, group;
    /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0}, adims[1] = {3}, adims2[2] = {32, 32}, start[2] = {8, 26}, count[2] = {4, 3},
            coords[3][2] = {{3, 2}, {3, 3}, {4, 4}};
    vehicle_t   wdata[2]; /* Write buffer */
    rvehicle_t *rdata;    /* Read buffer */
    color_t     val;
    sensor_t   *ptr;
    double      wdata2[32][32];
    int         ndims, i, j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataset to use for region references.
     */
    for (i = 0; i < 32; i++)
        for (j = 0; j < 32; j++)
            wdata2[i][j] = 70. + 0.1 * (i - 16.) + 0.1 * (j - 16.);
    space  = H5Screate_simple(2, adims2, NULL);
    dset   = H5Dcreate(file, "Ambient_Temperature", H5T_NATIVE_DOUBLE, space, H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_NATIVE_DOUBLE, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata2[0]);
    status = H5Dclose(dset);

    /*
     * Create groups to use for object references.
     */
    group  = H5Gcreate(file, "Land_Vehicles", H5P_DEFAULT);
    status = H5Gclose(group);
    group  = H5Gcreate(file, "Air_Vehicles", H5P_DEFAULT);
    status = H5Gclose(group);

    /*
     * Initialize variable-length compound in the first data element.
     */
    wdata[0].sensors.len = 4;
    ptr                  = (sensor_t *)malloc(wdata[0].sensors.len * sizeof(sensor_t));
    ptr[0].serial_no     = 1153;
    ptr[0].location      = "Exterior (static)";
    ptr[0].temperature   = 53.23;
    ptr[0].pressure      = 24.57;
    ptr[1].serial_no     = 1184;
    ptr[1].location      = "Intake";
    ptr[1].temperature   = 55.12;
    ptr[1].pressure      = 22.95;
    ptr[2].serial_no     = 1027;
    ptr[2].location      = "Intake manifold";
    ptr[2].temperature   = 103.55;
    ptr[2].pressure      = 31.23;
    ptr[3].serial_no     = 1313;
    ptr[3].location      = "Exhaust manifold";
    ptr[3].temperature   = 1252.89;
    ptr[3].pressure      = 84.11;
    wdata[0].sensors.p   = (void *)ptr;

    /*
     * Initialize other fields in the first data element.
     */
    wdata[0].name        = "Airplane";
    wdata[0].color       = GREEN;
    wdata[0].location[0] = -103234.21;
    wdata[0].location[1] = 422638.78;
    wdata[0].location[2] = 5996.43;
    status               = H5Rcreate(&wdata[0].group, file, "Air_Vehicles", H5R_OBJECT, -1);
    status               = H5Sselect_elements(space, H5S_SELECT_SET, 3, coords[0]);
    status = H5Rcreate(&wdata[0].surveyed_areas, file, "Ambient_Temperature", H5R_DATASET_REGION, space);

    /*
     * Initialize variable-length compound in the second data element.
     */
    wdata[1].sensors.len = 1;
    ptr                  = (sensor_t *)malloc(wdata[1].sensors.len * sizeof(sensor_t));
    ptr[0].serial_no     = 3244;
    ptr[0].location      = "Roof";
    ptr[0].temperature   = 83.82;
    ptr[0].pressure      = 29.92;
    wdata[1].sensors.p   = (void *)ptr;

    /*
     * Initialize other fields in the second data element.
     */
    wdata[1].name        = "Automobile";
    wdata[1].color       = RED;
    wdata[1].location[0] = 326734.36;
    wdata[1].location[1] = 221568.23;
    wdata[1].location[2] = 432.36;
    status               = H5Rcreate(&wdata[1].group, file, "Land_Vehicles", H5R_OBJECT, -1);
    status               = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, NULL, count, NULL);
    status = H5Rcreate(&wdata[1].surveyed_areas, file, "Ambient_Temperature", H5R_DATASET_REGION, space);

    status = H5Sclose(space);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the nested compound datatype.
     */
    sensortype = H5Tcreate(H5T_COMPOUND, sizeof(sensor_t));
    status     = H5Tinsert(sensortype, "Serial number", HOFFSET(sensor_t, serial_no), H5T_NATIVE_INT);
    status     = H5Tinsert(sensortype, "Location", HOFFSET(sensor_t, location), strtype);
    status     = H5Tinsert(sensortype, "Temperature (F)", HOFFSET(sensor_t, temperature), H5T_NATIVE_DOUBLE);
    status     = H5Tinsert(sensortype, "Pressure (inHg)", HOFFSET(sensor_t, pressure), H5T_NATIVE_DOUBLE);

    /*
     * Create the variable-length datatype.
     */
    sensorstype = H5Tvlen_create(sensortype);

    /*
     * Create the enumerated datatype.
     */
    colortype = H5Tenum_create(H5T_NATIVE_INT);
    val       = (color_t)RED;
    status    = H5Tenum_insert(colortype, "Red", &val);
    val       = (color_t)GREEN;
    status    = H5Tenum_insert(colortype, "Green", &val);
    val       = (color_t)BLUE;
    status    = H5Tenum_insert(colortype, "Blue", &val);

    /*
     * Create the array datatype.
     */
    loctype = H5Tarray_create(H5T_NATIVE_DOUBLE, 1, adims, NULL);

    /*
     * Create the main compound datatype.
     */
    vehicletype = H5Tcreate(H5T_COMPOUND, sizeof(vehicle_t));
    status      = H5Tinsert(vehicletype, "Sensors", HOFFSET(vehicle_t, sensors), sensorstype);
    status      = H5Tinsert(vehicletype, "Name", HOFFSET(vehicle_t, name), strtype);
    status      = H5Tinsert(vehicletype, "Color", HOFFSET(vehicle_t, color), colortype);
    status      = H5Tinsert(vehicletype, "Location", HOFFSET(vehicle_t, location), loctype);
    status      = H5Tinsert(vehicletype, "Group", HOFFSET(vehicle_t, group), H5T_STD_REF_OBJ);
    status =
        H5Tinsert(vehicletype, "Surveyed areas", HOFFSET(vehicle_t, surveyed_areas), H5T_STD_REF_DSETREG);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the compound data to it.
     */
    dset   = H5Dcreate(file, DATASET, vehicletype, space, H5P_DEFAULT);
    status = H5Dwrite(dset, vehicletype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.  Note that we cannot use
     * H5Dvlen_reclaim as it would attempt to free() the string
     * constants used to initialize the name fields in wdata.  We must
     * therefore manually free() only the data previously allocated
     * through malloc().
     */
    for (i = 0; i < dims[0]; i++)
        free(wdata[i].sensors.p);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(strtype);
    status = H5Tclose(sensortype);
    status = H5Tclose(sensorstype);
    status = H5Tclose(colortype);
    status = H5Tclose(loctype);
    status = H5Tclose(vehicletype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().  We will only read back the variable length strings.
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the nested compound datatype for reading.  Even though it
     * has only one field, it must still be defined as a compound type
     * so the library can match the correct field in the file type.
     * This matching is done by name.  However, we do not need to
     * define a structure for the read buffer as we can simply treat it
     * as a char *.
     */
    rsensortype = H5Tcreate(H5T_COMPOUND, sizeof(char *));
    status      = H5Tinsert(rsensortype, "Location", 0, strtype);

    /*
     * Create the variable-length datatype for reading.
     */
    rsensorstype = H5Tvlen_create(rsensortype);

    /*
     * Create the main compound datatype for reading.
     */
    rvehicletype = H5Tcreate(H5T_COMPOUND, sizeof(rvehicle_t));
    status       = H5Tinsert(rvehicletype, "Sensors", HOFFSET(rvehicle_t, sensors), rsensorstype);
    status       = H5Tinsert(rvehicletype, "Name", HOFFSET(rvehicle_t, name), strtype);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (rvehicle_t *)malloc(dims[0] * sizeof(rvehicle_t));

    /*
     * Read the data.
     */
    status = H5Dread(dset, rvehicletype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%d]:\n", DATASET, i);
        printf("   Vehicle name :\n      %s\n", rdata[i].name);
        printf("   Sensor locations :\n");
        for (j = 0; j < rdata[i].sensors.len; j++)
            printf("      %s\n", ((char **)rdata[i].sensors.p)[j]);
    }

    /*
     * Close and release resources.  H5Dvlen_reclaim will automatically
     * traverse the structure and free any vlen data (including
     * strings).
     */
    status = H5Dvlen_reclaim(rvehicletype, space, H5P_DEFAULT, rdata);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(strtype);
    status = H5Tclose(rsensortype);
    status = H5Tclose(rsensorstype);
    status = H5Tclose(rvehicletype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_cpxcmpdatt.c`

```c
/************************************************************

  This example shows how to read and write a complex
  compound datatype to an attribute.  The program first
  writes complex compound structures to an attribute with a
  dataspace of DIM0, then closes the file.  Next, it reopens
  the file, reads back selected fields in the structure, and
  outputs them to the screen.

  Unlike the other datatype examples, in this example we
  save to the file using native datatypes to simplify the
  type definitions here.  To save using standard types you
  must manually calculate the sizes and offsets of compound
  types as shown in h5ex_t_cmpd.c, and convert enumerated
  values as shown in h5ex_t_enum.c.

  The datatype defined here consists of a compound
  containing a variable-length list of compound types, as
  well as a variable-length string, enumeration, double
  array, object reference and region reference.  The nested
  compound type contains an int, variable-length string and
  two doubles.

  This file is intended for use with HDF5 Library version 1.8

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_cpxcmpdatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      2

typedef struct {
    int    serial_no;
    char  *location;
    double temperature;
    double pressure;
} sensor_t; /* Nested compound type */

typedef enum { RED, GREEN, BLUE } color_t; /* Enumerated type */

typedef struct {
    hvl_t           sensors;
    char           *name;
    color_t         color;
    double          location[3];
    hobj_ref_t      group;
    hdset_reg_ref_t surveyed_areas;
} vehicle_t; /* Main compound type */

typedef struct {
    hvl_t sensors;
    char *name;
} rvehicle_t; /* Read type */

int
main(void)
{
    hid_t file, vehicletype, colortype, sensortype, sensorstype, loctype, strtype, rvehicletype, rsensortype,
        rsensorstype, space, dset, group, attr;
    /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0}, adims[1] = {3}, adims2[2] = {32, 32}, start[2] = {8, 26}, count[2] = {4, 3},
            coords[3][2] = {{3, 2}, {3, 3}, {4, 4}};
    vehicle_t   wdata[2]; /* Write buffer */
    rvehicle_t *rdata;    /* Read buffer */
    color_t     val;
    sensor_t   *ptr;
    double      wdata2[32][32];
    int         ndims, i, j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataset to use for region references.
     */
    for (i = 0; i < 32; i++)
        for (j = 0; j < 32; j++)
            wdata2[i][j] = 70. + 0.1 * (i - 16.) + 0.1 * (j - 16.);
    space  = H5Screate_simple(2, adims2, NULL);
    dset   = H5Dcreate(file, "Ambient_Temperature", H5T_NATIVE_DOUBLE, space, H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_NATIVE_DOUBLE, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata2[0]);
    status = H5Dclose(dset);

    /*
     * Create groups to use for object references.
     */
    group  = H5Gcreate(file, "Land_Vehicles", H5P_DEFAULT);
    status = H5Gclose(group);
    group  = H5Gcreate(file, "Air_Vehicles", H5P_DEFAULT);
    status = H5Gclose(group);

    /*
     * Initialize variable-length compound in the first data element.
     */
    wdata[0].sensors.len = 4;
    ptr                  = (sensor_t *)malloc(wdata[0].sensors.len * sizeof(sensor_t));
    ptr[0].serial_no     = 1153;
    ptr[0].location      = "Exterior (static)";
    ptr[0].temperature   = 53.23;
    ptr[0].pressure      = 24.57;
    ptr[1].serial_no     = 1184;
    ptr[1].location      = "Intake";
    ptr[1].temperature   = 55.12;
    ptr[1].pressure      = 22.95;
    ptr[2].serial_no     = 1027;
    ptr[2].location      = "Intake manifold";
    ptr[2].temperature   = 103.55;
    ptr[2].pressure      = 31.23;
    ptr[3].serial_no     = 1313;
    ptr[3].location      = "Exhaust manifold";
    ptr[3].temperature   = 1252.89;
    ptr[3].pressure      = 84.11;
    wdata[0].sensors.p   = (void *)ptr;

    /*
     * Initialize other fields in the first data element.
     */
    wdata[0].name        = "Airplane";
    wdata[0].color       = GREEN;
    wdata[0].location[0] = -103234.21;
    wdata[0].location[1] = 422638.78;
    wdata[0].location[2] = 5996.43;
    status               = H5Rcreate(&wdata[0].group, file, "Air_Vehicles", H5R_OBJECT, -1);
    status               = H5Sselect_elements(space, H5S_SELECT_SET, 3, coords[0]);
    status = H5Rcreate(&wdata[0].surveyed_areas, file, "Ambient_Temperature", H5R_DATASET_REGION, space);

    /*
     * Initialize variable-length compound in the second data element.
     */
    wdata[1].sensors.len = 1;
    ptr                  = (sensor_t *)malloc(wdata[1].sensors.len * sizeof(sensor_t));
    ptr[0].serial_no     = 3244;
    ptr[0].location      = "Roof";
    ptr[0].temperature   = 83.82;
    ptr[0].pressure      = 29.92;
    wdata[1].sensors.p   = (void *)ptr;

    /*
     * Initialize other fields in the second data element.
     */
    wdata[1].name        = "Automobile";
    wdata[1].color       = RED;
    wdata[1].location[0] = 326734.36;
    wdata[1].location[1] = 221568.23;
    wdata[1].location[2] = 432.36;
    status               = H5Rcreate(&wdata[1].group, file, "Land_Vehicles", H5R_OBJECT, -1);
    status               = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, NULL, count, NULL);
    status = H5Rcreate(&wdata[1].surveyed_areas, file, "Ambient_Temperature", H5R_DATASET_REGION, space);

    status = H5Sclose(space);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the nested compound datatype.
     */
    sensortype = H5Tcreate(H5T_COMPOUND, sizeof(sensor_t));
    status     = H5Tinsert(sensortype, "Serial number", HOFFSET(sensor_t, serial_no), H5T_NATIVE_INT);
    status     = H5Tinsert(sensortype, "Location", HOFFSET(sensor_t, location), strtype);
    status     = H5Tinsert(sensortype, "Temperature (F)", HOFFSET(sensor_t, temperature), H5T_NATIVE_DOUBLE);
    status     = H5Tinsert(sensortype, "Pressure (inHg)", HOFFSET(sensor_t, pressure), H5T_NATIVE_DOUBLE);

    /*
     * Create the variable-length datatype.
     */
    sensorstype = H5Tvlen_create(sensortype);

    /*
     * Create the enumerated datatype.
     */
    colortype = H5Tenum_create(H5T_NATIVE_INT);
    val       = (color_t)RED;
    status    = H5Tenum_insert(colortype, "Red", &val);
    val       = (color_t)GREEN;
    status    = H5Tenum_insert(colortype, "Green", &val);
    val       = (color_t)BLUE;
    status    = H5Tenum_insert(colortype, "Blue", &val);

    /*
     * Create the array datatype.
     */
    loctype = H5Tarray_create(H5T_NATIVE_DOUBLE, 1, adims, NULL);

    /*
     * Create the main compound datatype.
     */
    vehicletype = H5Tcreate(H5T_COMPOUND, sizeof(vehicle_t));
    status      = H5Tinsert(vehicletype, "Sensors", HOFFSET(vehicle_t, sensors), sensorstype);
    status      = H5Tinsert(vehicletype, "Name", HOFFSET(vehicle_t, name), strtype);
    status      = H5Tinsert(vehicletype, "Color", HOFFSET(vehicle_t, color), colortype);
    status      = H5Tinsert(vehicletype, "Location", HOFFSET(vehicle_t, location), loctype);
    status      = H5Tinsert(vehicletype, "Group", HOFFSET(vehicle_t, group), H5T_STD_REF_OBJ);
    status =
        H5Tinsert(vehicletype, "Surveyed areas", HOFFSET(vehicle_t, surveyed_areas), H5T_STD_REF_DSETREG);

    /*
     * Create dataset with a scalar dataspace. to serve as the parent
     * for the attribute.
     */
    space  = H5Screate(H5S_SCALAR);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the compound data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, vehicletype, space, H5P_DEFAULT);
    status = H5Awrite(attr, vehicletype, wdata);

    /*
     * Close and release resources.  Note that we cannot use
     * H5Dvlen_reclaim as it would attempt to free() the string
     * constants used to initialize the name fields in wdata.  We must
     * therefore manually free() only the data previously allocated
     * through malloc().
     */
    for (i = 0; i < dims[0]; i++)
        free(wdata[i].sensors.p);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(strtype);
    status = H5Tclose(sensortype);
    status = H5Tclose(sensorstype);
    status = H5Tclose(colortype);
    status = H5Tclose(loctype);
    status = H5Tclose(vehicletype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().  We will only read back the variable length strings.
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);
    attr = H5Aopen_name(dset, ATTRIBUTE);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the nested compound datatype for reading.  Even though it
     * has only one field, it must still be defined as a compound type
     * so the library can match the correct field in the file type.
     * This matching is done by name.  However, we do not need to
     * define a structure for the read buffer as we can simply treat it
     * as a char *.
     */
    rsensortype = H5Tcreate(H5T_COMPOUND, sizeof(char *));
    status      = H5Tinsert(rsensortype, "Location", 0, strtype);

    /*
     * Create the variable-length datatype for reading.
     */
    rsensorstype = H5Tvlen_create(rsensortype);

    /*
     * Create the main compound datatype for reading.
     */
    rvehicletype = H5Tcreate(H5T_COMPOUND, sizeof(rvehicle_t));
    status       = H5Tinsert(rvehicletype, "Sensors", HOFFSET(rvehicle_t, sensors), rsensorstype);
    status       = H5Tinsert(rvehicletype, "Name", HOFFSET(rvehicle_t, name), strtype);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (rvehicle_t *)malloc(dims[0] * sizeof(rvehicle_t));

    /*
     * Read the data.
     */
    status = H5Aread(attr, rvehicletype, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%d]:\n", ATTRIBUTE, i);
        printf("   Vehicle name :\n      %s\n", rdata[i].name);
        printf("   Sensor locations :\n");
        for (j = 0; j < rdata[i].sensors.len; j++)
            printf("      %s\n", ((char **)rdata[i].sensors.p)[j]);
    }

    /*
     * Close and release resources.  H5Dvlen_reclaim will automatically
     * traverse the structure and free any vlen data (including
     * strings).
     */
    status = H5Dvlen_reclaim(rvehicletype, space, H5P_DEFAULT, rdata);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(strtype);
    status = H5Tclose(rsensortype);
    status = H5Tclose(rsensorstype);
    status = H5Tclose(rvehicletype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_enum.c`

```c
/************************************************************

  This example shows how to read and write enumerated
  datatypes to a dataset.  The program first writes
  enumerated values to a dataset with a dataspace of
  DIM0xDIM1, then closes the file.  Next, it reopens the
  file, reads back the data, and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME      "h5ex_t_enum.h5"
#define DATASET       "DS1"
#define DIM0          4
#define DIM1          7
#define F_BASET       H5T_STD_I16BE  /* File base type */
#define M_BASET       H5T_NATIVE_INT /* Memory base type */
#define NAME_BUF_SIZE 16

typedef enum { SOLID, LIQUID, GAS, PLASMA } phase_t; /* Enumerated type */

int
main(void)
{
    hid_t file, filetype, memtype, space, dset;
    /* Handles */
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    phase_t wdata[DIM0][DIM1], /* Write buffer */
        **rdata,               /* Read buffer */
        val;
    char *names[4] = {"SOLID", "LIQUID", "GAS", "PLASMA"}, name[NAME_BUF_SIZE];
    int   ndims, i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = (phase_t)((i + 1) * j - j) % (int)(PLASMA + 1);

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create the enumerated datatypes for file and memory.  This
     * process is simplified if native types are used for the file,
     * as only one type must be defined.
     */
    filetype = H5Tenum_create(F_BASET);
    memtype  = H5Tenum_create(M_BASET);

    for (i = (int)SOLID; i <= (int)PLASMA; i++) {
        /*
         * Insert enumerated value for memtype.
         */
        val    = (phase_t)i;
        status = H5Tenum_insert(memtype, names[i], &val);
        /*
         * Insert enumerated value for filetype.  We must first convert
         * the numerical value val to the base type of the destination.
         */
        status = H5Tconvert(M_BASET, F_BASET, 1, &val, NULL, H5P_DEFAULT);
        status = H5Tenum_insert(filetype, names[i], &val);
    }

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset and write the enumerated data to it.
     */
    dset   = H5Dcreate(file, DATASET, filetype, space, H5P_DEFAULT);
    status = H5Dwrite(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().  For simplicity, we do not rebuild memtype.
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (phase_t **)malloc(dims[0] * sizeof(phase_t *));

    /*
     * Allocate space for enumerated data.
     */
    rdata[0] = (phase_t *)malloc(dims[0] * dims[1] * sizeof(phase_t));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Dread(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++) {

            /*
             * Get the name of the enumeration member.
             */
            status = H5Tenum_nameof(memtype, &rdata[i][j], name, NAME_BUF_SIZE);
            printf(" %-6s", name);
        }
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_enumatt.c`

```c
/************************************************************

  This example shows how to read and write enumerated
  datatypes to an attribute.  The program first writes
  enumerated values to an attribute with a dataspace of
  DIM0xDIM1, then closes the file.  Next, it reopens the
  file, reads back the data, and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME      "h5ex_t_enumatt.h5"
#define DATASET       "DS1"
#define ATTRIBUTE     "A1"
#define DIM0          4
#define DIM1          7
#define F_BASET       H5T_STD_I16BE  /* File base type */
#define M_BASET       H5T_NATIVE_INT /* Memory base type */
#define NAME_BUF_SIZE 16

typedef enum { SOLID, LIQUID, GAS, PLASMA } phase_t; /* Enumerated type */

int
main(void)
{
    hid_t file, filetype, memtype, space, dset, attr;
    /* Handles */
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    phase_t wdata[DIM0][DIM1], /* Write buffer */
        **rdata,               /* Read buffer */
        val;
    char *names[4] = {"SOLID", "LIQUID", "GAS", "PLASMA"}, name[NAME_BUF_SIZE];
    int   ndims, i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = (phase_t)((i + 1) * j - j) % (int)(PLASMA + 1);

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create the enumerated datatypes for file and memory.  This
     * process is simplified if native types are used for the file,
     * as only one type must be defined.
     */
    filetype = H5Tenum_create(F_BASET);
    memtype  = H5Tenum_create(M_BASET);

    for (i = (int)SOLID; i <= (int)PLASMA; i++) {
        /*
         * Insert enumerated value for memtype.
         */
        val    = (phase_t)i;
        status = H5Tenum_insert(memtype, names[i], &val);
        /*
         * Insert enumerated value for filetype.  We must first convert
         * the numerical value val to the base type of the destination.
         */
        status = H5Tconvert(M_BASET, F_BASET, 1, &val, NULL, H5P_DEFAULT);
        status = H5Tenum_insert(filetype, names[i], &val);
    }

    /*
     * Create dataset with a scalar dataspace.
     */
    space  = H5Screate(H5S_SCALAR);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the attribute and write the enumerated data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, filetype, space, H5P_DEFAULT);
    status = H5Awrite(attr, memtype, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().  For simplicity, we do not rebuild memtype.
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);
    attr = H5Aopen_name(dset, ATTRIBUTE);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional attribute so the dynamic allocation must be done
     * in steps.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (phase_t **)malloc(dims[0] * sizeof(phase_t *));

    /*
     * Allocate space for enumerated data.
     */
    rdata[0] = (phase_t *)malloc(dims[0] * dims[1] * sizeof(phase_t));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Aread(attr, memtype, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", ATTRIBUTE);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++) {

            /*
             * Get the name of the enumeration member.
             */
            status = H5Tenum_nameof(memtype, &rdata[i][j], name, NAME_BUF_SIZE);
            printf(" %-6s", name);
        }
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_float.c`

```c
/************************************************************

  This example shows how to read and write float datatypes
  to a dataset.  The program first writes floats to a
  dataset with a dataspace of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_float.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7

int
main(void)
{
    hid_t   file, space, dset; /* Handles */
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    double  wdata[DIM0][DIM1], /* Write buffer */
        **rdata;               /* Read buffer */
    int ndims, i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = (double)i / (j + 0.5) + j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset and write the floating point data to it.  In
     * this example we will save the data as 64 bit little endian IEEE
     * floating point numbers, regardless of the native type.  The HDF5
     * library automatically converts between different floating point
     * types.
     */
    dset   = H5Dcreate(file, DATASET, H5T_IEEE_F64LE, space, H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_NATIVE_DOUBLE, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (double **)malloc(dims[0] * sizeof(double *));

    /*
     * Allocate space for floating point data.
     */
    rdata[0] = (double *)malloc(dims[0] * dims[1] * sizeof(double));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Dread(dset, H5T_NATIVE_DOUBLE, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %6.4f", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_floatatt.c`

```c
/************************************************************

  This example shows how to read and write floating point
  datatypes to an attribute.  The program first writes
  floating point numbers to an attribute with a dataspace of
  DIM0xDIM1, then closes the file.  Next, it reopens the
  file, reads back the data, and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_floatatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4
#define DIM1      7

int
main(void)
{
    hid_t   file, space, dset, attr; /* Handles */
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    double  wdata[DIM0][DIM1], /* Write buffer */
        **rdata;               /* Read buffer */
    int ndims, i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = (double)i / (j + 0.5) + j;
    ;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataset with a scalar dataspace.
     */
    space  = H5Screate(H5S_SCALAR);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the attribute and write the floating point data to it.
     * In this example we will save the data as 64 bit little endian
     * IEEE floating point numbers, regardless of the native type.  The
     * HDF5 library automatically converts between different floating
     * point types.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, H5T_IEEE_F64LE, space, H5P_DEFAULT);
    status = H5Awrite(attr, H5T_NATIVE_DOUBLE, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);
    attr = H5Aopen_name(dset, ATTRIBUTE);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional attribute so the dynamic allocation must be done
     * in steps.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (double **)malloc(dims[0] * sizeof(double *));

    /*
     * Allocate space for floating point data.
     */
    rdata[0] = (double *)malloc(dims[0] * dims[1] * sizeof(double));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Aread(attr, H5T_NATIVE_DOUBLE, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", ATTRIBUTE);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %6.4f", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_int.c`

```c
/************************************************************

  This example shows how to read and write integer datatypes
  to a dataset.  The program first writes integers to a
  dataset with a dataspace of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_int.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7

int
main(void)
{
    hid_t   file, space, dset; /* Handles */
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    int     wdata[DIM0][DIM1], /* Write buffer */
        **rdata,               /* Read buffer */
        ndims, i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset and write the integer data to it.  In this
     * example we will save the data as 64 bit big endian integers,
     * regardless of the native integer type.  The HDF5 library
     * automatically converts between different integer types.
     */
    dset   = H5Dcreate(file, DATASET, H5T_STD_I64BE, space, H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (int **)malloc(dims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_intatt.c`

```c
/************************************************************

  This example shows how to read and write integer datatypes
  to an attribute.  The program first writes integers to an
  attribute with a dataspace of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_intatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4
#define DIM1      7

int
main(void)
{
    hid_t   file, space, dset, attr; /* Handles */
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    int     wdata[DIM0][DIM1], /* Write buffer */
        **rdata,               /* Read buffer */
        ndims, i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataset with a scalar dataspace.
     */
    space  = H5Screate(H5S_SCALAR);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the attribute and write the integer data to it.  In this
     * example we will save the data as 64 bit big endian integers,
     * regardless of the native integer type.  The HDF5 library
     * automatically converts between different integer types.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, H5T_STD_I64BE, space, H5P_DEFAULT);
    status = H5Awrite(attr, H5T_NATIVE_INT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);
    attr = H5Aopen_name(dset, ATTRIBUTE);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional attribute so the dynamic allocation must be done
     * in steps.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (int **)malloc(dims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Aread(attr, H5T_NATIVE_INT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", ATTRIBUTE);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_objref.c`

```c
/************************************************************

  This example shows how to read and write object references
  to a dataset.  The program first creates objects in the
  file and writes references to those objects to a dataset
  with a dataspace of DIM0, then closes the file.  Next, it
  reopens the file, dereferences the references, and outputs
  the names of their targets to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_objref.h5"
#define DATASET  "DS1"
#define DIM0     2

int
main(void)
{
    hid_t      file, space, dset, obj; /* Handles */
    herr_t     status;
    hsize_t    dims[1] = {DIM0};
    hobj_ref_t wdata[DIM0], /* Write buffer */
        *rdata;             /* Read buffer */
    H5G_obj_t objtype;
    ssize_t   size;
    char     *name;
    int       ndims, i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create a dataset with a scalar dataspace.
     */
    space  = H5Screate(H5S_SCALAR);
    obj    = H5Dcreate(file, "DS2", H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Dclose(obj);
    status = H5Sclose(space);

    /*
     * Create a group.
     */
    obj    = H5Gcreate(file, "G1", H5P_DEFAULT);
    status = H5Gclose(obj);

    /*
     * Create references to the previously created objects.  Passing -1
     * as space_id causes this parameter to be ignored.  Other values
     * besides valid dataspaces result in an error.
     */
    status = H5Rcreate(&wdata[0], file, "G1", H5R_OBJECT, -1);
    status = H5Rcreate(&wdata[1], file, "DS2", H5R_OBJECT, -1);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the object references to it.
     */
    dset   = H5Dcreate(file, DATASET, H5T_STD_REF_OBJ, space, H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_STD_REF_OBJ, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (hobj_ref_t *)malloc(dims[0] * sizeof(hobj_ref_t));

    /*
     * Read the data.
     */
    status = H5Dread(dset, H5T_STD_REF_OBJ, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%d]:\n  ->", DATASET, i);
        /*
         * Open the referenced object, get its name and type.
         */
        obj     = H5Rdereference(dset, H5R_OBJECT, &rdata[i]);
        objtype = H5Rget_obj_type(dset, H5R_OBJECT, &rdata[i]);

        /*
         * Get the length of the name, allocate space, then retrieve
         * the name.
         */
        size = 1 + H5Iget_name(obj, NULL, 0);
        name = (char *)malloc(size);
        size = H5Iget_name(obj, name, size);

        /*
         * Print the object type and close the object.
         */
        switch (objtype) {
            case H5G_GROUP:
                printf("Group");
                status = H5Gclose(obj);
                break;
            case H5G_DATASET:
                printf("Dataset");
                status = H5Dclose(obj);
                break;
            case H5G_TYPE:
                printf("Named Datatype");
                status = H5Tclose(obj);
        }

        /*
         * Print the name and deallocate space for the name.
         */
        printf(": %s\n", name);
        free(name);
    }

    /*
     * Close and release resources.
     */
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_objrefatt.c`

```c
/************************************************************

  This example shows how to read and write object references
  to an attribute.  The program first creates objects in the
  file and writes references to those objects to an
  attribute with a dataspace of DIM0, then closes the file.
  Next, it reopens the file, dereferences the references,
  and outputs the names of their targets to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_objrefatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      2

int
main(void)
{
    hid_t       file;  /* File Handle */
    hid_t       space; /* Dataspace Handle */
    hid_t       dset;  /* Dataset Handle */
    hid_t       obj;   /* Object Handle */
    hid_t       attr;  /* Attribute Handle */
    herr_t      status;
    hsize_t     dims[1] = {DIM0};
    hobj_ref_t  wdata[DIM0];  /* Write buffer */
    hobj_ref_t *rdata = NULL; /* Read buffer */
    H5G_obj_t   objtype;
    ssize_t     size;
    char       *name = NULL;
    int         ndims;
    int         i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create a dataset with a scalar dataspace.
     */
    space  = H5Screate(H5S_SCALAR);
    obj    = H5Dcreate(file, "DS2", H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Dclose(obj);
    status = H5Sclose(space);

    /*
     * Create a group.
     */
    obj    = H5Gcreate(file, "G1", H5P_DEFAULT);
    status = H5Gclose(obj);

    /*
     * Create references to the previously created objects.  Passing -1
     * as space_id causes this parameter to be ignored.  Other values
     * besides valid dataspaces result in an error.
     */
    status = H5Rcreate(&wdata[0], file, "G1", H5R_OBJECT, -1);
    status = H5Rcreate(&wdata[1], file, "DS2", H5R_OBJECT, -1);

    /*
     * Create dataset with a scalar dataspace to serve as the parent
     * for the attribute.
     */
    space  = H5Screate(H5S_SCALAR);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the object references to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, H5T_STD_REF_OBJ, space, H5P_DEFAULT);
    status = H5Awrite(attr, H5T_STD_REF_OBJ, wdata);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);
    attr = H5Aopen_name(dset, ATTRIBUTE);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (hobj_ref_t *)malloc(dims[0] * sizeof(hobj_ref_t));

    /*
     * Read the data.
     */
    status = H5Aread(attr, H5T_STD_REF_OBJ, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%d]:\n  ->", ATTRIBUTE, i);

        /*
         * Open the referenced object, get its name and type.
         */
        obj     = H5Rdereference(dset, H5R_OBJECT, &rdata[i]);
        objtype = H5Rget_obj_type(dset, H5R_OBJECT, &rdata[i]);

        /*
         * Get the length of the name, allocate space, then retrieve
         * the name.
         */
        size = 1 + H5Iget_name(obj, NULL, 0);
        name = (char *)malloc(size);
        size = H5Iget_name(obj, name, size);

        /*
         * Print the object type and close the object.
         */
        switch (objtype) {
            case H5G_GROUP:
                printf("Group");
                status = H5Gclose(obj);
                break;
            case H5G_DATASET:
                printf("Dataset");
                status = H5Dclose(obj);
                break;
            case H5G_TYPE:
                printf("Named Datatype");
                status = H5Tclose(obj);
        }

        /*
         * Print the name and deallocate space for the name.
         */
        printf(": %s\n", name);
        free(name);
    }

    /*
     * Close and release resources.
     */
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_opaque.c`

```c
/************************************************************

  This example shows how to read and write opaque datatypes
  to a dataset.  The program first writes opaque data to a
  dataset with a dataspace of DIM0, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_opaque.h5"
#define DATASET  "DS1"
#define DIM0     4
#define LEN      7

int
main(void)
{
    hid_t   file, space, dtype, dset; /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0};
    size_t  len;
    char    wdata[DIM0 * LEN], /* Write buffer */
        *rdata,                /* Read buffer */
        str[LEN] = "OPAQUE", *tag;
    int      ndims, i, j;
    unsigned majnum, minnum, relnum;

    /* Get library version to differentiate between acceptable version methods
     * to free the tag returned by H5Tget_tag. */
    H5get_libversion(&majnum, &minnum, &relnum);

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++) {
        for (j = 0; j < LEN - 1; j++)
            wdata[j + i * LEN] = str[j];
        wdata[LEN - 1 + i * LEN] = (char)i + '0';
    }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create opaque datatype and set the tag to something appropriate.
     * For this example we will write and view the data as a character
     * array.
     */
    dtype  = H5Tcreate(H5T_OPAQUE, LEN);
    status = H5Tset_tag(dtype, "Character array");

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the opaque data to it.
     */
    dset   = H5Dcreate(file, DATASET, dtype, space, H5P_DEFAULT);
    status = H5Dwrite(dset, dtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(dtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get datatype and properties for the datatype.  Note that H5Tget_tag
     * allocates space for the string in tag, so we must remember to free() it
     * later.
     */
    dtype = H5Dget_type(dset);
    len   = H5Tget_size(dtype);
    tag   = H5Tget_tag(dtype);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (char *)malloc(dims[0] * len);

    /*
     * Read the data.
     */
    status = H5Dread(dset, dtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the data to the screen.
     */
    printf("Datatype tag for %s is: \"%s\"\n", DATASET, tag);
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%u]: ", DATASET, i);
        for (j = 0; j < len; j++)
            printf("%c", rdata[j + i * len]);
        printf("\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata);
    /* H5free_memory is available in 1.8.16 and above.
     * Last version for 1.6 was 1.6.10. */
    if (minnum > 8 || relnum > 15)
        H5free_memory(tag);
    else
        free(tag);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(dtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_opaqueatt.c`

```c
/************************************************************

  This example shows how to read and write opaque datatypes
  to an attribute.  The program first writes opaque data to
  an attribute with a dataspace of DIM0, then closes the
  file. Next, it reopens the file, reads back the data, and
  outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_opaqueatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4
#define LEN       7

int
main(void)
{
    hid_t   file, space, dtype, dset, attr; /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0};
    size_t  len;
    char    wdata[DIM0 * LEN], /* Write buffer */
        *rdata,                /* Read buffer */
        str[LEN] = "OPAQUE", *tag;
    int      ndims, i, j;
    unsigned majnum, minnum, relnum;

    /* Get library version to differentiate between acceptable version methods
     * to free the tag returned by H5Tget_tag. */
    H5get_libversion(&majnum, &minnum, &relnum);

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++) {
        for (j = 0; j < LEN - 1; j++)
            wdata[j + i * LEN] = str[j];
        wdata[LEN - 1 + i * LEN] = (char)i + '0';
    }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataset with a scalar dataspace.
     */
    space  = H5Screate(H5S_SCALAR);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create opaque datatype and set the tag to something appropriate.
     * For this example we will write and view the data as a character
     * array.
     */
    dtype  = H5Tcreate(H5T_OPAQUE, LEN);
    status = H5Tset_tag(dtype, "Character array");

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the opaque data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, dtype, space, H5P_DEFAULT);
    status = H5Awrite(attr, dtype, wdata);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(dtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);
    attr = H5Aopen_name(dset, ATTRIBUTE);

    /*
     * Get datatype and properties for the datatype.  Note that H5Tget_tag
     * allocates space for the string in tag, so we must remember to free() it
     * later.
     */
    dtype = H5Aget_type(attr);
    len   = H5Tget_size(dtype);
    tag   = H5Tget_tag(dtype);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (char *)malloc(dims[0] * len);

    /*
     * Read the data.
     */
    status = H5Aread(attr, dtype, rdata);

    /*
     * Output the data to the screen.
     */
    printf("Datatype tag for %s is: \"%s\"\n", ATTRIBUTE, tag);
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%u]: ", ATTRIBUTE, i);
        for (j = 0; j < len; j++)
            printf("%c", rdata[j + i * len]);
        printf("\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata);
    /* H5free_memory is available in 1.8.16 and above.
     * Last version for 1.6 was 1.6.10. */
    if (minnum > 8 || relnum > 15)
        H5free_memory(tag);
    else
        free(tag);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(dtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_regref.c`

```c
/************************************************************

  This example shows how to read and write region references
  to a dataset.  The program first creates a dataset
  containing characters and writes references to region of
  the dataset to a new dataset with a dataspace of DIM0,
  then closes the file.  Next, it reopens the file,
  dereferences the references, and outputs the referenced
  regions to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_regref.h5"
#define DATASET  "DS1"
#define DATASET2 "DS2"
#define DIM0     2
#define DS2DIM0  3
#define DS2DIM1  16

int
main(void)
{
    hid_t file, space, memspace, dset, dset2;
    /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0}, dims2[2] = {DS2DIM0, DS2DIM1}, coords[4][2] = {{0, 1}, {2, 11}, {1, 0}, {2, 4}},
            start[2] = {0, 0}, stride[2] = {2, 11}, count[2] = {2, 2}, block[2] = {1, 3};
    hssize_t        npoints;
    hdset_reg_ref_t wdata[DIM0], /* Write buffer */
        *rdata;                  /* Read buffer */
    ssize_t size;
    char wdata2[DS2DIM0][DS2DIM1] = {"The quick brown", "fox jumps over ", "the 5 lazy dogs"}, *rdata2, *name;
    int  ndims, i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create a dataset with character data.
     */
    space  = H5Screate_simple(2, dims2, NULL);
    dset2  = H5Dcreate(file, DATASET2, H5T_STD_I8LE, space, H5P_DEFAULT);
    status = H5Dwrite(dset2, H5T_NATIVE_CHAR, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata2);

    /*
     * Create reference to a list of elements in dset2.
     */
    status = H5Sselect_elements(space, H5S_SELECT_SET, 4, coords[0]);
    status = H5Rcreate(&wdata[0], file, DATASET2, H5R_DATASET_REGION, space);

    /*
     * Create reference to a hyperslab in dset2, close dataspace.
     */
    status = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, stride, count, block);
    status = H5Rcreate(&wdata[1], file, DATASET2, H5R_DATASET_REGION, space);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the region references to it.
     */
    dset   = H5Dcreate(file, DATASET, H5T_STD_REF_DSETREG, space, H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_STD_REF_DSETREG, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Dclose(dset2);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space  = H5Dget_space(dset);
    ndims  = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata  = (hdset_reg_ref_t *)malloc(dims[0] * sizeof(hdset_reg_ref_t));
    status = H5Sclose(space);

    /*
     * Read the data.
     */
    status = H5Dread(dset, H5T_STD_REF_DSETREG, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%d]:\n  ->", DATASET, i);

        /*
         * Open the referenced object, retrieve its region as a
         * dataspace selection.
         */
        dset2 = H5Rdereference(dset, H5R_DATASET_REGION, &rdata[i]);
        space = H5Rget_region(dset, H5R_DATASET_REGION, &rdata[i]);

        /*
         * Get the length of the object's name, allocate space, then
         * retrieve the name.
         */
        size = 1 + H5Iget_name(dset2, NULL, 0);
        name = (char *)malloc(size);
        size = 1 + H5Iget_name(dset2, name, size);
        if (size <= 1)
            name[0] = '\0';

        /*
         * Allocate space for the read buffer.  We will only allocate
         * enough space for the selection, plus a null terminator.  The
         * read buffer will be 1-dimensional.
         */
        npoints = H5Sget_select_npoints(space);
        rdata2  = (char *)malloc(npoints + 1);

        /*
         * Read the dataset region, and add a null terminator so we can
         * print it as a string.
         */
        memspace        = H5Screate_simple(1, (hsize_t *)&npoints, NULL);
        status          = H5Dread(dset2, H5T_NATIVE_CHAR, memspace, space, H5P_DEFAULT, rdata2);
        rdata2[npoints] = '\0';

        /*
         * Print the name and region data, close and release resources.
         */
        printf(" %s: %s\n", name, rdata2);
        free(rdata2);
        free(name);
        status = H5Sclose(space);
        status = H5Sclose(memspace);
        status = H5Dclose(dset2);
    }

    /*
     * Close and release resources.
     */
    free(rdata);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_regrefatt.c`

```c
/************************************************************

  This example shows how to read and write region references
  to an attribute.  The program first creates a dataset
  containing characters and writes references to region of
  the dataset to a new attribute with a dataspace of DIM0,
  then closes the file.  Next, it reopens the file,
  dereferences the references, and outputs the referenced
  regions to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_regrefatt.h5"
#define DATASET   "DS1"
#define DATASET2  "DS2"
#define ATTRIBUTE "A1"
#define DIM0      2
#define DS2DIM0   3
#define DS2DIM1   16

int
main(void)
{
    hid_t file, space, memspace, dset, dset2, attr;
    /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0}, dims2[2] = {DS2DIM0, DS2DIM1}, coords[4][2] = {{0, 1}, {2, 11}, {1, 0}, {2, 4}},
            start[2] = {0, 0}, stride[2] = {2, 11}, count[2] = {2, 2}, block[2] = {1, 3};
    hssize_t        npoints;
    hdset_reg_ref_t wdata[DIM0], /* Write buffer */
        *rdata;                  /* Read buffer */
    ssize_t size;
    char wdata2[DS2DIM0][DS2DIM1] = {"The quick brown", "fox jumps over ", "the 5 lazy dogs"}, *rdata2, *name;
    int  ndims, i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create a dataset with character data.
     */
    space  = H5Screate_simple(2, dims2, NULL);
    dset2  = H5Dcreate(file, DATASET2, H5T_STD_I8LE, space, H5P_DEFAULT);
    status = H5Dwrite(dset2, H5T_NATIVE_CHAR, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata2);

    /*
     * Create reference to a list of elements in dset2.
     */
    status = H5Sselect_elements(space, H5S_SELECT_SET, 4, coords[0]);
    status = H5Rcreate(&wdata[0], file, DATASET2, H5R_DATASET_REGION, space);

    /*
     * Create reference to a hyperslab in dset2, close dataspace.
     */
    status = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, stride, count, block);
    status = H5Rcreate(&wdata[1], file, DATASET2, H5R_DATASET_REGION, space);
    status = H5Sclose(space);

    /*
     * Create dataset with a scalar dataspace to serve as the parent
     * for the attribute.
     */
    space  = H5Screate(H5S_SCALAR);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the region references to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, H5T_STD_REF_DSETREG, space, H5P_DEFAULT);
    status = H5Awrite(attr, H5T_STD_REF_DSETREG, wdata);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Dclose(dset2);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);
    attr = H5Aopen_name(dset, ATTRIBUTE);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space  = H5Aget_space(attr);
    ndims  = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata  = (hdset_reg_ref_t *)malloc(dims[0] * sizeof(hdset_reg_ref_t));
    status = H5Sclose(space);

    /*
     * Read the data.
     */
    status = H5Aread(attr, H5T_STD_REF_DSETREG, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%d]:\n  ->", ATTRIBUTE, i);

        /*
         * Open the referenced object, retrieve its region as a
         * dataspace selection.
         */
        dset2 = H5Rdereference(dset, H5R_DATASET_REGION, &rdata[i]);
        space = H5Rget_region(dset, H5R_DATASET_REGION, &rdata[i]);

        /*
         * Get the length of the object's name, allocate space, then
         * retrieve the name.
         */
        size = 1 + H5Iget_name(dset2, NULL, 0);
        name = (char *)malloc(size);
        size = H5Iget_name(dset2, name, size);
        if (size <= 1)
            name[0] = '\0';

        /*
         * Allocate space for the read buffer.  We will only allocate
         * enough space for the selection, plus a null terminator.  The
         * read buffer will be 1-dimensional.
         */
        npoints = H5Sget_select_npoints(space);
        rdata2  = (char *)malloc(npoints + 1);

        /*
         * Read the dataset region, and add a null terminator so we can
         * print it as a string.
         */
        memspace        = H5Screate_simple(1, (hsize_t *)&npoints, NULL);
        status          = H5Dread(dset2, H5T_NATIVE_CHAR, memspace, space, H5P_DEFAULT, rdata2);
        rdata2[npoints] = '\0';

        /*
         * Print the name and region data, close and release resources.
         */
        printf(" %s: %s\n", name, rdata2);
        free(rdata2);
        free(name);
        status = H5Sclose(space);
        status = H5Sclose(memspace);
        status = H5Dclose(dset2);
    }

    /*
     * Close and release resources.
     */
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_string.c`

```c
/************************************************************

  This example shows how to read and write string datatypes
  to a dataset.  The program first writes strings to a
  dataset with a dataspace of DIM0, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_string.h5"
#define DATASET  "DS1"
#define DIM0     4
#define SDIM     8

int
main(void)
{
    hid_t file, filetype, memtype, space, dset;
    /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0};
    size_t  sdim;
    char    wdata[DIM0][SDIM] = {"Parting", "is such", "sweet", "sorrow."},
         /* Write buffer */
        **rdata; /* Read buffer */
    int ndims, i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create file and memory datatypes.  For this example we will save
     * the strings as FORTRAN strings, therefore they do not need space
     * for the null terminator in the file.
     */
    filetype = H5Tcopy(H5T_FORTRAN_S1);
    status   = H5Tset_size(filetype, SDIM - 1);
    memtype  = H5Tcopy(H5T_C_S1);
    status   = H5Tset_size(memtype, SDIM);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the string data to it.
     */
    dset   = H5Dcreate(file, DATASET, filetype, space, H5P_DEFAULT);
    status = H5Dwrite(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset and string have the same name and rank, but can have
     * any size.  Therefore we must allocate a new array to read in
     * data using malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get the datatype and its size.
     */
    filetype = H5Dget_type(dset);
    sdim     = H5Tget_size(filetype);
    sdim++; /* Make room for null terminator */

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (char **)malloc(dims[0] * sizeof(char *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (char *)malloc(dims[0] * sdim * sizeof(char));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * sdim;

    /*
     * Create the memory datatype.
     */
    memtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(memtype, sdim);

    /*
     * Read the data.
     */
    status = H5Dread(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++)
        printf("%s[%d]: %s\n", DATASET, i, rdata[i]);

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_stringatt.c`

```c
/************************************************************

  This example shows how to read and write string datatypes
  to an attribute.  The program first writes strings to an
  attribute with a dataspace of DIM0, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_stringatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4
#define SDIM      8

int
main(void)
{
    hid_t file, filetype, memtype, space, dset, attr;
    /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0};
    size_t  sdim;
    char    wdata[DIM0][SDIM] = {"Parting", "is such", "sweet", "sorrow."},
         /* Write buffer */
        **rdata; /* Read buffer */
    int ndims, i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create file and memory datatypes.  For this example we will save
     * the strings as FORTRAN strings, therefore they do not need space
     * for the null terminator in the file.
     */
    filetype = H5Tcopy(H5T_FORTRAN_S1);
    status   = H5Tset_size(filetype, SDIM - 1);
    memtype  = H5Tcopy(H5T_C_S1);
    status   = H5Tset_size(memtype, SDIM);

    /*
     * Create dataset with a scalar dataspace.
     */
    space  = H5Screate(H5S_SCALAR);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the string data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, filetype, space, H5P_DEFAULT);
    status = H5Awrite(attr, memtype, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute and string have the same name and rank, but can
     * have any size.  Therefore we must allocate a new array to read
     * in data using malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);
    attr = H5Aopen_name(dset, ATTRIBUTE);

    /*
     * Get the datatype and its size.
     */
    filetype = H5Aget_type(attr);
    sdim     = H5Tget_size(filetype);
    sdim++; /* Make room for null terminator */

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional attribute so the dynamic allocation must be done
     * in steps.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (char **)malloc(dims[0] * sizeof(char *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (char *)malloc(dims[0] * sdim * sizeof(char));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * sdim;

    /*
     * Create the memory datatype.
     */
    memtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(memtype, sdim);

    /*
     * Read the data.
     */
    status = H5Aread(attr, memtype, rdata[0]);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++)
        printf("%s[%d]: %s\n", ATTRIBUTE, i, rdata[i]);

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_vlen.c`

```c
/************************************************************

  This example shows how to read and write variable-length
  datatypes to a dataset.  The program first writes two
  variable-length integer arrays to a dataset then closes
  the file.  Next, it reopens the file, reads back the data,
  and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_vlen.h5"
#define DATASET  "DS1"
#define LEN0     3
#define LEN1     12

int
main(void)
{
    hid_t file, filetype, memtype, space, dset;
    /* Handles */
    herr_t status;
    hvl_t  wdata[2], /* Array of vlen structures */
        *rdata;      /* Pointer to vlen structures */
    hsize_t dims[1] = {2};
    int    *ptr, ndims, i, j;

    /*
     * Initialize variable-length data.  wdata[0] is a countdown of
     * length LEN0, wdata[1] is a Fibonacci sequence of length LEN1.
     */
    wdata[0].len = LEN0;
    ptr          = (int *)malloc(wdata[0].len * sizeof(int));
    for (i = 0; i < wdata[0].len; i++)
        ptr[i] = wdata[0].len - i; /* 3 2 1 */
    wdata[0].p = (void *)ptr;

    wdata[1].len = LEN1;
    ptr          = (int *)malloc(wdata[1].len * sizeof(int));
    ptr[0]       = 1;
    ptr[1]       = 1;
    for (i = 2; i < wdata[1].len; i++)
        ptr[i] = ptr[i - 1] + ptr[i - 2]; /* 1 1 2 3 5 8 etc. */
    wdata[1].p = (void *)ptr;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create variable-length datatype for file and memory.
     */
    filetype = H5Tvlen_create(H5T_STD_I32LE);
    memtype  = H5Tvlen_create(H5T_NATIVE_INT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the variable-length data to it.
     */
    dset   = H5Dcreate(file, DATASET, filetype, space, H5P_DEFAULT);
    status = H5Dwrite(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.  Note the use of H5Dvlen_reclaim
     * removes the need to manually free() the previously malloc'ed
     * data.
     */
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, wdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get dataspace and allocate memory for array of vlen structures.
     * This does not actually allocate memory for the vlen data, that
     * will be done by the library.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (hvl_t *)malloc(dims[0] * sizeof(hvl_t));

    /*
     * Create the memory datatype.
     */
    memtype = H5Tvlen_create(H5T_NATIVE_INT);

    /*
     * Read the data.
     */
    status = H5Dread(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the variable-length data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%u]:\n  {", DATASET, i);
        ptr = rdata[i].p;
        for (j = 0; j < rdata[i].len; j++) {
            printf(" %d", ptr[j]);
            if ((j + 1) < rdata[i].len)
                printf(",");
        }
        printf(" }\n");
    }

    /*
     * Close and release resources.  Note we must still free the
     * top-level pointer "rdata", as H5Dvlen_reclaim only frees the
     * actual variable-length data, and not the structures themselves.
     */
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, rdata);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_vlenatt.c`

```c
/************************************************************

  This example shows how to read and write variable-length
  datatypes to an attribute.  The program first writes two
  variable-length integer arrays to the attribute then
  closes the file.  Next, it reopens the file, reads back
  the data, and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_vlenatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define LEN0      3
#define LEN1      12

int
main(void)
{
    hid_t file, filetype, memtype, space, dset, attr;
    /* Handles */
    herr_t status;
    hvl_t  wdata[2], /* Array of vlen structures */
        *rdata;      /* Pointer to vlen structures */
    hsize_t dims[1] = {2};
    int    *ptr, ndims, i, j;

    /*
     * Initialize variable-length data.  wdata[0] is a countdown of
     * length LEN0, wdata[1] is a Fibonacci sequence of length LEN1.
     */
    wdata[0].len = LEN0;
    ptr          = (int *)malloc(wdata[0].len * sizeof(int));
    for (i = 0; i < wdata[0].len; i++)
        ptr[i] = wdata[0].len - i; /* 3 2 1 */
    wdata[0].p = (void *)ptr;

    wdata[1].len = LEN1;
    ptr          = (int *)malloc(wdata[1].len * sizeof(int));
    ptr[0]       = 1;
    ptr[1]       = 1;
    for (i = 2; i < wdata[1].len; i++)
        ptr[i] = ptr[i - 1] + ptr[i - 2]; /* 1 1 2 3 5 8 etc. */
    wdata[1].p = (void *)ptr;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create variable-length datatype for file and memory.
     */
    filetype = H5Tvlen_create(H5T_STD_I32LE);
    memtype  = H5Tvlen_create(H5T_NATIVE_INT);

    /*
     * Create dataset with a scalar dataspace.
     */
    space  = H5Screate(H5S_SCALAR);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the variable-length data to it
     */
    attr   = H5Acreate(dset, ATTRIBUTE, filetype, space, H5P_DEFAULT);
    status = H5Awrite(attr, memtype, wdata);

    /*
     * Close and release resources.  Note the use of H5Dvlen_reclaim
     * removes the need to manually free() the previously malloc'ed
     * data.
     */
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, wdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);
    attr = H5Aopen_name(dset, ATTRIBUTE);

    /*
     * Get dataspace and allocate memory for array of vlen structures.
     * This does not actually allocate memory for the vlen data, that
     * will be done by the library.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (hvl_t *)malloc(dims[0] * sizeof(hvl_t));

    /*
     * Create the memory datatype.
     */
    memtype = H5Tvlen_create(H5T_NATIVE_INT);

    /*
     * Read the data.
     */
    status = H5Aread(attr, memtype, rdata);

    /*
     * Output the variable-length data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%u]:\n  {", ATTRIBUTE, i);
        ptr = rdata[i].p;
        for (j = 0; j < rdata[i].len; j++) {
            printf(" %d", ptr[j]);
            if ((j + 1) < rdata[i].len)
                printf(",");
        }
        printf(" }\n");
    }

    /*
     * Close and release resources.  Note we must still free the
     * top-level pointer "rdata", as H5Dvlen_reclaim only frees the
     * actual variable-length data, and not the structures themselves.
     */
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, rdata);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_vlstring.c`

```c
/************************************************************

  This example shows how to read and write variable-length
  string datatypes to a dataset.  The program first writes
  variable-length strings to a dataset with a dataspace of
  DIM0, then closes the file.  Next, it reopens the file,
  reads back the data, and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_vlstring.h5"
#define DATASET  "DS1"
#define DIM0     4

int
main(void)
{
    hid_t file, filetype, memtype, space, dset;
    /* Handles */
    herr_t  status;
    hsize_t dims[1]     = {DIM0};
    char   *wdata[DIM0] = {"Parting", "is such", "sweet", "sorrow."},
         /* Write buffer */
        **rdata; /* Read buffer */
    int ndims, i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create file and memory datatypes.  For this example we will save
     * the strings as FORTRAN strings.
     */
    filetype = H5Tcopy(H5T_FORTRAN_S1);
    status   = H5Tset_size(filetype, H5T_VARIABLE);
    memtype  = H5Tcopy(H5T_C_S1);
    status   = H5Tset_size(memtype, H5T_VARIABLE);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the variable-length string data to
     * it.
     */
    dset   = H5Dcreate(file, DATASET, filetype, space, H5P_DEFAULT);
    status = H5Dwrite(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);

    /*
     * Get the datatype.
     */
    filetype = H5Dget_type(dset);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (char **)malloc(dims[0] * sizeof(char *));

    /*
     * Create the memory datatype.
     */
    memtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(memtype, H5T_VARIABLE);

    /*
     * Read the data.
     */
    status = H5Dread(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++)
        printf("%s[%d]: %s\n", DATASET, i, rdata[i]);

    /*
     * Close and release resources.  Note that H5Dvlen_reclaim works
     * for variable-length strings as well as variable-length arrays.
     * Also note that we must still free the array of pointers stored
     * in rdata, as H5Tvlen_reclaim only frees the data these point to.
     */
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, rdata);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/16/h5ex_t_vlstringatt.c`

```c
/************************************************************

  This example shows how to read and write variable-length
  string datatypes to an attribute.  The program first
  writes variable-length strings to an attribute with a
  dataspace of DIM0, then closes the file.  Next, it reopens
  the file, reads back the data, and outputs it to the
  screen.

  This file is intended for use with HDF5 Library version 1.6

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_vlstringatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4

int
main(void)
{
    hid_t file, filetype, memtype, space, dset, attr;
    /* Handles */
    herr_t  status;
    hsize_t dims[1]     = {DIM0};
    char   *wdata[DIM0] = {"Parting", "is such", "sweet", "sorrow."},
         /* Write buffer */
        **rdata; /* Read buffer */
    int ndims, i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create file and memory datatypes.  For this example we will save
     * the strings as FORTRAN strings.
     */
    filetype = H5Tcopy(H5T_FORTRAN_S1);
    status   = H5Tset_size(filetype, H5T_VARIABLE);
    memtype  = H5Tcopy(H5T_C_S1);
    status   = H5Tset_size(memtype, H5T_VARIABLE);

    /*
     * Create dataset with a scalar dataspace.
     */
    space  = H5Screate(H5S_SCALAR);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the variable-length string data
     * to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, filetype, space, H5P_DEFAULT);
    status = H5Awrite(attr, memtype, wdata);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET);
    attr = H5Aopen_name(dset, ATTRIBUTE);

    /*
     * Get the datatype.
     */
    filetype = H5Aget_type(attr);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (char **)malloc(dims[0] * sizeof(char *));

    /*
     * Create the memory datatype.
     */
    memtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(memtype, H5T_VARIABLE);

    /*
     * Read the data.
     */
    status = H5Aread(attr, memtype, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++)
        printf("%s[%d]: %s\n", ATTRIBUTE, i, rdata[i]);

    /*
     * Close and release resources.  Note that H5Dvlen_reclaim works
     * for variable-length strings as well as variable-length arrays.
     * Also note that we must still free the array of pointers stored
     * in rdata, as H5Tvlen_reclaim only frees the data these point to.
     */
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, rdata);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/200/h5ex_t_complex.c`

```c
/************************************************************

  This example shows how to read and write complex number
  datatypes to a dataset.  The program first writes float
  complex values to a dataset with a dataspace of DIM0xDIM1,
  then closes the file.  Next, it reopens the file, reads
  back the data, and outputs it to the screen. This example
  assumes the C99 complex number types are supported. For an
  example that uses MSVC's complex number types, see the
  h5ex_t_complex_msvc.c example file.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>
#include <complex.h>

#define FILENAME "h5ex_t_complex.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7

int
main(void)
{
    float _Complex wdata[DIM0][DIM1];   /* Write buffer */
    float _Complex **rdata;             /* Read buffer */
    hid_t            file, space, dset; /* Handles */
    herr_t           status;
    hsize_t          dims[2] = {DIM0, DIM1};
    int              ndims;
    hsize_t          i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            float real      = (float)i / (j + 0.5) + j;
            float imaginary = (float)i / (j + 0.5) + j + 1;
            wdata[i][j]     = real + imaginary * I;
        }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset and write the complex number data to it.  In
     * this example we will save the data as complex numbers of 2 64-bit
     * little endian IEEE floating point numbers, regardless of the native
     * type.  The HDF5 library automatically converts between different
     * complex number types.
     */
    dset   = H5Dcreate(file, DATASET, H5T_COMPLEX_IEEE_F64LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_NATIVE_FLOAT_COMPLEX, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = malloc(dims[0] * sizeof(float _Complex *));

    /*
     * Allocate space for complex number data.
     */
    rdata[0] = malloc(dims[0] * dims[1] * sizeof(float _Complex));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Dread(dset, H5T_NATIVE_FLOAT_COMPLEX, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++) {
            printf(" %6.4f%+6.4fi", crealf(rdata[i][j]), cimagf(rdata[i][j]));
        }
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/200/h5ex_t_complex_custom.c`

```c
/************************************************************

  This example shows how to read and write complex number
  datatypes to a dataset.  The program first writes float
  complex values to a dataset with a dataspace of DIM0xDIM1,
  then closes the file.  Next, it reopens the file, reads
  back the data, and outputs it to the screen. This example
  assumes the C99 complex number types are supported. For an
  example that uses MSVC's complex number types, see the
  h5ex_t_complex_msvc.c example file.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>
#include <complex.h>

#define FILENAME "h5ex_t_complex_custom.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7

int
main(void)
{
    float _Complex wdata[DIM0][DIM1];          /* Write buffer */
    float _Complex **rdata;                    /* Read buffer */
    hid_t            file, space, dset, dtype; /* Handles */
    herr_t           status;
    hsize_t          dims[2] = {DIM0, DIM1};
    int              ndims;
    hsize_t          i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            float real      = (float)i / (j + 0.5) + j;
            float imaginary = (float)i / (j + 0.5) + j + 1;
            wdata[i][j]     = real + imaginary * I;
        }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset and write the complex number data to it.  In
     * this example we will save the data as complex numbers of 2 64-bit
     * little endian IEEE floating point numbers, regardless of the native
     * type.  The HDF5 library automatically converts between different
     * complex number types.
     */
    dset = H5Dcreate(file, DATASET, H5T_COMPLEX_IEEE_F64LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create a datatype for writing to the dataset. This datatype is a
     * complex number equivalent to the H5T_NATIVE_FLOAT_COMPLEX type.
     */
    dtype = H5Tcomplex_create(H5T_NATIVE_FLOAT);

    status = H5Dwrite(dset, dtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = malloc(dims[0] * sizeof(float _Complex *));

    /*
     * Allocate space for complex number data.
     */
    rdata[0] = malloc(dims[0] * dims[1] * sizeof(float _Complex));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Dread(dset, dtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++) {
            printf(" %6.4f%+6.4fi", crealf(rdata[i][j]), cimagf(rdata[i][j]));
        }
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(dtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/200/h5ex_t_complex_msvc.c`

```c
/************************************************************

  This example shows how to read and write complex number
  datatypes to a dataset.  The program first writes float
  complex values to a dataset with a dataspace of DIM0xDIM1,
  then closes the file.  Next, it reopens the file, reads
  back the data, and outputs it to the screen. This example
  assumes MSVC's complex number types are supported rather
  than the C99 complex number types. For an example that uses
  the C99 complex number types, see the h5ex_t_complex.c
  example file.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>
#include <complex.h>

#define FILENAME "h5ex_t_complex_msvc.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7

int
main(void)
{
    _Fcomplex   wdata[DIM0][DIM1]; /* Write buffer */
    _Fcomplex **rdata;             /* Read buffer */
    hid_t       file, space, dset; /* Handles */
    herr_t      status;
    hsize_t     dims[2] = {DIM0, DIM1};
    int         ndims;
    hsize_t     i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            float real      = (float)i / (j + 0.5) + j;
            float imaginary = (float)i / (j + 0.5) + j + 1;
            wdata[i][j]     = _FCbuild(real, imaginary);
        }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset and write the complex number data to it.  In
     * this example we will save the data as complex numbers of 2 64-bit
     * little endian IEEE floating point numbers, regardless of the native
     * type.  The HDF5 library automatically converts between different
     * complex number types.
     */
    dset   = H5Dcreate(file, DATASET, H5T_COMPLEX_IEEE_F64LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_NATIVE_FLOAT_COMPLEX, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = malloc(dims[0] * sizeof(_Fcomplex *));

    /*
     * Allocate space for complex number data.
     */
    rdata[0] = malloc(dims[0] * dims[1] * sizeof(_Fcomplex));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Dread(dset, H5T_NATIVE_FLOAT_COMPLEX, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++) {
            printf(" %6.4f%+6.4fi", crealf(rdata[i][j]), cimagf(rdata[i][j]));
        }
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_C_H5T C)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (C_sourcefiles.cmake)

foreach (example_name ${common_examples})
  if (${H5_LIBVER_DIR} EQUAL 16 OR ${EXAMPLE_VARNAME}_USE_16_API)
    add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/16/${example_name}.c)
  else ()
    add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
  endif ()
  target_compile_options (${EXAMPLE_VARNAME}_${example_name}
      PRIVATE
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  if (H5_HAVE_PARALLEL)
    target_include_directories (${EXAMPLE_VARNAME}_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
  endif ()
  target_link_libraries (${EXAMPLE_VARNAME}_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
  if (H5EXAMPLE_BUILD_TESTING)
    add_custom_command (
        TARGET     ${EXAMPLE_VARNAME}_${example_name}
        POST_BUILD
        COMMAND    ${CMAKE_COMMAND}
        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/16/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
    )
  endif ()
endforeach ()

#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8")
#  foreach (example_name ${1_8_examples})
#    add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
#    target_compile_options(${EXAMPLE_VARNAME}_${example_name}
#        PRIVATE
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
#    )
#    if (H5_HAVE_PARALLEL)
#      target_include_directories (${EXAMPLE_VARNAME}_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
#    endif ()
#    target_link_libraries (${EXAMPLE_VARNAME}_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
#  foreach (example_name ${1_10_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
#  foreach (example_name ${1_12_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#          add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
#  foreach (example_name ${1_14_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
  foreach (example_name ${2_0_examples})
    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
    endif ()
  endforeach ()
endif ()

if (HDF5_PROVIDES_TOOLS)
  foreach (example_name ${common_examples})
    if (NOT ${example_name} STREQUAL "h5ex_t_convert")
        if (${example_name} STREQUAL "h5ex_t_vlen" OR ${example_name} STREQUAL "h5ex_t_vlenatt")
          if (HDF5_VERSION_STRING VERSION_GREATER_EQUAL "1.14.3")
            if ((${EXAMPLE_VARNAME}_USE_16_API OR ${H5_LIBVER_DIR} EQUAL 16) AND ${example_name} STREQUAL "h5ex_t_vlenatt")
              add_custom_command (
                  TARGET     ${EXAMPLE_VARNAME}_${example_name}
                  POST_BUILD
                  COMMAND    ${CMAKE_COMMAND}
                  ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/16/114/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
              )
            else ()
              add_custom_command (
                  TARGET     ${EXAMPLE_VARNAME}_${example_name}
                  POST_BUILD
                  COMMAND    ${CMAKE_COMMAND}
                  ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
              )
            endif ()
          elseif (${EXAMPLE_VARNAME}_USE_16_API OR ${H5_LIBVER_DIR} EQUAL 16)
            add_custom_command (
                TARGET     ${EXAMPLE_VARNAME}_${example_name}
                POST_BUILD
                COMMAND    ${CMAKE_COMMAND}
                ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/16/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
            )
          else ()
            add_custom_command (
                TARGET     ${EXAMPLE_VARNAME}_${example_name}
                POST_BUILD
                COMMAND    ${CMAKE_COMMAND}
                ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
            )
          endif ()
        elseif ((${example_name} STREQUAL "h5ex_t_objref" OR ${example_name} STREQUAL "h5ex_t_objrefatt") OR (${example_name} STREQUAL "h5ex_t_regref" OR ${example_name} STREQUAL "h5ex_t_regrefatt"))
          if (${EXAMPLE_VARNAME}_USE_16_API OR ${EXAMPLE_VARNAME}_USE_18_API OR ${EXAMPLE_VARNAME}_USE_110_API)
            if (HDF5_VERSION_MAJOR VERSION_EQUAL "1.8")
              if (HDF5_VERSION_STRING VERSION_LESS_EQUAL "1.8.21")
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}21.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              else ()
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              endif ()
            elseif (HDF5_VERSION_MAJOR VERSION_EQUAL "1.10")
              if (HDF5_VERSION_STRING VERSION_LESS_EQUAL "1.10.6")
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}06.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              else ()
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              endif ()
            elseif (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
              if (${EXAMPLE_VARNAME}_USE_16_API OR ${H5_LIBVER_DIR} EQUAL 16)
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/16/112/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              else ()
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/112/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              endif ()
            else ()
              if (${EXAMPLE_VARNAME}_USE_16_API OR ${H5_LIBVER_DIR} EQUAL 16)
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/16/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              else ()
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              endif ()
            endif ()
          else ()
            if (HDF5_VERSION_MAJOR VERSION_EQUAL "1.8")
              if (HDF5_VERSION_STRING VERSION_LESS_EQUAL "1.8.21")
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}21.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              else ()
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              endif ()
            elseif (HDF5_VERSION_MAJOR VERSION_EQUAL "1.10")
              if (HDF5_VERSION_STRING VERSION_LESS_EQUAL "1.10.6")
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}06.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              else ()
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              endif ()
            elseif (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
            else ()
              add_custom_command (
                  TARGET     ${EXAMPLE_VARNAME}_${example_name}
                  POST_BUILD
                  COMMAND    ${CMAKE_COMMAND}
                  ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
              )
            endif ()
          endif ()
        elseif (${H5_LIBVER_DIR} EQUAL 16)
          add_custom_command (
              TARGET     ${EXAMPLE_VARNAME}_${example_name}
              POST_BUILD
              COMMAND    ${CMAKE_COMMAND}
              ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/16/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
          )
        else ()
          if (${EXAMPLE_VARNAME}_USE_16_API)
            add_custom_command (
                TARGET     ${EXAMPLE_VARNAME}_${example_name}
                POST_BUILD
                COMMAND    ${CMAKE_COMMAND}
                ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/16/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
            )
          else ()
            add_custom_command (
                TARGET     ${EXAMPLE_VARNAME}_${example_name}
                POST_BUILD
                COMMAND    ${CMAKE_COMMAND}
                ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
            )
          endif ()
        endif ()
      endif ()
  endforeach ()

#  foreach (example_name ${1_8_examples})
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
#  endforeach ()
#  foreach (example_name ${1_10_examples})
#  endforeach ()
#  foreach (example_name ${1_12_examples})
#  endforeach ()
#  foreach (example_name ${1_14_examples})
#  endforeach ()
#  foreach (example_name ${2_0_examples})
  #  add_custom_command (
  #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
  #      POST_BUILD
  #      COMMAND    ${CMAKE_COMMAND}
  #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
  #  )
#  endforeach ()
endif ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
    )
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${ARGN};${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-${testname} PROPERTIES
            DEPENDS ${EXAMPLE_VARNAME}_${testname}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
      endif ()
    endif ()
  endmacro ()

  macro (ADD_H5_CMP_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
    )
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>"
              -D "TEST_ARGS:STRING=${ARGN}"
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
    endif ()
  endmacro ()

  foreach (example_name ${common_examples})
    if (${example_name} STREQUAL "h5ex_t_convert")
      ADD_H5_CMP_TEST (${example_name})
    elseif (${example_name} STREQUAL "h5ex_t_cpxcmpd" OR ${example_name} STREQUAL "h5ex_t_cpxcmpdatt")
      ADD_H5_TEST (${example_name} -n)
    else ()
      ADD_H5_TEST (${example_name})
    endif ()
  endforeach ()

#  foreach (example_name ${2_0_examples})
#      ADD_H5_TEST (${example_name})
#  endforeach ()
endif ()
```

### `HDF5Examples/C/H5T/C_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (common_examples
    h5ex_t_array
    h5ex_t_arrayatt
    h5ex_t_bit
    h5ex_t_bitatt
    h5ex_t_cmpd
    h5ex_t_cmpdatt
    h5ex_t_enum
    h5ex_t_enumatt
    h5ex_t_float
    h5ex_t_floatatt
    h5ex_t_int
    h5ex_t_intatt
    h5ex_t_objref
    h5ex_t_objrefatt
    h5ex_t_opaque
    h5ex_t_opaqueatt
    h5ex_t_regref
    h5ex_t_regrefatt
    h5ex_t_string
    h5ex_t_stringatt
    h5ex_t_vlen
    h5ex_t_vlenatt
    h5ex_t_vlstring
    h5ex_t_vlstringatt
    h5ex_t_cpxcmpd
    h5ex_t_cpxcmpdatt
    h5ex_t_commit
    h5ex_t_convert
)

set (2_0_examples
    h5ex_t_complex
    h5ex_t_complex_custom
    h5ex_t_complex_msvc
)
```

### `HDF5Examples/C/H5T/h5ex_t_array.c`

```c
/************************************************************

  This example shows how to read and write array datatypes
  to a dataset.  The program first writes integers arrays of
  dimension ADIM0xADIM1 to a dataset with a dataspace of
  DIM0, then closes the  file.  Next, it reopens the file,
  reads back the data, and outputs it to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_array.h5"
#define DATASET  "DS1"
#define DIM0     4
#define ADIM0    3
#define ADIM1    5

int
main(void)
{
    hid_t file     = H5I_INVALID_HID; /* File Handle */
    hid_t space    = H5I_INVALID_HID; /* Dataspace Handle */
    hid_t dset     = H5I_INVALID_HID; /* Dataset Handle */
    hid_t filetype = H5I_INVALID_HID;
    hid_t memtype  = H5I_INVALID_HID;
    /* Handles */
    herr_t  status;
    hsize_t dims[1]  = {DIM0};
    hsize_t adims[2] = {ADIM0, ADIM1};
    int     wdata[DIM0][ADIM0][ADIM1]; /* Write buffer */
    int  ***rdata = NULL;              /* Read buffer */
    int     ndims;
    hsize_t i, j, k;

    /*
     * Initialize data.  i is the element in the dataspace, j and k the
     * elements within the array datatype.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < ADIM0; j++)
            for (k = 0; k < ADIM1; k++)
                wdata[i][j][k] = i * j - j * k + i * k;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create array datatypes for file and memory.
     */
    filetype = H5Tarray_create(H5T_STD_I64LE, 2, adims);
    memtype  = H5Tarray_create(H5T_NATIVE_INT, 2, adims);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the array data to it.
     */
    dset   = H5Dcreate(file, DATASET, filetype, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0][0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset and array have the same name and rank, but can have
     * any size.  Therefore we must allocate a new array to read in
     * data using malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get the datatype and its dimensions.
     */
    filetype = H5Dget_type(dset);
    ndims    = H5Tget_array_dims(filetype, adims);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * three dimensional dataset when the array datatype is included so
     * the dynamic allocation must be done in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to two-dimensional arrays (the
     * elements of the dataset.
     */
    rdata = (int ***)malloc(dims[0] * sizeof(int **));

    /*
     * Allocate two dimensional array of pointers to rows in the data
     * elements.
     */
    rdata[0] = (int **)malloc(dims[0] * adims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0][0] = (int *)malloc(dims[0] * adims[0] * adims[1] * sizeof(int));

    /*
     * Set the members of the pointer arrays allocated above to point
     * to the correct locations in their respective arrays.
     */
    for (i = 0; i < dims[0]; i++) {
        rdata[i] = rdata[0] + i * adims[0];
        for (j = 0; j < adims[0]; j++)
            rdata[i][j] = rdata[0][0] + (adims[0] * adims[1] * i) + (adims[1] * j);
    }

    /*
     * Create the memory datatype.
     */
    memtype = H5Tarray_create(H5T_NATIVE_INT, 2, adims);

    /*
     * Read the data.
     */
    status = H5Dread(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0][0]);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]:\n", DATASET, i);
        for (j = 0; j < adims[0]; j++) {
            printf(" [");
            for (k = 0; k < adims[1]; k++)
                printf(" %3d", rdata[i][j][k]);
            printf("]\n");
        }
        printf("\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0][0]);
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_arrayatt.c`

```c
/************************************************************

  This example shows how to read and write array datatypes
  to an attribute.  The program first writes integers arrays
  of dimension ADIM0xADIM1 to an attribute with a dataspace
  of DIM0, then closes the  file.  Next, it reopens the
  file, reads back the data, and outputs it to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_arrayatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4
#define ADIM0     3
#define ADIM1     5

int
main(void)
{
    hid_t file     = H5I_INVALID_HID; /* File Handle */
    hid_t space    = H5I_INVALID_HID; /* Dataspace Handle */
    hid_t dset     = H5I_INVALID_HID; /* Dataset Handle */
    hid_t filetype = H5I_INVALID_HID;
    hid_t memtype  = H5I_INVALID_HID;
    hid_t attr     = H5I_INVALID_HID; /* Attribute Handle */
    /* Handles */
    herr_t  status;
    hsize_t dims[1]  = {DIM0};
    hsize_t adims[2] = {ADIM0, ADIM1};
    int     wdata[DIM0][ADIM0][ADIM1]; /* Write buffer */
    int  ***rdata = NULL;              /* Read buffer */
    int     ndims;
    hsize_t i, j, k;

    /*
     * Initialize data.  i is the element in the dataspace, j and k the
     * elements within the array datatype.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < ADIM0; j++)
            for (k = 0; k < ADIM1; k++)
                wdata[i][j][k] = i * j - j * k + i * k;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create array datatypes for file and memory.
     */
    filetype = H5Tarray_create(H5T_STD_I64LE, 2, adims);
    memtype  = H5Tarray_create(H5T_NATIVE_INT, 2, adims);

    /*
     * Create dataset with a null dataspace.
     */
    space  = H5Screate(H5S_NULL);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the array data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, filetype, space, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Awrite(attr, memtype, wdata[0][0]);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute and array have the same name and rank, but can
     * have any size.  Therefore we must allocate a new array to read
     * in data using malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    attr = H5Aopen(dset, ATTRIBUTE, H5P_DEFAULT);

    /*
     * Get the datatype and its dimensions.
     */
    filetype = H5Aget_type(attr);
    ndims    = H5Tget_array_dims(filetype, adims);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * three dimensional attribute when the array datatype is included
     * so the dynamic allocation must be done in steps.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to two-dimensional arrays (the
     * elements of the attribute.
     */
    rdata = (int ***)malloc(dims[0] * sizeof(int **));

    /*
     * Allocate two dimensional array of pointers to rows in the data
     * elements.
     */
    rdata[0] = (int **)malloc(dims[0] * adims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0][0] = (int *)malloc(dims[0] * adims[0] * adims[1] * sizeof(int));

    /*
     * Set the members of the pointer arrays allocated above to point
     * to the correct locations in their respective arrays.
     */
    for (i = 0; i < dims[0]; i++) {
        rdata[i] = rdata[0] + i * adims[0];
        for (j = 0; j < adims[0]; j++)
            rdata[i][j] = rdata[0][0] + (adims[0] * adims[1] * i) + (adims[1] * j);
    }

    /*
     * Create the memory datatype.
     */
    memtype = H5Tarray_create(H5T_NATIVE_INT, 2, adims);

    /*
     * Read the data.
     */
    status = H5Aread(attr, memtype, rdata[0][0]);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]:\n", ATTRIBUTE, i);
        for (j = 0; j < adims[0]; j++) {
            printf(" [");
            for (k = 0; k < adims[1]; k++)
                printf(" %3d", rdata[i][j][k]);
            printf("]\n");
        }
        printf("\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0][0]);
    free(rdata[0]);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_bit.c`

```c
/************************************************************

  This example shows how to read and write bitfield
  datatypes to a dataset.  The program first writes bit
  fields to a dataset with a dataspace of DIM0xDIM1, then
  closes the file.  Next, it reopens the file, reads back
  the data, and outputs it to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_bit.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7

int
main(void)
{
    hid_t         file, space, dset; /* Handles */
    herr_t        status;
    hsize_t       dims[2] = {DIM0, DIM1};
    unsigned char wdata[DIM0][DIM1], /* Write buffer */
        **rdata;                     /* Read buffer */
    int     ndims, A, B, C, D;
    hsize_t i, j;

    /*
     * Initialize data.  We will manually pack 4 2-bit integers into
     * each unsigned char data element.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            wdata[i][j] = 0;
            wdata[i][j] |= (i * j - j) & 0x03;    /* Field "A" */
            wdata[i][j] |= (i & 0x03) << 2;       /* Field "B" */
            wdata[i][j] |= (j & 0x03) << 4;       /* Field "C" */
            wdata[i][j] |= ((i + j) & 0x03) << 6; /* Field "D" */
        }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset and write the bitfield data to it.
     */
    dset   = H5Dcreate(file, DATASET, H5T_STD_B8BE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_NATIVE_B8, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (unsigned char **)malloc(dims[0] * sizeof(unsigned char *));

    /*
     * Allocate space for bitfield data.
     */
    rdata[0] = (unsigned char *)malloc(dims[0] * dims[1] * sizeof(unsigned char));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Dread(dset, H5T_NATIVE_B8, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++) {
            A = rdata[i][j] & 0x03;        /* Retrieve field "A" */
            B = (rdata[i][j] >> 2) & 0x03; /* Retrieve field "B" */
            C = (rdata[i][j] >> 4) & 0x03; /* Retrieve field "C" */
            D = (rdata[i][j] >> 6) & 0x03; /* Retrieve field "D" */
            printf(" {%d, %d, %d, %d}", A, B, C, D);
        }
        printf(" ]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_bitatt.c`

```c
/************************************************************

  This example shows how to read and write bitfield
  datatypes to an attribute.  The program first writes bit
  fields to an attribute with a dataspace of DIM0xDIM1, then
  closes the file.  Next, it reopens the file, reads back
  the data, and outputs it to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_bitatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4
#define DIM1      7

int
main(void)
{
    hid_t         file, space, dset, attr; /* Handles */
    herr_t        status;
    hsize_t       dims[2] = {DIM0, DIM1};
    unsigned char wdata[DIM0][DIM1], /* Write buffer */
        **rdata;                     /* Read buffer */
    int     ndims, A, B, C, D;
    hsize_t i, j;

    /*
     * Initialize data.  We will manually pack 4 2-bit integers into
     * each unsigned char data element.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++) {
            wdata[i][j] = 0;
            wdata[i][j] |= (i * j - j) & 0x03;    /* Field "A" */
            wdata[i][j] |= (i & 0x03) << 2;       /* Field "B" */
            wdata[i][j] |= (j & 0x03) << 4;       /* Field "C" */
            wdata[i][j] |= ((i + j) & 0x03) << 6; /* Field "D" */
        }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataset with a null dataspace.
     */
    space  = H5Screate(H5S_NULL);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the attribute and write the bitfield data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, H5T_STD_B8BE, space, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Awrite(attr, H5T_NATIVE_B8, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    attr = H5Aopen(dset, ATTRIBUTE, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional attribute so the dynamic allocation must be done
     * in steps.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (unsigned char **)malloc(dims[0] * sizeof(unsigned char *));

    /*
     * Allocate space for bitfield data.
     */
    rdata[0] = (unsigned char *)malloc(dims[0] * dims[1] * sizeof(unsigned char));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Aread(attr, H5T_NATIVE_B8, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", ATTRIBUTE);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++) {
            A = rdata[i][j] & 0x03;        /* Retrieve field "A" */
            B = (rdata[i][j] >> 2) & 0x03; /* Retrieve field "B" */
            C = (rdata[i][j] >> 4) & 0x03; /* Retrieve field "C" */
            D = (rdata[i][j] >> 6) & 0x03; /* Retrieve field "D" */
            printf(" {%d, %d, %d, %d}", A, B, C, D);
        }
        printf(" ]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_cmpd.c`

```c
/************************************************************

  This example shows how to read and write compound
  datatypes to a dataset.  The program first writes
  compound structures to a dataset with a dataspace of DIM0,
  then closes the file.  Next, it reopens the file, reads
  back the data, and outputs it to the screen.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_cmpd.h5"
#define DATASET  "DS1"
#define DIM0     4

typedef struct {
    int    serial_no;
    char  *location;
    double temperature;
    double pressure;
} sensor_t; /* Compound type */

int
main(void)
{
    hid_t file, filetype, memtype, strtype, space, dset;
    /* Handles */
    herr_t   status;
    hsize_t  dims[1] = {DIM0};
    sensor_t wdata[DIM0], /* Write buffer */
        *rdata;           /* Read buffer */
    int     ndims;
    hsize_t i;

    /*
     * Initialize data.
     */
    wdata[0].serial_no   = 1153;
    wdata[0].location    = "Exterior (static)";
    wdata[0].temperature = 53.23;
    wdata[0].pressure    = 24.57;
    wdata[1].serial_no   = 1184;
    wdata[1].location    = "Intake";
    wdata[1].temperature = 55.12;
    wdata[1].pressure    = 22.95;
    wdata[2].serial_no   = 1027;
    wdata[2].location    = "Intake manifold";
    wdata[2].temperature = 103.55;
    wdata[2].pressure    = 31.23;
    wdata[3].serial_no   = 1313;
    wdata[3].location    = "Exhaust manifold";
    wdata[3].temperature = 1252.89;
    wdata[3].pressure    = 84.11;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the compound datatype for memory.
     */
    memtype = H5Tcreate(H5T_COMPOUND, sizeof(sensor_t));
    status  = H5Tinsert(memtype, "Serial number", HOFFSET(sensor_t, serial_no), H5T_NATIVE_INT);
    status  = H5Tinsert(memtype, "Location", HOFFSET(sensor_t, location), strtype);
    status  = H5Tinsert(memtype, "Temperature (F)", HOFFSET(sensor_t, temperature), H5T_NATIVE_DOUBLE);
    status  = H5Tinsert(memtype, "Pressure (inHg)", HOFFSET(sensor_t, pressure), H5T_NATIVE_DOUBLE);

    /*
     * Create the compound datatype for the file.  Because the standard
     * types we are using for the file may have different sizes than
     * the corresponding native types, we must manually calculate the
     * offset of each member.
     */
    filetype = H5Tcreate(H5T_COMPOUND, 8 + sizeof(hvl_t) + 8 + 8);
    status   = H5Tinsert(filetype, "Serial number", 0, H5T_STD_I64BE);
    status   = H5Tinsert(filetype, "Location", 8, strtype);
    status   = H5Tinsert(filetype, "Temperature (F)", 8 + sizeof(hvl_t), H5T_IEEE_F64BE);
    status   = H5Tinsert(filetype, "Pressure (inHg)", 8 + sizeof(hvl_t) + 8, H5T_IEEE_F64BE);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the compound data to it.
     */
    dset   = H5Dcreate(file, DATASET, filetype, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().  For simplicity, we do not rebuild memtype.
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (sensor_t *)malloc(dims[0] * sizeof(sensor_t));

    /*
     * Read the data.
     */
    status = H5Dread(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]:\n", DATASET, i);
        printf("Serial number   : %d\n", rdata[i].serial_no);
        printf("Location        : %s\n", rdata[i].location);
        printf("Temperature (F) : %f\n", rdata[i].temperature);
        printf("Pressure (inHg) : %f\n\n", rdata[i].pressure);
    }

    /*
     * Close and release resources.  H5Dvlen_reclaim will automatically
     * traverse the structure and free any vlen data (strings in this
     * case).
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Treclaim(memtype, space, H5P_DEFAULT, rdata);
#else
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, rdata);
#endif
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(memtype);
    status = H5Tclose(strtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_cmpdatt.c`

```c
/************************************************************

  This example shows how to read and write compound
  datatypes to an attribute.  The program first writes
  compound structures to an attribute with a dataspace of
  DIM0, then closes the file.  Next, it reopens the file,
  reads back the data, and outputs it to the screen.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_cmpdatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4

typedef struct {
    int    serial_no;
    char  *location;
    double temperature;
    double pressure;
} sensor_t; /* Compound type */

int
main(void)
{
    hid_t file, filetype, memtype, strtype, space, dset, attr;
    /* Handles */
    herr_t   status;
    hsize_t  dims[1] = {DIM0};
    sensor_t wdata[DIM0], /* Write buffer */
        *rdata;           /* Read buffer */
    int     ndims;
    hsize_t i;

    /*
     * Initialize data.
     */
    wdata[0].serial_no   = 1153;
    wdata[0].location    = "Exterior (static)";
    wdata[0].temperature = 53.23;
    wdata[0].pressure    = 24.57;
    wdata[1].serial_no   = 1184;
    wdata[1].location    = "Intake";
    wdata[1].temperature = 55.12;
    wdata[1].pressure    = 22.95;
    wdata[2].serial_no   = 1027;
    wdata[2].location    = "Intake manifold";
    wdata[2].temperature = 103.55;
    wdata[2].pressure    = 31.23;
    wdata[3].serial_no   = 1313;
    wdata[3].location    = "Exhaust manifold";
    wdata[3].temperature = 1252.89;
    wdata[3].pressure    = 84.11;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the compound datatype for memory.
     */
    memtype = H5Tcreate(H5T_COMPOUND, sizeof(sensor_t));
    status  = H5Tinsert(memtype, "Serial number", HOFFSET(sensor_t, serial_no), H5T_NATIVE_INT);
    status  = H5Tinsert(memtype, "Location", HOFFSET(sensor_t, location), strtype);
    status  = H5Tinsert(memtype, "Temperature (F)", HOFFSET(sensor_t, temperature), H5T_NATIVE_DOUBLE);
    status  = H5Tinsert(memtype, "Pressure (inHg)", HOFFSET(sensor_t, pressure), H5T_NATIVE_DOUBLE);

    /*
     * Create the compound datatype for the file.  Because the standard
     * types we are using for the file may have different sizes than
     * the corresponding native types, we must manually calculate the
     * offset of each member.
     */
    filetype = H5Tcreate(H5T_COMPOUND, 8 + sizeof(hvl_t) + 8 + 8);
    status   = H5Tinsert(filetype, "Serial number", 0, H5T_STD_I64BE);
    status   = H5Tinsert(filetype, "Location", 8, strtype);
    status   = H5Tinsert(filetype, "Temperature (F)", 8 + sizeof(hvl_t), H5T_IEEE_F64BE);
    status   = H5Tinsert(filetype, "Pressure (inHg)", 8 + sizeof(hvl_t) + 8, H5T_IEEE_F64BE);

    /*
     * Create dataset with a null dataspace.
     */
    space  = H5Screate(H5S_NULL);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the compound data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, filetype, space, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Awrite(attr, memtype, wdata);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().  For simplicity, we do not rebuild memtype.
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    attr = H5Aopen(dset, ATTRIBUTE, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (sensor_t *)malloc(dims[0] * sizeof(sensor_t));

    /*
     * Read the data.
     */
    status = H5Aread(attr, memtype, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]:\n", ATTRIBUTE, i);
        printf("Serial number   : %d\n", rdata[i].serial_no);
        printf("Location        : %s\n", rdata[i].location);
        printf("Temperature (F) : %f\n", rdata[i].temperature);
        printf("Pressure (inHg) : %f\n\n", rdata[i].pressure);
    }

    /*
     * Close and release resources.  H5Dvlen_reclaim will automatically
     * traverse the structure and free any vlen data (strings in this
     * case).
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Treclaim(memtype, space, H5P_DEFAULT, rdata);
#else
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, rdata);
#endif
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(memtype);
    status = H5Tclose(strtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_commit.c`

```c
/************************************************************

  This example shows how to commit a named datatype to a
  file, and read back that datatype.  The program first
  defines a compound datatype, commits it to a file, then
  closes the file.  Next, it reopens the file, opens the
  datatype, and outputs the names of its fields to the
  screen.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_commit.h5"
#define DATATYPE "Sensor_Type"

int
main(void)
{
    hid_t file, filetype, strtype;
    /* Handles */
    herr_t      status;
    H5T_class_t typeclass;
    char       *name;
    int         nmembs, i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the compound datatype.  Because the standard types we are
     * using may have different sizes than the corresponding native
     * types, we must manually calculate the offset of each member.
     */
    filetype = H5Tcreate(H5T_COMPOUND, 8 + sizeof(char *) + 8 + 8);
    status   = H5Tinsert(filetype, "Serial number", 0, H5T_STD_I64BE);
    status   = H5Tinsert(filetype, "Location", 8, strtype);
    status   = H5Tinsert(filetype, "Temperature (F)", 8 + sizeof(char *), H5T_IEEE_F64BE);
    status   = H5Tinsert(filetype, "Pressure (inHg)", 8 + sizeof(char *) + 8, H5T_IEEE_F64BE);

    /*
     * Commit the compound datatype to the file, creating a named
     * datatype.
     */
    status = H5Tcommit(file, DATATYPE, filetype, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Close and release resources.
     */
    status = H5Tclose(filetype);
    status = H5Tclose(strtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);

    /*
     * Open the named datatype.
     */
    filetype = H5Topen(file, DATATYPE, H5P_DEFAULT);

    /*
     * Output the data to the screen.
     */
    printf("Named datatype: %s:\n", DATATYPE);
    /*
     * Get datatype class.  If it isn't compound, we won't print
     * anything.
     */
    typeclass = H5Tget_class(filetype);
    if (typeclass == H5T_COMPOUND) {
        printf("   Class: H5T_COMPOUND\n");
        nmembs = H5Tget_nmembers(filetype);
        /*
         * Iterate over compound datatype members.
         */
        for (i = 0; i < nmembs; i++) {
            /*
             * Get the member name and print it.  Note that
             * H5Tget_member_name allocates space for the string in
             * name, so we must release it after use.
             */
            name = H5Tget_member_name(filetype, i);
            printf("   %s\n", name);
#if H5_VERSION_GE(1, 10, 0) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
            H5free_memory(name);
#else
            free(name);
#endif
        }
    }

    /*
     * Close and release resources.
     */
    status = H5Tclose(filetype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_convert.c`

```c
/************************************************************

  This example shows how to convert between different
  datatypes in memory.  The program converts DIM0 elements
  of compound type sourcetype to desttype, then outputs the
  converted data to the screen.  A background buffer is used
  to fill in the elements of desttype that are not in
  sourcetype.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define DIM0 4

typedef struct {
    double temperature;
    double pressure;
} reading_t; /* Source type */

typedef struct {
    int    serial_no;
    char  *location;
    double temperature;
    double pressure;
} sensor_t; /* Destination type */

int
main(void)
{
    hid_t sourcetype, desttype, strtype, space;
    /* Handles */
    herr_t     status;
    hsize_t    dims[1] = {DIM0};
    reading_t *reading; /* Conversion buffer */
    sensor_t  *sensor,  /* Conversion buffer */
        bkgrd[DIM0];    /* Background buffer */
    hsize_t i;

    /*
     * Allocate memory for conversion buffer.  We will allocate space
     * for it to hold DIM0 elements of the destination type, as the
     * type conversion is performed in place.  Of course, if the
     * destination type were smaller than the source type, we would
     * allocate space to hold DIM0 elements of the source type.
     */
    reading = (reading_t *)malloc(DIM0 * sizeof(sensor_t));

    /*
     * Assign the allocated space to a pointer of the destination type,
     * to allow the buffer to be accessed correctly after the
     * conversion has taken place.
     */
    sensor = (sensor_t *)reading;

    /*
     * Initialize data.
     */
    bkgrd[0].serial_no   = 1153;
    bkgrd[0].location    = "Exterior (static)";
    bkgrd[0].temperature = 53.23;
    bkgrd[0].pressure    = 24.57;
    bkgrd[1].serial_no   = 1184;
    bkgrd[1].location    = "Intake";
    bkgrd[1].temperature = 55.12;
    bkgrd[1].pressure    = 22.95;
    bkgrd[2].serial_no   = 1027;
    bkgrd[2].location    = "Intake manifold";
    bkgrd[2].temperature = 103.55;
    bkgrd[2].pressure    = 31.23;
    bkgrd[3].serial_no   = 1313;
    bkgrd[3].location    = "Exhaust manifold";
    bkgrd[3].temperature = 1252.89;
    bkgrd[3].pressure    = 84.11;

    reading[0].temperature = 54.84;
    reading[0].pressure    = 24.76;
    reading[1].temperature = 56.63;
    reading[1].pressure    = 23.10;
    reading[2].temperature = 102.69;
    reading[2].pressure    = 30.97;
    reading[3].temperature = 1238.27;
    reading[3].pressure    = 82.15;

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the compound datatype for memory.
     */
    sourcetype = H5Tcreate(H5T_COMPOUND, sizeof(reading_t));
    status     = H5Tinsert(sourcetype, "Temperature (F)", HOFFSET(reading_t, temperature), H5T_NATIVE_DOUBLE);
    status     = H5Tinsert(sourcetype, "Pressure (inHg)", HOFFSET(reading_t, pressure), H5T_NATIVE_DOUBLE);

    desttype = H5Tcreate(H5T_COMPOUND, sizeof(sensor_t));
    status   = H5Tinsert(desttype, "Serial number", HOFFSET(sensor_t, serial_no), H5T_NATIVE_INT);
    status   = H5Tinsert(desttype, "Location", HOFFSET(sensor_t, location), strtype);
    status   = H5Tinsert(desttype, "Temperature (F)", HOFFSET(sensor_t, temperature), H5T_NATIVE_DOUBLE);
    status   = H5Tinsert(desttype, "Pressure (inHg)", HOFFSET(sensor_t, pressure), H5T_NATIVE_DOUBLE);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Convert the buffer in reading from sourcetype to desttype.
     * After this conversion we will use sensor to access the buffer,
     * as the buffer now matches its type.
     */
    status = H5Tconvert(sourcetype, desttype, DIM0, reading, bkgrd, H5P_DEFAULT);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < DIM0; i++) {
        printf("sensor[%" PRIuHSIZE "]:\n", i);
        printf("Serial number   : %d\n", sensor[i].serial_no);
        printf("Location        : %s\n", sensor[i].location);
        printf("Temperature (F) : %f\n", sensor[i].temperature);
        printf("Pressure (inHg) : %f\n\n", sensor[i].pressure);
    }

    /*
     * Close and release resources.  In this case H5Tconvert preserves
     * the memory locations of the variable-length strings in
     * "location", so we do not need to free those strings as they were
     * initialized as string constants.
     */
    free(sensor);
    status = H5Sclose(space);
    status = H5Tclose(sourcetype);
    status = H5Tclose(desttype);
    status = H5Tclose(strtype);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_cpxcmpd.c`

```c
/************************************************************

  This example shows how to read and write a complex
  compound datatype to a dataset.  The program first writes
  complex compound structures to a dataset with a dataspace
  of DIM0, then closes the file.  Next, it reopens the file,
  reads back selected fields in the structure, and outputs
  them to the screen.

  Unlike the other datatype examples, in this example we
  save to the file using native datatypes to simplify the
  type definitions here.  To save using standard types you
  must manually calculate the sizes and offsets of compound
  types as shown in h5ex_t_cmpd.c, and convert enumerated
  values as shown in h5ex_t_enum.c.

  The datatype defined here consists of a compound
  containing a variable-length list of compound types, as
  well as a variable-length string, enumeration, double
  array, object reference and region reference.  The nested
  compound type contains an int, variable-length string and
  two doubles.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_cpxcmpd.h5"
#define DATASET  "DS1"
#define DIM0     2

typedef struct {
    int    serial_no;
    char  *location;
    double temperature;
    double pressure;
} sensor_t; /* Nested compound type */

typedef enum { RED, GREEN, BLUE } color_t; /* Enumerated type */

typedef struct {
    hvl_t           sensors;
    char           *name;
    color_t         color;
    double          location[3];
    hobj_ref_t      group;
    hdset_reg_ref_t surveyed_areas;
} vehicle_t; /* Main compound type */

typedef struct {
    hvl_t sensors;
    char *name;
} rvehicle_t; /* Read type */

int
main(void)
{
    hid_t file, vehicletype, colortype, sensortype, sensorstype, loctype, strtype, rvehicletype, rsensortype,
        rsensorstype, space, dset, group;
    /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0}, adims[1] = {3}, adims2[2] = {32, 32}, start[2] = {8, 26}, count[2] = {4, 3},
            coords[3][2] = {{3, 2}, {3, 3}, {4, 4}};
    vehicle_t   wdata[2]; /* Write buffer */
    rvehicle_t *rdata;    /* Read buffer */
    color_t     val;
    sensor_t   *ptr;
    double      wdata2[32][32];
    int         ndims;
    hsize_t     i, j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataset to use for region references.
     */
    for (i = 0; i < 32; i++)
        for (j = 0; j < 32; j++)
            wdata2[i][j] = 70. + 0.1 * (i - 16.) + 0.1 * (j - 16.);
    space  = H5Screate_simple(2, adims2, NULL);
    dset   = H5Dcreate(file, "Ambient_Temperature", H5T_NATIVE_DOUBLE, space, H5P_DEFAULT, H5P_DEFAULT,
                       H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_NATIVE_DOUBLE, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata2[0]);
    status = H5Dclose(dset);

    /*
     * Create groups to use for object references.
     */
    group  = H5Gcreate(file, "Land_Vehicles", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Gclose(group);
    group  = H5Gcreate(file, "Air_Vehicles", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Gclose(group);

    /*
     * Initialize variable-length compound in the first data element.
     */
    wdata[0].sensors.len = 4;
    ptr                  = (sensor_t *)malloc(wdata[0].sensors.len * sizeof(sensor_t));
    ptr[0].serial_no     = 1153;
    ptr[0].location      = "Exterior (static)";
    ptr[0].temperature   = 53.23;
    ptr[0].pressure      = 24.57;
    ptr[1].serial_no     = 1184;
    ptr[1].location      = "Intake";
    ptr[1].temperature   = 55.12;
    ptr[1].pressure      = 22.95;
    ptr[2].serial_no     = 1027;
    ptr[2].location      = "Intake manifold";
    ptr[2].temperature   = 103.55;
    ptr[2].pressure      = 31.23;
    ptr[3].serial_no     = 1313;
    ptr[3].location      = "Exhaust manifold";
    ptr[3].temperature   = 1252.89;
    ptr[3].pressure      = 84.11;
    wdata[0].sensors.p   = (void *)ptr;

    /*
     * Initialize other fields in the first data element.
     */
    wdata[0].name        = "Airplane";
    wdata[0].color       = GREEN;
    wdata[0].location[0] = -103234.21;
    wdata[0].location[1] = 422638.78;
    wdata[0].location[2] = 5996.43;
    status               = H5Rcreate(&wdata[0].group, file, "Air_Vehicles", H5R_OBJECT, -1);
    status               = H5Sselect_elements(space, H5S_SELECT_SET, 3, coords[0]);
    status = H5Rcreate(&wdata[0].surveyed_areas, file, "Ambient_Temperature", H5R_DATASET_REGION, space);

    /*
     * Initialize variable-length compound in the second data element.
     */
    wdata[1].sensors.len = 1;
    ptr                  = (sensor_t *)malloc(wdata[1].sensors.len * sizeof(sensor_t));
    ptr[0].serial_no     = 3244;
    ptr[0].location      = "Roof";
    ptr[0].temperature   = 83.82;
    ptr[0].pressure      = 29.92;
    wdata[1].sensors.p   = (void *)ptr;

    /*
     * Initialize other fields in the second data element.
     */
    wdata[1].name        = "Automobile";
    wdata[1].color       = RED;
    wdata[1].location[0] = 326734.36;
    wdata[1].location[1] = 221568.23;
    wdata[1].location[2] = 432.36;
    status               = H5Rcreate(&wdata[1].group, file, "Land_Vehicles", H5R_OBJECT, -1);
    status               = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, NULL, count, NULL);
    status = H5Rcreate(&wdata[1].surveyed_areas, file, "Ambient_Temperature", H5R_DATASET_REGION, space);

    status = H5Sclose(space);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the nested compound datatype.
     */
    sensortype = H5Tcreate(H5T_COMPOUND, sizeof(sensor_t));
    status     = H5Tinsert(sensortype, "Serial number", HOFFSET(sensor_t, serial_no), H5T_NATIVE_INT);
    status     = H5Tinsert(sensortype, "Location", HOFFSET(sensor_t, location), strtype);
    status     = H5Tinsert(sensortype, "Temperature (F)", HOFFSET(sensor_t, temperature), H5T_NATIVE_DOUBLE);
    status     = H5Tinsert(sensortype, "Pressure (inHg)", HOFFSET(sensor_t, pressure), H5T_NATIVE_DOUBLE);

    /*
     * Create the variable-length datatype.
     */
    sensorstype = H5Tvlen_create(sensortype);

    /*
     * Create the enumerated datatype.
     */
    colortype = H5Tenum_create(H5T_NATIVE_INT);
    val       = (color_t)RED;
    status    = H5Tenum_insert(colortype, "Red", &val);
    val       = (color_t)GREEN;
    status    = H5Tenum_insert(colortype, "Green", &val);
    val       = (color_t)BLUE;
    status    = H5Tenum_insert(colortype, "Blue", &val);

    /*
     * Create the array datatype.
     */
    loctype = H5Tarray_create(H5T_NATIVE_DOUBLE, 1, adims);

    /*
     * Create the main compound datatype.
     */
    vehicletype = H5Tcreate(H5T_COMPOUND, sizeof(vehicle_t));
    status      = H5Tinsert(vehicletype, "Sensors", HOFFSET(vehicle_t, sensors), sensorstype);
    status      = H5Tinsert(vehicletype, "Name", HOFFSET(vehicle_t, name), strtype);
    status      = H5Tinsert(vehicletype, "Color", HOFFSET(vehicle_t, color), colortype);
    status      = H5Tinsert(vehicletype, "Location", HOFFSET(vehicle_t, location), loctype);
    status      = H5Tinsert(vehicletype, "Group", HOFFSET(vehicle_t, group), H5T_STD_REF_OBJ);
    status =
        H5Tinsert(vehicletype, "Surveyed areas", HOFFSET(vehicle_t, surveyed_areas), H5T_STD_REF_DSETREG);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the compound data to it.
     */
    dset   = H5Dcreate(file, DATASET, vehicletype, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, vehicletype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.  Note that we cannot use
     * H5Dvlen_reclaim as it would attempt to free() the string
     * constants used to initialize the name fields in wdata.  We must
     * therefore manually free() only the data previously allocated
     * through malloc().
     */
    for (i = 0; i < dims[0]; i++)
        free(wdata[i].sensors.p);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(strtype);
    status = H5Tclose(sensortype);
    status = H5Tclose(sensorstype);
    status = H5Tclose(colortype);
    status = H5Tclose(loctype);
    status = H5Tclose(vehicletype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().  We will only read back the variable length strings.
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the nested compound datatype for reading.  Even though it
     * has only one field, it must still be defined as a compound type
     * so the library can match the correct field in the file type.
     * This matching is done by name.  However, we do not need to
     * define a structure for the read buffer as we can simply treat it
     * as a char *.
     */
    rsensortype = H5Tcreate(H5T_COMPOUND, sizeof(char *));
    status      = H5Tinsert(rsensortype, "Location", 0, strtype);

    /*
     * Create the variable-length datatype for reading.
     */
    rsensorstype = H5Tvlen_create(rsensortype);

    /*
     * Create the main compound datatype for reading.
     */
    rvehicletype = H5Tcreate(H5T_COMPOUND, sizeof(rvehicle_t));
    status       = H5Tinsert(rvehicletype, "Sensors", HOFFSET(rvehicle_t, sensors), rsensorstype);
    status       = H5Tinsert(rvehicletype, "Name", HOFFSET(rvehicle_t, name), strtype);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (rvehicle_t *)malloc(dims[0] * sizeof(rvehicle_t));

    /*
     * Read the data.
     */
    status = H5Dread(dset, rvehicletype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]:\n", DATASET, i);
        printf("   Vehicle name :\n      %s\n", rdata[i].name);
        printf("   Sensor locations :\n");
        for (j = 0; j < rdata[i].sensors.len; j++)
            printf("      %s\n", ((char **)rdata[i].sensors.p)[j]);
    }

    /*
     * Close and release resources.  H5Dvlen_reclaim will automatically
     * traverse the structure and free any vlen data (including
     * strings).
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Treclaim(rvehicletype, space, H5P_DEFAULT, rdata);
#else
    status = H5Dvlen_reclaim(rvehicletype, space, H5P_DEFAULT, rdata);
#endif
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(strtype);
    status = H5Tclose(rsensortype);
    status = H5Tclose(rsensorstype);
    status = H5Tclose(rvehicletype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_cpxcmpdatt.c`

```c
/************************************************************

  This example shows how to read and write a complex
  compound datatype to an attribute.  The program first
  writes complex compound structures to an attribute with a
  dataspace of DIM0, then closes the file.  Next, it reopens
  the file, reads back selected fields in the structure, and
  outputs them to the screen.

  Unlike the other datatype examples, in this example we
  save to the file using native datatypes to simplify the
  type definitions here.  To save using standard types you
  must manually calculate the sizes and offsets of compound
  types as shown in h5ex_t_cmpd.c, and convert enumerated
  values as shown in h5ex_t_enum.c.

  The datatype defined here consists of a compound
  containing a variable-length list of compound types, as
  well as a variable-length string, enumeration, double
  array, object reference and region reference.  The nested
  compound type contains an int, variable-length string and
  two doubles.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_cpxcmpdatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      2

typedef struct {
    int    serial_no;
    char  *location;
    double temperature;
    double pressure;
} sensor_t; /* Nested compound type */

typedef enum { RED, GREEN, BLUE } color_t; /* Enumerated type */

typedef struct {
    hvl_t           sensors;
    char           *name;
    color_t         color;
    double          location[3];
    hobj_ref_t      group;
    hdset_reg_ref_t surveyed_areas;
} vehicle_t; /* Main compound type */

typedef struct {
    hvl_t sensors;
    char *name;
} rvehicle_t; /* Read type */

int
main(void)
{
    hid_t file, vehicletype, colortype, sensortype, sensorstype, loctype, strtype, rvehicletype, rsensortype,
        rsensorstype, space, dset, group, attr;
    /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0}, adims[1] = {3}, adims2[2] = {32, 32}, start[2] = {8, 26}, count[2] = {4, 3},
            coords[3][2] = {{3, 2}, {3, 3}, {4, 4}};
    vehicle_t   wdata[2]; /* Write buffer */
    rvehicle_t *rdata;    /* Read buffer */
    color_t     val;
    sensor_t   *ptr;
    double      wdata2[32][32];
    int         ndims;
    hsize_t     i, j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataset to use for region references.
     */
    for (i = 0; i < 32; i++)
        for (j = 0; j < 32; j++)
            wdata2[i][j] = 70. + 0.1 * (i - 16.) + 0.1 * (j - 16.);
    space  = H5Screate_simple(2, adims2, NULL);
    dset   = H5Dcreate(file, "Ambient_Temperature", H5T_NATIVE_DOUBLE, space, H5P_DEFAULT, H5P_DEFAULT,
                       H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_NATIVE_DOUBLE, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata2[0]);
    status = H5Dclose(dset);

    /*
     * Create groups to use for object references.
     */
    group  = H5Gcreate(file, "Land_Vehicles", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Gclose(group);
    group  = H5Gcreate(file, "Air_Vehicles", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Gclose(group);

    /*
     * Initialize variable-length compound in the first data element.
     */
    wdata[0].sensors.len = 4;
    ptr                  = (sensor_t *)malloc(wdata[0].sensors.len * sizeof(sensor_t));
    ptr[0].serial_no     = 1153;
    ptr[0].location      = "Exterior (static)";
    ptr[0].temperature   = 53.23;
    ptr[0].pressure      = 24.57;
    ptr[1].serial_no     = 1184;
    ptr[1].location      = "Intake";
    ptr[1].temperature   = 55.12;
    ptr[1].pressure      = 22.95;
    ptr[2].serial_no     = 1027;
    ptr[2].location      = "Intake manifold";
    ptr[2].temperature   = 103.55;
    ptr[2].pressure      = 31.23;
    ptr[3].serial_no     = 1313;
    ptr[3].location      = "Exhaust manifold";
    ptr[3].temperature   = 1252.89;
    ptr[3].pressure      = 84.11;
    wdata[0].sensors.p   = (void *)ptr;

    /*
     * Initialize other fields in the first data element.
     */
    wdata[0].name        = "Airplane";
    wdata[0].color       = GREEN;
    wdata[0].location[0] = -103234.21;
    wdata[0].location[1] = 422638.78;
    wdata[0].location[2] = 5996.43;
    status               = H5Rcreate(&wdata[0].group, file, "Air_Vehicles", H5R_OBJECT, -1);
    status               = H5Sselect_elements(space, H5S_SELECT_SET, 3, coords[0]);
    status = H5Rcreate(&wdata[0].surveyed_areas, file, "Ambient_Temperature", H5R_DATASET_REGION, space);

    /*
     * Initialize variable-length compound in the second data element.
     */
    wdata[1].sensors.len = 1;
    ptr                  = (sensor_t *)malloc(wdata[1].sensors.len * sizeof(sensor_t));
    ptr[0].serial_no     = 3244;
    ptr[0].location      = "Roof";
    ptr[0].temperature   = 83.82;
    ptr[0].pressure      = 29.92;
    wdata[1].sensors.p   = (void *)ptr;

    /*
     * Initialize other fields in the second data element.
     */
    wdata[1].name        = "Automobile";
    wdata[1].color       = RED;
    wdata[1].location[0] = 326734.36;
    wdata[1].location[1] = 221568.23;
    wdata[1].location[2] = 432.36;
    status               = H5Rcreate(&wdata[1].group, file, "Land_Vehicles", H5R_OBJECT, -1);
    status               = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, NULL, count, NULL);
    status = H5Rcreate(&wdata[1].surveyed_areas, file, "Ambient_Temperature", H5R_DATASET_REGION, space);

    status = H5Sclose(space);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the nested compound datatype.
     */
    sensortype = H5Tcreate(H5T_COMPOUND, sizeof(sensor_t));
    status     = H5Tinsert(sensortype, "Serial number", HOFFSET(sensor_t, serial_no), H5T_NATIVE_INT);
    status     = H5Tinsert(sensortype, "Location", HOFFSET(sensor_t, location), strtype);
    status     = H5Tinsert(sensortype, "Temperature (F)", HOFFSET(sensor_t, temperature), H5T_NATIVE_DOUBLE);
    status     = H5Tinsert(sensortype, "Pressure (inHg)", HOFFSET(sensor_t, pressure), H5T_NATIVE_DOUBLE);

    /*
     * Create the variable-length datatype.
     */
    sensorstype = H5Tvlen_create(sensortype);

    /*
     * Create the enumerated datatype.
     */
    colortype = H5Tenum_create(H5T_NATIVE_INT);
    val       = (color_t)RED;
    status    = H5Tenum_insert(colortype, "Red", &val);
    val       = (color_t)GREEN;
    status    = H5Tenum_insert(colortype, "Green", &val);
    val       = (color_t)BLUE;
    status    = H5Tenum_insert(colortype, "Blue", &val);

    /*
     * Create the array datatype.
     */
    loctype = H5Tarray_create(H5T_NATIVE_DOUBLE, 1, adims);

    /*
     * Create the main compound datatype.
     */
    vehicletype = H5Tcreate(H5T_COMPOUND, sizeof(vehicle_t));
    status      = H5Tinsert(vehicletype, "Sensors", HOFFSET(vehicle_t, sensors), sensorstype);
    status      = H5Tinsert(vehicletype, "Name", HOFFSET(vehicle_t, name), strtype);
    status      = H5Tinsert(vehicletype, "Color", HOFFSET(vehicle_t, color), colortype);
    status      = H5Tinsert(vehicletype, "Location", HOFFSET(vehicle_t, location), loctype);
    status      = H5Tinsert(vehicletype, "Group", HOFFSET(vehicle_t, group), H5T_STD_REF_OBJ);
    status =
        H5Tinsert(vehicletype, "Surveyed areas", HOFFSET(vehicle_t, surveyed_areas), H5T_STD_REF_DSETREG);

    /*
     * Create dataset with a null dataspace. to serve as the parent for
     * the attribute.
     */
    space  = H5Screate(H5S_NULL);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the compound data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, vehicletype, space, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Awrite(attr, vehicletype, wdata);

    /*
     * Close and release resources.  Note that we cannot use
     * H5Dvlen_reclaim as it would attempt to free() the string
     * constants used to initialize the name fields in wdata.  We must
     * therefore manually free() only the data previously allocated
     * through malloc().
     */
    for (i = 0; i < dims[0]; i++)
        free(wdata[i].sensors.p);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(strtype);
    status = H5Tclose(sensortype);
    status = H5Tclose(sensorstype);
    status = H5Tclose(colortype);
    status = H5Tclose(loctype);
    status = H5Tclose(vehicletype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().  We will only read back the variable length strings.
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    attr = H5Aopen(dset, ATTRIBUTE, H5P_DEFAULT);

    /*
     * Create variable-length string datatype.
     */
    strtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(strtype, H5T_VARIABLE);

    /*
     * Create the nested compound datatype for reading.  Even though it
     * has only one field, it must still be defined as a compound type
     * so the library can match the correct field in the file type.
     * This matching is done by name.  However, we do not need to
     * define a structure for the read buffer as we can simply treat it
     * as a char *.
     */
    rsensortype = H5Tcreate(H5T_COMPOUND, sizeof(char *));
    status      = H5Tinsert(rsensortype, "Location", 0, strtype);

    /*
     * Create the variable-length datatype for reading.
     */
    rsensorstype = H5Tvlen_create(rsensortype);

    /*
     * Create the main compound datatype for reading.
     */
    rvehicletype = H5Tcreate(H5T_COMPOUND, sizeof(rvehicle_t));
    status       = H5Tinsert(rvehicletype, "Sensors", HOFFSET(rvehicle_t, sensors), rsensorstype);
    status       = H5Tinsert(rvehicletype, "Name", HOFFSET(rvehicle_t, name), strtype);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (rvehicle_t *)malloc(dims[0] * sizeof(rvehicle_t));

    /*
     * Read the data.
     */
    status = H5Aread(attr, rvehicletype, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]:\n", ATTRIBUTE, i);
        printf("   Vehicle name :\n      %s\n", rdata[i].name);
        printf("   Sensor locations :\n");
        for (j = 0; j < rdata[i].sensors.len; j++)
            printf("      %s\n", ((char **)rdata[i].sensors.p)[j]);
    }

    /*
     * Close and release resources.  H5Dvlen_reclaim will automatically
     * traverse the structure and free any vlen data (including
     * strings).
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Treclaim(rvehicletype, space, H5P_DEFAULT, rdata);
#else
    status = H5Dvlen_reclaim(rvehicletype, space, H5P_DEFAULT, rdata);
#endif
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(strtype);
    status = H5Tclose(rsensortype);
    status = H5Tclose(rsensorstype);
    status = H5Tclose(rvehicletype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_enum.c`

```c
/************************************************************

  This example shows how to read and write enumerated
  datatypes to a dataset.  The program first writes
  enumerated values to a dataset with a dataspace of
  DIM0xDIM1, then closes the file.  Next, it reopens the
  file, reads back the data, and outputs it to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME      "h5ex_t_enum.h5"
#define DATASET       "DS1"
#define DIM0          4
#define DIM1          7
#define F_BASET       H5T_STD_I16BE  /* File base type */
#define M_BASET       H5T_NATIVE_INT /* Memory base type */
#define NAME_BUF_SIZE 16

typedef enum { SOLID, LIQUID, GAS, PLASMA } phase_t; /* Enumerated type */

int
main(void)
{
    hid_t file, filetype, memtype, space, dset;
    /* Handles */
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    phase_t wdata[DIM0][DIM1], /* Write buffer */
        **rdata,               /* Read buffer */
        val;
    char   *names[4] = {"SOLID", "LIQUID", "GAS", "PLASMA"}, name[NAME_BUF_SIZE];
    int     ndims;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = (phase_t)((i + 1) * j - j) % (int)(PLASMA + 1);

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create the enumerated datatypes for file and memory.  This
     * process is simplified if native types are used for the file,
     * as only one type must be defined.
     */
    filetype = H5Tenum_create(F_BASET);
    memtype  = H5Tenum_create(M_BASET);

    for (i = (int)SOLID; i <= (int)PLASMA; i++) {
        /*
         * Insert enumerated value for memtype.
         */
        val    = (phase_t)i;
        status = H5Tenum_insert(memtype, names[i], &val);
        /*
         * Insert enumerated value for filetype.  We must first convert
         * the numerical value val to the base type of the destination.
         */
        status = H5Tconvert(M_BASET, F_BASET, 1, &val, NULL, H5P_DEFAULT);
        status = H5Tenum_insert(filetype, names[i], &val);
    }

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset and write the enumerated data to it.
     */
    dset   = H5Dcreate(file, DATASET, filetype, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().  For simplicity, we do not rebuild memtype.
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (phase_t **)malloc(dims[0] * sizeof(phase_t *));

    /*
     * Allocate space for enumerated data.
     */
    rdata[0] = (phase_t *)malloc(dims[0] * dims[1] * sizeof(phase_t));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Dread(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++) {

            /*
             * Get the name of the enumeration member.
             */
            status = H5Tenum_nameof(memtype, &rdata[i][j], name, NAME_BUF_SIZE);
            printf(" %-6s", name);
        }
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_enumatt.c`

```c
/************************************************************

  This example shows how to read and write enumerated
  datatypes to an attribute.  The program first writes
  enumerated values to an attribute with a dataspace of
  DIM0xDIM1, then closes the file.  Next, it reopens the
  file, reads back the data, and outputs it to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME      "h5ex_t_enumatt.h5"
#define DATASET       "DS1"
#define ATTRIBUTE     "A1"
#define DIM0          4
#define DIM1          7
#define F_BASET       H5T_STD_I16BE  /* File base type */
#define M_BASET       H5T_NATIVE_INT /* Memory base type */
#define NAME_BUF_SIZE 16

typedef enum { SOLID, LIQUID, GAS, PLASMA } phase_t; /* Enumerated type */

int
main(void)
{
    hid_t file, filetype, memtype, space, dset, attr;
    /* Handles */
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    phase_t wdata[DIM0][DIM1], /* Write buffer */
        **rdata,               /* Read buffer */
        val;
    char   *names[4] = {"SOLID", "LIQUID", "GAS", "PLASMA"}, name[NAME_BUF_SIZE];
    int     ndims;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = (phase_t)((i + 1) * j - j) % (int)(PLASMA + 1);

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create the enumerated datatypes for file and memory.  This
     * process is simplified if native types are used for the file,
     * as only one type must be defined.
     */
    filetype = H5Tenum_create(F_BASET);
    memtype  = H5Tenum_create(M_BASET);

    for (i = (int)SOLID; i <= (int)PLASMA; i++) {
        /*
         * Insert enumerated value for memtype.
         */
        val    = (phase_t)i;
        status = H5Tenum_insert(memtype, names[i], &val);
        /*
         * Insert enumerated value for filetype.  We must first convert
         * the numerical value val to the base type of the destination.
         */
        status = H5Tconvert(M_BASET, F_BASET, 1, &val, NULL, H5P_DEFAULT);
        status = H5Tenum_insert(filetype, names[i], &val);
    }

    /*
     * Create dataset with a null dataspace.
     */
    space  = H5Screate(H5S_NULL);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the attribute and write the enumerated data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, filetype, space, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Awrite(attr, memtype, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().  For simplicity, we do not rebuild memtype.
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    attr = H5Aopen(dset, ATTRIBUTE, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional attribute so the dynamic allocation must be done
     * in steps.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (phase_t **)malloc(dims[0] * sizeof(phase_t *));

    /*
     * Allocate space for enumerated data.
     */
    rdata[0] = (phase_t *)malloc(dims[0] * dims[1] * sizeof(phase_t));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Aread(attr, memtype, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", ATTRIBUTE);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++) {

            /*
             * Get the name of the enumeration member.
             */
            status = H5Tenum_nameof(memtype, &rdata[i][j], name, NAME_BUF_SIZE);
            printf(" %-6s", name);
        }
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_float.c`

```c
/************************************************************

  This example shows how to read and write float datatypes
  to a dataset.  The program first writes floats to a
  dataset with a dataspace of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_float.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7

int
main(void)
{
    hid_t   file, space, dset; /* Handles */
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    double  wdata[DIM0][DIM1], /* Write buffer */
        **rdata;               /* Read buffer */
    int     ndims;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = (double)i / (j + 0.5) + j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset and write the floating point data to it.  In
     * this example we will save the data as 64 bit little endian IEEE
     * floating point numbers, regardless of the native type.  The HDF5
     * library automatically converts between different floating point
     * types.
     */
    dset   = H5Dcreate(file, DATASET, H5T_IEEE_F64LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_NATIVE_DOUBLE, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (double **)malloc(dims[0] * sizeof(double *));

    /*
     * Allocate space for floating point data.
     */
    rdata[0] = (double *)malloc(dims[0] * dims[1] * sizeof(double));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Dread(dset, H5T_NATIVE_DOUBLE, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %6.4f", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_floatatt.c`

```c
/************************************************************

  This example shows how to read and write floating point
  datatypes to an attribute.  The program first writes
  floating point numbers to an attribute with a dataspace of
  DIM0xDIM1, then closes the file.  Next, it reopens the
  file, reads back the data, and outputs it to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_floatatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4
#define DIM1      7

int
main(void)
{
    hid_t   file, space, dset, attr; /* Handles */
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    double  wdata[DIM0][DIM1], /* Write buffer */
        **rdata;               /* Read buffer */
    int     ndims;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = (double)i / (j + 0.5) + j;
    ;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataset with a null dataspace.
     */
    space  = H5Screate(H5S_NULL);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the attribute and write the floating point data to it.
     * In this example we will save the data as 64 bit little endian
     * IEEE floating point numbers, regardless of the native type.  The
     * HDF5 library automatically converts between different floating
     * point types.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, H5T_IEEE_F64LE, space, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Awrite(attr, H5T_NATIVE_DOUBLE, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    attr = H5Aopen(dset, ATTRIBUTE, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional attribute so the dynamic allocation must be done
     * in steps.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (double **)malloc(dims[0] * sizeof(double *));

    /*
     * Allocate space for floating point data.
     */
    rdata[0] = (double *)malloc(dims[0] * dims[1] * sizeof(double));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Aread(attr, H5T_NATIVE_DOUBLE, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", ATTRIBUTE);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %6.4f", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_int.c`

```c
/************************************************************

  This example shows how to read and write integer datatypes
  to a dataset.  The program first writes integers to a
  dataset with a dataspace of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_int.h5"
#define DATASET  "DS1"
#define DIM0     4
#define DIM1     7

int
main(void)
{
    hid_t   file, space, dset; /* Handles */
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    int     wdata[DIM0][DIM1], /* Write buffer */
        **rdata,               /* Read buffer */
        ndims;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the dataset and write the integer data to it.  In this
     * example we will save the data as 64 bit big endian integers,
     * regardless of the native integer type.  The HDF5 library
     * automatically converts between different integer types.
     */
    dset   = H5Dcreate(file, DATASET, H5T_STD_I64BE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (int **)malloc(dims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", DATASET);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_intatt.c`

```c
/************************************************************

  This example shows how to read and write integer datatypes
  to an attribute.  The program first writes integers to an
  attribute with a dataspace of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_intatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4
#define DIM1      7

int
main(void)
{
    hid_t   file, space, dset, attr; /* Handles */
    herr_t  status;
    hsize_t dims[2] = {DIM0, DIM1};
    int     wdata[DIM0][DIM1], /* Write buffer */
        **rdata,               /* Read buffer */
        ndims;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i * j - j;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataset with a null dataspace.
     */
    space  = H5Screate(H5S_NULL);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(2, dims, NULL);

    /*
     * Create the attribute and write the integer data to it.  In this
     * example we will save the data as 64 bit big endian integers,
     * regardless of the native integer type.  The HDF5 library
     * automatically converts between different integer types.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, H5T_STD_I64BE, space, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Awrite(attr, H5T_NATIVE_INT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    attr = H5Aopen(dset, ATTRIBUTE, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional attribute so the dynamic allocation must be done
     * in steps.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (int **)malloc(dims[0] * sizeof(int *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (int *)malloc(dims[0] * dims[1] * sizeof(int));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * dims[1];

    /*
     * Read the data.
     */
    status = H5Aread(attr, H5T_NATIVE_INT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf("%s:\n", ATTRIBUTE);
    for (i = 0; i < dims[0]; i++) {
        printf(" [");
        for (j = 0; j < dims[1]; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_objref.c`

```c
/************************************************************

  This example shows how to read and write object references
  to a dataset.  The program first creates objects in the
  file and writes references to those objects to a dataset
  with a dataspace of DIM0, then closes the file.  Next, it
  reopens the file, dereferences the references, and outputs
  the names of their targets to the screen.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_objref.h5"
#define DATASET  "DS1"
#define DIM0     2
#define RANK     1

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID; /* File Handle */
    hid_t   space = H5I_INVALID_HID; /* Dataspace Handle */
    hid_t   dset  = H5I_INVALID_HID; /* Dataset Handle */
    hid_t   obj   = H5I_INVALID_HID; /* Object Handle */
    herr_t  status;
    hsize_t dims[1] = {DIM0};
    ssize_t size;
    char   *name = NULL;
    int     ndims;
    hsize_t i;

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    hid_t      ref_type = H5T_STD_REF; /* Reference datatype */
    H5R_ref_t  wdata[DIM0];            /* buffer to write to disk */
    H5R_ref_t *rdata = NULL;           /* buffer to read into*/
    H5O_type_t objtype;                /* Reference type */
#else
    hid_t       ref_type = H5T_STD_REF_OBJ; /* Reference datatype */
    hobj_ref_t  wdata[DIM0];                /* Write buffer */
    hobj_ref_t *rdata = NULL;               /* Read buffer */
    H5O_type_t  objtype;
#endif

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create a dataset with a null dataspace.
     */
    space  = H5Screate(H5S_NULL);
    obj    = H5Dcreate(file, "DS2", H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dclose(obj);
    status = H5Sclose(space);

    /*
     * Create a group.
     */
    obj    = H5Gcreate(file, "G1", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Gclose(obj);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(RANK, dims, NULL);

    /*
     * Create references to the previously created objects.
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Rcreate_object(file, "G1", H5R_OBJECT, &wdata[0]);
    status = H5Rcreate_object(file, "DS2", H5R_OBJECT, &wdata[1]);
#else
    /*
     * Passing -1
     * as space_id causes this parameter to be ignored.  Other values
     * besides valid dataspaces result in an error.
     */
    status = H5Rcreate(&wdata[0], file, "G1", H5R_OBJECT, -1);
    status = H5Rcreate(&wdata[1], file, "DS2", H5R_OBJECT, -1);
#endif

    /*
     * Create the dataset and write the object references to it.
     */
    dset   = H5Dcreate(file, DATASET, ref_type, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, ref_type, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Rdestroy(&wdata[0]);
    status = H5Rdestroy(&wdata[1]);
#endif
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    rdata = (H5R_ref_t *)malloc(dims[0] * sizeof(H5R_ref_t));
#else
    rdata = (hobj_ref_t *)malloc(dims[0] * sizeof(hobj_ref_t));
#endif
    /*
     * Read the data.
     */
    status = H5Dread(dset, ref_type, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]:\n  ->", DATASET, i);

        /*
         * Open the referenced object, get its name and type.
         */
#if H5_VERSION_GE(1, 10, 0) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
        obj    = H5Ropen_object(&rdata[i], H5P_DEFAULT, H5P_DEFAULT);
        status = H5Rget_obj_type3(&rdata[i], H5P_DEFAULT, &objtype);
#else
        obj    = H5Rdereference(dset, H5P_DEFAULT, H5R_OBJECT, &rdata[i]);
        status = H5Rget_obj_type(dset, H5R_OBJECT, &rdata[i], &objtype);
#endif
#else
        obj    = H5Rdereference(dset, H5R_OBJECT, &rdata[i]);
        status = H5Rget_obj_type(dset, H5R_OBJECT, &rdata[i], &objtype);
#endif

        /*
         * Get the length of the name, allocate space, then retrieve
         * the name.
         */
        size = 1 + H5Iget_name(obj, NULL, 0);
        name = (char *)malloc(size);
        size = H5Iget_name(obj, name, size);

        /*
         * Print the object type and close the object.
         */
        switch (objtype) {
            case H5O_TYPE_GROUP:
                printf("Group");
                break;
            case H5O_TYPE_DATASET:
                printf("Dataset");
                break;
            case H5O_TYPE_NAMED_DATATYPE:
                printf("Named Datatype");
                break;
            case H5O_TYPE_MAP:
                printf("Map Object");
                break;
            case H5O_TYPE_UNKNOWN:
            case H5O_TYPE_NTYPES:
                printf("Unknown");
        }
        status = H5Oclose(obj);

        /*
         * Print the name and deallocate space for the name.
         */
        printf(": %s\n", name);
        free(name);

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
        status = H5Rdestroy(&rdata[i]);
#endif
    }

    /*
     * Close and release resources.
     */
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_objrefatt.c`

```c
/************************************************************

  This example shows how to read and write object references
  to an attribute.  The program first creates objects in the
  file and writes references to those objects to an
  attribute with a dataspace of DIM0, then closes the file.
  Next, it reopens the file, dereferences the references,
  and outputs the names of their targets to the screen.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_objrefatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      2
#define RANK      1

int
main(void)
{
    hid_t   file  = H5I_INVALID_HID; /* File Handle */
    hid_t   space = H5I_INVALID_HID; /* Dataspace Handle */
    hid_t   dset  = H5I_INVALID_HID; /* Dataset Handle */
    hid_t   obj   = H5I_INVALID_HID; /* Object Handle */
    hid_t   attr  = H5I_INVALID_HID; /* Attribute Handle */
    herr_t  status;
    hsize_t dims[1] = {DIM0};
    ssize_t size;
    char   *name = NULL;
    int     ndims;
    hsize_t i;

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    hid_t      ref_type = H5T_STD_REF; /* Reference datatype */
    H5R_ref_t  wdata[DIM0];            /* buffer to write to disk */
    H5R_ref_t *rdata = NULL;           /* buffer to read into*/
    H5O_type_t objtype;                /* Reference type */
#else
    hid_t       ref_type = H5T_STD_REF_OBJ; /* Reference datatype */
    hobj_ref_t  wdata[DIM0];                /* Write buffer */
    hobj_ref_t *rdata = NULL;               /* Read buffer */
    H5O_type_t  objtype;
#endif

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create a dataset with a null dataspace.
     */
    space  = H5Screate(H5S_NULL);
    obj    = H5Dcreate(file, "DS2", H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dclose(obj);
    status = H5Sclose(space);

    /*
     * Create a group.
     */
    obj    = H5Gcreate(file, "G1", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Gclose(obj);

    /*
     * Create dataset with a null dataspace to serve as the parent for
     * the attribute.
     */
    space  = H5Screate(H5S_NULL);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(RANK, dims, NULL);

    /*
     * Create references to the previously created objects.
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Rcreate_object(file, "G1", H5R_OBJECT, &wdata[0]);
    status = H5Rcreate_object(file, "DS2", H5R_OBJECT, &wdata[1]);
#else
    /*
     * Passing -1
     * as space_id causes this parameter to be ignored.  Other values
     * besides valid dataspaces result in an error.
     */
    status = H5Rcreate(&wdata[0], file, "G1", H5R_OBJECT, -1);
    status = H5Rcreate(&wdata[1], file, "DS2", H5R_OBJECT, -1);
#endif

    /*
     * Create the attribute and write the object references to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, ref_type, space, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Awrite(attr, ref_type, wdata);

    /*
     * Close and release resources.
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Rdestroy(&wdata[0]);
    status = H5Rdestroy(&wdata[1]);
#endif
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    attr = H5Aopen(dset, ATTRIBUTE, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    rdata = (H5R_ref_t *)malloc(dims[0] * sizeof(H5R_ref_t));
#else
    rdata = (hobj_ref_t *)malloc(dims[0] * sizeof(hobj_ref_t));
#endif
    /*
     * Read the data.
     */
    status = H5Aread(attr, ref_type, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]:\n  ->", ATTRIBUTE, i);

        /*
         * Open the referenced object, get its name and type.
         */
#if H5_VERSION_GE(1, 10, 0) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
        obj    = H5Ropen_object(&rdata[i], H5P_DEFAULT, H5P_DEFAULT);
        status = H5Rget_obj_type3(&rdata[i], H5P_DEFAULT, &objtype);
#else
        obj    = H5Rdereference(dset, H5P_DEFAULT, H5R_OBJECT, &rdata[i]);
        status = H5Rget_obj_type(dset, H5R_OBJECT, &rdata[i], &objtype);
#endif
#else
        obj    = H5Rdereference(dset, H5R_OBJECT, &rdata[i]);
        status = H5Rget_obj_type(dset, H5R_OBJECT, &rdata[i], &objtype);
#endif

        /*
         * Get the length of the name, allocate space, then retrieve
         * the name.
         */
        size = 1 + H5Iget_name(obj, NULL, 0);
        name = (char *)malloc(size);
        size = H5Iget_name(obj, name, size);

        /*
         * Print the object type and close the object.
         */
        switch (objtype) {
            case H5O_TYPE_GROUP:
                printf("Group");
                break;
            case H5O_TYPE_DATASET:
                printf("Dataset");
                break;
            case H5O_TYPE_NAMED_DATATYPE:
                printf("Named Datatype");
                break;
            case H5O_TYPE_MAP:
                printf("Map Object");
                break;
            case H5O_TYPE_UNKNOWN:
            case H5O_TYPE_NTYPES:
                printf("Unknown");
        }
        status = H5Oclose(obj);

        /*
         * Print the name and deallocate space for the name.
         */
        printf(": %s\n", name);
        free(name);

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
        status = H5Rdestroy(&rdata[i]);
#endif
    }

    /*
     * Close and release resources.
     */
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_opaque.c`

```c
/************************************************************

  This example shows how to read and write opaque datatypes
  to a dataset.  The program first writes opaque data to a
  dataset with a dataspace of DIM0, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_opaque.h5"
#define DATASET  "DS1"
#define DIM0     4
#define LEN      7

int
main(void)
{
    hid_t   file, space, dtype, dset; /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0};
    size_t  len;
    char    wdata[DIM0 * LEN], /* Write buffer */
        *rdata,                /* Read buffer */
        str[LEN] = "OPAQUE", *tag;
    int     ndims;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++) {
        for (j = 0; j < LEN - 1; j++)
            wdata[j + i * LEN] = str[j];
        wdata[LEN - 1 + i * LEN] = (char)i + '0';
    }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create opaque datatype and set the tag to something appropriate.
     * For this example we will write and view the data as a character
     * array.
     */
    dtype  = H5Tcreate(H5T_OPAQUE, LEN);
    status = H5Tset_tag(dtype, "Character array");

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the opaque data to it.
     */
    dset   = H5Dcreate(file, DATASET, dtype, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, dtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(dtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get datatype and properties for the datatype.  Note that H5Tget_tag
     * allocates space for the string in tag, so we must remember to release it
     * later.
     */
    dtype = H5Dget_type(dset);
    len   = H5Tget_size(dtype);
    tag   = H5Tget_tag(dtype);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (char *)malloc(dims[0] * len);

    /*
     * Read the data.
     */
    status = H5Dread(dset, dtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the data to the screen.
     */
    printf("Datatype tag for %s is: \"%s\"\n", DATASET, tag);
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]: ", DATASET, i);
        for (j = 0; j < len; j++)
            printf("%c", rdata[j + i * len]);
        printf("\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata);
#if H5_VERSION_GE(1, 10, 0) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    H5free_memory(tag);
#else
    free(tag);
#endif
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(dtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_opaqueatt.c`

```c
/************************************************************

  This example shows how to read and write opaque datatypes
  to an attribute.  The program first writes opaque data to
  an attribute with a dataspace of DIM0, then closes the
  file. Next, it reopens the file, reads back the data, and
  outputs it to the screen.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_opaqueatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4
#define LEN       7

int
main(void)
{
    hid_t   file, space, dtype, dset, attr; /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0};
    size_t  len;
    char    wdata[DIM0 * LEN], /* Write buffer */
        *rdata,                /* Read buffer */
        str[LEN] = "OPAQUE", *tag;
    int     ndims;
    hsize_t i, j;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++) {
        for (j = 0; j < LEN - 1; j++)
            wdata[j + i * LEN] = str[j];
        wdata[LEN - 1 + i * LEN] = (char)i + '0';
    }

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataset with a null dataspace.
     */
    space  = H5Screate(H5S_NULL);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create opaque datatype and set the tag to something appropriate.
     * For this example we will write and view the data as a character
     * array.
     */
    dtype  = H5Tcreate(H5T_OPAQUE, LEN);
    status = H5Tset_tag(dtype, "Character array");

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the opaque data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, dtype, space, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Awrite(attr, dtype, wdata);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(dtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    attr = H5Aopen(dset, ATTRIBUTE, H5P_DEFAULT);

    /*
     * Get datatype and properties for the datatype.  Note that H5Tget_tag
     * allocates space for the string in tag, so we must remember to release it
     * later.
     */
    dtype = H5Aget_type(attr);
    len   = H5Tget_size(dtype);
    tag   = H5Tget_tag(dtype);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (char *)malloc(dims[0] * len);

    /*
     * Read the data.
     */
    status = H5Aread(attr, dtype, rdata);

    /*
     * Output the data to the screen.
     */
    printf("Datatype tag for %s is: \"%s\"\n", ATTRIBUTE, tag);
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]: ", ATTRIBUTE, i);
        for (j = 0; j < len; j++)
            printf("%c", rdata[j + i * len]);
        printf("\n");
    }

    /*
     * Close and release resources.
     */
    free(rdata);
#if H5_VERSION_GE(1, 10, 0) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    H5free_memory(tag);
#else
    free(tag);
#endif
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(dtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_regref.c`

```c
/************************************************************

  This example shows how to read and write region references
  to a dataset.  The program first creates a dataset
  containing characters and writes references to region of
  the dataset to a new dataset with a dataspace of DIM0,
  then closes the file.  Next, it reopens the file,
  dereferences the references, and outputs the referenced
  regions to the screen.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_regref.h5"
#define DATASET  "DS1"
#define DATASET2 "DS2"
#define DIM0     2
#define DS2DIM0  3
#define DS2DIM1  16

int
main(void)
{
    hid_t    file     = H5I_INVALID_HID; /* File Handle */
    hid_t    space    = H5I_INVALID_HID; /* Dataspace Handle */
    hid_t    dset     = H5I_INVALID_HID; /* Dataset Handle */
    hid_t    dset2    = H5I_INVALID_HID; /* Dataset Handle */
    hid_t    memspace = H5I_INVALID_HID; /* Mem dataspace */
    herr_t   status;
    hsize_t  dims[1]      = {DIM0};
    hsize_t  dims2[2]     = {DS2DIM0, DS2DIM1};
    hsize_t  coords[4][2] = {{0, 1}, {2, 11}, {1, 0}, {2, 4}};
    hsize_t  start[2]     = {0, 0};
    hsize_t  stride[2]    = {2, 11};
    hsize_t  count[2]     = {2, 2};
    hsize_t  block[2]     = {1, 3};
    hssize_t npoints;
    ssize_t  size;
    char    *name = NULL;
    int      ndims;
    hsize_t  i;
    char     wdata2[DS2DIM0][DS2DIM1] = {"The quick brown", "fox jumps over ", "the 5 lazy dogs"};
    char    *rdata2                   = NULL;

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    hid_t      ref_type = H5T_STD_REF; /* Reference datatype */
    H5R_ref_t  wdata[DIM0];            /* buffer to write to disk */
    H5R_ref_t *rdata = NULL;           /* buffer to read into*/
    H5R_type_t objtype;                /* Reference type */
#else
    hid_t            ref_type = H5T_STD_REF_DSETREG; /* Reference datatype */
    hdset_reg_ref_t  wdata[DIM0];                    /* Write buffer */
    hdset_reg_ref_t *rdata = NULL;                   /* Read buffer */
    H5O_type_t       objtype;
#endif

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (file < 0)
        goto done;

    /*
     * Create a dataset with character data.
     */
    space = H5Screate_simple(2, dims2, NULL);
    dset2 = H5Dcreate(file, DATASET2, H5T_STD_I8LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    if (dset2 < 0)
        goto done;
    status = H5Dwrite(dset2, H5T_NATIVE_CHAR, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata2);

    /*
     * Create reference to a list of elements in dset2.
     */
    status = H5Sselect_elements(space, H5S_SELECT_SET, 4, coords[0]);

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Rcreate_region(file, DATASET2, space, H5P_DEFAULT, &wdata[0]);
#else
    status = H5Rcreate(&wdata[0], file, DATASET2, H5R_DATASET_REGION, space);
#endif
    if (status < 0)
        goto done;

    /*
     * Create reference to a hyperslab in dset2, close dataspace.
     */
    status = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, stride, count, block);

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Rcreate_region(file, DATASET2, space, H5P_DEFAULT, &wdata[1]);
#else
    status = H5Rcreate(&wdata[1], file, DATASET2, H5R_DATASET_REGION, space);
#endif
    if (status < 0)
        goto done;

    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the region references to it.
     */
    dset = H5Dcreate(file, DATASET, ref_type, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    if (dset < 0)
        goto done;
    status = H5Dwrite(dset, ref_type, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Rdestroy(&wdata[0]);
    status = H5Rdestroy(&wdata[1]);
#endif
    status = H5Dclose(dset);
    status = H5Dclose(dset2);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    if (file < 0)
        goto done;

    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    if (dset < 0)
        goto done;

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    rdata = (H5R_ref_t *)malloc(dims[0] * sizeof(H5R_ref_t));
#else
    rdata = (hdset_reg_ref_t *)malloc(dims[0] * sizeof(hdset_reg_ref_t));
#endif

    status = H5Sclose(space);

    /*
     * Read the data.
     */
    status = H5Dread(dset, ref_type, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]:\n  ->", DATASET, i);

        /*
         * Open the referenced object, retrieve its region as a
         * dataspace selection.
         */
#if H5_VERSION_GE(1, 10, 0) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
        dset2 = H5Ropen_object(&rdata[i], H5P_DEFAULT, H5P_DEFAULT);
        space = H5Ropen_region(&rdata[i], H5P_DEFAULT, H5P_DEFAULT);
#else
        dset2 = H5Rdereference(dset, H5P_DEFAULT, H5R_DATASET_REGION, &rdata[i]);
        space = H5Rget_region(dset, H5R_DATASET_REGION, &rdata[i]);
#endif
#else
        dset2 = H5Rdereference(dset, H5R_DATASET_REGION, &rdata[i]);
        space = H5Rget_region(dset, H5R_DATASET_REGION, &rdata[i]);
#endif
        if (dset2 < 0)
            goto done;

        /*
         * Get the length of the object's name, allocate space, then
         * retrieve the name.
         */
        size = 1 + H5Iget_name(dset2, NULL, 0);
        name = (char *)malloc(size);
        size = H5Iget_name(dset2, name, size);

        /*
         * Allocate space for the read buffer.  We will only allocate
         * enough space for the selection, plus a null terminator.  The
         * read buffer will be 1-dimensional.
         */
        npoints = H5Sget_select_npoints(space);
        rdata2  = (char *)malloc(npoints + 1);

        /*
         * Read the dataset region, and add a null terminator so we can
         * print it as a string.
         */
        memspace        = H5Screate_simple(1, (hsize_t *)&npoints, NULL);
        status          = H5Dread(dset2, H5T_NATIVE_CHAR, memspace, space, H5P_DEFAULT, rdata2);
        rdata2[npoints] = '\0';

        /*
         * Print the name and region data, close and release resources.
         */
        printf(" %s: %s\n", name, rdata2);
        free(rdata2);
        free(name);

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
        status = H5Rdestroy(&rdata[i]);
#endif

        status = H5Sclose(space);
        status = H5Sclose(memspace);
        status = H5Dclose(dset2);
    }

    /*
     * Close and release resources.
     */
    free(rdata);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;

done:
    return 1;
}
```

### `HDF5Examples/C/H5T/h5ex_t_regrefatt.c`

```c
/************************************************************

  This example shows how to read and write region references
  to an attribute.  The program first creates a dataset
  containing characters and writes references to region of
  the dataset to a new attribute with a dataspace of DIM0,
  then closes the file.  Next, it reopens the file,
  dereferences the references, and outputs the referenced
  regions to the screen.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_regrefatt.h5"
#define DATASET   "DS1"
#define DATASET2  "DS2"
#define ATTRIBUTE "A1"
#define DIM0      2
#define DS2DIM0   3
#define DS2DIM1   16

int
main(void)
{
    hid_t    file     = H5I_INVALID_HID; /* File Handle */
    hid_t    space    = H5I_INVALID_HID; /* Dataspace Handle */
    hid_t    dset     = H5I_INVALID_HID; /* Dataset Handle */
    hid_t    dset2    = H5I_INVALID_HID; /* Dataset Handle */
    hid_t    memspace = H5I_INVALID_HID; /* Mem dataspace */
    hid_t    attr     = H5I_INVALID_HID; /* Attribute dataspace */
    herr_t   status;
    hsize_t  dims[1]      = {DIM0};
    hsize_t  dims2[2]     = {DS2DIM0, DS2DIM1};
    hsize_t  coords[4][2] = {{0, 1}, {2, 11}, {1, 0}, {2, 4}};
    hsize_t  start[2]     = {0, 0};
    hsize_t  stride[2]    = {2, 11};
    hsize_t  count[2]     = {2, 2};
    hsize_t  block[2]     = {1, 3};
    hssize_t npoints;
    ssize_t  size;
    char    *name = NULL;
    int      ndims;
    hsize_t  i;
    char     wdata2[DS2DIM0][DS2DIM1] = {"The quick brown", "fox jumps over ", "the 5 lazy dogs"};
    char    *rdata2                   = NULL;

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    hid_t      ref_type = H5T_STD_REF; /* Reference datatype */
    H5R_ref_t  wdata[DIM0];            /* buffer to write to disk */
    H5R_ref_t *rdata = NULL;           /* buffer to read into*/
    H5R_type_t objtype;                /* Reference type */
#else
    hid_t            ref_type = H5T_STD_REF_DSETREG; /* Reference datatype */
    hdset_reg_ref_t  wdata[DIM0];                    /* Write buffer */
    hdset_reg_ref_t *rdata = NULL;                   /* Read buffer */
    H5O_type_t       objtype;
#endif

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (file < 0)
        goto done;

    /*
     * Create a dataset with character data.
     */
    space = H5Screate_simple(2, dims2, NULL);
    dset2 = H5Dcreate(file, DATASET2, H5T_STD_I8LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    if (dset2 < 0)
        goto done;
    status = H5Dwrite(dset2, H5T_NATIVE_CHAR, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata2);

    /*
     * Create reference to a list of elements in dset2.
     */
    status = H5Sselect_elements(space, H5S_SELECT_SET, 4, coords[0]);

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Rcreate_region(file, DATASET2, space, H5P_DEFAULT, &wdata[0]);
#else
    status = H5Rcreate(&wdata[0], file, DATASET2, H5R_DATASET_REGION, space);
#endif
    if (status < 0)
        goto done;

    /*
     * Create reference to a hyperslab in dset2, close dataspace.
     */
    status = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, stride, count, block);

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Rcreate_region(file, DATASET2, space, H5P_DEFAULT, &wdata[1]);
#else
    status = H5Rcreate(&wdata[1], file, DATASET2, H5R_DATASET_REGION, space);
#endif
    if (status < 0)
        goto done;

    status = H5Sclose(space);

    /*
     * Create dataset with a null dataspace to serve as the parent for
     * the attribute.
     */
    space  = H5Screate(H5S_NULL);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the region references to it.
     */
    attr = H5Acreate(dset, ATTRIBUTE, ref_type, space, H5P_DEFAULT, H5P_DEFAULT);
    if (attr < 0)
        goto done;
    status = H5Awrite(attr, ref_type, wdata);

    /*
     * Close and release resources.
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Rdestroy(&wdata[0]);
    status = H5Rdestroy(&wdata[1]);
#endif
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Dclose(dset2);
    status = H5Sclose(space);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    if (file < 0)
        goto done;

    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    if (dset < 0)
        goto done;

    attr = H5Aopen(dset, ATTRIBUTE, H5P_DEFAULT);
    if (attr < 0)
        goto done;

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    rdata = (H5R_ref_t *)malloc(dims[0] * sizeof(H5R_ref_t));
#else
    rdata = (hdset_reg_ref_t *)malloc(dims[0] * sizeof(hdset_reg_ref_t));
#endif

    status = H5Sclose(space);

    /*
     * Read the data.
     */
    status = H5Aread(attr, ref_type, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]:\n  ->", ATTRIBUTE, i);

        /*
         * Open the referenced object, retrieve its region as a
         * dataspace selection.
         */
#if H5_VERSION_GE(1, 10, 0) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
        dset2 = H5Ropen_object(&rdata[i], H5P_DEFAULT, H5P_DEFAULT);
        space = H5Ropen_region(&rdata[i], H5P_DEFAULT, H5P_DEFAULT);
#else
        dset2 = H5Rdereference(dset, H5P_DEFAULT, H5R_DATASET_REGION, &rdata[i]);
        space = H5Rget_region(dset, H5R_DATASET_REGION, &rdata[i]);
#endif
#else
        dset2 = H5Rdereference(dset, H5R_DATASET_REGION, &rdata[i]);
        space = H5Rget_region(dset, H5R_DATASET_REGION, &rdata[i]);
#endif
        if (dset2 < 0)
            goto done;

        /*
         * Get the length of the object's name, allocate space, then
         * retrieve the name.
         */
        size = 1 + H5Iget_name(dset2, NULL, 0);
        name = (char *)malloc(size);
        size = H5Iget_name(dset2, name, size);

        /*
         * Allocate space for the read buffer.  We will only allocate
         * enough space for the selection, plus a null terminator.  The
         * read buffer will be 1-dimensional.
         */
        npoints = H5Sget_select_npoints(space);
        rdata2  = (char *)malloc(npoints + 1);

        /*
         * Read the dataset region, and add a null terminator so we can
         * print it as a string.
         */
        memspace        = H5Screate_simple(1, (hsize_t *)&npoints, NULL);
        status          = H5Dread(dset2, H5T_NATIVE_CHAR, memspace, space, H5P_DEFAULT, rdata2);
        rdata2[npoints] = '\0';

        /*
         * Print the name and region data, close and release resources.
         */
        printf(" %s: %s\n", name, rdata2);
        free(rdata2);
        free(name);

#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
        status = H5Rdestroy(&rdata[i]);
#endif

        status = H5Sclose(space);
        status = H5Sclose(memspace);
        status = H5Dclose(dset2);
    }

    /*
     * Close and release resources.
     */
    free(rdata);
    status = H5Aclose(attr);

#if H5_VERSION_LE(1, 11, 0)
    status = H5Dclose(dset);
#endif

    status = H5Fclose(file);

    return 0;

done:
    return 1;
}
```

### `HDF5Examples/C/H5T/h5ex_t_string.c`

```c
/************************************************************

  This example shows how to read and write string datatypes
  to a dataset.  The program first writes strings to a
  dataset with a dataspace of DIM0, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.8

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_string.h5"
#define DATASET  "DS1"
#define DIM0     4
#define SDIM     8

int
main(void)
{
    hid_t file, filetype, memtype, space, dset;
    /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0};
    size_t  sdim;
    char    wdata[DIM0][SDIM] = {"Parting", "is such", "sweet", "sorrow."},
         /* Write buffer */
        **rdata; /* Read buffer */
    int ndims, i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create file and memory datatypes.  For this example we will save
     * the strings as FORTRAN strings, therefore they do not need space
     * for the null terminator in the file.
     */
    filetype = H5Tcopy(H5T_FORTRAN_S1);
    status   = H5Tset_size(filetype, SDIM - 1);
    memtype  = H5Tcopy(H5T_C_S1);
    status   = H5Tset_size(memtype, SDIM);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the string data to it.
     */
    dset   = H5Dcreate(file, DATASET, filetype, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset and string have the same name and rank, but can have
     * any size.  Therefore we must allocate a new array to read in
     * data using malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get the datatype and its size.
     */
    filetype = H5Dget_type(dset);
    sdim     = H5Tget_size(filetype);
    sdim++; /* Make room for null terminator */

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional dataset so the dynamic allocation must be done
     * in steps.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (char **)malloc(dims[0] * sizeof(char *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (char *)malloc(dims[0] * sdim * sizeof(char));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * sdim;

    /*
     * Create the memory datatype.
     */
    memtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(memtype, sdim);

    /*
     * Read the data.
     */
    status = H5Dread(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++)
        printf("%s[%d]: %s\n", DATASET, i, rdata[i]);

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_stringatt.c`

```c
/************************************************************

  This example shows how to read and write string datatypes
  to an attribute.  The program first writes strings to an
  attribute with a dataspace of DIM0, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.8

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_stringatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4
#define SDIM      8

int
main(void)
{
    hid_t file, filetype, memtype, space, dset, attr;
    /* Handles */
    herr_t  status;
    hsize_t dims[1] = {DIM0};
    size_t  sdim;
    char    wdata[DIM0][SDIM] = {"Parting", "is such", "sweet", "sorrow."},
         /* Write buffer */
        **rdata; /* Read buffer */
    int ndims, i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create file and memory datatypes.  For this example we will save
     * the strings as FORTRAN strings, therefore they do not need space
     * for the null terminator in the file.
     */
    filetype = H5Tcopy(H5T_FORTRAN_S1);
    status   = H5Tset_size(filetype, SDIM - 1);
    memtype  = H5Tcopy(H5T_C_S1);
    status   = H5Tset_size(memtype, SDIM);

    /*
     * Create dataset with a null dataspace.
     */
    space  = H5Screate(H5S_NULL);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the string data to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, filetype, space, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Awrite(attr, memtype, wdata[0]);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute and string have the same name and rank, but can
     * have any size.  Therefore we must allocate a new array to read
     * in data using malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    attr = H5Aopen(dset, ATTRIBUTE, H5P_DEFAULT);

    /*
     * Get the datatype and its size.
     */
    filetype = H5Aget_type(attr);
    sdim     = H5Tget_size(filetype);
    sdim++; /* Make room for null terminator */

    /*
     * Get dataspace and allocate memory for read buffer.  This is a
     * two dimensional attribute so the dynamic allocation must be done
     * in steps.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);

    /*
     * Allocate array of pointers to rows.
     */
    rdata = (char **)malloc(dims[0] * sizeof(char *));

    /*
     * Allocate space for integer data.
     */
    rdata[0] = (char *)malloc(dims[0] * sdim * sizeof(char));

    /*
     * Set the rest of the pointers to rows to the correct addresses.
     */
    for (i = 1; i < dims[0]; i++)
        rdata[i] = rdata[0] + i * sdim;

    /*
     * Create the memory datatype.
     */
    memtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(memtype, sdim);

    /*
     * Read the data.
     */
    status = H5Aread(attr, memtype, rdata[0]);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++)
        printf("%s[%d]: %s\n", ATTRIBUTE, i, rdata[i]);

    /*
     * Close and release resources.
     */
    free(rdata[0]);
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_vlen.c`

```c
/************************************************************

  This example shows how to read and write variable-length
  datatypes to a dataset.  The program first writes two
  variable-length integer arrays to a dataset then closes
  the file.  Next, it reopens the file, reads back the data,
  and outputs it to the screen.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_vlen.h5"
#define DATASET  "DS1"
#define LEN0     3
#define LEN1     12

int
main(void)
{
    hid_t file, filetype, memtype, space, dset;
    /* Handles */
    herr_t status;
    hvl_t  wdata[2], /* Array of vlen structures */
        *rdata;      /* Pointer to vlen structures */
    hsize_t dims[1] = {2};
    int    *ptr, ndims;
    hsize_t i, j;

    /*
     * Initialize variable-length data.  wdata[0] is a countdown of
     * length LEN0, wdata[1] is a Fibonacci sequence of length LEN1.
     */
    wdata[0].len = LEN0;
    ptr          = (int *)malloc(wdata[0].len * sizeof(int));
    for (i = 0; i < wdata[0].len; i++)
        ptr[i] = wdata[0].len - (size_t)i; /* 3 2 1 */
    wdata[0].p = (void *)ptr;

    wdata[1].len = LEN1;
    ptr          = (int *)malloc(wdata[1].len * sizeof(int));
    ptr[0]       = 1;
    ptr[1]       = 1;
    for (i = 2; i < wdata[1].len; i++)
        ptr[i] = ptr[i - 1] + ptr[i - 2]; /* 1 1 2 3 5 8 etc. */
    wdata[1].p = (void *)ptr;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create variable-length datatype for file and memory.
     */
    filetype = H5Tvlen_create(H5T_STD_I32LE);
    memtype  = H5Tvlen_create(H5T_NATIVE_INT);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the variable-length data to it.
     */
    dset   = H5Dcreate(file, DATASET, filetype, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.  Note the use of H5Dvlen_reclaim
     * removes the need to manually free() the previously malloc'ed
     * data.
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Treclaim(memtype, space, H5P_DEFAULT, wdata);
#else
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, wdata);
#endif
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for array of vlen structures.
     * This does not actually allocate memory for the vlen data, that
     * will be done by the library.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (hvl_t *)malloc(dims[0] * sizeof(hvl_t));

    /*
     * Create the memory datatype.
     */
    memtype = H5Tvlen_create(H5T_NATIVE_INT);

    /*
     * Read the data.
     */
    status = H5Dread(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the variable-length data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]:\n  {", DATASET, i);
        ptr = rdata[i].p;
        for (j = 0; j < rdata[i].len; j++) {
            printf(" %d", ptr[j]);
            if ((j + 1) < rdata[i].len)
                printf(",");
        }
        printf(" }\n");
    }

    /*
     * Close and release resources.  Note we must still free the
     * top-level pointer "rdata", as H5Dvlen_reclaim only frees the
     * actual variable-length data, and not the structures themselves.
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Treclaim(memtype, space, H5P_DEFAULT, rdata);
#else
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, rdata);
#endif
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_vlenatt.c`

```c
/************************************************************

  This example shows how to read and write variable-length
  datatypes to an attribute.  The program first writes two
  variable-length integer arrays to the attribute then
  closes the file.  Next, it reopens the file, reads back
  the data, and outputs it to the screen.

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_vlenatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define LEN0      3
#define LEN1      12

int
main(void)
{
    hid_t file, filetype, memtype, space, dset, attr;
    /* Handles */
    herr_t status;
    hvl_t  wdata[2], /* Array of vlen structures */
        *rdata;      /* Pointer to vlen structures */
    hsize_t dims[1] = {2};
    int    *ptr, ndims;
    hsize_t i, j;

    /*
     * Initialize variable-length data.  wdata[0] is a countdown of
     * length LEN0, wdata[1] is a Fibonacci sequence of length LEN1.
     */
    wdata[0].len = LEN0;
    ptr          = (int *)malloc(wdata[0].len * sizeof(int));
    for (i = 0; i < wdata[0].len; i++)
        ptr[i] = wdata[0].len - (size_t)i; /* 3 2 1 */
    wdata[0].p = (void *)ptr;

    wdata[1].len = LEN1;
    ptr          = (int *)malloc(wdata[1].len * sizeof(int));
    ptr[0]       = 1;
    ptr[1]       = 1;
    for (i = 2; i < wdata[1].len; i++)
        ptr[i] = ptr[i - 1] + ptr[i - 2]; /* 1 1 2 3 5 8 etc. */
    wdata[1].p = (void *)ptr;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create variable-length datatype for file and memory.
     */
    filetype = H5Tvlen_create(H5T_STD_I32LE);
    memtype  = H5Tvlen_create(H5T_NATIVE_INT);

    /*
     * Create dataset with a null dataspace.
     */
    space  = H5Screate(H5S_NULL);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the variable-length data to it
     */
    attr   = H5Acreate(dset, ATTRIBUTE, filetype, space, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Awrite(attr, memtype, wdata);

    /*
     * Close and release resources.  Note the use of H5Dvlen_reclaim
     * removes the need to manually free() the previously malloc'ed
     * data.
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Treclaim(memtype, space, H5P_DEFAULT, wdata);
#else
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, wdata);
#endif
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    attr = H5Aopen(dset, ATTRIBUTE, H5P_DEFAULT);

    /*
     * Get dataspace and allocate memory for array of vlen structures.
     * This does not actually allocate memory for the vlen data, that
     * will be done by the library.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (hvl_t *)malloc(dims[0] * sizeof(hvl_t));

    /*
     * Create the memory datatype.
     */
    memtype = H5Tvlen_create(H5T_NATIVE_INT);

    /*
     * Read the data.
     */
    status = H5Aread(attr, memtype, rdata);

    /*
     * Output the variable-length data to the screen.
     */
    for (i = 0; i < dims[0]; i++) {
        printf("%s[%" PRIuHSIZE "]:\n  {", ATTRIBUTE, i);
        ptr = rdata[i].p;
        for (j = 0; j < rdata[i].len; j++) {
            printf(" %d", ptr[j]);
            if ((j + 1) < rdata[i].len)
                printf(",");
        }
        printf(" }\n");
    }

    /*
     * Close and release resources.  Note we must still free the
     * top-level pointer "rdata", as H5Dvlen_reclaim only frees the
     * actual variable-length data, and not the structures themselves.
     */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Treclaim(memtype, space, H5P_DEFAULT, rdata);
#else
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, rdata);
#endif
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_vlstring.c`

```c
/************************************************************

  This example shows how to read and write variable-length
  string datatypes to a dataset.  The program first writes
  variable-length strings to a dataset with a dataspace of
  DIM0, then closes the file.  Next, it reopens the file,
  reads back the data, and outputs it to the screen.

  This file is intended for use with HDF5 Library version 1.8

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_t_vlstring.h5"
#define DATASET  "DS1"
#define DIM0     4

int
main(void)
{
    hid_t file, filetype, memtype, space, dset;
    /* Handles */
    herr_t  status;
    hsize_t dims[1]     = {DIM0};
    char   *wdata[DIM0] = {"Parting", "is such", "sweet", "sorrow."},
         /* Write buffer */
        **rdata; /* Read buffer */
    int ndims, i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create file and memory datatypes.  For this example we will save
     * the strings as FORTRAN strings.
     */
    filetype = H5Tcopy(H5T_FORTRAN_S1);
    status   = H5Tset_size(filetype, H5T_VARIABLE);
    memtype  = H5Tcopy(H5T_C_S1);
    status   = H5Tset_size(memtype, H5T_VARIABLE);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the dataset and write the variable-length string data to
     * it.
     */
    dset   = H5Dcreate(file, DATASET, filetype, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);

    /*
     * Close and release resources.
     */
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the dataset has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file and dataset.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);

    /*
     * Get the datatype.
     */
    filetype = H5Dget_type(dset);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Dget_space(dset);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (char **)malloc(dims[0] * sizeof(char *));

    /*
     * Create the memory datatype.
     */
    memtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(memtype, H5T_VARIABLE);

    /*
     * Read the data.
     */
    status = H5Dread(dset, memtype, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++)
        printf("%s[%d]: %s\n", DATASET, i, rdata[i]);

        /*
         * Close and release resources.  Note that H5Dvlen_reclaim works
         * for variable-length strings as well as variable-length arrays.
         * Also note that we must still free the array of pointers stored
         * in rdata, as H5Tvlen_reclaim only frees the data these point to.
         */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Treclaim(memtype, space, H5P_DEFAULT, rdata);
#else
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, rdata);
#endif
    free(rdata);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/h5ex_t_vlstringatt.c`

```c
/************************************************************

  This example shows how to read and write variable-length
  string datatypes to an attribute.  The program first
  writes variable-length strings to an attribute with a
  dataspace of DIM0, then closes the file.  Next, it reopens
  the file, reads back the data, and outputs it to the
  screen.

  This file is intended for use with HDF5 Library version 1.8

Note: This example includes older cases from previous versions
  of HDF5 for historical reference and to illustrate how to
  migrate older code to newer functions. However, readers are
  encouraged to avoid using deprecated functions and earlier
  schemas from those versions.

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME  "h5ex_t_vlstringatt.h5"
#define DATASET   "DS1"
#define ATTRIBUTE "A1"
#define DIM0      4

int
main(void)
{
    hid_t file, filetype, memtype, space, dset, attr;
    /* Handles */
    herr_t  status;
    hsize_t dims[1]     = {DIM0};
    char   *wdata[DIM0] = {"Parting", "is such", "sweet", "sorrow."},
         /* Write buffer */
        **rdata; /* Read buffer */
    int ndims, i;

    /*
     * Create a new file using the default properties.
     */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create file and memory datatypes.  For this example we will save
     * the strings as FORTRAN strings.
     */
    filetype = H5Tcopy(H5T_FORTRAN_S1);
    status   = H5Tset_size(filetype, H5T_VARIABLE);
    memtype  = H5Tcopy(H5T_C_S1);
    status   = H5Tset_size(memtype, H5T_VARIABLE);

    /*
     * Create dataset with a null dataspace.
     */
    space  = H5Screate(H5S_NULL);
    dset   = H5Dcreate(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Sclose(space);

    /*
     * Create dataspace.  Setting maximum size to NULL sets the maximum
     * size to be the current size.
     */
    space = H5Screate_simple(1, dims, NULL);

    /*
     * Create the attribute and write the variable-length string data
     * to it.
     */
    attr   = H5Acreate(dset, ATTRIBUTE, filetype, space, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Awrite(attr, memtype, wdata);

    /*
     * Close and release resources.
     */
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.  Here we assume
     * the attribute has the same name and rank, but can have any size.
     * Therefore we must allocate a new array to read in data using
     * malloc().
     */

    /*
     * Open file, dataset, and attribute.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen(file, DATASET, H5P_DEFAULT);
    attr = H5Aopen(dset, ATTRIBUTE, H5P_DEFAULT);

    /*
     * Get the datatype.
     */
    filetype = H5Aget_type(attr);

    /*
     * Get dataspace and allocate memory for read buffer.
     */
    space = H5Aget_space(attr);
    ndims = H5Sget_simple_extent_dims(space, dims, NULL);
    rdata = (char **)malloc(dims[0] * sizeof(char *));

    /*
     * Create the memory datatype.
     */
    memtype = H5Tcopy(H5T_C_S1);
    status  = H5Tset_size(memtype, H5T_VARIABLE);

    /*
     * Read the data.
     */
    status = H5Aread(attr, memtype, rdata);

    /*
     * Output the data to the screen.
     */
    for (i = 0; i < dims[0]; i++)
        printf("%s[%d]: %s\n", ATTRIBUTE, i, rdata[i]);

        /*
         * Close and release resources.  Note that H5Dvlen_reclaim works
         * for variable-length strings as well as variable-length arrays.
         * Also note that we must still free the array of pointers stored
         * in rdata, as H5Tvlen_reclaim only frees the data these point to.
         */
#if H5_VERSION_GE(1, 12, 0) && !defined(H5_USE_110_API) && !defined(H5_USE_18_API) && !defined(H5_USE_16_API)
    status = H5Treclaim(memtype, space, H5P_DEFAULT, rdata);
#else
    status = H5Dvlen_reclaim(memtype, space, H5P_DEFAULT, rdata);
#endif
    free(rdata);
    status = H5Aclose(attr);
    status = H5Dclose(dset);
    status = H5Sclose(space);
    status = H5Tclose(filetype);
    status = H5Tclose(memtype);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5T/test-pc.sh`

```bash
#! /bin/sh
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.

# This file is for use of h5cc created with the CMake process.
# Environment variable, HDF5_HOME is expected to be set.
# $1 is the path name of the source directory.
# $2 is the path name of the build directory.
# $3 is the current path name.

top_srcdir=$1
top_builddir=$2
currentpath=$3
verbose=yes
nerrors=0

echo "Current build directory: $top_builddir/$currentpath"

# HDF5 compile commands, assuming they are in your $PATH.
H5CC=$HDF5_HOME/bin/h5cc
LD_LIBRARY_PATH=$HDF5_HOME/lib
export LD_LIBRARY_PATH

if ! test -f $H5CC; then
    echo "Set paths for H5CC and LD_LIBRARY_PATH in test.sh"
    echo "Set environment variable HDF5_HOME to the hdf5 install dir"
    echo "h5cc was not found at $H5CC"
    exit $EXIT_FAILURE
fi

H5DUMP=`echo $H5CC | sed -e 's/\/[^/]*$/\/h5dump/'`;
H5_LIBVER=$($H5CC -showconfig | grep -i "HDF5 Version:" | sed 's/^.* //g' | sed 's/[-].*//g')
H5_APIVER=$($H5CC -showconfig | grep -i "Default API mapping:" | sed 's/^.* //g' | sed 's/v//g' | sed 's/1/1_/')

H5_MAJORVER=$(echo $H5_LIBVER | cut -f1 -d'.'  | sed -E 's/\./_/g')
H5_MINORVER=$(echo $H5_LIBVER | cut -f2 -d'.'  | sed -E 's/\./_/g')
H5_RELEASEVER=$(echo $H5_LIBVER | cut -f3 -d'.'  | sed -E 's/\./_/g')
H5_LIBVER_DIR=$H5_MAJORVER$H5_MINORVER

# Shell commands used in Makefiles
RM="rm -rf"
DIFF="diff -c"
CMP="cmp -s"
GREP='grep'
CP="cp -p"  # Use -p to preserve mode,ownership,timestamps
DIRNAME='dirname'
LS='ls'
AWK='awk'

# setup plugin path
ENVCMD="env HDF5_PLUGIN_PATH=$LD_LIBRARY_PATH/plugin"

TESTDIR=$top_builddir/$currentpath


case `echo "testing\c"; echo 1,2,3`,`echo -n testing; echo 1,2,3` in
  *c*,-n*) ECHO_N= ECHO_C='
' ;;
  *c*,*  ) ECHO_N=-n ECHO_C= ;;
  *)       ECHO_N= ECHO_C='\c' ;;
esac
ECHO_N="echo $ECHO_N"


exout() {
    cd $TESTDIR
    "$@"
}

dumpout() {
    cd $TESTDIR
    $H5DUMP "$@"
}

compileout() {
    cd $TESTDIR
    $H5CC "$@"
}

# compare current version, required version.
# returns if cur_ver < req_ver is true.
version_compare() {
  version_lt=0
  if [ ! "$(printf '%s\n' "$1" "$2" | sort -V | head -n1)" = "$2" ]; then
          version_lt=1
  fi
}


topics="array arrayatt bit bitatt cmpd cmpdatt cpxcmpd cpxcmpdatt enum enumatt float floatatt \
int intatt opaque opaqueatt string stringatt vlstring vlstringatt \
commit"

return_val=0

for topic in $topics
do
    compileout $top_srcdir/$currentpath/h5ex_t_$topic.c -o h5ex_t_$topic
done

for topic in $topics
do
    fname=h5ex_t_$topic
    $ECHO_N "Testing C/H5T/$fname...$ECHO_C"
    exout ./$fname >tmp.test
    cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/16/$fname.tst
    status=$?
    if test $status -ne 0
    then
        echo "  FAILED!"
    else
        if [ "$fname" = "h5ex_t_cpxcmpd" -o "$fname" = "h5ex_t_cpxcmpdatt" ]; then
            targ="-n"
        else
            targ=""
        fi
        dumpout $targ $fname.h5 >tmp.test
        rm -f $TESTDIR/$fname.h5
        cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.ddl
        status=$?
        if test $status -ne 0
        then
            echo "  FAILED!"
        else
            echo "  Passed"
        fi
    fi
    return_val=`expr $status + $return_val`
done


#######Non-standard tests#######

USE_ALT=""
if [ "$H5_LIBVER_DIR" = "110" ]; then
   # check if HDF5 version is < 1.10.7
   version_compare "$H5_LIBVER" "1.10.7"
   if [ "$version_lt" = 1 ]; then
      USE_ALT="06"
   fi
else
  if [ "$H5_LIBVER_DIR" = "18" ]; then
   # check if HDF5 version is < 1.8.22
   version_compare "$H5_LIBVER" "1.8.22"
   if [ "$version_lt" = 1 ]; then
      USE_ALT="21"
   fi
  fi
fi

topics="objref objrefatt regref regrefatt"

for topic in $topics
do
    compileout $top_srcdir/$currentpath/h5ex_t_$topic.c -o h5ex_t_$topic
done

for topic in $topics
do
    fname=h5ex_t_$topic
    $ECHO_N "Testing C/H5T/$fname...$ECHO_C"
    exout ./$fname >tmp.test
    cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/16/$fname.tst
    status=$?
    if test $status -ne 0
    then
        echo "  FAILED!"
    else
        dumpout $fname.h5 >tmp.test
        rm -f $TESTDIR/$fname.h5
        version_compare "$H5_LIBVER" "1.10.0"
        if [ "$version_lt" = 1 ]; then
            cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname$USE_ALT.ddl
        else
            version_compare "$H5_LIBVER" "1.12.0"
            if [ "$version_lt" = 1 ]; then
               version_compare "$H5_LIBVER" "1.10.7"
               if [ "$version_lt" = 1 ]; then
                  cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/110/$fname$USE_ALT.ddl
               else
                  cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.ddl
               fi
            else
                cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/112/$fname.ddl
            fi
        fi
        status=$?
        if test $status -ne 0
        then
            echo "  FAILED!"
        else
            echo "  Passed"
        fi
    fi
    return_val=`expr $status + $return_val`
done

topics="vlen vlenatt"

for topic in $topics
do
    compileout $top_srcdir/$currentpath/h5ex_t_$topic.c -o h5ex_t_$topic
done

for topic in $topics
do
    fname=h5ex_t_$topic
    $ECHO_N "Testing C/H5T/$fname...$ECHO_C"
    exout ./$fname >tmp.test
    cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/16/$fname.tst
    status=$?
    if test $status -ne 0
    then
        echo "  FAILED!"
    else
        dumpout $fname.h5 >tmp.test
        rm -f $TESTDIR/$fname.h5
        version_compare "$H5_LIBVER" "1.14.3"
        if [ "$version_lt" = 1 ]; then
            cmp -s tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.ddl
        else
            cmp -s tmp.test $top_srcdir/$currentpath/tfiles/114/$fname.ddl
        fi
        status=$?
        if test $status -ne 0
        then
            echo "  FAILED!"
        else
            echo "  Passed"
        fi
    fi
    return_val=`expr $status + $return_val`
done

compileout $top_srcdir/$currentpath/h5ex_t_convert.c -o h5ex_t_convert

fname=h5ex_t_convert
$ECHO_N "Testing C/H5T/$fname...$ECHO_C"
exout ./$fname >tmp.test
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/16/$fname.tst
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
    echo "  Passed"
fi
return_val=`expr $status + $return_val`


rm -f $TESTDIR/tmp.test
echo "$return_val tests failed in C/H5T/"
exit $return_val
```

### `HDF5Examples/C/H5VDS/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_C_H5VDS C)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (C_sourcefiles.cmake)

#foreach (example_name ${examples})
#endforeach ()

if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
  foreach (example_name ${1_10_examples})
    add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
    target_compile_options (${EXAMPLE_VARNAME}_${example_name}
        PRIVATE
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
    )
    if (H5_HAVE_PARALLEL)
      target_include_directories (${EXAMPLE_VARNAME}_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
    endif ()
    target_link_libraries (${EXAMPLE_VARNAME}_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
    if (H5EXAMPLE_BUILD_TESTING)
      add_custom_command (
          TARGET     ${EXAMPLE_VARNAME}_${example_name}
          POST_BUILD
          COMMAND    ${CMAKE_COMMAND}
          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
      )
    endif ()
  endforeach ()
endif ()

#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
#  foreach (example_name ${1_12_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#          add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
#  foreach (example_name ${1_14_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
#  foreach (example_name ${2_0_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()

if (HDF5_PROVIDES_TOOLS)
#  foreach (example_name ${examples})
#  endforeach ()

  foreach (example_name ${1_10_examples})
    if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
      add_custom_command (
          TARGET     ${EXAMPLE_VARNAME}_${example_name}
          POST_BUILD
          COMMAND    ${CMAKE_COMMAND}
          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      )
    endif ()
    #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
    #  add_custom_command (
    #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
    #      POST_BUILD
    #      COMMAND    ${CMAKE_COMMAND}
    #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
    #  )
    #endif ()
    #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
    #  add_custom_command (
    #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
    #      POST_BUILD
    #      COMMAND    ${CMAKE_COMMAND}
    #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
    #  )
    #endif ()
    #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
    #  add_custom_command (
    #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
    #      POST_BUILD
    #      COMMAND    ${CMAKE_COMMAND}
    #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
    #  )
    #endif ()
  endforeach ()

#  foreach (example_name ${1_12_examples})
#  endforeach ()
#  foreach (example_name ${1_14_examples})
#  endforeach ()
#  foreach (example_name ${2_0_examples})
#  endforeach ()
endif ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}*.h5
    )
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${ARGN};${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-${testname} PROPERTIES
            DEPENDS ${EXAMPLE_VARNAME}_${testname}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
      endif ()
    endif ()
  endmacro ()

  foreach (example_name ${examples} ${1_10_examples})
    ADD_H5_TEST (${example_name})
  endforeach ()
endif ()
```

### `HDF5Examples/C/H5VDS/C_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (examples)

if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10" AND NOT ${EXAMPLE_VARNAME}_USE_16_API AND NOT ${EXAMPLE_VARNAME}_USE_18_API)
  set (1_10_examples
    h5ex_vds
    h5ex_vds-exc
#    h5ex_vds-exclim
    h5ex_vds-eiger
    h5ex_vds-simpleIO
    h5ex_vds-percival
    h5ex_vds-percival-unlim
    h5ex_vds-percival-unlim-maxmin
  )
else ()
  set (1_10_examples)
endif ()
```

### `HDF5Examples/C/H5VDS/h5ex_vds-eiger.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
/************************************************************

  This example illustrates the concept of the virtual dataset.
  Eiger use case. Every 5 frames 10x10 are in the source
  dataset "/A" in file with the name f-<#>.h5
  This file is intended for use with HDF5 Library version 1.10

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_vds-eiger.h5"
#define DATASET  "VDS-Eiger"
#define VDSDIM0  5
#define VDSDIM1  10
#define VDSDIM2  10
#define DIM0     5
#define DIM1     10
#define DIM2     10
#define RANK     3

int
main(void)
{
    hid_t        file      = H5I_INVALID_HID;
    hid_t        space     = H5I_INVALID_HID;
    hid_t        dset      = H5I_INVALID_HID;
    hid_t        src_space = H5I_INVALID_HID;
    hid_t        vspace    = H5I_INVALID_HID;
    hid_t        dcpl      = H5I_INVALID_HID;
    herr_t       status;
    hsize_t      vdsdims[3]     = {VDSDIM0, VDSDIM1, VDSDIM2};
    hsize_t      vdsdims_max[3] = {H5S_UNLIMITED, VDSDIM1, VDSDIM1};
    hsize_t      dims[3]        = {DIM0, DIM1, DIM2};
    hsize_t      start[3], stride[3], count[3], block[3];                 /* Hyperslab parameters */
    hsize_t      start_out[3], stride_out[3], count_out[3], block_out[3]; /* Hyperslab parameter out */
    int          rdata[VDSDIM0][VDSDIM1][VDSDIM2]; /* Read buffer for virtual dataset */
    int          i, j, k;
    H5D_layout_t layout;  /* Storage layout */
    size_t       num_map; /* Number of mappings */
    ssize_t      len;     /* Length of the string; also a return value */
    char        *filename = NULL;
    char        *dsetname = NULL;

    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create VDS dataspace.  */
    vspace = H5Screate_simple(RANK, vdsdims, vdsdims_max);

    /* Create dataspaces for the source dataset. */
    src_space = H5Screate_simple(RANK, dims, NULL);

    /* Create VDS creation property */
    dcpl = H5Pcreate(H5P_DATASET_CREATE);

    /* Initialize hyperslab values */

    start[0]  = 0;
    start[1]  = 0;
    start[2]  = 0;
    stride[0] = DIM0;
    stride[1] = 1;
    stride[2] = 1;
    count[0]  = H5S_UNLIMITED;
    count[1]  = 1;
    count[2]  = 1;
    block[0]  = DIM0;
    block[1]  = DIM1;
    block[2]  = DIM2;

    /*
     * Build the mappings
     *
     */
    status = H5Sselect_hyperslab(vspace, H5S_SELECT_SET, start, stride, count, block);
    status = H5Pset_virtual(dcpl, vspace, "f-%b.h5", "/A", src_space);

    /* Create a virtual dataset */
    dset   = H5Dcreate2(file, DATASET, H5T_STD_I32LE, vspace, H5P_DEFAULT, dcpl, H5P_DEFAULT);
    status = H5Sclose(vspace);
    status = H5Sclose(src_space);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen2(file, DATASET, H5P_DEFAULT);

    /*
     * Get creation property list and mapping properties.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Get storage layout.
     */
    layout = H5Pget_layout(dcpl);
    if (H5D_VIRTUAL == layout)
        printf(" Dataset has a virtual layout \n");
    else
        printf("Wrong layout found \n");

    /*
     * Find the number of mappings.
     */
    status = H5Pget_virtual_count(dcpl, &num_map);
    printf(" Number of mappings is %lu\n", (unsigned long)num_map);

    /*
     * Get mapping parameters for each mapping.
     */
    for (i = 0; i < (int)num_map; i++) {
        printf(" Mapping %d \n", i);
        printf("         Selection in the virtual dataset \n");
        /* Get selection in the virtual dataset */
        vspace = H5Pget_virtual_vspace(dcpl, (size_t)i);

        /* Make sure that this is a hyperslab selection and then print information. */
        if (H5Sget_select_type(vspace) == H5S_SEL_HYPERSLABS) {
            if (H5Sis_regular_hyperslab(vspace)) {
                status = H5Sget_regular_hyperslab(vspace, start_out, stride_out, count_out, block_out);
                printf("         start  = [%llu, %llu, %llu] \n", (unsigned long long)start_out[0],
                       (unsigned long long)start_out[1], (unsigned long long)start_out[2]);
                printf("         stride = [%llu, %llu, %llu] \n", (unsigned long long)stride_out[0],
                       (unsigned long long)stride_out[1], (unsigned long long)stride_out[2]);
                printf("         count  = [%llu, %llu, %llu] \n", (unsigned long long)count_out[0],
                       (unsigned long long)count_out[1], (unsigned long long)count_out[2]);
                printf("         block  = [%llu, %llu, %llu] \n", (unsigned long long)block_out[0],
                       (unsigned long long)block_out[1], (unsigned long long)block_out[2]);
            }
        }
        /* Get source file name */
        len      = H5Pget_virtual_filename(dcpl, (size_t)i, NULL, 0);
        filename = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_filename(dcpl, (size_t)i, filename, len + 1);
        printf("         Source filename %s\n", filename);

        /* Get source dataset name */
        len      = H5Pget_virtual_dsetname(dcpl, (size_t)i, NULL, 0);
        dsetname = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_dsetname(dcpl, (size_t)i, dsetname, len + 1);
        printf("         Source dataset name %s\n", dsetname);

        /* Get selection in the source dataset */
        printf("         Selection in the source dataset ");
        src_space = H5Pget_virtual_srcspace(dcpl, (size_t)i);

        /* Make sure it is ALL selection and then print the coordinates. */
        if (H5Sget_select_type(src_space) == H5S_SEL_ALL) {
            printf("H5S_ALL \n");
        }
        H5Sclose(vspace);
        H5Sclose(src_space);
        free(filename);
        free(dsetname);
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf(" VDS Data:\n");
    for (i = 0; i < VDSDIM0; i++) {
        printf(" [");
        for (j = 0; j < VDSDIM1; j++) {
            printf(" [");
            for (k = 0; k < VDSDIM1; k++)
                printf(" %3d", rdata[i][j][k]);
            printf("]");
        }
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5VDS/h5ex_vds-exc.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
/************************************************************

  This example illustrates the concept of the virtual dataset.
  Excalibur use case with k=2 and m=3.
  This file is intended for use with HDF5 Library version 1.10

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_vds-exc.h5"
#define DATASET  "VDS-Excalibur"
#define VDSDIM0  0
#define VDSDIM1  15
#define VDSDIM2  6
#define LDIM0    0
#define LDIM1    2
#define LDIM2    6
#define NDIM0    0
#define NDIM1    3
#define NDIM2    6
#define RANK     3

const char *SRC_FILE[] = {"ae.h5", "be.h5", "ce.h5", "de.h5", "ee.h5", "fe.h5"};

const char *SRC_DATASET[] = {"A", "B", "C", "D", "E", "F"};

int
main(void)
{
    hid_t        file       = H5I_INVALID_HID;
    hid_t        space      = H5I_INVALID_HID;
    hid_t        dset       = H5I_INVALID_HID;
    hid_t        src_space  = H5I_INVALID_HID;
    hid_t        lsrc_space = H5I_INVALID_HID;
    hid_t        nsrc_space = H5I_INVALID_HID;
    hid_t        vspace     = H5I_INVALID_HID;
    hid_t        dcpl       = H5I_INVALID_HID;
    herr_t       status;
    hsize_t      vdsdims[3]     = {VDSDIM0, VDSDIM1, VDSDIM2};
    hsize_t      vdsdims_max[3] = {H5S_UNLIMITED, VDSDIM1, VDSDIM2};
    hsize_t      ldims[3]       = {LDIM0, LDIM1, LDIM2};
    hsize_t      ldims_max[3]   = {H5S_UNLIMITED, LDIM1, LDIM2};
    hsize_t      ndims[3]       = {NDIM0, NDIM1, NDIM2};
    hsize_t      ndims_max[3]   = {H5S_UNLIMITED, NDIM1, NDIM2};
    hsize_t      start[3], count[3], block[3]; /* Hyperslab parameters */
    hsize_t      start_out[3], stride_out[3], count_out[3], block_out[3];
    int          rdata[40][VDSDIM1][VDSDIM2]; /* Read buffer for virtual dataset */
    int          l = 2;
    int          n = 3;
    int          i, j, k;
    H5D_layout_t layout;  /* Storage layout */
    size_t       num_map; /* Number of mappings */
    ssize_t      len;     /* Length of the string; also a return value */
    char        *filename = NULL;
    char        *dsetname = NULL;

    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create VDS dataspace.  */
    space = H5Screate_simple(RANK, vdsdims, vdsdims_max);
    /* Create dataspaces for A, C, and E datasets. */
    lsrc_space = H5Screate_simple(RANK, ldims, ldims_max);
    /* Create dataspaces for B, D, and F datasets. */
    nsrc_space = H5Screate_simple(RANK, ndims, ndims_max);

    /* Create VDS creation property */
    dcpl = H5Pcreate(H5P_DATASET_CREATE);

    /* Initialize hyperslab values */

    start[0] = 0;
    start[1] = 0;
    start[2] = 0;
    count[0] = H5S_UNLIMITED;
    count[1] = 1;
    count[2] = 1;
    block[0] = 1;
    block[1] = l;
    block[2] = VDSDIM2;

    /*
     * Build the mappings for A, C and E source datasets.
     * Unlimited hyperslab selection is the same in the source datasets.
     * Unlimited hyperslab selections in the virtual dataset have different offsets.
     */
    status = H5Sselect_hyperslab(lsrc_space, H5S_SELECT_SET, start, NULL, count, block);
    for (i = 0; i < 3; i++) {
        start[1] = (hsize_t)((l + n) * i);
        status   = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, NULL, count, block);
        status   = H5Pset_virtual(dcpl, space, SRC_FILE[2 * i], SRC_DATASET[2 * i], lsrc_space);
    }

    /* Reinitialize start[1] and block[1] to build the second set of mappings. */
    start[1] = 0;
    block[1] = n;
    /*
     * Build the mappings for B, D and F source datasets.
     * Unlimited hyperslab selection is the same in the source datasets.
     * Unlimited hyperslab selections in the virtual dataset have different offsets.
     */
    status = H5Sselect_hyperslab(nsrc_space, H5S_SELECT_SET, start, NULL, count, block);
    for (i = 0; i < 3; i++) {
        start[1] = (hsize_t)(l + (l + n) * i);
        status   = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, NULL, count, block);
        status   = H5Pset_virtual(dcpl, space, SRC_FILE[2 * i + 1], SRC_DATASET[2 * i + 1], nsrc_space);
    }

    /* Create a virtual dataset */
    dset   = H5Dcreate2(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);
    status = H5Sclose(space);
    status = H5Sclose(nsrc_space);
    status = H5Sclose(lsrc_space);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen2(file, DATASET, H5P_DEFAULT);

    /*
     * Get creation property list and mapping properties.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Get storage layout.
     */
    layout = H5Pget_layout(dcpl);
    if (H5D_VIRTUAL == layout)
        printf(" Dataset has a virtual layout \n");
    else
        printf("Wrong layout found \n");

    /*
     * Find the number of mappings.
     */
    status = H5Pget_virtual_count(dcpl, &num_map);
    printf(" Number of mappings is %lu\n", (unsigned long)num_map);

    /*
     * Get mapping parameters for each mapping.
     */
    for (i = 0; i < (int)num_map; i++) {
        printf(" Mapping %d \n", i);
        printf("         Selection in the virtual dataset \n");
        /* Get selection in the virtual dataset */
        vspace = H5Pget_virtual_vspace(dcpl, (size_t)i);

        /* Make sure that this is a hyperslab selection and then print information. */
        if (H5Sget_select_type(vspace) == H5S_SEL_HYPERSLABS) {
            if (H5Sis_regular_hyperslab(vspace)) {
                status = H5Sget_regular_hyperslab(vspace, start_out, stride_out, count_out, block_out);
                printf("         start  = [%llu, %llu, %llu] \n", (unsigned long long)start_out[0],
                       (unsigned long long)start_out[1], (unsigned long long)start_out[2]);
                printf("         stride = [%llu, %llu, %llu] \n", (unsigned long long)stride_out[0],
                       (unsigned long long)stride_out[1], (unsigned long long)stride_out[2]);
                printf("         count  = [%llu, %llu, %llu] \n", (unsigned long long)count_out[0],
                       (unsigned long long)count_out[1], (unsigned long long)count_out[2]);
                printf("         block  = [%llu, %llu, %llu] \n", (unsigned long long)block_out[0],
                       (unsigned long long)block_out[1], (unsigned long long)block_out[2]);
            }
        }
        /* Get source file name */
        len      = H5Pget_virtual_filename(dcpl, (size_t)i, NULL, 0);
        filename = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_filename(dcpl, (size_t)i, filename, len + 1);
        printf("         Source filename %s\n", filename);

        /* Get source dataset name */
        len      = H5Pget_virtual_dsetname(dcpl, (size_t)i, NULL, 0);
        dsetname = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_dsetname(dcpl, (size_t)i, dsetname, len + 1);
        printf("         Source dataset name %s\n", dsetname);

        /* Get selection in the source dataset */
        printf("         Selection in the source dataset \n");
        src_space = H5Pget_virtual_srcspace(dcpl, (size_t)i);
        if (H5Sget_select_type(src_space) == H5S_SEL_HYPERSLABS) {
            if (H5Sis_regular_hyperslab(vspace)) {
                status = H5Sget_regular_hyperslab(src_space, start_out, stride_out, count_out, block_out);
                printf("         start  = [%llu, %llu, %llu] \n", (unsigned long long)start_out[0],
                       (unsigned long long)start_out[1], (unsigned long long)start_out[2]);
                printf("         stride = [%llu, %llu, %llu] \n", (unsigned long long)stride_out[0],
                       (unsigned long long)stride_out[1], (unsigned long long)stride_out[2]);
                printf("         count  = [%llu, %llu, %llu] \n", (unsigned long long)count_out[0],
                       (unsigned long long)count_out[1], (unsigned long long)count_out[2]);
                printf("         block  = [%llu, %llu, %llu] \n", (unsigned long long)block_out[0],
                       (unsigned long long)block_out[1], (unsigned long long)block_out[2]);
            }
        }
        H5Sclose(vspace);
        H5Sclose(src_space);
        free(filename);
        free(dsetname);
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf(" VDS Data:\n");
    for (i = 0; i < VDSDIM0; i++) {
        printf(" [");
        for (j = 0; j < VDSDIM1; j++) {
            printf(" [");
            for (k = 0; k < VDSDIM1; k++)
                printf(" %3d", rdata[i][j][k]);
            printf("]");
        }
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5VDS/h5ex_vds-exclim.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
/************************************************************

  This example illustrates the concept of the virtual dataset.
  Excalibur use case with k=2 and m=3 and only 3 planes in
  Z-direction (i.e., not unlimited).
  This file is intended for use with HDF5 Library version 1.10

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_vds-exclim.h5"
#define DATASET  "VDS-Excaliburlim"
#define VDSDIM0  3
#define VDSDIM1  15
#define VDSDIM2  6
#define LDIM0    3
#define LDIM1    2
#define LDIM2    6
#define NDIM0    3
#define NDIM1    3
#define NDIM2    6
#define RANK     3

const char *SRC_FILE[] = {"ael.h5", "bel.h5", "cel.h5", "del.h5", "eel.h5", "fel.h5"};

const char *SRC_DATASET[] = {"A", "B", "C", "D", "E", "F"};

int
main(void)
{
    hid_t        file       = H5I_INVALID_HID;
    hid_t        space      = H5I_INVALID_HID;
    hid_t        dset       = H5I_INVALID_HID;
    hid_t        src_space  = H5I_INVALID_HID;
    hid_t        lsrc_space = H5I_INVALID_HID;
    hid_t        nsrc_space = H5I_INVALID_HID;
    hid_t        vspace     = H5I_INVALID_HID;
    hid_t        dcpl       = H5I_INVALID_HID;
    herr_t       status;
    hsize_t      vdsdims[3] = {VDSDIM0, VDSDIM1, VDSDIM2};
    hsize_t      ldims[3]   = {LDIM0, LDIM1, LDIM2};
    hsize_t      ndims[3]   = {NDIM0, NDIM1, NDIM2};
    hsize_t      start[3], count[3], block[3]; /* Hyperslab parameters */
    hsize_t      start_out[3], stride_out[3], count_out[3], block_out[3];
    int          rdata[VDSDIM0][VDSDIM1][VDSDIM2]; /* Read buffer for virtual dataset */
    int          l = 2;
    int          n = 3;
    int          i, j, k;
    H5D_layout_t layout;  /* Storage layout */
    size_t       num_map; /* Number of mappings */
    ssize_t      len;     /* Length of the string; also a return value */
    char        *filename = NULL;
    char        *dsetname = NULL;

    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create VDS dataspace.  */
    space = H5Screate_simple(RANK, vdsdims, NULL);
    /* Create dataspaces for A, C, and E datasets. */
    lsrc_space = H5Screate_simple(RANK, ldims, NULL);
    /* Create dataspaces for B, D, and F datasets. */
    nsrc_space = H5Screate_simple(RANK, ndims, NULL);

    /* Create VDS creation property */
    dcpl = H5Pcreate(H5P_DATASET_CREATE);

    /* Initialize hyperslab values */

    start[0] = 0;
    start[1] = 0;
    start[2] = 0;
    count[0] = VDSDIM0;
    count[1] = 1;
    count[2] = 1;
    block[0] = 1;
    block[1] = l;
    block[2] = VDSDIM2;

    /*
     * Build the mappings for A, C and E source datasets.
     */
    status = H5Sselect_hyperslab(lsrc_space, H5S_SELECT_SET, start, NULL, count, block);
    for (i = 0; i < 3; i++) {
        start[1] = (hsize_t)((l + n) * i);
        status   = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, NULL, count, block);
        status   = H5Pset_virtual(dcpl, space, SRC_FILE[2 * i], SRC_DATASET[2 * i], lsrc_space);
    }

    /* Reinitialize start[0] and block[1] */
    start[0] = 0;
    block[1] = n;
    /*
     * Build the mappings for B, D and F source datasets.
     */
    status = H5Sselect_hyperslab(nsrc_space, H5S_SELECT_SET, start, NULL, count, block);
    for (i = 0; i < 3; i++) {
        start[1] = (hsize_t)(l + (l + n) * i);
        status   = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, NULL, count, block);
        status   = H5Pset_virtual(dcpl, space, SRC_FILE[2 * i + 1], SRC_DATASET[2 * i + 1], nsrc_space);
    }

    /* Create a virtual dataset */
    dset   = H5Dcreate2(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);
    status = H5Sclose(space);
    status = H5Sclose(nsrc_space);
    status = H5Sclose(lsrc_space);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen2(file, DATASET, H5P_DEFAULT);

    /*
     * Get creation property list and mapping properties.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Get storage layout.
     */
    layout = H5Pget_layout(dcpl);
    if (H5D_VIRTUAL == layout)
        printf(" Dataset has a virtual layout \n");
    else
        printf("Wrong layout found \n");

    /*
     * Find the number of mappings.
     */
    status = H5Pget_virtual_count(dcpl, &num_map);
    printf(" Number of mappings is %lu\n", (unsigned long)num_map);

    /*
     * Get mapping parameters for each mapping.
     */
    for (i = 0; i < (int)num_map; i++) {
        printf(" Mapping %d \n", i);
        printf("         Selection in the virtual dataset \n");
        /* Get selection in the virtual dataset */
        vspace = H5Pget_virtual_vspace(dcpl, (size_t)i);

        /* Make sure that this is a hyperslab selection and then print information. */
        if (H5Sget_select_type(vspace) == H5S_SEL_HYPERSLABS) {
            if (H5Sis_regular_hyperslab(vspace)) {
                status = H5Sget_regular_hyperslab(vspace, start_out, stride_out, count_out, block_out);
                printf("         start  = [%llu, %llu, %llu] \n", (unsigned long long)start_out[0],
                       (unsigned long long)start_out[1], (unsigned long long)start_out[2]);
                printf("         stride = [%llu, %llu, %llu] \n", (unsigned long long)stride_out[0],
                       (unsigned long long)stride_out[1], (unsigned long long)stride_out[2]);
                printf("         count  = [%llu, %llu, %llu] \n", (unsigned long long)count_out[0],
                       (unsigned long long)count_out[1], (unsigned long long)count_out[2]);
                printf("         block  = [%llu, %llu, %llu] \n", (unsigned long long)block_out[0],
                       (unsigned long long)block_out[1], (unsigned long long)block_out[2]);
            }
        }
        /* Get source file name */
        len      = H5Pget_virtual_filename(dcpl, (size_t)i, NULL, 0);
        filename = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_filename(dcpl, (size_t)i, filename, len + 1);
        printf("         Source filename %s\n", filename);

        /* Get source dataset name */
        len      = H5Pget_virtual_dsetname(dcpl, (size_t)i, NULL, 0);
        dsetname = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_dsetname(dcpl, (size_t)i, dsetname, len + 1);
        printf("         Source dataset name %s\n", dsetname);

        /* Get selection in the source dataset */
        printf("         Selection in the source dataset \n");
        src_space = H5Pget_virtual_srcspace(dcpl, (size_t)i);
        if (H5Sget_select_type(src_space) == H5S_SEL_HYPERSLABS) {
            if (H5Sis_regular_hyperslab(vspace)) {
                status = H5Sget_regular_hyperslab(vspace, start_out, stride_out, count_out, block_out);
                printf("         start  = [%llu, %llu, %llu] \n", (unsigned long long)start_out[0],
                       (unsigned long long)start_out[1], (unsigned long long)start_out[2]);
                printf("         stride = [%llu, %llu, %llu] \n", (unsigned long long)stride_out[0],
                       (unsigned long long)stride_out[1], (unsigned long long)stride_out[2]);
                printf("         count  = [%llu, %llu, %llu] \n", (unsigned long long)count_out[0],
                       (unsigned long long)count_out[1], (unsigned long long)count_out[2]);
                printf("         block  = [%llu, %llu, %llu] \n", (unsigned long long)block_out[0],
                       (unsigned long long)block_out[1], (unsigned long long)block_out[2]);
            }
        }
        H5Sclose(vspace);
        H5Sclose(src_space);
        free(filename);
        free(dsetname);
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf(" VDS Data:\n");
    for (i = 0; i < VDSDIM0; i++) {
        printf(" [");
        for (j = 0; j < VDSDIM1; j++) {
            printf(" [");
            for (k = 0; k < VDSDIM1; k++)
                printf(" %3d", rdata[i][j][k]);
            printf("]");
        }
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5VDS/h5ex_vds-percival-unlim-maxmin.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
/************************************************************

  This example illustrates the concept of the virtual dataset.
  Percival use case. Every fifth 10x10 plane in VDS is stored in
  the corresponding 3D unlimited dataset.
  There are 4 source datasets total.
  Each of the source datasets is extended to different sizes.
  VDS access property can be used to get max and min extent.
  This file is intended for use with HDF5 Library version 1.10

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define VFILE   "h5ex_vds-percival-unlim-maxmin.h5"
#define DATASET "VDS-Percival-unlim-maxmin"
#define VDSDIM0 H5S_UNLIMITED
#define VDSDIM1 10
#define VDSDIM2 10

#define DIM0         H5S_UNLIMITED
#define DIM0_1       4 /* Initial size of the source datasets */
#define DIM1         10
#define DIM2         10
#define RANK         3
#define PLANE_STRIDE 4

const char *SRC_FILE[] = {"apum.h5", "bpum.h5", "cpum.h5", "dpum.h5"};

const char *SRC_DATASET[] = {"A", "B", "C", "D"};

int
main(void)
{
    hid_t   file      = H5I_INVALID_HID;
    hid_t   vfile     = H5I_INVALID_HID;
    hid_t   space     = H5I_INVALID_HID;
    hid_t   dset      = H5I_INVALID_HID;
    hid_t   vdset     = H5I_INVALID_HID;
    hid_t   src_space = H5I_INVALID_HID;
    hid_t   mem_space = H5I_INVALID_HID;
    hid_t   vspace    = H5I_INVALID_HID;
    hid_t   dcpl      = H5I_INVALID_HID;
    hid_t   dapl      = H5I_INVALID_HID;
    herr_t  status;
    hsize_t vdsdims[3]     = {4 * DIM0_1, VDSDIM1, VDSDIM2};
    hsize_t vdsdims_max[3] = {VDSDIM0, VDSDIM1, VDSDIM2};
    hsize_t dims[3]        = {DIM0_1, DIM1, DIM2};
    hsize_t memdims[3]     = {DIM0_1, DIM1, DIM2};
    hsize_t extdims[3]     = {0, DIM1, DIM2}; /* Dimensions of the extended source datasets */
    hsize_t chunk_dims[3]  = {DIM0_1, DIM1, DIM2};
    hsize_t dims_max[3]    = {DIM0, DIM1, DIM2};
    hsize_t vdsdims_out[3];
    hsize_t vdsdims_max_out[3], start[3], stride[3], count[3], src_count[3],
        block[3];                                                         /* Hyperslab parameters */
    hsize_t      start_out[3], stride_out[3], count_out[3], block_out[3]; /* Hyperslab parameter out */
    int          wdata[DIM0_1 * DIM1 * DIM2];
    int          rdata[8 * DIM0_1][VDSDIM1][VDSDIM2]; /* Read buffer for virtual dataset */
    int          i, j, k;
    H5D_layout_t layout;  /* Storage layout */
    size_t       num_map; /* Number of mappings */
    ssize_t      len;     /* Length of the string; also a return value */
    char        *filename = NULL;
    char        *dsetname = NULL;

    /*
     * Create source files and datasets. This step is optional.
     */
    for (i = 0; i < PLANE_STRIDE; i++) {
        /*
         * Initialize data for i-th source dataset.
         */
        for (j = 0; j < DIM0_1 * DIM1 * DIM2; j++)
            wdata[j] = i + 1;

        /*
         * Create the source files and  datasets. Write data to each dataset and
         * close all resources.
         */

        file      = H5Fcreate(SRC_FILE[i], H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
        src_space = H5Screate_simple(RANK, dims, dims_max);
        dcpl      = H5Pcreate(H5P_DATASET_CREATE);
        status    = H5Pset_chunk(dcpl, RANK, chunk_dims);
        dset   = H5Dcreate2(file, SRC_DATASET[i], H5T_STD_I32LE, src_space, H5P_DEFAULT, dcpl, H5P_DEFAULT);
        status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);
        status = H5Sclose(src_space);
        status = H5Pclose(dcpl);
        status = H5Dclose(dset);
        status = H5Fclose(file);
    }

    vfile = H5Fcreate(VFILE, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create VDS dataspace.  */
    vspace = H5Screate_simple(RANK, vdsdims, vdsdims_max);

    /* Create dataspaces for the source dataset. */
    src_space = H5Screate_simple(RANK, dims, dims_max);

    /* Create VDS creation property */
    dcpl = H5Pcreate(H5P_DATASET_CREATE);

    /* Initialize hyperslab values */

    start[0]     = 0;
    start[1]     = 0;
    start[2]     = 0;
    stride[0]    = PLANE_STRIDE; /* we will select every fifth plane in VDS */
    stride[1]    = 1;
    stride[2]    = 1;
    count[0]     = H5S_UNLIMITED;
    count[1]     = 1;
    count[2]     = 1;
    src_count[0] = H5S_UNLIMITED;
    src_count[1] = 1;
    src_count[2] = 1;
    block[0]     = 1;
    block[1]     = DIM1;
    block[2]     = DIM2;

    /*
     * Build the mappings
     */
    status = H5Sselect_hyperslab(src_space, H5S_SELECT_SET, start, NULL, src_count, block);
    for (i = 0; i < PLANE_STRIDE; i++) {
        status = H5Sselect_hyperslab(vspace, H5S_SELECT_SET, start, stride, count, block);
        status = H5Pset_virtual(dcpl, vspace, SRC_FILE[i], SRC_DATASET[i], src_space);
        start[0]++;
    }

    H5Sselect_none(vspace);

    /* Create a virtual dataset */
    vdset  = H5Dcreate2(vfile, DATASET, H5T_STD_I32LE, vspace, H5P_DEFAULT, dcpl, H5P_DEFAULT);
    status = H5Sclose(vspace);
    status = H5Sclose(src_space);
    status = H5Pclose(dcpl);

    /* Let's add data to the source datasets and check new dimensions for VDS */
    /* We will add only one plane to the first source dataset, two planes to the
       second one, three to the third, and four to the forth.                 */

    /* Let's add data to the source datasets and check new dimensions for VDS */
    for (i = 0; i < PLANE_STRIDE; i++) {
        /*
         * Initialize data for i-th source dataset.
         */
        for (j = 0; j < (i + 1) * DIM1 * DIM2; j++)
            wdata[j] = 10 * (i + 1);

        /*
         * Open the source files and datasets. Append data to each dataset and
         * close all resources.
         */

        file       = H5Fopen(SRC_FILE[i], H5F_ACC_RDWR, H5P_DEFAULT);
        dset       = H5Dopen2(file, SRC_DATASET[i], H5P_DEFAULT);
        extdims[0] = DIM0_1 + i + 1;
        status     = H5Dset_extent(dset, extdims);
        src_space  = H5Dget_space(dset);
        start[0]   = DIM0_1;
        start[1]   = 0;
        start[2]   = 0;
        count[0]   = 1;
        count[1]   = 1;
        count[2]   = 1;
        block[0]   = i + 1;
        block[1]   = DIM1;
        block[2]   = DIM2;

        memdims[0] = i + 1;
        mem_space  = H5Screate_simple(RANK, memdims, NULL);
        status     = H5Sselect_hyperslab(src_space, H5S_SELECT_SET, start, NULL, count, block);
        status     = H5Dwrite(dset, H5T_NATIVE_INT, mem_space, src_space, H5P_DEFAULT, wdata);
        status     = H5Sclose(src_space);
        status     = H5Dclose(dset);
        status     = H5Fclose(file);
    }

    status = H5Dclose(vdset);
    status = H5Fclose(vfile);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    vfile = H5Fopen(VFILE, H5F_ACC_RDONLY, H5P_DEFAULT);

    /*
     * Open VDS using different access properties to use max or
     * min extents depending on the sizes of the underlying datasets
     */
    dapl = H5Pcreate(H5P_DATASET_ACCESS);

    for (i = 0; i < 2; i++) {
        status = H5Pset_virtual_view(dapl, i ? H5D_VDS_LAST_AVAILABLE : H5D_VDS_FIRST_MISSING);
        vdset  = H5Dopen2(vfile, DATASET, dapl);

        /* Let's get space of the VDS and its dimension; we should get 32(or 20)x10x10 */
        vspace = H5Dget_space(vdset);
        H5Sget_simple_extent_dims(vspace, vdsdims_out, vdsdims_max_out);
        printf("VDS dimensions, bounds = H5D_VDS_%s: ", i ? "LAST_AVAILABLE" : "FIRST_MISSING");
        for (j = 0; j < RANK; j++)
            printf(" %d ", (int)vdsdims_out[j]);
        printf("\n");

        /* Close */
        status = H5Dclose(vdset);
        status = H5Sclose(vspace);
    }

    status = H5Pclose(dapl);

    vdset = H5Dopen2(vfile, DATASET, H5P_DEFAULT);

    /*
     * Get creation property list and mapping properties.
     */
    dcpl = H5Dget_create_plist(vdset);

    /*
     * Get storage layout.
     */
    layout = H5Pget_layout(dcpl);
    if (H5D_VIRTUAL == layout)
        printf(" Dataset has a virtual layout \n");
    else
        printf("Wrong layout found \n");

    /*
     * Find the number of mappings.
     */
    status = H5Pget_virtual_count(dcpl, &num_map);
    printf(" Number of mappings is %lu\n", (unsigned long)num_map);

    /*
     * Get mapping parameters for each mapping.
     */
    for (i = 0; i < (int)num_map; i++) {
        printf(" Mapping %d \n", i);
        printf("         Selection in the virtual dataset \n");
        /* Get selection in the virtual dataset */
        vspace = H5Pget_virtual_vspace(dcpl, (size_t)i);

        /* Make sure that this is a hyperslab selection and then print information. */
        if (H5Sget_select_type(vspace) == H5S_SEL_HYPERSLABS) {
            if (H5Sis_regular_hyperslab(vspace)) {
                status = H5Sget_regular_hyperslab(vspace, start_out, stride_out, count_out, block_out);
                printf("         start  = [%llu, %llu, %llu] \n", (unsigned long long)start_out[0],
                       (unsigned long long)start_out[1], (unsigned long long)start_out[2]);
                printf("         stride = [%llu, %llu, %llu] \n", (unsigned long long)stride_out[0],
                       (unsigned long long)stride_out[1], (unsigned long long)stride_out[2]);
                printf("         count  = [%llu, %llu, %llu] \n", (unsigned long long)count_out[0],
                       (unsigned long long)count_out[1], (unsigned long long)count_out[2]);
                printf("         block  = [%llu, %llu, %llu] \n", (unsigned long long)block_out[0],
                       (unsigned long long)block_out[1], (unsigned long long)block_out[2]);
            }
        }
        /* Get source file name */
        len      = H5Pget_virtual_filename(dcpl, (size_t)i, NULL, 0);
        filename = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_filename(dcpl, (size_t)i, filename, len + 1);
        printf("         Source filename %s\n", filename);

        /* Get source dataset name */
        len      = H5Pget_virtual_dsetname(dcpl, (size_t)i, NULL, 0);
        dsetname = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_dsetname(dcpl, (size_t)i, dsetname, len + 1);
        printf("         Source dataset name %s\n", dsetname);

        /* Get selection in the source dataset */
        printf("         Selection in the source dataset \n");
        src_space = H5Pget_virtual_srcspace(dcpl, (size_t)i);
        if (H5Sget_select_type(src_space) == H5S_SEL_HYPERSLABS) {
            if (H5Sis_regular_hyperslab(src_space)) {
                status = H5Sget_regular_hyperslab(src_space, start_out, stride_out, count_out, block_out);
                printf("         start  = [%llu, %llu, %llu] \n", (unsigned long long)start_out[0],
                       (unsigned long long)start_out[1], (unsigned long long)start_out[2]);
                printf("         stride = [%llu, %llu, %llu] \n", (unsigned long long)stride_out[0],
                       (unsigned long long)stride_out[1], (unsigned long long)stride_out[2]);
                printf("         count  = [%llu, %llu, %llu] \n", (unsigned long long)count_out[0],
                       (unsigned long long)count_out[1], (unsigned long long)count_out[2]);
                printf("         block  = [%llu, %llu, %llu] \n", (unsigned long long)block_out[0],
                       (unsigned long long)block_out[1], (unsigned long long)block_out[2]);
            }
        }
        H5Sclose(vspace);
        H5Sclose(src_space);
        free(filename);
        free(dsetname);
    }

    /*
     * Read data from VDS.
     */
    vspace = H5Dget_space(vdset);
    H5Sget_simple_extent_dims(vspace, vdsdims_out, vdsdims_max_out);
    printf("VDS dimensions second time \n");
    printf(" Current: ");
    for (i = 0; i < RANK; i++)
        printf(" %d ", (int)vdsdims_out[i]);
    printf("\n");

    /* Read all VDS data */

    /* We should be able to do it by using H5S_ALL instead of making selection
     * or using H5Sselect_all from vspace.
     */
    start[0] = 0;
    start[1] = 0;
    start[2] = 0;
    count[0] = 1;
    count[1] = 1;
    count[2] = 1;
    block[0] = vdsdims_out[0];
    block[1] = vdsdims_out[1];
    block[2] = vdsdims_out[2];

    status    = H5Sselect_hyperslab(vspace, H5S_SELECT_SET, start, NULL, count, block);
    mem_space = H5Screate_simple(RANK, vdsdims_out, NULL);
    status    = H5Dread(vdset, H5T_NATIVE_INT, mem_space, vspace, H5P_DEFAULT, rdata);
    printf(" All data: \n");
    for (i = 0; i < (int)vdsdims_out[0]; i++) {
        for (j = 0; j < (int)vdsdims_out[1]; j++) {
            printf("(%d, %d, 0)", i, j);
            for (k = 0; k < (int)vdsdims_out[2]; k++)
                printf(" %d ", rdata[i][j][k]);
            printf("\n");
        }
    }

    /*
     * Close and release resources.
     */
    status = H5Sclose(mem_space);
    status = H5Pclose(dcpl);
    status = H5Dclose(vdset);
    status = H5Fclose(vfile);

    return 0;
}
```

### `HDF5Examples/C/H5VDS/h5ex_vds-percival-unlim.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
/************************************************************

  This example illustrates the concept of the virtual dataset.
  Percival use case. Every fifth 10x10 plane in VDS is stored in
  the corresponding 3D unlimited dataset.
  There are 4 source datasets total.
  This file is intended for use with HDF5 Library version 1.10

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define VFILE   "h5ex_vds-percival-unlim.h5"
#define DATASET "VDS-Percival-unlim"
#define VDSDIM0 H5S_UNLIMITED
#define VDSDIM1 10
#define VDSDIM2 10

#define DIM0         H5S_UNLIMITED
#define DIM0_1       10 /* Initial size of the datasets */
#define DIM1         10
#define DIM2         10
#define RANK         3
#define PLANE_STRIDE 4

const char *SRC_FILE[] = {"apu.h5", "bpu.h5", "cpu.h5", "dpu.h5"};

const char *SRC_DATASET[] = {"A", "B", "C", "D"};

int
main(void)
{
    hid_t   file      = H5I_INVALID_HID;
    hid_t   vfile     = H5I_INVALID_HID;
    hid_t   space     = H5I_INVALID_HID;
    hid_t   dset      = H5I_INVALID_HID;
    hid_t   vdset     = H5I_INVALID_HID;
    hid_t   src_space = H5I_INVALID_HID;
    hid_t   mem_space = H5I_INVALID_HID;
    hid_t   vspace    = H5I_INVALID_HID;
    hid_t   dcpl      = H5I_INVALID_HID;
    herr_t  status;
    hsize_t vdsdims[3]     = {4 * DIM0_1, VDSDIM1, VDSDIM2};
    hsize_t vdsdims_max[3] = {VDSDIM0, VDSDIM1, VDSDIM2};
    hsize_t dims[3]        = {DIM0_1, DIM1, DIM2};
    hsize_t extdims[3]     = {2 * DIM0_1, DIM1, DIM2};
    hsize_t chunk_dims[3]  = {DIM0_1, DIM1, DIM2};
    hsize_t dims_max[3]    = {DIM0, DIM1, DIM2};
    hsize_t vdsdims_out[3];
    hsize_t vdsdims_max_out[3], start[3], stride[3], count[3], src_count[3],
        block[3];                                                         /* Hyperslab parameters */
    hsize_t      start_out[3], stride_out[3], count_out[3], block_out[3]; /* Hyperslab parameter out */
    int          wdata[DIM0_1 * DIM1 * DIM2];
    int          rdata[8 * DIM0_1][VDSDIM1][VDSDIM2];
    int          a_rdata[2 * DIM0_1][VDSDIM1][VDSDIM2];
    int          i, j, k;
    H5D_layout_t layout;  /* Storage layout */
    size_t       num_map; /* Number of mappings */
    ssize_t      len;     /* Length of the string; also a return value */
    char        *filename = NULL;
    char        *dsetname = NULL;

    /*
     * Create source files and datasets. This step is optional.
     */
    for (i = 0; i < PLANE_STRIDE; i++) {
        /*
         * Initialize data for i-th source dataset.
         */
        for (j = 0; j < DIM0_1 * DIM1 * DIM2; j++)
            wdata[j] = i + 1;

        /*
         * Create the source files and  datasets. Write data to each dataset and
         * close all resources.
         */

        file      = H5Fcreate(SRC_FILE[i], H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
        src_space = H5Screate_simple(RANK, dims, dims_max);
        dcpl      = H5Pcreate(H5P_DATASET_CREATE);
        status    = H5Pset_chunk(dcpl, RANK, chunk_dims);
        dset   = H5Dcreate2(file, SRC_DATASET[i], H5T_STD_I32LE, src_space, H5P_DEFAULT, dcpl, H5P_DEFAULT);
        status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);
        status = H5Sclose(src_space);
        status = H5Pclose(dcpl);
        status = H5Dclose(dset);
        status = H5Fclose(file);
    }

    vfile = H5Fcreate(VFILE, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create VDS dataspace.  */
    vspace = H5Screate_simple(RANK, vdsdims, vdsdims_max);

    /* Create dataspaces for the source dataset. */
    src_space = H5Screate_simple(RANK, dims, dims_max);

    /* Create VDS creation property */
    dcpl = H5Pcreate(H5P_DATASET_CREATE);

    /* Initialize hyperslab values */

    start[0]     = 0;
    start[1]     = 0;
    start[2]     = 0;
    stride[0]    = PLANE_STRIDE; /* we will select every fifth plane in VDS */
    stride[1]    = 1;
    stride[2]    = 1;
    count[0]     = H5S_UNLIMITED;
    count[1]     = 1;
    count[2]     = 1;
    src_count[0] = H5S_UNLIMITED;
    src_count[1] = 1;
    src_count[2] = 1;
    block[0]     = 1;
    block[1]     = DIM1;
    block[2]     = DIM2;

    /*
     * Build the mappings
     */
    status = H5Sselect_hyperslab(src_space, H5S_SELECT_SET, start, NULL, src_count, block);
    for (i = 0; i < PLANE_STRIDE; i++) {
        status = H5Sselect_hyperslab(vspace, H5S_SELECT_SET, start, stride, count, block);
        status = H5Pset_virtual(dcpl, vspace, SRC_FILE[i], SRC_DATASET[i], src_space);
        start[0]++;
    }

    H5Sselect_none(vspace);

    /* Create a virtual dataset */
    vdset  = H5Dcreate2(vfile, DATASET, H5T_STD_I32LE, vspace, H5P_DEFAULT, dcpl, H5P_DEFAULT);
    status = H5Sclose(vspace);
    status = H5Sclose(src_space);
    status = H5Pclose(dcpl);

    /* Let's get space of the VDS and its dimension; we should get 40x10x10 */
    vspace = H5Dget_space(vdset);
    H5Sget_simple_extent_dims(vspace, vdsdims_out, vdsdims_max_out);
    printf("VDS dimensions first time \n");
    printf(" Current: ");
    for (i = 0; i < RANK; i++)
        printf(" %d ", (int)vdsdims_out[i]);
    printf("\n");

    /* Let's add data to the source datasets and check new dimensions for VDS */
    for (i = 0; i < PLANE_STRIDE; i++) {
        /*
         * Initialize data for i-th source dataset.
         */
        for (j = 0; j < DIM0_1 * DIM1 * DIM2; j++)
            wdata[j] = 10 * (i + 1);

        /*
         * Create the source files and  datasets. Write data to each dataset and
         * close all resources.
         */
        file      = H5Fopen(SRC_FILE[i], H5F_ACC_RDWR, H5P_DEFAULT);
        dset      = H5Dopen2(file, SRC_DATASET[i], H5P_DEFAULT);
        status    = H5Dset_extent(dset, extdims);
        src_space = H5Dget_space(dset);
        start[0]  = DIM0_1;
        start[1]  = 0;
        start[2]  = 0;
        count[0]  = 1;
        count[1]  = 1;
        count[2]  = 1;
        block[0]  = DIM0_1;
        block[1]  = DIM1;
        block[2]  = DIM2;

        mem_space = H5Screate_simple(RANK, dims, NULL);
        status    = H5Sselect_hyperslab(src_space, H5S_SELECT_SET, start, NULL, count, block);
        status    = H5Dwrite(dset, H5T_NATIVE_INT, mem_space, src_space, H5P_DEFAULT, wdata);
        status    = H5Sclose(src_space);
        status    = H5Dclose(dset);
        status    = H5Fclose(file);
    }

    status = H5Dclose(vdset);
    status = H5Fclose(vfile);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    vfile = H5Fopen(VFILE, H5F_ACC_RDONLY, H5P_DEFAULT);
    vdset = H5Dopen2(vfile, DATASET, H5P_DEFAULT);

    /*
     * Get creation property list and mapping properties.
     */
    dcpl = H5Dget_create_plist(vdset);

    /*
     * Get storage layout.
     */
    layout = H5Pget_layout(dcpl);
    if (H5D_VIRTUAL == layout)
        printf(" Dataset has a virtual layout \n");
    else
        printf("Wrong layout found \n");

    /*
     * Find the number of mappings.
     */
    status = H5Pget_virtual_count(dcpl, &num_map);
    printf(" Number of mappings is %lu\n", (unsigned long)num_map);

    /*
     * Get mapping parameters for each mapping.
     */
    for (i = 0; i < (int)num_map; i++) {
        printf(" Mapping %d \n", i);
        printf("         Selection in the virtual dataset \n");
        /* Get selection in the virtual dataset */
        vspace = H5Pget_virtual_vspace(dcpl, (size_t)i);

        /* Make sure that this is a hyperslab selection and then print information. */
        if (H5Sget_select_type(vspace) == H5S_SEL_HYPERSLABS) {
            if (H5Sis_regular_hyperslab(vspace)) {
                status = H5Sget_regular_hyperslab(vspace, start_out, stride_out, count_out, block_out);
                printf("         start  = [%llu, %llu, %llu] \n", (unsigned long long)start_out[0],
                       (unsigned long long)start_out[1], (unsigned long long)start_out[2]);
                printf("         stride = [%llu, %llu, %llu] \n", (unsigned long long)stride_out[0],
                       (unsigned long long)stride_out[1], (unsigned long long)stride_out[2]);
                printf("         count  = [%llu, %llu, %llu] \n", (unsigned long long)count_out[0],
                       (unsigned long long)count_out[1], (unsigned long long)count_out[2]);
                printf("         block  = [%llu, %llu, %llu] \n", (unsigned long long)block_out[0],
                       (unsigned long long)block_out[1], (unsigned long long)block_out[2]);
            }
        }
        /* Get source file name */
        len      = H5Pget_virtual_filename(dcpl, (size_t)i, NULL, 0);
        filename = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_filename(dcpl, (size_t)i, filename, len + 1);
        printf("         Source filename %s\n", filename);

        /* Get source dataset name */
        len      = H5Pget_virtual_dsetname(dcpl, (size_t)i, NULL, 0);
        dsetname = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_dsetname(dcpl, (size_t)i, dsetname, len + 1);
        printf("         Source dataset name %s\n", dsetname);

        /* Get selection in the source dataset */
        printf("         Selection in the source dataset \n");
        src_space = H5Pget_virtual_srcspace(dcpl, (size_t)i);
        if (H5Sget_select_type(src_space) == H5S_SEL_HYPERSLABS) {
            if (H5Sis_regular_hyperslab(src_space)) {
                status = H5Sget_regular_hyperslab(src_space, start_out, stride_out, count_out, block_out);
                printf("         start  = [%llu, %llu, %llu] \n", (unsigned long long)start_out[0],
                       (unsigned long long)start_out[1], (unsigned long long)start_out[2]);
                printf("         stride = [%llu, %llu, %llu] \n", (unsigned long long)stride_out[0],
                       (unsigned long long)stride_out[1], (unsigned long long)stride_out[2]);
                printf("         count  = [%llu, %llu, %llu] \n", (unsigned long long)count_out[0],
                       (unsigned long long)count_out[1], (unsigned long long)count_out[2]);
                printf("         block  = [%llu, %llu, %llu] \n", (unsigned long long)block_out[0],
                       (unsigned long long)block_out[1], (unsigned long long)block_out[2]);
            }
        }
        H5Sclose(vspace);
        H5Sclose(src_space);
        free(filename);
        free(dsetname);
    }

    /*
     * Read data from VDS.
     */
    vspace = H5Dget_space(vdset);
    H5Sget_simple_extent_dims(vspace, vdsdims_out, vdsdims_max_out);
    printf("VDS dimensions second time \n");
    printf(" Current: ");
    for (i = 0; i < RANK; i++)
        printf(" %d ", (int)vdsdims_out[i]);
    printf("\n");

    /* Read all VDS data */

    /* We should be able to do it by using H5S_ALL instead of making selection
     * or using H5Sselect_all from vspace.
     */
    start[0] = 0;
    start[1] = 0;
    start[2] = 0;
    count[0] = 1;
    count[1] = 1;
    count[2] = 1;
    block[0] = vdsdims_out[0];
    block[1] = vdsdims_out[1];
    block[2] = vdsdims_out[2];

    status    = H5Sselect_hyperslab(vspace, H5S_SELECT_SET, start, NULL, count, block);
    mem_space = H5Screate_simple(RANK, vdsdims_out, NULL);
    status    = H5Dread(vdset, H5T_NATIVE_INT, mem_space, vspace, H5P_DEFAULT, rdata);
    printf(" All data: \n");
    for (i = 0; i < (int)vdsdims_out[0]; i++) {
        for (j = 0; j < (int)vdsdims_out[1]; j++) {
            printf("(%d, %d, 0)", i, j);
            for (k = 0; k < (int)vdsdims_out[2]; k++)
                printf(" %d ", rdata[i][j][k]);
            printf("\n");
        }
    }
    /* Read VDS, but only data mapped to dataset a.h5 */
    start[0]  = 0;
    start[1]  = 0;
    start[2]  = 0;
    stride[0] = PLANE_STRIDE;
    stride[1] = 1;
    stride[2] = 1;
    count[0]  = 2 * DIM0_1;
    count[1]  = 1;
    count[2]  = 1;
    block[0]  = 1;
    block[1]  = vdsdims_out[1];
    block[2]  = vdsdims_out[2];
    dims[0]   = 2 * DIM0_1;
    status    = H5Sselect_hyperslab(vspace, H5S_SELECT_SET, start, stride, count, block);
    mem_space = H5Screate_simple(RANK, dims, NULL);
    status    = H5Dread(vdset, H5T_NATIVE_INT, mem_space, vspace, H5P_DEFAULT, a_rdata);
    printf(" All data: \n");
    for (i = 0; i < 2 * DIM0_1; i++) {
        for (j = 0; j < (int)vdsdims_out[1]; j++) {
            printf("(%d, %d, 0)", i, j);
            for (k = 0; k < (int)vdsdims_out[2]; k++)
                printf(" %d ", a_rdata[i][j][k]);
            printf("\n");
        }
    }

    /*
     * Close and release resources.
     */
    status = H5Sclose(mem_space);
    status = H5Pclose(dcpl);
    status = H5Dclose(vdset);
    status = H5Fclose(vfile);

    return 0;
}
```

### `HDF5Examples/C/H5VDS/h5ex_vds-percival.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
/************************************************************

  This example illustrates the concept of the virtual dataset.
  Percival use case. Every fifth 10x10 plane in VDS is stored in
  the corresponding 3D unlimited dataset.
  There are 4 source datasets total.
  This file is intended for use with HDF5 Library version 1.10

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME     "h5ex_vds-percival.h5"
#define DATASET      "VDS-Percival"
#define VDSDIM0      40
#define VDSDIM1      10
#define VDSDIM2      10
#define DIM0         10
#define DIM1         10
#define DIM2         10
#define RANK         3
#define PLANE_STRIDE 4

const char *SRC_FILE[] = {"ap.h5", "bp.h5", "cp.h5", "dp.h5"};

const char *SRC_DATASET[] = {"A", "B", "C", "D"};

int
main(void)
{
    hid_t   file      = H5I_INVALID_HID;
    hid_t   space     = H5I_INVALID_HID;
    hid_t   dset      = H5I_INVALID_HID;
    hid_t   src_space = H5I_INVALID_HID;
    hid_t   vspace    = H5I_INVALID_HID;
    hid_t   dcpl      = H5I_INVALID_HID;
    herr_t  status;
    hsize_t vdsdims[3]     = {VDSDIM0, VDSDIM1, VDSDIM2};
    hsize_t vdsdims_max[3] = {VDSDIM0, VDSDIM1, VDSDIM2};
    hsize_t dims[3]        = {DIM0, DIM1, DIM2};
    hsize_t dims_max[3]    = {DIM0, DIM1, DIM2};
    hsize_t start[3], stride[3], count[3], src_count[3], block[3];   /* Hyperslab start parameter for VDS */
    hsize_t start_out[3], stride_out[3], count_out[3], block_out[3]; /* Hyperslab parameter out */
    int     wdata[DIM0 * DIM1 * DIM2];
    int     rdata[VDSDIM0][VDSDIM1][VDSDIM2]; /* Read buffer for virtual dataset */
    int     i, j, k;
    H5D_layout_t layout;  /* Storage layout */
    size_t       num_map; /* Number of mappings */
    ssize_t      len;     /* Length of the string; also a return value */
    char        *filename = NULL;
    char        *dsetname = NULL;

    /*
     * Create source files and datasets. This step is optional.
     */
    for (i = 0; i < PLANE_STRIDE; i++) {
        /*
         * Initialize data for i-th source dataset.
         */
        for (j = 0; j < DIM0 * DIM1 * DIM2; j++)
            wdata[j] = i + 1;

        /*
         * Create the source files and  datasets. Write data to each dataset and
         * close all resources.
         */

        file      = H5Fcreate(SRC_FILE[i], H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
        src_space = H5Screate_simple(RANK, dims, NULL);
        dset =
            H5Dcreate2(file, SRC_DATASET[i], H5T_STD_I32LE, src_space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
        status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);
        status = H5Sclose(src_space);
        status = H5Dclose(dset);
        status = H5Fclose(file);
    }

    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create VDS dataspace.  */
    vspace = H5Screate_simple(RANK, vdsdims, vdsdims_max);

    /* Create dataspaces for the source dataset. */
    src_space = H5Screate_simple(RANK, dims, dims_max);

    /* Create VDS creation property */
    dcpl = H5Pcreate(H5P_DATASET_CREATE);

    /* Initialize hyperslab values */

    start[0]     = 0;
    start[1]     = 0;
    start[2]     = 0;
    stride[0]    = PLANE_STRIDE; /* we will select every fifth plane in VDS */
    stride[1]    = 1;
    stride[2]    = 1;
    count[0]     = VDSDIM0 / 4;
    count[1]     = 1;
    count[2]     = 1;
    src_count[0] = DIM0;
    src_count[1] = 1;
    src_count[2] = 1;
    block[0]     = 1;
    block[1]     = DIM1;
    block[2]     = DIM2;

    /*
     * Build the mappings
     */
    status = H5Sselect_hyperslab(src_space, H5S_SELECT_SET, start, NULL, src_count, block);
    for (i = 0; i < PLANE_STRIDE; i++) {
        status = H5Sselect_hyperslab(vspace, H5S_SELECT_SET, start, stride, count, block);
        status = H5Pset_virtual(dcpl, vspace, SRC_FILE[i], SRC_DATASET[i], src_space);
        start[0]++;
    }

    H5Sselect_none(vspace);

    /* Create a virtual dataset */
    dset   = H5Dcreate2(file, DATASET, H5T_STD_I32LE, vspace, H5P_DEFAULT, dcpl, H5P_DEFAULT);
    status = H5Sclose(vspace);
    status = H5Sclose(src_space);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen2(file, DATASET, H5P_DEFAULT);

    /*
     * Get creation property list and mapping properties.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Get storage layout.
     */
    layout = H5Pget_layout(dcpl);
    if (H5D_VIRTUAL == layout)
        printf(" Dataset has a virtual layout \n");
    else
        printf("Wrong layout found \n");

    /*
     * Find the number of mappings.
     */
    status = H5Pget_virtual_count(dcpl, &num_map);
    printf(" Number of mappings is %lu\n", (unsigned long)num_map);

    /*
     * Get mapping parameters for each mapping.
     */
    for (i = 0; i < (int)num_map; i++) {
        printf(" Mapping %d \n", i);
        printf("         Selection in the virtual dataset \n");
        /* Get selection in the virtual dataset */
        vspace = H5Pget_virtual_vspace(dcpl, (size_t)i);

        /* Make sure that this is a hyperslab selection and then print information. */
        if (H5Sget_select_type(vspace) == H5S_SEL_HYPERSLABS) {
            if (H5Sis_regular_hyperslab(vspace)) {
                status = H5Sget_regular_hyperslab(vspace, start_out, stride_out, count_out, block_out);
                printf("         start  = [%llu, %llu, %llu] \n", (unsigned long long)start_out[0],
                       (unsigned long long)start_out[1], (unsigned long long)start_out[2]);
                printf("         stride = [%llu, %llu, %llu] \n", (unsigned long long)stride_out[0],
                       (unsigned long long)stride_out[1], (unsigned long long)stride_out[2]);
                printf("         count  = [%llu, %llu, %llu] \n", (unsigned long long)count_out[0],
                       (unsigned long long)count_out[1], (unsigned long long)count_out[2]);
                printf("         block  = [%llu, %llu, %llu] \n", (unsigned long long)block_out[0],
                       (unsigned long long)block_out[1], (unsigned long long)block_out[2]);
            }
        }
        /* Get source file name */
        len      = H5Pget_virtual_filename(dcpl, (size_t)i, NULL, 0);
        filename = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_filename(dcpl, (size_t)i, filename, len + 1);
        printf("         Source filename %s\n", filename);

        /* Get source dataset name */
        len      = H5Pget_virtual_dsetname(dcpl, (size_t)i, NULL, 0);
        dsetname = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_dsetname(dcpl, (size_t)i, dsetname, len + 1);
        printf("         Source dataset name %s\n", dsetname);

        /* Get selection in the source dataset */
        printf("         Selection in the source dataset \n");
        src_space = H5Pget_virtual_srcspace(dcpl, (size_t)i);
        if (H5Sget_select_type(src_space) == H5S_SEL_HYPERSLABS) {
            if (H5Sis_regular_hyperslab(src_space)) {
                status = H5Sget_regular_hyperslab(src_space, start_out, stride_out, count_out, block_out);
                printf("         start  = [%llu, %llu, %llu] \n", (unsigned long long)start_out[0],
                       (unsigned long long)start_out[1], (unsigned long long)start_out[2]);
                printf("         stride = [%llu, %llu, %llu] \n", (unsigned long long)stride_out[0],
                       (unsigned long long)stride_out[1], (unsigned long long)stride_out[2]);
                printf("         count  = [%llu, %llu, %llu] \n", (unsigned long long)count_out[0],
                       (unsigned long long)count_out[1], (unsigned long long)count_out[2]);
                printf("         block  = [%llu, %llu, %llu] \n", (unsigned long long)block_out[0],
                       (unsigned long long)block_out[1], (unsigned long long)block_out[2]);
            }
        }
        H5Sclose(vspace);
        H5Sclose(src_space);
        free(filename);
        free(dsetname);
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf(" VDS Data:\n");
    for (i = 0; i < VDSDIM0; i++) {
        printf(" [");
        for (j = 0; j < VDSDIM1; j++) {
            printf(" [");
            for (k = 0; k < VDSDIM1; k++)
                printf(" %3d", rdata[i][j][k]);
            printf("]");
        }
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5VDS/h5ex_vds-simpleIO.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
/************************************************************

  This example illustrates the concept of virtual dataset I/O
  The program  creates 2-dim source dataset and writes
  data to it. Then it creates 2-dim virtual dataset that has
  the same dimension sizes and maps the all elements of the
  virtual dataset to all elements of the source dataset.
  Then VDS is read back.

  This file is intended for use with HDF5 Library version 1.10

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_vds-simpleIO.h5"
#define DATASET  "VDS"
#define DIM1     6
#define DIM0     4
#define RANK     2

#define SRC_FILE    "as.h5"
#define SRC_DATASET "/A"

int
main(void)
{
    hid_t        file      = H5I_INVALID_HID;
    hid_t        space     = H5I_INVALID_HID;
    hid_t        dset      = H5I_INVALID_HID;
    hid_t        src_space = H5I_INVALID_HID;
    hid_t        vspace    = H5I_INVALID_HID;
    hid_t        dcpl      = H5I_INVALID_HID;
    herr_t       status;
    hsize_t      vdsdims[2] = {DIM0, DIM1}; /* Virtual dataset dimension */
    hsize_t      dims[2]    = {DIM0, DIM1}; /* Source dataset dimensions */
    int          wdata[DIM0][DIM1];         /* Write buffer for source dataset */
    int          rdata[DIM0][DIM1];         /* Read buffer for virtual dataset */
    int          i, j;
    H5D_layout_t layout;  /* Storage layout */
    size_t       num_map; /* Number of mappings */
    ssize_t      len;     /* Length of the string; also a return value */
    char        *filename = NULL;
    char        *dsetname = NULL;

    /*
     * Initialize data.
     */
    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            wdata[i][j] = i + 1;

    /*
     * Create the source file and the dataset. Write data to the source dataset
     * and close all resources.
     */
    file   = H5Fcreate(SRC_FILE, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    space  = H5Screate_simple(RANK, dims, NULL);
    dset   = H5Dcreate2(file, SRC_DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata[0]);
    status = H5Sclose(space);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    /* Create file in which virtual dataset will be stored. */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create VDS dataspace.  */
    vspace = H5Screate_simple(RANK, vdsdims, NULL);

    /* Set VDS creation property. */
    dcpl = H5Pcreate(H5P_DATASET_CREATE);

    /*
     * Build the mappings.
     * Selections in the source datasets are H5S_ALL.
     * In the virtual dataset we select the first, the second and the third rows
     * and map each row to the data in the corresponding source dataset.
     */
    src_space = H5Screate_simple(RANK, dims, NULL);
    status    = H5Pset_virtual(dcpl, vspace, SRC_FILE, SRC_DATASET, src_space);

    /* Create a virtual dataset */
    dset   = H5Dcreate2(file, DATASET, H5T_STD_I32LE, vspace, H5P_DEFAULT, dcpl, H5P_DEFAULT);
    status = H5Sclose(vspace);
    status = H5Sclose(src_space);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open the file and virtual dataset
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen2(file, DATASET, H5P_DEFAULT);

    /*
     * Get creation property list and mapping properties.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Get storage layout.
     */
    layout = H5Pget_layout(dcpl);
    if (H5D_VIRTUAL == layout)
        printf(" Dataset has a virtual layout \n");
    else
        printf("Wrong layout found \n");

    /*
     * Find the number of mappings.
     */
    status = H5Pget_virtual_count(dcpl, &num_map);
    printf(" Number of mappings is %lu\n", (unsigned long)num_map);

    /*
     * Get mapping parameters for each mapping.
     */
    for (i = 0; i < (int)num_map; i++) {
        printf(" Mapping %d \n", i);
        printf("         Selection in the virtual dataset ");
        /* Get selection in the virtual dataset */
        vspace = H5Pget_virtual_vspace(dcpl, (size_t)i);

        /* Make sure it is ALL selection and then print selection. */
        if (H5Sget_select_type(vspace) == H5S_SEL_ALL) {
            printf("Selection is H5S_ALL \n");
        }
        /* Get source file name */
        len      = H5Pget_virtual_filename(dcpl, (size_t)i, NULL, 0);
        filename = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_filename(dcpl, (size_t)i, filename, len + 1);
        printf("         Source filename %s\n", filename);

        /* Get source dataset name */
        len      = H5Pget_virtual_dsetname(dcpl, (size_t)i, NULL, 0);
        dsetname = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_dsetname(dcpl, (size_t)i, dsetname, len + 1);
        printf("         Source dataset name %s\n", dsetname);

        /* Get selection in the source dataset */
        printf("         Selection in the source dataset ");
        src_space = H5Pget_virtual_srcspace(dcpl, (size_t)i);

        /* Make sure it is ALL selection and then print selection */
        if (H5Sget_select_type(src_space) == H5S_SEL_ALL) {
            printf("Selection is H5S_ALL \n");
        }
        H5Sclose(vspace);
        H5Sclose(src_space);
        free(filename);
        free(dsetname);
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf(" VDS Data:\n");
    for (i = 0; i < DIM0; i++) {
        printf(" [");
        for (j = 0; j < DIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5VDS/h5ex_vds.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
/************************************************************

  This example illustrates the concept of virtual dataset.
  The program  creates three 1-dim source datasets and writes
  data to them. Then it creates a 2-dim virtual dataset and
  maps the first three rows of the virtual dataset to the data
  in the source datasets. Elements of a row are mapped to all
  elements of the corresponding source dataset.
  The fourth row is not mapped and will be filled with the fill
  values when virtual dataset is read back.

  The program closes all datasets, and then reopens the virtual
  dataset, and finds and prints its creation properties.
  Then it reads the values.

  This file is intended for use with HDF5 Library version 1.10

 ************************************************************/

#include "hdf5.h"
#include <stdio.h>
#include <stdlib.h>

#define FILENAME "h5ex_vds.h5"
#define DATASET  "VDS"
#define VDSDIM1  6
#define VDSDIM0  4
#define DIM0     6
#define RANK1    1
#define RANK2    2

const char *SRC_FILE[] = {"h5ex_vds_a.h5", "h5ex_vds_b.h5", "h5ex_vds_c.h5"};

const char *SRC_DATASET[] = {"A", "B", "C"};

int
main(void)
{
    hid_t        file      = H5I_INVALID_HID;
    hid_t        space     = H5I_INVALID_HID;
    hid_t        dset      = H5I_INVALID_HID;
    hid_t        src_space = H5I_INVALID_HID;
    hid_t        vspace    = H5I_INVALID_HID;
    hid_t        dcpl      = H5I_INVALID_HID;
    herr_t       status;
    hsize_t      vdsdims[2] = {VDSDIM0, VDSDIM1}; /* Virtual datasets dimension */
    hsize_t      dims[1]    = {DIM0};             /* Source datasets dimensions */
    hsize_t      start[2], count[2], block[2];    /* Hyperslab parameters */
    hsize_t      start_out[2], stride_out[2], count_out[2], block_out[2];
    int          wdata[DIM0];             /* Write buffer for source dataset */
    int          rdata[VDSDIM0][VDSDIM1]; /* Read buffer for virtual dataset */
    int          i, j, k, l, block_inc;
    int          fill_value = -1; /* Fill value for VDS */
    H5D_layout_t layout;          /* Storage layout */
    size_t       num_map;         /* Number of mappings */
    ssize_t      len;             /* Length of the string; also a return value */
    hsize_t     *buf      = NULL; /* Buffer to hold hyperslab coordinates */
    char        *filename = NULL;
    char        *dsetname = NULL;
    hsize_t      nblocks;

    /*
     * Create source files and datasets. This step is optional.
     */
    for (i = 0; i < 3; i++) {
        /*
         * Initialize data for i-th source dataset.
         */
        for (j = 0; j < DIM0; j++)
            wdata[j] = i + 1;

        /*
         * Create the source files and  datasets. Write data to each dataset and
         * close all resources.
         */

        file  = H5Fcreate(SRC_FILE[i], H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
        space = H5Screate_simple(RANK1, dims, NULL);
        dset  = H5Dcreate2(file, SRC_DATASET[i], H5T_STD_I32LE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
        status = H5Dwrite(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, wdata);
        status = H5Sclose(space);
        status = H5Dclose(dset);
        status = H5Fclose(file);
    }

    /* Create file in which virtual dataset will be stored. */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create VDS dataspace.  */
    space = H5Screate_simple(RANK2, vdsdims, NULL);

    /* Set VDS creation property. */
    dcpl   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_fill_value(dcpl, H5T_NATIVE_INT, &fill_value);

    /* Initialize hyperslab values. */
    start[0] = 0;
    start[1] = 0;
    count[0] = 1;
    count[1] = 1;
    block[0] = 1;
    block[1] = VDSDIM1;

    /*
     * Build the mappings.
     * Selections in the source datasets are H5S_ALL.
     * In the virtual dataset we select the first, the second and the third rows
     * and map each row to the data in the corresponding source dataset.
     */
    src_space = H5Screate_simple(RANK1, dims, NULL);
    for (i = 0; i < 3; i++) {
        start[0] = (hsize_t)i;
        /* Select i-th row in the virtual dataset; selection in the source datasets is the same. */
        status = H5Sselect_hyperslab(space, H5S_SELECT_SET, start, NULL, count, block);
        status = H5Pset_virtual(dcpl, space, SRC_FILE[i], SRC_DATASET[i], src_space);
    }

    /* Create a virtual dataset. */
    dset   = H5Dcreate2(file, DATASET, H5T_STD_I32LE, space, H5P_DEFAULT, dcpl, H5P_DEFAULT);
    status = H5Sclose(space);
    status = H5Sclose(src_space);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    /*
     * Now we begin the read section of this example.
     */

    /*
     * Open file and dataset using the default properties.
     */
    file = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset = H5Dopen2(file, DATASET, H5P_DEFAULT);

    /*
     * Get creation property list and mapping properties.
     */
    dcpl = H5Dget_create_plist(dset);

    /*
     * Get storage layout.
     */
    layout = H5Pget_layout(dcpl);
    if (H5D_VIRTUAL == layout)
        printf(" Dataset has a virtual layout \n");
    else
        printf("Wrong layout found \n");

    /*
     * Find the number of mappings.
     */
    status = H5Pget_virtual_count(dcpl, &num_map);
    printf(" Number of mappings is %lu\n", (unsigned long)num_map);

    /*
     * Get mapping parameters for each mapping.
     */
    for (i = 0; i < (int)num_map; i++) {
        printf(" Mapping %d \n", i);
        printf("         Selection in the virtual dataset ");
        /* Get selection in the virtual dataset */
        vspace = H5Pget_virtual_vspace(dcpl, (size_t)i);

        /* Make sure that this is a hyperslab selection and then print information. */
        if (H5Sget_select_type(vspace) == H5S_SEL_HYPERSLABS) {
            nblocks = H5Sget_select_hyper_nblocks(vspace);
            buf     = (hsize_t *)malloc(sizeof(hsize_t) * 2 * RANK2 * nblocks);
            status  = H5Sget_select_hyper_blocklist(vspace, (hsize_t)0, nblocks, buf);
            for (l = 0; l < nblocks; l++) {
                block_inc = 2 * RANK2 * l;
                printf("(");
                for (k = 0; k < RANK2 - 1; k++)
                    printf("%d,", (int)buf[block_inc + k]);
                printf("%d) - (", (int)buf[block_inc + k]);
                for (k = 0; k < RANK2 - 1; k++)
                    printf("%d,", (int)buf[block_inc + RANK2 + k]);
                printf("%d)\n", (int)buf[block_inc + RANK2 + k]);
            }
            /* We also can use new APIs to get start, stride, count and block */
            if (H5Sis_regular_hyperslab(vspace)) {
                status = H5Sget_regular_hyperslab(vspace, start_out, stride_out, count_out, block_out);
                printf("         start  = [%llu, %llu] \n", (unsigned long long)start_out[0],
                       (unsigned long long)start_out[1]);
                printf("         stride = [%llu, %llu] \n", (unsigned long long)stride_out[0],
                       (unsigned long long)stride_out[1]);
                printf("         count  = [%llu, %llu] \n", (unsigned long long)count_out[0],
                       (unsigned long long)count_out[1]);
                printf("         block  = [%llu, %llu] \n", (unsigned long long)block_out[0],
                       (unsigned long long)block_out[1]);
            }
        }
        /* Get source file name */
        len      = H5Pget_virtual_filename(dcpl, (size_t)i, NULL, 0);
        filename = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_filename(dcpl, (size_t)i, filename, len + 1);
        printf("         Source filename %s\n", filename);

        /* Get source dataset name */
        len      = H5Pget_virtual_dsetname(dcpl, (size_t)i, NULL, 0);
        dsetname = (char *)malloc((size_t)len * sizeof(char) + 1);
        H5Pget_virtual_dsetname(dcpl, (size_t)i, dsetname, len + 1);
        printf("         Source dataset name %s\n", dsetname);

        /* Get selection in the source dataset */
        printf("         Selection in the source dataset ");
        src_space = H5Pget_virtual_srcspace(dcpl, (size_t)i);

        /* Make sure it is ALL selection and then print the coordinates. */
        if (H5Sget_select_type(src_space) == H5S_SEL_ALL) {
            printf("(0) - (%d) \n", DIM0 - 1);
        }
        H5Sclose(vspace);
        H5Sclose(src_space);
        free(filename);
        free(dsetname);
        free(buf);
    }

    /*
     * Read the data using the default properties.
     */
    status = H5Dread(dset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata[0]);

    /*
     * Output the data to the screen.
     */
    printf(" VDS Data:\n");
    for (i = 0; i < VDSDIM0; i++) {
        printf(" [");
        for (j = 0; j < VDSDIM1; j++)
            printf(" %3d", rdata[i][j]);
        printf("]\n");
    }

    /*
     * Close and release resources.
     */
    status = H5Pclose(dcpl);
    status = H5Dclose(dset);
    status = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/H5VDS/test-pc.sh`

```bash
#! /bin/sh
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.

# This file is for use of h5cc created with the CMake process.
# Environment variable, HDF5_HOME is expected to be set.
# $1 is the path name of the source directory.
# $2 is the path name of the build directory.
# $3 is the current path name.

top_srcdir=$1
top_builddir=$2
currentpath=$3
verbose=yes
nerrors=0

echo "Current build directory: $top_builddir/$currentpath"

# HDF5 compile commands, assuming they are in your $PATH.
H5CC=$HDF5_HOME/bin/h5cc
LD_LIBRARY_PATH=$HDF5_HOME/lib
export LD_LIBRARY_PATH

if ! test -f $H5CC; then
    echo "Set paths for H5CC and LD_LIBRARY_PATH in test.sh"
    echo "Set environment variable HDF5_HOME to the hdf5 install dir"
    echo "h5cc was not found at $H5CC"
    exit $EXIT_FAILURE
fi

H5DUMP=`echo $H5CC | sed -e 's/\/[^/]*$/\/h5dump/'`;
H5_LIBVER=$($H5CC -showconfig | grep -i "HDF5 Version:" | sed 's/^.* //g' | sed 's/[-].*//g')
H5_APIVER=$($H5CC -showconfig | grep -i "Default API mapping:" | sed 's/^.* //g' | sed 's/v//g' | sed 's/1/1_/')

H5_MAJORVER=$(echo $H5_LIBVER | cut -f1 -d'.'  | sed -E 's/\./_/g')
H5_MINORVER=$(echo $H5_LIBVER | cut -f2 -d'.'  | sed -E 's/\./_/g')
H5_RELEASEVER=$(echo $H5_LIBVER | cut -f3 -d'.'  | sed -E 's/\./_/g')
H5_LIBVER_DIR=$H5_MAJORVER$H5_MINORVER

# Shell commands used in Makefiles
RM="rm -rf"
DIFF="diff -c"
CMP="cmp -s"
GREP='grep'
CP="cp -p"  # Use -p to preserve mode,ownership,timestamps
DIRNAME='dirname'
LS='ls'
AWK='awk'

# setup plugin path
ENVCMD="env HDF5_PLUGIN_PATH=$LD_LIBRARY_PATH/plugin"

TESTDIR=$top_builddir/$currentpath


case `echo "testing\c"; echo 1,2,3`,`echo -n testing; echo 1,2,3` in
  *c*,-n*) ECHO_N= ECHO_C='
' ;;
  *c*,*  ) ECHO_N=-n ECHO_C= ;;
  *)       ECHO_N= ECHO_C='\c' ;;
esac
ECHO_N="echo $ECHO_N"


exout() {
    cd $TESTDIR
    "$@"
}

dumpout() {
    cd $TESTDIR
    $H5DUMP "$@"
}

compileout() {
    cd $TESTDIR
    $H5CC "$@"
}

# compare current version, required version.
# returns if cur_ver < req_ver is true.
version_compare() {
  version_lt=0
  if [ ! "$(printf '%s\n' "$1" "$2" | sort -V | head -n1)" = "$2" ]; then
          version_lt=1
  fi
}


topics=""
topics110="vds vds-exc vds-eiger vds-simpleIO vds-percival vds-percival-unlim vds-percival-unlim-maxmin"
# not tested vds-exclim
return_val=0

version_compare "$H5_LIBVER" "1.10.0"
if [ "$version_lt" = 0 ]; then
  for topic in $topics110
  do
      compileout $top_srcdir/$currentpath/h5ex_$topic.c -o h5ex_$topic
  done

  for topic in $topics110
  do
    fname=h5ex_$topic
    $ECHO_N "Testing C/H5VDS/$fname...$ECHO_C"
    exout ./$fname >tmp.test
    status=$?
    if test $status -eq 1
    then
        echo "  Unsupported feature"
        status=0
    else
        cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/110/$fname.tst
        status=$?
        if test $status -ne 0
        then
            echo "  FAILED!"
        else
          dumpout $fname.h5 >tmp.test
          rm -f $TESTDIR/$fname.h5
          cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/110/$fname.ddl
          status=$?
          if test $status -ne 0
          then
              echo "  FAILED!"
          else
              echo "  Passed"
          fi
        fi
        return_val=`expr $status + $return_val`
    fi
  done
fi


rm -f $TESTDIR/tmp.test
echo "$return_val tests failed in C/H5VDS/"
exit $return_val
```

### `HDF5Examples/C/HL/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_C_HL C)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (C_sourcefiles.cmake)

foreach (example_name ${common_examples})
  add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
  target_compile_options (${EXAMPLE_VARNAME}_${example_name}
      PRIVATE
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  if (H5_HAVE_PARALLEL)
    target_include_directories (${EXAMPLE_VARNAME}_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
  endif ()
  target_link_libraries (${EXAMPLE_VARNAME}_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
  if (H5EXAMPLE_BUILD_TESTING)
    add_custom_command (
        TARGET     ${EXAMPLE_VARNAME}_${example_name}
        POST_BUILD
        COMMAND    ${CMAKE_COMMAND}
        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
    )
  endif ()
endforeach ()

#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8")
#  foreach (example_name ${1_8_examples})
#    add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
#    target_compile_options(${EXAMPLE_VARNAME}_${example_name}
#        PRIVATE
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
#    )
#    if (H5_HAVE_PARALLEL)
#      target_include_directories (${EXAMPLE_VARNAME}_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
#    endif ()
#    target_link_libraries (${EXAMPLE_VARNAME}_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
#  foreach (example_name ${1_10_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
#  foreach (example_name ${1_12_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#          add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
#  foreach (example_name ${1_14_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
#  foreach (example_name ${2_0_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()

if (HDF5_PROVIDES_TOOLS)
  add_custom_command (
      TARGET     ${EXAMPLE_VARNAME}_h5ex_image2
      POST_BUILD
      COMMAND    ${CMAKE_COMMAND}
      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/image8.txt ${PROJECT_BINARY_DIR}/image8.txt
  )
  add_custom_command (
      TARGET     ${EXAMPLE_VARNAME}_h5ex_image2
      POST_BUILD
      COMMAND    ${CMAKE_COMMAND}
      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/image24pixel.txt ${PROJECT_BINARY_DIR}/image24pixel.txt
  )
  foreach (example_name ${common_examples})
    if (NOT ${example_name} STREQUAL "h5ex_lite1" AND NOT ${example_name} STREQUAL "h5ex_lite2")
      if (H5EXAMPLE_WORDS_BIGENDIAN)
        add_custom_command (
            TARGET     ${EXAMPLE_VARNAME}_${example_name}
            POST_BUILD
            COMMAND    ${CMAKE_COMMAND}
            ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/${example_name}BE.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
        )
      else ()
        add_custom_command (
            TARGET     ${EXAMPLE_VARNAME}_${example_name}
            POST_BUILD
            COMMAND    ${CMAKE_COMMAND}
            ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
        )
      endif ()
    endif ()
  endforeach ()

#  foreach (example_name ${1_8_examples})
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
#  endforeach ()
#  foreach (example_name ${1_10_examples})
#  endforeach ()
#  foreach (example_name ${1_12_examples})
#  endforeach ()
#  foreach (example_name ${1_14_examples})
#  endforeach ()
#  foreach (example_name ${2_0_examples})
#  endforeach ()
endif ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
    )
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${ARGN};${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-${testname} PROPERTIES
            DEPENDS ${EXAMPLE_VARNAME}_${testname}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
      endif ()
    endif ()
  endmacro ()

  macro (ADD_H5_NOCMP_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
    )
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>"
              -D "TEST_ARGS:STRING=${ARGN}"
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${ARGN};${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_SKIP_COMPARE=TRUE"
                #-D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-${testname} PROPERTIES
            DEPENDS ${EXAMPLE_VARNAME}_${testname}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
      endif ()
    endif ()
  endmacro ()

  foreach (example_name ${common_examples})
    if (${example_name} STREQUAL "h5ex_ds1")
      ADD_H5_NOCMP_TEST (${example_name})
    elseif (NOT ${example_name} STREQUAL "h5ex_lite1" AND NOT ${example_name} STREQUAL "h5ex_lite2")
      ADD_H5_TEST (${example_name})
    endif ()
  endforeach ()

  # special test for h5ex_lite1 and h5ex_lite2
  add_test (
      NAME ${EXAMPLE_VARNAME}_h5ex_lite-clearall
      COMMAND ${CMAKE_COMMAND} -E remove h5ex_lite1.h5
  )
  if (HDF5_ENABLE_USING_MEMCHECKER)
    add_test (NAME ${EXAMPLE_VARNAME}_h5ex_lite1 COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_h5ex_lite1>)
    set_tests_properties (${EXAMPLE_VARNAME}_h5ex_lite1 PROPERTIES
        DEPENDS ${EXAMPLE_VARNAME}_h5ex_lite-clearall
        ENVIRONMENT "${CROSSCOMPILING_PATH}"
    )
    add_test (NAME ${EXAMPLE_VARNAME}_h5ex_lite2 COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_h5ex_lite2>)
    set_tests_properties (${EXAMPLE_VARNAME}_h5ex_lite2 PROPERTIES
        DEPENDS ${EXAMPLE_VARNAME}_h5ex_lite1
        ENVIRONMENT "${CROSSCOMPILING_PATH}"
    )
  else ()
    add_test (
        NAME ${EXAMPLE_VARNAME}_h5ex_lite1
        COMMAND "${CMAKE_COMMAND}"
            -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_h5ex_lite1>"
            -D "TEST_ARGS:STRING="
            -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
            -D "TEST_EXPECT=0"
            -D "TEST_OUTPUT=h5ex_lite1.out"
            -D "TEST_REFERENCE=h5ex_lite1.tst"
            -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
            -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
    )
    set_tests_properties (${EXAMPLE_VARNAME}_h5ex_lite1 PROPERTIES
        DEPENDS ${EXAMPLE_VARNAME}_h5ex_lite-clearall
        ENVIRONMENT "${CROSSCOMPILING_PATH}"
    )
    add_test (
        NAME ${EXAMPLE_VARNAME}_h5ex_lite2
        COMMAND "${CMAKE_COMMAND}"
            -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_h5ex_lite2>"
            -D "TEST_ARGS:STRING="
            -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
            -D "TEST_EXPECT=0"
            -D "TEST_OUTPUT=h5ex_lite2.out"
            -D "TEST_REFERENCE=h5ex_lite2.tst"
            -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
            -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
    )
    set_tests_properties (${EXAMPLE_VARNAME}_h5ex_lite2 PROPERTIES
        DEPENDS ${EXAMPLE_VARNAME}_h5ex_lite1
        ENVIRONMENT "${CROSSCOMPILING_PATH}"
    )
  endif ()
endif ()
```

### `HDF5Examples/C/HL/C_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (common_examples
    h5ex_lite1 h5ex_lite2 h5ex_lite3 h5ex_packet_table_FL
    h5ex_image1 h5ex_image2
    h5ex_table_01 h5ex_table_02 h5ex_table_03 h5ex_table_04
    h5ex_table_05 h5ex_table_06 h5ex_table_07 h5ex_table_08
    h5ex_table_09 h5ex_table_10 h5ex_table_11 h5ex_table_12
    h5ex_ds1
)
```

### `HDF5Examples/C/HL/h5ex_ds1.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"

#define RANK      2
#define DIM_DATA  12
#define DIM1_SIZE 3
#define DIM2_SIZE 4
#define DIM0      0
#define DIM1      1

#define FILENAME  "h5ex_ds1.h5"
#define DSET_NAME "Mydata"
#define DS_1_NAME "Yaxis"
#define DS_2_NAME "Xaxis"

int
main(void)
{

    hid_t   fid;                                                          /* file ID */
    hid_t   did;                                                          /* dataset ID */
    hid_t   dsid;                                                         /* DS dataset ID */
    int     rank               = RANK;                                    /* rank of data dataset */
    int     rankds             = 1;                                       /* rank of DS dataset */
    hsize_t dims[RANK]         = {DIM1_SIZE, DIM2_SIZE};                  /* size of data dataset */
    int     buf[DIM_DATA]      = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}; /* data of data dataset */
    hsize_t s1_dim[1]          = {DIM1_SIZE};                             /* size of DS 1 dataset */
    hsize_t s2_dim[1]          = {DIM2_SIZE};                             /* size of DS 2 dataset */
    float   s1_wbuf[DIM1_SIZE] = {10, 20, 30};                            /* data of DS 1 dataset */
    int     s2_wbuf[DIM2_SIZE] = {10, 20, 50, 100};                       /* data of DS 2 dataset */

    /* create a file using default properties */
    if ((fid = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT)) < 0)
        goto out;

    /* make a dataset */
    if (H5LTmake_dataset_int(fid, DSET_NAME, rank, dims, buf) < 0)
        goto out;

    /* make a DS dataset for the first dimension */
    if (H5LTmake_dataset_float(fid, DS_1_NAME, rankds, s1_dim, s1_wbuf) < 0)
        goto out;

    /* make a DS dataset for the second dimension */
    if (H5LTmake_dataset_int(fid, DS_2_NAME, rankds, s2_dim, s2_wbuf) < 0)
        goto out;

    /*-------------------------------------------------------------------------
     * attach the DS_1_NAME dimension scale to DSET_NAME at dimension 0
     *-------------------------------------------------------------------------
     */

    /* get the dataset id for DSET_NAME */
    if ((did = H5Dopen2(fid, DSET_NAME, H5P_DEFAULT)) < 0)
        goto out;

    /* get the DS dataset id */
    if ((dsid = H5Dopen2(fid, DS_1_NAME, H5P_DEFAULT)) < 0)
        goto out;

    /* attach the DS_1_NAME dimension scale to DSET_NAME at dimension index 0 */
    if (H5DSattach_scale(did, dsid, DIM0) < 0)
        goto out;

    /* close DS id */
    if (H5Dclose(dsid) < 0)
        goto out;

    /*-------------------------------------------------------------------------
     * attach the DS_2_NAME dimension scale to DSET_NAME
     *-------------------------------------------------------------------------
     */

    /* get the DS dataset id */
    if ((dsid = H5Dopen2(fid, DS_2_NAME, H5P_DEFAULT)) < 0)
        goto out;

    /* attach the DS_2_NAME dimension scale to DSET_NAME as the 2nd dimension (index 1)  */
    if (H5DSattach_scale(did, dsid, DIM1) < 0)
        goto out;

    /* close DS ids */
    if (H5Dclose(dsid) < 0)
        goto out;
    if (H5Dclose(did) < 0)
        goto out;

    /* close file */
    H5Fclose(fid);

    return 0;

out:
    printf("Error on return function...Exiting\n");
    return 1;
}
```

### `HDF5Examples/C/HL/h5ex_image1.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"

#define FILENAME    "h5ex_image1.h5"
#define WIDTH       400
#define HEIGHT      200
#define PAL_ENTRIES 9
static unsigned char buf[WIDTH * HEIGHT];

int
main(void)
{
    hid_t         file_id;
    hsize_t       pal_dims[] = {PAL_ENTRIES, 3};
    size_t        i, j;
    int           n, space;
    unsigned char pal[PAL_ENTRIES * 3] = {               /* create a palette with 9 colors */
                                          0,   0,   168, /* dark blue */
                                          0,   0,   252, /* blue */
                                          0,   168, 252, /* ocean blue */
                                          84,  252, 252, /* light blue */
                                          168, 252, 168, /* light green */
                                          0,   252, 168, /* green */
                                          252, 252, 84,  /* yellow */
                                          252, 168, 0,   /* orange */
                                          252, 0,   0};  /* red */

    /* create an image of 9 values divided evenly by the array */
    space = WIDTH * HEIGHT / PAL_ENTRIES;
    for (i = 0, j = 0, n = 0; i < WIDTH * HEIGHT; i++, j++) {
        buf[i] = n;
        if (j > space) {
            n++;
            j = 0;
        }
        if (n > PAL_ENTRIES - 1)
            n = 0;
    }

    /* create a new HDF5 file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* make the image */
    H5IMmake_image_8bit(file_id, "image1", (hsize_t)WIDTH, (hsize_t)HEIGHT, buf);

    /* make a palette */
    H5IMmake_palette(file_id, "palette", pal_dims, pal);

    /* attach the palette to the image */
    H5IMlink_palette(file_id, "image1", "palette");

    /* close the file. */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_image2.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"
#include <stdlib.h>
#include <string.h>

#define FILENAME    "h5ex_image2.h5"
#define DATA_FILE1  "image8.txt"
#define DATA_FILE2  "image24pixel.txt"
#define IMAGE1_NAME "image8bit"
#define IMAGE2_NAME "image24bitpixel"
#define PAL_NAME    "palette"
#define PAL_ENTRIES 256

static int            read_data(const char *file_name, hsize_t *width, hsize_t *height);
static unsigned char *gbuf = NULL; /* global buffer for image data */

int
main(void)
{
    hid_t         file_id;                        /* HDF5 file identifier */
    hsize_t       width;                          /* width of image */
    hsize_t       height;                         /* height of image */
    unsigned char pal[PAL_ENTRIES * 3];           /* palette array */
    hsize_t       pal_dims[2] = {PAL_ENTRIES, 3}; /* palette dimensions */
    herr_t        i, n;

    /* create a new HDF5 file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* read first data file */
    if (read_data(DATA_FILE1, &width, &height) < 0)
        goto out;

    /* make the image */
    H5IMmake_image_8bit(file_id, IMAGE1_NAME, width, height, gbuf);
    if (gbuf) {
        free(gbuf);
        gbuf = NULL;
    }

    /*-------------------------------------------------------------------------
     * define a palette, blue to red tones
     *-------------------------------------------------------------------------
     */
    for (i = 0, n = 0; i < PAL_ENTRIES * 3; i += 3, n++) {
        pal[i]     = n;       /* red */
        pal[i + 1] = 0;       /* green */
        pal[i + 2] = 255 - n; /* blue */
    }

    /* make a palette */
    H5IMmake_palette(file_id, PAL_NAME, pal_dims, pal);

    /* attach the palette to the image */
    H5IMlink_palette(file_id, IMAGE1_NAME, PAL_NAME);

    /*-------------------------------------------------------------------------
     * True color image example with pixel interlace
     *-------------------------------------------------------------------------
     */

    /* read second data file */
    if (read_data(DATA_FILE2, &width, &height) < 0)
        goto out;

    /* make dataset */
    H5IMmake_image_24bit(file_id, IMAGE2_NAME, width, height, "INTERLACE_PIXEL", gbuf);

    /* close the file. */
    H5Fclose(file_id);

    if (gbuf) {
        free(gbuf);
        gbuf = NULL;
    }

    return 0;

out:
    printf("Error on return function...Exiting\n");

    if (gbuf) {
        free(gbuf);
        gbuf = NULL;
    }

    return 1;
}

/*-------------------------------------------------------------------------
 * read_data
 * utility function to read ASCII image data
 * the files have a header of the type
 *
 *   components
 *   n
 *   height
 *   n
 *   width
 *   n
 *
 * followed by the image data
 *
 *-------------------------------------------------------------------------
 */

static int
read_data(const char *fname, /*IN*/
          hsize_t    *width, /*OUT*/
          hsize_t    *height /*OUT*/)
{
    int   i, n;
    int   color_planes;
    char  str[20];
    FILE *f;
    int   w, h;
    char *srcdir         = getenv("srcdir"); /* the source directory */
    char  data_file[512] = "";               /* buffer to hold name of existing data file */

    /*-------------------------------------------------------------------------
     * compose the name of the file to open, using "srcdir", if appropriate
     *-------------------------------------------------------------------------
     */
    strcpy(data_file, "");
    if (srcdir) {
        strcpy(data_file, srcdir);
        strcat(data_file, "/");
    }
    strcat(data_file, fname);

    /*-------------------------------------------------------------------------
     * read
     *-------------------------------------------------------------------------
     */

    f = fopen(data_file, "r");
    if (f == NULL) {
        printf("Could not open file %s. Try set $srcdir \n", data_file);
        return -1;
    }

    fscanf(f, "%s", str);
    fscanf(f, "%d", &color_planes);
    fscanf(f, "%s", str);
    fscanf(f, "%d", &h);
    fscanf(f, "%s", str);
    fscanf(f, "%d", &w);

    *width  = (hsize_t)w;
    *height = (hsize_t)h;

    if (gbuf) {
        free(gbuf);
        gbuf = NULL;
    }

    gbuf = (unsigned char *)malloc(w * h * color_planes * sizeof(unsigned char));

    for (i = 0; i < h * w * color_planes; i++) {
        fscanf(f, "%d", &n);
        gbuf[i] = (unsigned char)n;
    }
    fclose(f);

    return 1;
}
```

### `HDF5Examples/C/HL/h5ex_lite1.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"

#define FILENAME "h5ex_lite1.h5"
#define RANK     2

int
main(void)
{
    hid_t   file_id;
    hsize_t dims[RANK] = {2, 3};
    int     data[6]    = {1, 2, 3, 4, 5, 6};

    /* create a HDF5 file */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* create and write an integer type dataset named "dset" */
    H5LTmake_dataset(file_id, "/dset", RANK, dims, H5T_NATIVE_INT, data);

    /* close file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_lite2.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"

#define FILENAME "h5ex_lite1.h5"

int
main(void)
{
    hid_t   file_id;
    int     data[6];
    hsize_t dims[2];
    size_t  i, j, nrow, n_values;

    /* open file from ex_lite1.c */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);

    /* read dataset */
    H5LTread_dataset_int(file_id, "/dset", data);

    /* get the dimensions of the dataset */
    H5LTget_dataset_info(file_id, "/dset", dims, NULL, NULL);

    /* print it by rows */
    n_values = (size_t)(dims[0] * dims[1]);
    nrow     = (size_t)dims[1];
    for (i = 0; i < n_values / nrow; i++) {
        for (j = 0; j < nrow; j++)
            printf("  %d", data[i * nrow + j]);
        printf("\n");
    }

    /* close file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_lite3.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"
#include <stdlib.h>

#define FILENAME  "h5ex_lite3.h5"
#define ATTR_SIZE 5

int
main(void)
{
    hid_t   file_id;
    hid_t   dset_id;
    hid_t   space_id;
    hsize_t dims[1]         = {ATTR_SIZE};
    int     data[ATTR_SIZE] = {1, 2, 3, 4, 5};
    int     i;

    /* create a file */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* create a data space  */
    space_id = H5Screate_simple(1, dims, NULL);

    /* create a dataset named "dset" */
    dset_id = H5Dcreate2(file_id, "dset", H5T_NATIVE_INT, space_id, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /* close */
    H5Dclose(dset_id);
    H5Sclose(space_id);

    /*-------------------------------------------------------------------------
     * example of H5LTset_attribute_int
     *-------------------------------------------------------------------------
     */

    /* create and write the attribute "attr1" on the dataset "dset" */
    H5LTset_attribute_int(file_id, "dset", "attr1", data, ATTR_SIZE);

    /*-------------------------------------------------------------------------
     * example of H5LTget_attribute_int
     *-------------------------------------------------------------------------
     */

    /* get the attribute "attr1" from the dataset "dset" */
    H5LTget_attribute_int(file_id, "dset", "attr1", data);

    for (i = 0; i < ATTR_SIZE; i++)
        printf("  %d", data[i]);
    printf("\n");

    /* close file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_packet_table_FL.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5_hl.h"
#include <stdlib.h>

/*-------------------------------------------------------------------------
 * Packet Table Fixed-Length Example
 *
 * Example program that creates a packet table and performs
 * writes and reads.
 *
 *-------------------------------------------------------------------------
 */

#define FILENAME "h5ex_packet_table_FL.h5"
int
main(void)
{
    hid_t fid;    /* File identifier */
    hid_t ptable; /* Packet table identifier */

    herr_t  err;   /* Function return status */
    hsize_t count; /* Number of records in the table */

    int x; /* Loop variable */

    /* Buffers to hold data */
    int writeBuffer[5];
    int readBuffer[5];

    /* Initialize buffers */
    for (x = 0; x < 5; x++) {
        writeBuffer[x] = x;
        readBuffer[x]  = -1;
    }

    /* Create a file using default properties */
    fid = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create a fixed-length packet table within the file */
    /* This table's "packets" will be simple integers and it will use compression
     * level 5. */
    ptable = H5PTcreate_fl(fid, "Packet Test Dataset", H5T_NATIVE_INT, (hsize_t)100, 5);
    if (ptable == H5I_INVALID_HID)
        goto out;

    /* Write one packet to the packet table */
    err = H5PTappend(ptable, (hsize_t)1, &(writeBuffer[0]));
    if (err < 0)
        goto out;

    /* Write several packets to the packet table */
    err = H5PTappend(ptable, (hsize_t)4, &(writeBuffer[1]));
    if (err < 0)
        goto out;

    /* Get the number of packets in the packet table.  This should be five. */
    err = H5PTget_num_packets(ptable, &count);
    if (err < 0)
        goto out;

    printf("Number of packets in packet table after five appends: %d\n", (int)count);

    /* Initialize packet table's "current record" */
    err = H5PTcreate_index(ptable);
    if (err < 0)
        goto out;

    /* Iterate through packets, read each one back */
    for (x = 0; x < 5; x++) {
        err = H5PTget_next(ptable, (hsize_t)1, &(readBuffer[x]));
        if (err < 0)
            goto out;

        printf("Packet %d's value is %d\n", x, readBuffer[x]);
    }

    /* Close the packet table */
    err = H5PTclose(ptable);
    if (err < 0)
        goto out;

    /* Close the file */
    H5Fclose(fid);

    return 0;

out: /* An error has occurred.  Clean up and exit. */
    H5PTclose(ptable);
    H5Fclose(fid);
    return -1;
}
```

### `HDF5Examples/C/HL/h5ex_table_01.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"
#include <stdlib.h>

/*-------------------------------------------------------------------------
 * Table API example
 *
 * H5TBmake_table
 * H5TBread_table
 *
 *-------------------------------------------------------------------------
 */

#define NFIELDS    (hsize_t)5
#define NRECORDS   (hsize_t)8
#define TABLE_NAME "table"
#define FILENAME   "h5ex_table_01.h5"

int
main(void)
{
    typedef struct Particle {
        char   name[16];
        int    lati;
        int    longi;
        float  pressure;
        double temperature;
    } Particle;

    Particle dst_buf[NRECORDS];

    /* Calculate the size and the offsets of our struct members in memory */
    size_t dst_size            = sizeof(Particle);
    size_t dst_offset[NFIELDS] = {HOFFSET(Particle, name), HOFFSET(Particle, lati), HOFFSET(Particle, longi),
                                  HOFFSET(Particle, pressure), HOFFSET(Particle, temperature)};

    size_t dst_sizes[NFIELDS] = {sizeof(dst_buf[0].name), sizeof(dst_buf[0].lati), sizeof(dst_buf[0].longi),
                                 sizeof(dst_buf[0].pressure), sizeof(dst_buf[0].temperature)};

    /* Define an array of Particles */
    Particle p_data[NRECORDS] = {{"zero", 0, 1, 0.2F, 3.0},    {"one", 10, 11, 1.2F, 13.0},
                                 {"two", 20, 21, 2.2F, 23.0},  {"three", 30, 31, 3.2F, 33.0},
                                 {"four", 40, 41, 4.2F, 43.0}, {"five", 50, 51, 5.2F, 53.0},
                                 {"six", 60, 61, 6.2F, 63.0},  {"seven", 70, 71, 7.2F, 73.0}};

    /* Define field information */
    const char *field_names[NFIELDS] = {"Name", "Latitude", "Longitude", "Pressure", "Temperature"};
    hid_t       field_type[NFIELDS];
    hid_t       string_type;
    hid_t       file_id;
    hsize_t     chunk_size = 10;
    int        *fill_data  = NULL;
    int         compress   = 0;
    int         i;

    /* Initialize field_type */
    string_type = H5Tcopy(H5T_C_S1);
    H5Tset_size(string_type, 16);
    field_type[0] = string_type;
    field_type[1] = H5T_NATIVE_INT;
    field_type[2] = H5T_NATIVE_INT;
    field_type[3] = H5T_NATIVE_FLOAT;
    field_type[4] = H5T_NATIVE_DOUBLE;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*-------------------------------------------------------------------------
     * H5TBmake_table
     *-------------------------------------------------------------------------
     */

    H5TBmake_table("Table Title", file_id, TABLE_NAME, NFIELDS, NRECORDS, dst_size, field_names, dst_offset,
                   field_type, chunk_size, fill_data, compress, p_data);

    /*-------------------------------------------------------------------------
     * H5TBread_table
     *-------------------------------------------------------------------------
     */

    H5TBread_table(file_id, TABLE_NAME, dst_size, dst_offset, dst_sizes, dst_buf);

    /* print it by rows */
    for (i = 0; i < NRECORDS; i++) {
        printf("%-5s %-5d %-5d %-5f %-5f", dst_buf[i].name, dst_buf[i].lati, dst_buf[i].longi,
               dst_buf[i].pressure, dst_buf[i].temperature);
        printf("\n");
    }

    /*-------------------------------------------------------------------------
     * end
     *-------------------------------------------------------------------------
     */

    /* close type */
    H5Tclose(string_type);

    /* close the file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_table_02.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"
#include <stdlib.h>

/*-------------------------------------------------------------------------
 * Table API example
 *
 * H5TBappend_records
 *
 *-------------------------------------------------------------------------
 */

#define NFIELDS      (hsize_t)5
#define NRECORDS     (hsize_t)8
#define NRECORDS_ADD (hsize_t)2
#define TABLE_NAME   "table"
#define FILENAME     "h5ex_table_02.h5"

int
main(void)
{
    typedef struct Particle {
        char   name[16];
        int    lati;
        int    longi;
        float  pressure;
        double temperature;
    } Particle;

    Particle dst_buf[NRECORDS + NRECORDS_ADD];

    /* Define an array of Particles */
    Particle p_data[NRECORDS] = {{"zero", 0, 1, 0.2F, 3.0},    {"one", 10, 11, 1.2F, 13.0},
                                 {"two", 20, 21, 2.2F, 23.0},  {"three", 30, 31, 3.2F, 33.0},
                                 {"four", 40, 41, 4.2F, 43.0}, {"five", 50, 51, 5.2F, 53.0},
                                 {"six", 60, 61, 6.2F, 63.0},  {"seven", 70, 71, 7.2F, 73.0}};

    /* Calculate the size and the offsets of our struct members in memory */
    size_t dst_size            = sizeof(Particle);
    size_t dst_offset[NFIELDS] = {HOFFSET(Particle, name), HOFFSET(Particle, lati), HOFFSET(Particle, longi),
                                  HOFFSET(Particle, pressure), HOFFSET(Particle, temperature)};

    size_t dst_sizes[NFIELDS] = {sizeof(p_data[0].name), sizeof(p_data[0].lati), sizeof(p_data[0].longi),
                                 sizeof(p_data[0].pressure), sizeof(p_data[0].temperature)};

    /* Define field information */
    const char *field_names[NFIELDS] = {"Name", "Latitude", "Longitude", "Pressure", "Temperature"};
    hid_t       field_type[NFIELDS];
    hid_t       string_type;
    hid_t       file_id;
    hsize_t     chunk_size = 10;
    int        *fill_data  = NULL;
    int         compress   = 0;
    int         i;

    /* Append particles */
    Particle particle_in[NRECORDS_ADD] = {{"eight", 80, 81, 8.2F, 80.3}, {"nine", 90, 91, 9.2F, 90.3}};

    /* Initialize the field field_type */
    string_type = H5Tcopy(H5T_C_S1);
    H5Tset_size(string_type, 16);
    field_type[0] = string_type;
    field_type[1] = H5T_NATIVE_INT;
    field_type[2] = H5T_NATIVE_INT;
    field_type[3] = H5T_NATIVE_FLOAT;
    field_type[4] = H5T_NATIVE_DOUBLE;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* make a table */
    H5TBmake_table("Table Title", file_id, TABLE_NAME, NFIELDS, NRECORDS, dst_size, field_names, dst_offset,
                   field_type, chunk_size, fill_data, compress, p_data);

    /* append two records */
    H5TBappend_records(file_id, TABLE_NAME, NRECORDS_ADD, dst_size, dst_offset, dst_sizes, &particle_in);

    /* read the table */
    H5TBread_table(file_id, TABLE_NAME, dst_size, dst_offset, dst_sizes, dst_buf);

    /* print it by rows */
    for (i = 0; i < NRECORDS + NRECORDS_ADD; i++) {
        printf("%-5s %-5d %-5d %-5f %-5f", dst_buf[i].name, dst_buf[i].lati, dst_buf[i].longi,
               dst_buf[i].pressure, dst_buf[i].temperature);
        printf("\n");
    }

    /* close type */
    H5Tclose(string_type);

    /* close the file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_table_03.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"
#include <stdlib.h>

/*-------------------------------------------------------------------------
 * Table API example
 *
 * H5TBwrite_records
 *
 *-------------------------------------------------------------------------
 */

#define NFIELDS        (hsize_t)5
#define NRECORDS       (hsize_t)8
#define NRECORDS_WRITE (hsize_t)2
#define TABLE_NAME     "table"
#define FILENAME       "h5ex_table_03.h5"

int
main(void)
{
    typedef struct Particle {
        char   name[16];
        int    lati;
        int    longi;
        float  pressure;
        double temperature;
    } Particle;

    Particle dst_buf[NRECORDS];

    /* Calculate the size and the offsets of our struct members in memory */
    size_t dst_size            = sizeof(Particle);
    size_t dst_offset[NFIELDS] = {HOFFSET(Particle, name), HOFFSET(Particle, lati), HOFFSET(Particle, longi),
                                  HOFFSET(Particle, pressure), HOFFSET(Particle, temperature)};

    Particle p                  = {"zero", 0, 1, 0.2F, 0.3};
    size_t   dst_sizes[NFIELDS] = {sizeof(p.name), sizeof(p.lati), sizeof(p.longi), sizeof(p.pressure),
                                   sizeof(p.temperature)};

    /* Define field information */
    const char *field_names[NFIELDS] = {"Name", "Latitude", "Longitude", "Pressure", "Temperature"};
    /* Fill value particle */
    Particle fill_data[1] = {{"no data", -1, -2, -99.0F, -98.0}};
    hid_t    field_type[NFIELDS];
    hid_t    string_type;
    hid_t    file_id;
    hsize_t  chunk_size = 10;
    hsize_t  start;    /* Record to start reading/writing */
    hsize_t  nrecords; /* Number of records to read/write */
    int      i;

    /* Define 2 new particles to write */
    Particle particle_in[NRECORDS_WRITE] = {{"zero", 0, 1, 0.2F, 0.3}, {"one", 10, 11, 1.2F, 10.3}};

    /* Initialize the field field_type */
    string_type = H5Tcopy(H5T_C_S1);
    H5Tset_size(string_type, 16);
    field_type[0] = string_type;
    field_type[1] = H5T_NATIVE_INT;
    field_type[2] = H5T_NATIVE_INT;
    field_type[3] = H5T_NATIVE_FLOAT;
    field_type[4] = H5T_NATIVE_DOUBLE;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Make the table */
    H5TBmake_table("Table Title", file_id, TABLE_NAME, NFIELDS, NRECORDS, dst_size, field_names, dst_offset,
                   field_type, chunk_size, fill_data, 0, /* no compression */
                   NULL);                                /* no data written */

    /* Overwrite 2 records starting at record 0 */
    start    = 0;
    nrecords = NRECORDS_WRITE;
    H5TBwrite_records(file_id, TABLE_NAME, start, nrecords, dst_size, dst_offset, dst_sizes, particle_in);

    /* read the table */
    H5TBread_table(file_id, TABLE_NAME, dst_size, dst_offset, dst_sizes, dst_buf);

    /* print it by rows */
    for (i = 0; i < NRECORDS; i++) {
        printf("%-5s %-5d %-5d %-5f %-5f", dst_buf[i].name, dst_buf[i].lati, dst_buf[i].longi,
               dst_buf[i].pressure, dst_buf[i].temperature);
        printf("\n");
    }

    /* close type */
    H5Tclose(string_type);

    /* close the file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_table_04.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"
#include <stdlib.h>

/*-------------------------------------------------------------------------
 * Table API example
 *
 * H5TBwrite_fields_name
 *
 *-------------------------------------------------------------------------
 */

#define NFIELDS      (hsize_t)5
#define NRECORDS     (hsize_t)8
#define NRECORDS_ADD (hsize_t)3
#define TABLE_NAME   "table"
#define FILENAME     "h5ex_table_04.h5"

int
main(void)
{
    typedef struct Particle {
        char   name[16];
        int    lati;
        int    longi;
        float  pressure;
        double temperature;
    } Particle;

    /* Define a subset of Particle, with latitude and longitude fields */
    typedef struct Position {
        int lati;
        int longi;
    } Position;

    /* Define a subset of Particle, with name and pressure fields */
    typedef struct NamePressure {
        char  name[16];
        float pressure;
    } NamePressure;

    Particle dst_buf[NRECORDS];
    /* Calculate the size and the offsets of our struct members in memory */
    size_t dst_size            = sizeof(Particle);
    size_t dst_offset[NFIELDS] = {HOFFSET(Particle, name), HOFFSET(Particle, lati), HOFFSET(Particle, longi),
                                  HOFFSET(Particle, pressure), HOFFSET(Particle, temperature)};
    size_t dst_sizes[NFIELDS]  = {sizeof(dst_buf[0].name), sizeof(dst_buf[0].lati), sizeof(dst_buf[0].longi),
                                  sizeof(dst_buf[0].pressure), sizeof(dst_buf[0].temperature)};
    size_t field_offset_pos[2] = {HOFFSET(Position, lati), HOFFSET(Position, longi)};
    const char *field_names[NFIELDS] = /* Define field information */
        {"Name", "Latitude", "Longitude", "Pressure", "Temperature"};
    hid_t     field_type[NFIELDS];
    hid_t     string_type;
    hid_t     file_id;
    hsize_t   chunk_size   = 10;
    Particle  fill_data[1] = {{"no data", -1, -2, -99.0F, -98.0}}; /* Fill value particle */
    hsize_t   start;                                               /* Record to start reading/writing */
    hsize_t   nrecords;                                            /* Number of records to read/write */
    int       compress = 0;
    int       i;
    Particle *p_data                    = NULL; /* Initially no data */
    float     pressure_in[NRECORDS_ADD] =       /* Define new values for the field "Pressure"  */
        {0.0F, 1.0F, 2.0F};
    Position     position_in[NRECORDS_ADD] = {/* Define new values for "Latitude,Longitude"  */
                                          {0, 1},
                                          {10, 11},
                                          {20, 21}};
    NamePressure namepre_in[NRECORDS_ADD]  = /* Define new values for "Name,Pressure"  */
        {
            {"zero", 0.0F},
            {"one", 1.0F},
            {"two", 2.0F},
        };
    size_t field_sizes_pos[2] = {sizeof(position_in[0].longi), sizeof(position_in[0].lati)};
    size_t field_sizes_pre[1] = {sizeof(namepre_in[0].pressure)};

    /* Initialize the field field_type */
    string_type = H5Tcopy(H5T_C_S1);
    H5Tset_size(string_type, 16);
    field_type[0] = string_type;
    field_type[1] = H5T_NATIVE_INT;
    field_type[2] = H5T_NATIVE_INT;
    field_type[3] = H5T_NATIVE_FLOAT;
    field_type[4] = H5T_NATIVE_DOUBLE;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Make the table */
    H5TBmake_table("Table Title", file_id, TABLE_NAME, NFIELDS, NRECORDS, dst_size, field_names, dst_offset,
                   field_type, chunk_size, fill_data, compress, p_data);

    /* Write the pressure field starting at record 2 */
    start    = 2;
    nrecords = NRECORDS_ADD;
    H5TBwrite_fields_name(file_id, TABLE_NAME, "Pressure", start, nrecords, sizeof(float), 0, field_sizes_pre,
                          pressure_in);

    /* Write the new longitude and latitude information starting at record 2 */
    start    = 2;
    nrecords = NRECORDS_ADD;
    H5TBwrite_fields_name(file_id, TABLE_NAME, "Latitude,Longitude", start, nrecords, sizeof(Position),
                          field_offset_pos, field_sizes_pos, position_in);

    /* read the table */
    H5TBread_table(file_id, TABLE_NAME, dst_size, dst_offset, dst_sizes, dst_buf);

    /* print it by rows */
    for (i = 0; i < NRECORDS; i++) {
        printf("%-5s %-5d %-5d %-5f %-5f", dst_buf[i].name, dst_buf[i].lati, dst_buf[i].longi,
               dst_buf[i].pressure, dst_buf[i].temperature);
        printf("\n");
    }

    /*-------------------------------------------------------------------------
     * end
     *-------------------------------------------------------------------------
     */

    /* close type */
    H5Tclose(string_type);

    /* close the file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_table_05.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"
#include <stdlib.h>

/*-------------------------------------------------------------------------
 * Table API example
 *
 * H5TBwrite_fields_index
 *
 *-------------------------------------------------------------------------
 */

#define NFIELDS      (hsize_t)5
#define NRECORDS     (hsize_t)8
#define NRECORDS_ADD (hsize_t)3
#define TABLE_NAME   "table"
#define FILENAME     "h5ex_table_05.h5"

int
main(void)
{
    typedef struct Particle {
        char   name[16];
        int    lati;
        int    longi;
        float  pressure;
        double temperature;
    } Particle;

    /* Define a subset of Particle, with latitude and longitude fields */
    typedef struct Position {
        int lati;
        int longi;
    } Position;

    /* Calculate the type_size and the offsets of our struct members */
    Particle dst_buf[NRECORDS];
    size_t   dst_size          = sizeof(Particle);
    size_t dst_offset[NFIELDS] = {HOFFSET(Particle, name), HOFFSET(Particle, lati), HOFFSET(Particle, longi),
                                  HOFFSET(Particle, pressure), HOFFSET(Particle, temperature)};
    size_t dst_sizes[NFIELDS]  = {sizeof(dst_buf[0].name), sizeof(dst_buf[0].lati), sizeof(dst_buf[0].longi),
                                  sizeof(dst_buf[0].pressure), sizeof(dst_buf[0].temperature)};

    size_t field_offset_pos[2] = {HOFFSET(Position, lati), HOFFSET(Position, longi)};

    /* Initially no data */
    Particle *p_data = NULL;

    /* Define field information */
    const char *field_names[NFIELDS] = {"Name", "Latitude", "Longitude", "Pressure", "Temperature"};
    hid_t       field_type[NFIELDS];
    hid_t       string_type;
    hid_t       file_id;
    hsize_t     chunk_size   = 10;
    Particle    fill_data[1] = {{"no data", -1, -2, -99.0F, -98.0}}; /* Fill value particle */
    int         compress     = 0;
    hsize_t     nfields;
    hsize_t     start;    /* Record to start reading/writing */
    hsize_t     nrecords; /* Number of records to read/write */
    int         i;

    /* Define new values for the field "Pressure"  */
    float pressure_in[NRECORDS_ADD] = {0.0F, 1.0F, 2.0F};
    int   field_index_pre[1]        = {3};
    int   field_index_pos[2]        = {1, 2};

    /* Define new values for the fields "Latitude,Longitude"  */
    Position position_in[NRECORDS_ADD] = {{0, 1}, {10, 11}, {20, 21}};

    size_t field_sizes_pos[2] = {sizeof(position_in[0].longi), sizeof(position_in[0].lati)};

    size_t field_sizes_pre[1] = {sizeof(float)};

    /* Initialize the field field_type */
    string_type = H5Tcopy(H5T_C_S1);
    H5Tset_size(string_type, 16);
    field_type[0] = string_type;
    field_type[1] = H5T_NATIVE_INT;
    field_type[2] = H5T_NATIVE_INT;
    field_type[3] = H5T_NATIVE_FLOAT;
    field_type[4] = H5T_NATIVE_DOUBLE;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Make the table */
    H5TBmake_table("Table Title", file_id, TABLE_NAME, NFIELDS, NRECORDS, dst_size, field_names, dst_offset,
                   field_type, chunk_size, fill_data, compress, p_data);

    /* Write the pressure field starting at record 2 */
    nfields  = 1;
    start    = 2;
    nrecords = NRECORDS_ADD;
    H5TBwrite_fields_index(file_id, TABLE_NAME, nfields, field_index_pre, start, nrecords, sizeof(float), 0,
                           field_sizes_pre, pressure_in);

    /* Write the new longitude and latitude information starting at record 2  */
    nfields  = 2;
    start    = 2;
    nrecords = NRECORDS_ADD;
    H5TBwrite_fields_index(file_id, TABLE_NAME, nfields, field_index_pos, start, nrecords, sizeof(Position),
                           field_offset_pos, field_sizes_pos, position_in);

    /* read the table */
    H5TBread_table(file_id, TABLE_NAME, dst_size, dst_offset, dst_sizes, dst_buf);

    /* print it by rows */
    for (i = 0; i < NRECORDS; i++) {
        printf("%-5s %-5d %-5d %-5f %-5f", dst_buf[i].name, dst_buf[i].lati, dst_buf[i].longi,
               dst_buf[i].pressure, dst_buf[i].temperature);
        printf("\n");
    }

    /* close type */
    H5Tclose(string_type);

    /* close the file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_table_06.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"
#include <stdlib.h>

/*-------------------------------------------------------------------------
 * Table API example
 *
 * H5TBget_table_info
 *
 *-------------------------------------------------------------------------
 */

#define NFIELDS    (hsize_t)5
#define NRECORDS   (hsize_t)8
#define TABLE_NAME "table"
#define FILENAME   "h5ex_table_06.h5"

int
main(void)
{
    typedef struct Particle {
        char   name[16];
        int    lati;
        int    longi;
        float  pressure;
        double temperature;
    } Particle;

    /* Calculate the size and the offsets of our struct members in memory */
    size_t dst_size            = sizeof(Particle);
    size_t dst_offset[NFIELDS] = {HOFFSET(Particle, name), HOFFSET(Particle, lati), HOFFSET(Particle, longi),
                                  HOFFSET(Particle, pressure), HOFFSET(Particle, temperature)};

    /* Define field information */
    const char *field_names[NFIELDS] = {"Name", "Latitude", "Longitude", "Pressure", "Temperature"};
    hid_t       field_type[NFIELDS];
    hid_t       string_type;
    hid_t       file_id;
    hsize_t     chunk_size   = 10;
    Particle    fill_data[1] = {{"no data", -1, -2, -99.0F, -98.0}}; /* Fill value particle */
    int         compress     = 0;
    hsize_t     nfields_out;
    hsize_t     nrecords_out;

    /* Initialize field_type */
    string_type = H5Tcopy(H5T_C_S1);
    H5Tset_size(string_type, 16);
    field_type[0] = string_type;
    field_type[1] = H5T_NATIVE_INT;
    field_type[2] = H5T_NATIVE_INT;
    field_type[3] = H5T_NATIVE_FLOAT;
    field_type[4] = H5T_NATIVE_DOUBLE;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Make a table */
    H5TBmake_table("Table Title", file_id, TABLE_NAME, NFIELDS, NRECORDS, dst_size, field_names, dst_offset,
                   field_type, chunk_size, fill_data, compress, NULL);

    /* Get table info  */
    H5TBget_table_info(file_id, TABLE_NAME, &nfields_out, &nrecords_out);

    /* print */
    printf("Table has %d fields and %d records\n", (int)nfields_out, (int)nrecords_out);

    /* close type */
    H5Tclose(string_type);

    /* close the file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_table_07.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"
#include <stdlib.h>

/*-------------------------------------------------------------------------
 * Table API example
 *
 * H5TBdelete_record
 *
 *-------------------------------------------------------------------------
 */

#define NFIELDS    (hsize_t)5
#define NRECORDS   (hsize_t)8
#define TABLE_NAME "table"
#define FILENAME   "h5ex_table_07.h5"

int
main(void)
{
    typedef struct Particle {
        char   name[16];
        int    lati;
        int    longi;
        float  pressure;
        double temperature;
    } Particle;

    /* Calculate the size and the offsets of our struct members in memory */
    size_t dst_size            = sizeof(Particle);
    size_t dst_offset[NFIELDS] = {HOFFSET(Particle, name), HOFFSET(Particle, lati), HOFFSET(Particle, longi),
                                  HOFFSET(Particle, pressure), HOFFSET(Particle, temperature)};

    /* Define an array of Particles */
    Particle p_data[NRECORDS] = {{"zero", 0, 1, 0.2F, 3.0},    {"one", 10, 11, 1.2F, 13.0},
                                 {"two", 20, 21, 2.2F, 23.0},  {"three", 30, 31, 3.2F, 33.0},
                                 {"four", 40, 41, 4.2F, 43.0}, {"five", 50, 51, 5.2F, 53.0},
                                 {"six", 60, 61, 6.2F, 63.0},  {"seven", 70, 71, 7.2F, 73.0}};

    const char *field_names[NFIELDS] = /* Define field information */
        {"Name", "Latitude", "Longitude", "Pressure", "Temperature"};
    hid_t    field_type[NFIELDS];
    hid_t    string_type;
    hid_t    file_id;
    hsize_t  chunk_size   = 10;
    int      compress     = 0;
    Particle fill_data[1] = {{"no data", -1, -2, -99.0F, -98.0}};
    hsize_t  start;    /* Record to start reading */
    hsize_t  nrecords; /* Number of records to insert/delete */
    hsize_t  nfields_out;
    hsize_t  nrecords_out;

    /* Initialize the field field_type */
    string_type = H5Tcopy(H5T_C_S1);
    H5Tset_size(string_type, 16);
    field_type[0] = string_type;
    field_type[1] = H5T_NATIVE_INT;
    field_type[2] = H5T_NATIVE_INT;
    field_type[3] = H5T_NATIVE_FLOAT;
    field_type[4] = H5T_NATIVE_DOUBLE;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Make the table */
    H5TBmake_table("Table Title", file_id, TABLE_NAME, NFIELDS, NRECORDS, dst_size, field_names, dst_offset,
                   field_type, chunk_size, fill_data, compress, p_data);

    /* Delete records  */
    start    = 3;
    nrecords = 3;
    H5TBdelete_record(file_id, TABLE_NAME, start, nrecords);

    /* Get table info  */
    H5TBget_table_info(file_id, TABLE_NAME, &nfields_out, &nrecords_out);

    /* print */
    printf("Table has %d fields and %d records\n", (int)nfields_out, (int)nrecords_out);

    /* close type */
    H5Tclose(string_type);

    /* close the file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_table_08.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5_hl.h"
#include <stdlib.h>

/*-------------------------------------------------------------------------
 * Table API example
 *
 * H5TBinsert_record
 *
 *-------------------------------------------------------------------------
 */

#define NFIELDS      (hsize_t)5
#define NRECORDS     (hsize_t)8
#define NRECORDS_INS (hsize_t)2
#define TABLE_NAME   "table"
#define FILENAME     "h5ex_table_08.h5"

int
main(void)
{
    typedef struct Particle {
        char   name[16];
        int    lati;
        int    longi;
        float  pressure;
        double temperature;
    } Particle;

    Particle dst_buf[NRECORDS + NRECORDS_INS];

    /* Define an array of Particles */
    Particle p_data[NRECORDS] = {{"zero", 0, 1, 0.2F, 3.0},    {"one", 10, 11, 1.2F, 13.0},
                                 {"two", 20, 21, 2.2F, 23.0},  {"three", 30, 31, 3.2F, 33.0},
                                 {"four", 40, 41, 4.2F, 43.0}, {"five", 50, 51, 5.2F, 53.0},
                                 {"six", 60, 61, 6.2F, 63.0},  {"seven", 70, 71, 7.2F, 73.0}};

    /* Calculate the size and the offsets of our struct members in memory */
    size_t dst_size            = sizeof(Particle);
    size_t dst_offset[NFIELDS] = {HOFFSET(Particle, name), HOFFSET(Particle, lati), HOFFSET(Particle, longi),
                                  HOFFSET(Particle, pressure), HOFFSET(Particle, temperature)};
    size_t dst_sizes[NFIELDS]  = {sizeof(p_data[0].name), sizeof(p_data[0].lati), sizeof(p_data[0].longi),
                                  sizeof(p_data[0].pressure), sizeof(p_data[0].temperature)};

    /* Define an array of Particles to insert */
    Particle p_data_insert[NRECORDS_INS] = {{"new", 80, 81, 8.2F, 83.0}, {"new", 90, 91, 9.2F, 93.0}};

    /* Define field information */
    const char *field_names[NFIELDS] = {"Name", "Latitude", "Longitude", "Pressure", "Temperature"};
    hid_t       field_type[NFIELDS];
    hid_t       string_type;
    hid_t       file_id;
    hsize_t     chunk_size = 10;
    int         compress   = 0;
    int        *fill_data  = NULL;
    hsize_t     start;    /* Record to start reading */
    hsize_t     nrecords; /* Number of records to insert/delete */
    hsize_t     nfields_out;
    hsize_t     nrecords_out;
    int         i;

    /* Initialize the field field_type */
    string_type = H5Tcopy(H5T_C_S1);
    H5Tset_size(string_type, 16);
    field_type[0] = string_type;
    field_type[1] = H5T_NATIVE_INT;
    field_type[2] = H5T_NATIVE_INT;
    field_type[3] = H5T_NATIVE_FLOAT;
    field_type[4] = H5T_NATIVE_DOUBLE;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Make the table */
    H5TBmake_table("Table Title", file_id, TABLE_NAME, NFIELDS, NRECORDS, dst_size, field_names, dst_offset,
                   field_type, chunk_size, fill_data, compress, p_data);

    /* Insert records */
    start    = 3;
    nrecords = NRECORDS_INS;
    H5TBinsert_record(file_id, TABLE_NAME, start, nrecords, dst_size, dst_offset, dst_sizes, p_data_insert);

    /* read the table */
    H5TBread_table(file_id, TABLE_NAME, dst_size, dst_offset, dst_sizes, dst_buf);

    /* get table info  */
    H5TBget_table_info(file_id, TABLE_NAME, &nfields_out, &nrecords_out);

    /* print */
    printf("Table has %d fields and %d records\n", (int)nfields_out, (int)nrecords_out);

    /* print it by rows */
    for (i = 0; i < nrecords_out; i++) {
        printf("%-5s %-5d %-5d %-5f %-5f", dst_buf[i].name, dst_buf[i].lati, dst_buf[i].longi,
               dst_buf[i].pressure, dst_buf[i].temperature);
        printf("\n");
    }

    /* close type */
    H5Tclose(string_type);

    /* close the file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_table_09.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5_hl.h"
#include <stdlib.h>

/*-------------------------------------------------------------------------
 * Table API example
 *
 * H5TBadd_records_from
 *
 *-------------------------------------------------------------------------
 */

#define NFIELDS      (hsize_t)5
#define NRECORDS     (hsize_t)8
#define NRECORDS_INS (hsize_t)2
#define TABLE1_NAME  "table1"
#define TABLE2_NAME  "table2"
#define FILENAME     "h5ex_table_09.h5"

int
main(void)
{
    typedef struct Particle {
        char   name[16];
        int    lati;
        int    longi;
        float  pressure;
        double temperature;
    } Particle;

    Particle dst_buf[NRECORDS + NRECORDS_INS];

    /* Calculate the size and the offsets of our struct members in memory */
    size_t dst_size            = sizeof(Particle);
    size_t dst_offset[NFIELDS] = {HOFFSET(Particle, name), HOFFSET(Particle, lati), HOFFSET(Particle, longi),
                                  HOFFSET(Particle, pressure), HOFFSET(Particle, temperature)};
    size_t dst_sizes[NFIELDS]  = {sizeof(dst_buf[0].name), sizeof(dst_buf[0].lati), sizeof(dst_buf[0].longi),
                                  sizeof(dst_buf[0].pressure), sizeof(dst_buf[0].temperature)};

    /* Define an array of Particles */
    Particle p_data[NRECORDS] = {{"zero", 0, 1, 0.2F, 3.0},    {"one", 10, 11, 1.2F, 13.0},
                                 {"two", 20, 21, 2.2F, 23.0},  {"three", 30, 31, 3.2F, 33.0},
                                 {"four", 40, 41, 4.2F, 43.0}, {"five", 50, 51, 5.2F, 53.0},
                                 {"six", 60, 61, 6.2F, 63.0},  {"seven", 70, 71, 7.2F, 73.0}};

    /* Define field information */
    const char *field_names[NFIELDS] = {"Name", "Latitude", "Longitude", "Pressure", "Temperature"};
    hid_t       field_type[NFIELDS];
    hid_t       string_type;
    hid_t       file_id;
    hsize_t     chunk_size   = 10;
    int         compress     = 0;
    Particle    fill_data[1] = {{"no data", -1, -2, -99.0F, -98.0}}; /* Fill value particle */
    hsize_t     start1;   /* Record to start reading from 1st table */
    hsize_t     nrecords; /* Number of records to insert */
    hsize_t     start2;   /* Record to start writing in 2nd table */
    int         i;
    hsize_t     nfields_out;
    hsize_t     nrecords_out;

    /* Initialize the field field_type */
    string_type = H5Tcopy(H5T_C_S1);
    H5Tset_size(string_type, 16);
    field_type[0] = string_type;
    field_type[1] = H5T_NATIVE_INT;
    field_type[2] = H5T_NATIVE_INT;
    field_type[3] = H5T_NATIVE_FLOAT;
    field_type[4] = H5T_NATIVE_DOUBLE;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Make 2 tables: TABLE2_NAME is empty  */
    H5TBmake_table("Table Title", file_id, TABLE1_NAME, NFIELDS, NRECORDS, dst_size, field_names, dst_offset,
                   field_type, chunk_size, fill_data, compress, p_data);

    H5TBmake_table("Table Title", file_id, TABLE2_NAME, NFIELDS, NRECORDS, dst_size, field_names, dst_offset,
                   field_type, chunk_size, fill_data, compress, NULL);

    /* Add 2 records from TABLE1_NAME to TABLE2_NAME  */
    start1   = 3;
    nrecords = NRECORDS_INS;
    start2   = 6;
    H5TBadd_records_from(file_id, TABLE1_NAME, start1, nrecords, TABLE2_NAME, start2);

    /* read TABLE2_NAME: it should have 2 more records now */
    H5TBread_table(file_id, TABLE2_NAME, dst_size, dst_offset, dst_sizes, dst_buf);

    /* Get table info  */
    H5TBget_table_info(file_id, TABLE2_NAME, &nfields_out, &nrecords_out);

    /* print */
    printf("Table has %d fields and %d records\n", (int)nfields_out, (int)nrecords_out);

    /* print it by rows */
    for (i = 0; i < nrecords_out; i++) {
        printf("%-5s %-5d %-5d %-5f %-5f", dst_buf[i].name, dst_buf[i].lati, dst_buf[i].longi,
               dst_buf[i].pressure, dst_buf[i].temperature);
        printf("\n");
    }

    /* close type */
    H5Tclose(string_type);

    /* close the file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_table_10.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5_hl.h"
#include <stdlib.h>

/*-------------------------------------------------------------------------
 * Table API example
 *
 * H5TBcombine_tables
 *
 *-------------------------------------------------------------------------
 */

#define NFIELDS     (hsize_t)5
#define NRECORDS    (hsize_t)8
#define TABLE1_NAME "table1"
#define TABLE2_NAME "table2"
#define TABLE3_NAME "table3"
#define FILENAME    "h5ex_table_10.h5"

int
main(void)
{
    typedef struct Particle {
        char   name[16];
        int    lati;
        int    longi;
        float  pressure;
        double temperature;
    } Particle;

    /* Define an array of Particles */
    Particle p_data[NRECORDS] = {{"zero", 0, 1, 0.2F, 3.0},    {"one", 10, 11, 1.2F, 13.0},
                                 {"two", 20, 21, 2.2F, 23.0},  {"three", 30, 31, 3.2F, 33.0},
                                 {"four", 40, 41, 4.2F, 43.0}, {"five", 50, 51, 5.2F, 53.0},
                                 {"six", 60, 61, 6.2F, 63.0},  {"seven", 70, 71, 7.2F, 73.0}};

    Particle dst_buf[2 * NRECORDS];
    /* Calculate the size and the offsets of our struct members in memory */
    size_t dst_size            = sizeof(Particle);
    size_t dst_offset[NFIELDS] = {HOFFSET(Particle, name), HOFFSET(Particle, lati), HOFFSET(Particle, longi),
                                  HOFFSET(Particle, pressure), HOFFSET(Particle, temperature)};
    size_t dst_sizes[NFIELDS]  = {sizeof(dst_buf[0].name), sizeof(dst_buf[0].lati), sizeof(dst_buf[0].longi),
                                  sizeof(dst_buf[0].pressure), sizeof(dst_buf[0].temperature)};

    /* Define field information */
    const char *field_names[NFIELDS] = {"Name", "Latitude", "Longitude", "Pressure", "Temperature"};
    hid_t       field_type[NFIELDS];
    hid_t       string_type;
    hid_t       file_id;
    hsize_t     chunk_size = 10;
    int         compress   = 0;
    int        *fill_data  = NULL;
    hsize_t     nfields_out;
    hsize_t     nrecords_out;
    int         i;

    /* Initialize the field field_type */
    string_type = H5Tcopy(H5T_C_S1);
    H5Tset_size(string_type, 16);
    field_type[0] = string_type;
    field_type[1] = H5T_NATIVE_INT;
    field_type[2] = H5T_NATIVE_INT;
    field_type[3] = H5T_NATIVE_FLOAT;
    field_type[4] = H5T_NATIVE_DOUBLE;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Make two tables */
    H5TBmake_table("Table Title", file_id, TABLE1_NAME, NFIELDS, NRECORDS, dst_size, field_names, dst_offset,
                   field_type, chunk_size, fill_data, compress, p_data);

    H5TBmake_table("Table Title", file_id, TABLE2_NAME, NFIELDS, NRECORDS, dst_size, field_names, dst_offset,
                   field_type, chunk_size, fill_data, compress, p_data);

    /* Combine the two tables into a third in the same file  */
    H5TBcombine_tables(file_id, TABLE1_NAME, file_id, TABLE2_NAME, TABLE3_NAME);

    /* read the combined table */
    H5TBread_table(file_id, TABLE3_NAME, dst_size, dst_offset, dst_sizes, dst_buf);

    /* Get table info  */
    H5TBget_table_info(file_id, TABLE3_NAME, &nfields_out, &nrecords_out);

    /* print */
    printf("Table has %d fields and %d records\n", (int)nfields_out, (int)nrecords_out);

    /* print it by rows */
    for (i = 0; i < nrecords_out; i++) {
        printf("%-5s %-5d %-5d %-5f %-5f", dst_buf[i].name, dst_buf[i].lati, dst_buf[i].longi,
               dst_buf[i].pressure, dst_buf[i].temperature);
        printf("\n");
    }

    /* close type */
    H5Tclose(string_type);

    /* close the file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_table_11.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5_hl.h"
#include <stdlib.h>

/*-------------------------------------------------------------------------
 * Table API example
 *
 * H5TBinsert_field
 *
 *-------------------------------------------------------------------------
 */

#define NFIELDS    (hsize_t)5
#define NRECORDS   (hsize_t)8
#define TABLE_NAME "table"
#define FILENAME   "h5ex_table_11.h5"

int
main(void)
{
    typedef struct Particle1 {
        char   name[16];
        int    lati;
        int    longi;
        float  pressure;
        double temperature;
    } Particle1;

    /* Define an array of Particles */
    Particle1 p_data[NRECORDS] = {{"zero", 0, 1, 0.2F, 3.0},    {"one", 10, 11, 1.2F, 13.0},
                                  {"two", 20, 21, 2.2F, 23.0},  {"three", 30, 31, 3.2F, 33.0},
                                  {"four", 40, 41, 4.2F, 43.0}, {"five", 50, 51, 5.2F, 53.0},
                                  {"six", 60, 61, 6.2F, 63.0},  {"seven", 70, 71, 7.2F, 73.0}};

    /* Calculate the size and the offsets of our struct members in memory */
    size_t dst_size1            = sizeof(Particle1);
    size_t dst_offset1[NFIELDS] = {HOFFSET(Particle1, name), HOFFSET(Particle1, lati),
                                   HOFFSET(Particle1, longi), HOFFSET(Particle1, pressure),
                                   HOFFSET(Particle1, temperature)};

    /* Define field information */
    const char *field_names[NFIELDS] = {"Name", "Latitude", "Longitude", "Pressure", "Temperature"};
    hid_t       field_type[NFIELDS];
    hid_t       string_type;
    hid_t       file_id;
    hsize_t     chunk_size       = 10;
    int         compress         = 0;
    Particle1   fill_data[1]     = {{"no data", -1, -2, -99.0F, -98.0}};
    int         fill_data_new[1] = {-100};
    hsize_t     position;
    hsize_t     nfields_out;
    hsize_t     nrecords_out;

    /* Define the inserted field information */
    hid_t field_type_new = H5T_NATIVE_INT;
    int   data[NRECORDS] = {0, 1, 2, 3, 4, 5, 6, 7};

    /* Initialize the field type */
    string_type = H5Tcopy(H5T_C_S1);
    H5Tset_size(string_type, 16);
    field_type[0] = string_type;
    field_type[1] = H5T_NATIVE_INT;
    field_type[2] = H5T_NATIVE_INT;
    field_type[3] = H5T_NATIVE_FLOAT;
    field_type[4] = H5T_NATIVE_DOUBLE;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Make the table */
    H5TBmake_table("Table Title", file_id, TABLE_NAME, NFIELDS, NRECORDS, dst_size1, field_names, dst_offset1,
                   field_type, chunk_size, fill_data, compress, p_data);

    /* Insert the new field at the end of the field list */
    position = NFIELDS;
    H5TBinsert_field(file_id, TABLE_NAME, "New Field", field_type_new, position, fill_data_new, data);

    /* Get table info  */
    H5TBget_table_info(file_id, TABLE_NAME, &nfields_out, &nrecords_out);

    /* print */
    printf("Table has %d fields and %d records\n", (int)nfields_out, (int)nrecords_out);

    /* close type */
    H5Tclose(string_type);

    /* close the file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/h5ex_table_12.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "hdf5.h"
#include "hdf5_hl.h"
#include <stdlib.h>

/*-------------------------------------------------------------------------
 * Table API example
 *
 * H5TBdelete_field
 *
 *-------------------------------------------------------------------------
 */

#define NFIELDS    (hsize_t)5
#define NRECORDS   (hsize_t)8
#define TABLE_NAME "table"
#define FILENAME   "h5ex_table_12.h5"

int
main(void)
{
    typedef struct Particle {
        char   name[16];
        int    lati;
        int    longi;
        float  pressure;
        double temperature;
    } Particle;

    /* Calculate the size and the offsets of our struct members in memory */
    size_t dst_size            = sizeof(Particle);
    size_t dst_offset[NFIELDS] = {HOFFSET(Particle, name), HOFFSET(Particle, lati), HOFFSET(Particle, longi),
                                  HOFFSET(Particle, pressure), HOFFSET(Particle, temperature)};

    /* Define an array of Particles */
    Particle p_data[NRECORDS] = {{"zero", 0, 1, 0.2F, 3.0},    {"one", 10, 11, 1.2F, 13.0},
                                 {"two", 20, 21, 2.2F, 23.0},  {"three", 30, 31, 3.2F, 33.0},
                                 {"four", 40, 41, 4.2F, 43.0}, {"five", 50, 51, 5.2F, 53.0},
                                 {"six", 60, 61, 6.2F, 63.0},  {"seven", 70, 71, 7.2F, 73.0}};

    /* Define field information */
    const char *field_names[NFIELDS] = {"Name", "Latitude", "Longitude", "Pressure", "Temperature"};
    hid_t       field_type[NFIELDS];
    hid_t       string_type;
    hid_t       file_id;
    hsize_t     chunk_size   = 10;
    int         compress     = 0;
    Particle    fill_data[1] = {{"no data", -1, -2, -99.0F, -98.0}};
    hsize_t     nfields_out;
    hsize_t     nrecords_out;

    /* Initialize the field type */
    string_type = H5Tcopy(H5T_C_S1);
    H5Tset_size(string_type, 16);
    field_type[0] = string_type;
    field_type[1] = H5T_NATIVE_INT;
    field_type[2] = H5T_NATIVE_INT;
    field_type[3] = H5T_NATIVE_FLOAT;
    field_type[4] = H5T_NATIVE_DOUBLE;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Make a table */
    H5TBmake_table("Table Title", file_id, TABLE_NAME, NFIELDS, NRECORDS, dst_size, field_names, dst_offset,
                   field_type, chunk_size, fill_data, compress, p_data);

    /* Delete the field */
    H5TBdelete_field(file_id, TABLE_NAME, "Pressure");

    /* Get table info  */
    H5TBget_table_info(file_id, TABLE_NAME, &nfields_out, &nrecords_out);

    /* print */
    printf("Table has %d fields and %d records\n", (int)nfields_out, (int)nrecords_out);

    /* close type */
    H5Tclose(string_type);

    /* close the file */
    H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/HL/pal_rgb.h`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/* clang-format off */
const unsigned char pal_rgb[256*3] = {
        255,255,255,
        0,0,131,
        0,0,135,
        0,0,139,
        0,0,143,
        0,0,147,
        0,0,151,
        0,0,155,
        0,0,159,
        0,0,163,
        0,0,167,
        0,0,171,
        0,0,175,
        0,0,179,
        0,0,183,
        0,0,187,
        0,0,191,
        0,0,195,
        0,0,199,
        0,0,203,
        0,0,207,
        0,0,211,
        0,0,215,
        0,0,219,
        0,0,223,
        0,0,227,
        0,0,231,
        0,0,235,
        0,0,239,
        0,0,243,
        0,0,247,
        0,0,251,
        0,0,255,
        0,0,255,
        0,3,255,
        0,7,255,
        0,11,255,
        0,15,255,
        0,19,255,
        0,23,255,
        0,27,255,
        0,31,255,
        0,35,255,
        0,39,255,
        0,43,255,
        0,47,255,
        0,51,255,
        0,55,255,
        0,59,255,
        0,63,255,
        0,67,255,
        0,71,255,
        0,75,255,
        0,79,255,
        0,83,255,
        0,87,255,
        0,91,255,
        0,95,255,
        0,99,255,
        0,103,255,
        0,107,255,
        0,111,255,
        0,115,255,
        0,119,255,
        0,123,255,
        0,127,255,
        0,131,255,
        0,135,255,
        0,139,255,
        0,143,255,
        0,147,255,
        0,151,255,
        0,155,255,
        0,159,255,
        0,163,255,
        0,167,255,
        0,171,255,
        0,175,255,
        0,179,255,
        0,183,255,
        0,187,255,
        0,191,255,
        0,195,255,
        0,199,255,
        0,203,255,
        0,207,255,
        0,211,255,
        0,215,255,
        0,219,255,
        0,223,255,
        0,227,255,
        0,231,255,
        0,235,255,
        0,239,255,
        0,243,255,
        0,247,255,
        0,251,255,
        0,255,255,
        0,255,255,
        3,255,251,
        7,255,247,
        11,255,243,
        15,255,239,
        19,255,235,
        23,255,231,
        27,255,227,
        31,255,223,
        35,255,219,
        39,255,215,
        43,255,211,
        47,255,207,
        51,255,203,
        55,255,199,
        59,255,195,
        63,255,191,
        67,255,187,
        71,255,183,
        75,255,179,
        79,255,175,
        83,255,171,
        87,255,167,
        91,255,163,
        95,255,159,
        99,255,155,
        103,255,151,
        107,255,147,
        111,255,143,
        115,255,139,
        119,255,135,
        123,255,131,
        127,255,127,
        131,255,123,
        135,255,119,
        139,255,115,
        143,255,111,
        147,255,107,
        151,255,103,
        155,255,99,
        159,255,95,
        163,255,91,
        167,255,87,
        171,255,83,
        175,255,79,
        179,255,75,
        183,255,71,
        187,255,67,
        191,255,63,
        195,255,59,
        199,255,55,
        203,255,51,
        207,255,47,
        211,255,43,
        215,255,39,
        219,255,35,
        223,255,31,
        227,255,27,
        231,255,23,
        235,255,19,
        239,255,15,
        243,255,11,
        247,255,7,
        251,255,3,
        255,255,0,
        255,251,0,
        255,247,0,
        255,243,0,
        255,239,0,
        255,235,0,
        255,231,0,
        255,227,0,
        255,223,0,
        255,219,0,
        255,215,0,
        255,211,0,
        255,207,0,
        255,203,0,
        255,199,0,
        255,195,0,
        255,191,0,
        255,187,0,
        255,183,0,
        255,179,0,
        255,175,0,
        255,171,0,
        255,167,0,
        255,163,0,
        255,159,0,
        255,155,0,
        255,151,0,
        255,147,0,
        255,143,0,
        255,139,0,
        255,135,0,
        255,131,0,
        255,127,0,
        255,123,0,
        255,119,0,
        255,115,0,
        255,111,0,
        255,107,0,
        255,103,0,
        255,99,0,
        255,95,0,
        255,91,0,
        255,87,0,
        255,83,0,
        255,79,0,
        255,75,0,
        255,71,0,
        255,67,0,
        255,63,0,
        255,59,0,
        255,55,0,
        255,51,0,
        255,47,0,
        255,43,0,
        255,39,0,
        255,35,0,
        255,31,0,
        255,27,0,
        255,23,0,
        255,19,0,
        255,15,0,
        255,11,0,
        255,7,0,
        255,3,0,
        255,0,0,
        250,0,0,
        246,0,0,
        241,0,0,
        237,0,0,
        233,0,0,
        228,0,0,
        224,0,0,
        219,0,0,
        215,0,0,
        211,0,0,
        206,0,0,
        202,0,0,
        197,0,0,
        193,0,0,
        189,0,0,
        184,0,0,
        180,0,0,
        175,0,0,
        171,0,0,
        167,0,0,
        162,0,0,
        158,0,0,
        153,0,0,
        149,0,0,
        145,0,0,
        140,0,0,
        136,0,0,
        131,0,0,
        127,0,0
};
/* clang-format on */
```

### `HDF5Examples/C/HL/test-pc.sh`

```bash
#! /bin/sh
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.

# This file is for use of h5cc created with the CMake process.
# Environment variable, HDF5_HOME is expected to be set.
# $1 is the path name of the source directory.
# $2 is the path name of the build directory.
# $3 is the current path name.

top_srcdir=$1
top_builddir=$2
currentpath=$3
verbose=yes
nerrors=0

echo "Current build directory: $top_builddir/$currentpath"

# HDF5 compile commands, assuming they are in your $PATH.
H5CC=$HDF5_HOME/bin/h5cc
LD_LIBRARY_PATH=$HDF5_HOME/lib
export LD_LIBRARY_PATH

if ! test -f $H5CC; then
    echo "Set paths for H5CC and LD_LIBRARY_PATH in test.sh"
    echo "Set environment variable HDF5_HOME to the hdf5 install dir"
    echo "h5cc was not found at $H5CC"
    exit $EXIT_FAILURE
fi

H5DUMP=`echo $H5CC | sed -e 's/\/[^/]*$/\/h5dump/'`;
H5_LIBVER=$($H5CC -showconfig | grep -i "HDF5 Version:" | sed 's/^.* //g' | sed 's/[-].*//g')
H5_APIVER=$($H5CC -showconfig | grep -i "Default API mapping:" | sed 's/^.* //g' | sed 's/v//g' | sed 's/1/1_/')

H5_MAJORVER=$(echo $H5_LIBVER | cut -f1 -d'.'  | sed -E 's/\./_/g')
H5_MINORVER=$(echo $H5_LIBVER | cut -f2 -d'.'  | sed -E 's/\./_/g')
H5_RELEASEVER=$(echo $H5_LIBVER | cut -f3 -d'.'  | sed -E 's/\./_/g')
H5_LIBVER_DIR=$H5_MAJORVER$H5_MINORVER

# Shell commands used in Makefiles
RM="rm -rf"
DIFF="diff -c"
CMP="cmp -s"
GREP='grep'
CP="cp -p"  # Use -p to preserve mode,ownership,timestamps
DIRNAME='dirname'
LS='ls'
AWK='awk'

# setup plugin path
ENVCMD="env HDF5_PLUGIN_PATH=$LD_LIBRARY_PATH/plugin"

TESTDIR=$top_builddir/$currentpath


case `echo "testing\c"; echo 1,2,3`,`echo -n testing; echo 1,2,3` in
  *c*,-n*) ECHO_N= ECHO_C='
' ;;
  *c*,*  ) ECHO_N=-n ECHO_C= ;;
  *)       ECHO_N= ECHO_C='\c' ;;
esac
ECHO_N="echo $ECHO_N"


exout() {
    cd $TESTDIR
    "$@"
}

dumpout() {
    cd $TESTDIR
    $H5DUMP "$@"
}

compileout() {
    cd $TESTDIR
    $H5CC "$@"
}

# compare current version, required version.
# returns if cur_ver < req_ver is true.
version_compare() {
  version_lt=0
  if [ ! "$(printf '%s\n' "$1" "$2" | sort -V | head -n1)" = "$2" ]; then
          version_lt=1
  fi
}

topics="h5ex_lite3 h5ex_packet_table_FL \
            h5ex_image1 h5ex_image2 \
            h5ex_table_01 h5ex_table_02 h5ex_table_03 h5ex_table_04 \
            h5ex_table_05 h5ex_table_06 h5ex_table_07 h5ex_table_08 \
            h5ex_table_09 h5ex_table_10 h5ex_table_11 h5ex_table_12 \
            h5ex_ds1"

return_val=0

for topic in $topics
do
    compileout $top_srcdir/$currentpath/$topic.c -o $topic
done

# h5ex_image2 needs data files
cp $top_srcdir/$currentpath/tfiles/image8.txt $TESTDIR/image8.txt
cp $top_srcdir/$currentpath/tfiles/image24pixel.txt $TESTDIR/image24pixel.txt

for topic in $topics
do
    fname=$topic
    $ECHO_N "Testing C/HL/$fname...$ECHO_C"
    exout ./$fname >tmp.test
    cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/$fname.tst
    status=$?
    if test $status -ne 0
    then
        echo "  FAILED!"
    else
        dumpout $fname.h5 >tmp.test
        rm -f $TESTDIR/$fname.h5
        if [ !"$fname" = "h5ex_ds1" ]; then
          cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/$fname.ddl
        fi
        status=$?
        if test $status -ne 0
        then
            echo "  FAILED!"
        else
            echo "  Passed"
        fi
    fi
    return_val=`expr $status + $return_val`
done


compileout $top_srcdir/$currentpath/h5ex_lite1.c -o h5ex_lite1
compileout $top_srcdir/$currentpath/h5ex_lite2.c -o h5ex_lite2

$ECHO_N "Testing C/HL/h5ex_lite1...$ECHO_C"
exout ./h5ex_lite1 >tmp.test
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/h5ex_lite1.tst
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
    echo "  Passed"
    $ECHO_N "Testing C/HL/h5ex_lite2...$ECHO_C"
    exout ./h5ex_lite2 >tmp.test
    cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/h5ex_lite2.tst
    status=$?
    if test $status -ne 0
    then
        echo "  FAILED!"
    else
        echo "  Passed"
    fi
fi
return_val=`expr $status + $return_val`


rm -f $TESTDIR/tmp.test
echo "$return_val tests failed in C/HL/"
exit $return_val
```

### `HDF5Examples/C/HL/tfiles/image24pixel.txt`

```
components
3
height
149
width
227
48
47
45
48
47
45
49
48
46
50
49
47
53
49
46
53
49
46
54
50
47
54
50
47
56
51
47
56
51
47
58
51
45
58
51
45
58
51
45
58
51
45
58
51
45
58
51
45
58
50
47
57
49
46
57
49
46
56
48
45
56
48
45
57
49
46
57
49
46
58
50
47
56
48
45
56
48
45
56
48
45
56
48
45
56
48
45
56
48
45
56
48
45
56
48
45
54
46
43
54
46
43
54
46
43
53
45
42
53
45
42
52
44
41
52
44
41
52
44
41
52
44
41
52
44
41
52
44
41
52
44
41
52
44
41
52
44
41
52
44
41
50
45
41
47
42
36
47
44
37
47
44
37
48
45
38
49
46
39
50
47
40
51
48
41
51
48
41
54
51
44
54
51
44
55
52
45
56
53
46
56
53
46
57
54
47
58
55
48
58
55
46
65
58
48
66
60
48
68
62
50
70
64
52
73
65
52
74
66
53
75
67
54
75
67
54
77
68
53
77
68
53
79
67
51
78
66
50
79
67
51
79
67
51
80
68
52
82
69
53
82
63
49
89
63
50
98
64
52
110
66
53
125
69
54
139
72
56
153
71
57
163
70
55
175
71
58
184
71
57
190
69
58
197
71
59
199
71
62
202
71
63
203
72
64
208
69
64
231
70
76
237
67
76
235
68
75
235
69
73
238
67
73
239
66
68
241
62
66
244
61
65
248
59
65
247
58
64
244
58
63
240
60
63
233
63
64
226
64
62
215
63
60
203
65
55
187
61
47
181
64
46
181
64
46
180
63
45
180
63
45
179
64
45
178
63
45
175
64
45
173
64
44
170
65
44
166
65
45
162
66
44
159
64
42
155
65
41
152
64
42
150
65
44
148
62
45
146
63
47
146
63
47
145
62
46
145
62
44
144
61
43
146
60
43
146
60
43
148
63
43
148
63
43
148
61
42
148
61
42
148
61
42
149
62
43
150
63
44
150
64
47
150
63
54
149
62
53
151
61
50
159
63
49
169
66
51
179
68
51
188
68
52
191
68
52
185
63
48
181
65
50
172
70
56
158
72
59
135
72
57
111
65
50
84
56
42
67
51
38
61
53
50
57
53
52
57
53
52
55
51
50
55
49
49
54
48
48
53
48
45
53
48
45
55
50
47
55
50
47
55
50
47
54
49
46
54
49
45
53
48
44
53
48
44
52
49
44
47
47
45
46
48
45
47
47
45
47
47
45
48
47
45
48
47
45
50
46
45
50
46
45
53
44
45
52
43
44
52
42
43
51
41
42
55
40
43
61
46
49
69
54
57
80
58
60
106
66
67
124
73
72
141
77
77
149
79
79
159
87
91
170
100
110
170
111
131
159
116
144
148
123
163
135
127
174
128
134
186
126
136
189
115
121
173
96
97
145
84
75
118
80
68
104
83
74
103
79
71
94
77
72
94
84
81
98
102
100
111
124
125
130
141
145
144
153
158
152
163
169
159
162
168
154
158
160
147
143
146
129
123
124
108
102
103
85
88
87
69
81
78
69
85
82
91
85
81
98
85
81
98
48
47
45
48
47
45
49
48
46
49
48
46
52
48
45
53
49
46
54
50
47
54
50
47
56
51
47
56
51
47
58
51
45
58
51
45
58
51
45
58
51
45
58
51
45
58
51
45
57
49
46
57
49
46
56
48
45
56
48
45
56
48
45
56
48
45
57
49
46
57
49
46
56
48
45
56
48
45
56
48
45
56
48
45
56
48
45
56
48
45
56
48
45
56
48
45
54
46
43
54
46
43
53
45
42
53
45
42
53
45
42
52
44
41
52
44
41
52
44
41
53
45
42
53
45
42
53
45
42
53
45
42
53
45
42
53
45
42
53
45
42
51
46
42
48
43
37
48
45
38
48
45
38
49
46
39
50
47
40
51
48
41
52
49
42
52
49
42
54
51
44
54
51
44
55
52
45
55
52
45
56
53
46
57
54
47
58
55
48
58
55
46
64
57
47
65
59
47
67
61
49
69
63
51
72
64
51
73
65
52
74
66
53
74
66
53
76
67
52
76
67
52
77
65
49
77
65
49
77
65
49
78
66
50
79
67
51
81
68
52
80
62
48
85
63
49
94
65
51
106
67
52
120
68
54
132
69
52
146
69
53
156
67
51
167
67
52
175
65
52
181
65
52
188
66
55
192
68
58
196
69
60
197
70
61
205
67
64
225
69
73
231
66
73
232
67
73
234
68
72
237
68
71
239
66
68
243
64
67
246
63
67
248
59
65
247
58
64
244
58
63
240
60
63
233
63
64
224
65
62
213
64
60
202
64
54
185
62
47
181
64
46
180
63
45
180
63
45
180
63
45
178
63
44
176
63
45
175
64
45
173
64
44
169
64
43
166
65
45
161
65
43
159
64
42
155
65
41
152
64
42
150
65
44
148
62
45
146
63
47
145
62
46
145
62
46
145
62
44
144
61
43
146
60
43
146
60
43
148
63
43
147
62
42
148
61
42
148
61
42
148
61
42
149
62
43
150
63
44
150
64
47
148
64
53
149
62
53
152
62
51
160
62
49
171
65
51
181
68
52
189
67
52
191
68
53
187
65
50
183
67
54
172
70
56
156
73
59
133
71
58
108
64
51
83
57
42
66
52
41
62
54
51
58
54
53
57
53
52
56
52
51
56
50
50
55
49
49
54
49
46
54
49
46
55
50
47
55
50
47
55
50
47
54
49
46
54
49
45
53
48
44
53
48
44
52
49
44
47
47
45
46
48
45
47
47
45
47
47
45
48
47
45
48
47
45
50
46
45
50
46
45
51
45
45
53
44
45
52
42
43
51
41
42
53
41
43
60
45
48
67
52
55
78
56
58
100
62
61
118
69
65
134
74
73
143
78
76
154
84
86
164
97
106
165
108
125
157
114
142
145
123
162
134
128
174
127
133
185
123
132
187
111
117
169
91
91
141
81
72
117
79
67
105
79
70
101
79
71
95
80
72
95
82
78
95
91
89
102
110
111
116
131
135
136
148
153
149
153
158
151
154
160
148
153
157
143
143
145
131
126
127
111
106
107
89
93
91
76
86
83
74
88
83
90
86
82
97
88
81
97
47
46
44
47
46
44
48
47
45
49
48
46
52
48
45
52
48
45
53
49
46
53
49
46
55
50
46
55
50
46
55
50
46
55
50
46
57
50
44
57
50
44
57
50
44
57
50
44
57
49
46
56
48
46
55
47
45
55
47
45
55
47
45
55
47
45
56
48
46
57
49
47
56
48
46
56
48
46
56
48
46
56
48
46
56
48
46
56
48
46
56
48
46
56
48
46
54
46
44
53
45
43
53
45
43
53
45
43
52
44
42
52
44
42
52
44
42
52
44
42
53
45
43
53
45
43
53
45
43
53
45
43
53
45
43
53
45
43
53
45
43
51
46
42
50
45
39
49
46
39
50
47
40
51
48
41
51
48
41
52
49
42
52
49
42
53
50
43
54
51
44
54
51
44
54
51
44
55
52
45
56
53
46
57
54
47
57
54
47
58
55
46
63
56
46
64
58
46
66
60
48
68
62
50
71
63
52
72
64
53
72
64
51
72
64
51
73
64
49
73
64
49
75
63
49
75
63
49
75
63
47
76
64
48
77
65
49
78
66
50
77
65
49
81
65
49
89
66
50
100
67
50
113
67
51
124
67
50
135
66
50
146
65
48
152
60
45
160
58
43
167
59
46
175
61
50
181
64
54
186
68
58
191
70
61
197
68
62
217
67
69
225
64
70
227
66
71
231
66
70
234
68
70
240
67
69
244
65
68
248
63
68
250
59
64
249
58
63
245
59
64
240
61
64
232
64
64
220
65
61
209
64
59
198
64
53
183
61
46
179
64
45
179
64
45
178
63
44
178
63
45
176
63
45
175
62
44
173
64
44
170
63
45
167
64
45
165
64
44
161
65
43
157
65
44
154
63
42
152
64
42
149
64
43
148
62
45
145
62
46
145
62
46
145
62
46
144
61
43
144
61
43
144
61
43
144
61
43
145
62
44
145
62
44
146
60
43
146
60
43
147
61
44
147
61
44
148
63
43
149
63
46
149
65
54
150
63
53
154
62
51
163
62
50
175
65
52
185
67
53
191
68
53
193
67
53
192
68
56
184
70
59
174
73
61
153
74
61
129
71
59
104
64
52
81
58
44
66
53
44
63
55
52
61
55
55
60
54
54
58
52
52
57
52
49
56
51
48
55
50
47
55
50
47
54
50
47
54
50
47
54
50
47
53
49
46
53
50
45
52
49
44
52
49
44
50
49
44
46
48
45
44
49
45
46
48
45
46
48
45
47
47
45
47
47
45
48
47
45
50
46
45
53
47
47
52
46
46
53
44
45
52
42
43
52
42
43
57
45
47
62
50
52
73
53
54
91
57
55
108
63
57
124
69
64
134
73
68
144
79
77
152
91
96
155
102
118
148
109
136
140
119
158
131
125
173
125
130
185
117
126
183
102
109
164
84
86
137
77
69
118
76
65
107
77
65
101
79
70
99
81
72
99
77
72
94
78
75
92
91
91
101
113
116
121
133
137
136
135
140
134
138
143
136
141
144
133
134
138
124
120
122
108
102
105
88
90
91
77
86
83
76
90
85
91
90
84
96
91
83
96
46
45
43
47
46
44
47
46
44
48
47
45
51
47
44
52
48
45
52
48
45
52
48
45
54
49
45
54
49
45
54
49
45
54
49
45
56
49
43
56
49
43
56
49
43
56
49
43
55
47
45
55
47
45
54
46
44
54
46
44
54
46
44
54
46
44
55
47
45
55
47
45
56
48
46
56
48
46
56
48
46
56
48
46
56
48
46
56
48
46
56
48
46
56
48
46
53
45
43
53
45
43
53
45
43
52
44
42
52
44
42
52
44
42
51
43
41
51
43
41
54
46
44
54
46
44
54
46
44
54
46
44
54
46
44
54
46
44
54
46
44
52
47
44
51
48
43
51
48
41
51
48
41
52
49
42
52
49
42
53
50
43
53
50
43
53
50
43
53
50
43
53
50
43
54
51
44
55
52
45
56
53
46
56
53
46
57
54
47
57
54
45
62
55
45
63
57
45
64
58
46
66
60
48
69
61
50
69
61
50
70
62
49
70
62
49
71
61
49
71
62
47
72
60
46
73
61
47
73
61
47
74
62
46
76
64
48
74
65
48
75
68
50
78
69
52
85
68
52
94
68
51
105
68
50
115
66
49
126
65
47
134
63
45
140
57
41
148
57
39
157
57
42
163
60
45
172
64
51
179
69
56
184
72
60
193
70
62
210
68
67
218
65
67
222
66
67
226
68
67
231
67
68
236
66
67
243
64
67
247
63
65
250
59
64
249
58
63
245
59
64
238
62
64
229
65
64
218
66
61
205
64
57
194
64
51
181
61
45
176
64
44
176
64
44
176
64
44
175
62
44
174
63
44
174
63
44
172
63
43
169
64
45
166
63
44
163
64
43
160
64
42
156
64
43
152
64
42
150
64
41
148
63
42
145
62
44
145
62
46
145
62
46
144
61
45
144
61
43
144
61
43
143
60
42
143
60
42
144
61
43
144
61
43
143
60
42
143
60
42
146
60
43
145
62
44
148
62
45
146
63
45
149
65
54
150
63
53
156
62
50
166
64
52
178
66
54
188
67
56
193
67
55
194
66
55
194
70
60
186
74
63
172
76
64
151
75
62
124
70
58
98
64
52
76
58
46
65
55
45
64
56
53
62
57
54
61
55
55
60
55
52
58
53
50
57
52
49
56
51
48
56
51
48
54
50
47
54
50
47
54
50
47
51
50
46
53
50
45
50
49
44
50
49
44
49
50
45
45
50
46
45
50
46
47
49
46
47
49
46
48
48
46
48
48
46
49
48
46
49
48
46
53
49
48
54
48
48
53
47
47
53
44
45
52
43
44
54
44
45
58
48
49
68
50
50
84
55
49
99
59
51
113
65
55
123
69
59
130
72
68
137
82
85
141
95
108
139
105
130
131
115
152
125
121
171
120
125
181
111
119
181
96
102
162
81
82
139
76
69
121
76
64
112
76
64
104
80
69
103
81
70
102
76
67
94
71
66
86
77
76
90
94
94
102
106
110
113
117
121
120
120
125
119
122
128
118
119
122
111
107
111
97
96
98
84
87
89
76
84
84
76
93
87
89
93
86
93
93
86
94
45
45
45
45
45
45
46
45
43
47
46
44
48
47
45
49
48
46
51
47
44
52
48
45
51
47
44
51
47
44
52
47
43
52
47
43
52
47
43
52
47
43
54
47
41
54
47
41
52
47
44
52
47
44
51
46
43
51
46
43
51
46
43
51
46
43
52
47
44
52
47
44
53
48
45
53
48
45
53
48
45
53
48
45
53
48
45
53
48
45
53
48
45
53
48
45
51
46
43
51
46
43
50
45
42
50
45
42
50
45
42
49
44
41
49
44
41
49
44
41
52
47
44
52
47
44
52
47
44
52
47
44
52
47
44
52
47
44
52
47
44
52
47
44
52
49
44
52
49
44
52
49
44
52
49
44
52
49
44
53
50
45
53
50
45
53
50
45
53
50
45
53
50
45
54
51
46
54
51
46
55
52
47
56
53
48
57
54
49
57
54
47
60
53
45
61
54
44
63
56
46
64
57
47
66
58
47
67
59
48
67
59
48
67
59
46
68
58
46
68
58
46
70
58
46
71
59
45
72
60
46
73
61
47
74
62
48
73
64
49
74
71
54
77
72
53
82
71
53
89
70
53
98
69
51
106
68
49
117
66
47
125
62
44
130
59
41
139
58
41
147
58
42
154
61
44
163
66
50
172
70
56
178
74
61
187
73
63
204
71
66
212
68
67
216
68
66
221
69
66
226
66
66
233
65
65
240
61
64
244
60
62
249
58
63
248
58
60
244
60
62
237
63
62
225
66
62
211
67
58
197
65
53
185
63
48
178
63
45
174
63
44
174
63
44
174
63
44
173
62
45
172
62
45
172
62
45
170
63
45
168
62
46
164
63
45
160
63
44
158
63
43
154
63
44
151
63
43
149
62
42
147
62
42
145
62
44
144
63
46
143
62
45
143
62
45
143
62
45
142
61
44
142
61
44
142
61
44
142
61
44
142
61
44
142
61
44
142
61
44
143
60
44
143
62
45
145
62
46
144
63
46
148
66
52
151
65
52
157
64
49
168
64
51
182
65
55
191
67
57
197
66
56
196
65
55
190
67
59
183
72
63
166
76
67
144
74
64
116
69
59
90
62
50
69
57
45
60
53
43
64
57
51
64
56
53
63
55
53
62
54
51
61
53
50
60
52
49
57
52
48
56
51
47
54
50
47
52
51
47
52
51
47
50
51
46
51
50
46
49
50
45
49
50
45
48
50
45
45
50
46
44
50
46
45
50
46
45
50
46
47
49
46
47
49
46
48
48
46
49
48
46
51
50
48
53
49
48
53
49
48
52
46
46
50
44
44
52
43
44
55
46
47
61
47
46
78
53
46
89
57
46
101
61
49
110
64
51
115
66
59
121
75
75
128
90
101
131
103
128
127
112
151
122
120
170
117
123
183
107
115
180
95
100
166
84
84
148
79
71
131
76
66
119
78
65
109
78
66
104
77
65
101
74
65
96
71
66
89
71
70
88
75
75
87
77
80
85
93
99
99
96
102
98
98
105
97
96
102
92
90
96
84
87
91
77
85
88
77
88
88
80
94
89
86
96
87
88
96
87
90
44
44
44
44
44
44
46
45
43
46
45
43
47
46
44
48
47
45
51
47
44
51
47
44
50
46
43
50
46
43
51
46
42
51
46
42
51
46
42
51
46
42
53
46
40
53
45
42
51
46
43
51
46
43
50
45
42
49
44
41
49
44
41
50
45
42
51
46
43
51
46
43
51
46
43
51
46
43
51
46
43
51
46
43
51
46
43
51
46
43
51
46
43
51
46
43
50
45
42
50
45
42
50
45
42
50
45
42
49
44
41
49
44
41
49
44
41
48
43
40
51
46
43
51
46
43
51
46
43
51
46
43
51
46
43
51
46
43
51
46
43
51
46
43
52
49
44
52
49
44
52
49
44
52
49
44
52
49
44
52
49
44
52
49
44
52
49
44
52
49
44
53
50
45
53
50
45
54
51
46
55
52
47
56
53
48
56
53
48
56
53
46
59
52
44
60
53
43
61
54
44
62
55
45
64
55
46
65
57
46
65
57
46
65
57
46
67
57
47
67
57
45
69
57
45
70
58
46
71
59
47
72
60
46
74
62
48
72
65
49
74
71
54
73
72
54
78
71
53
86
70
54
92
69
51
100
67
50
110
64
48
118
62
45
123
60
43
130
59
41
140
59
42
148
61
44
156
64
49
163
70
53
170
74
58
180
74
61
196
72
64
204
69
65
209
70
65
214
69
64
222
67
65
228
64
62
236
62
63
241
58
60
247
59
60
244
58
59
240
60
61
233
64
61
220
67
61
205
67
56
190
64
50
177
62
44
174
63
44
171
64
44
170
63
43
170
63
43
170
63
45
169
62
44
169
62
44
168
63
44
165
62
45
162
63
44
160
63
44
155
62
44
151
63
43
149
62
42
147
62
41
145
63
42
143
62
43
143
62
45
143
62
45
143
62
45
142
61
44
142
61
44
142
61
44
141
60
43
142
61
44
139
60
43
141
60
43
139
60
43
139
60
45
140
61
44
141
62
47
142
63
46
148
66
52
151
65
50
160
64
50
172
66
53
185
67
57
194
67
58
197
66
58
194
65
59
186
65
57
177
73
64
163
78
71
141
78
69
111
71
61
83
63
52
65
59
47
57
54
45
64
57
51
65
55
53
63
55
52
62
54
51
60
52
49
57
52
48
56
51
47
55
52
47
52
51
47
52
51
47
51
52
47
50
51
46
50
51
46
48
50
45
48
50
45
46
51
45
45
51
47
45
51
47
46
51
47
46
51
47
48
50
47
48
50
47
49
49
47
49
49
47
50
49
47
51
50
48
51
50
48
52
48
47
50
46
45
49
45
44
52
46
46
56
46
44
71
52
45
79
52
41
88
56
43
96
60
48
101
63
54
109
71
70
121
89
100
130
107
133
128
117
159
124
124
178
119
124
188
110
117
187
100
104
175
91
91
161
82
77
143
76
66
126
79
65
114
76
64
104
73
61
99
72
62
96
72
66
94
69
67
88
64
66
79
59
64
70
66
71
74
68
77
74
74
81
74
74
81
73
73
81
70
75
81
69
79
85
73
86
87
79
94
89
83
96
89
83
96
88
85
43
43
43
43
43
43
44
44
44
45
45
45
47
46
44
47
46
44
48
47
45
48
47
45
49
45
42
49
45
42
49
45
42
49
45
42
50
45
41
50
45
41
50
45
41
50
45
41
50
45
42
50
44
44
49
43
43
49
43
43
49
43
43
49
43
43
50
44
44
50
44
44
49
43
43
49
43
43
49
43
43
49
43
43
49
43
43
49
43
43
49
43
43
49
43
43
50
44
44
50
44
44
50
44
44
49
43
43
49
43
43
49
43
43
48
42
42
48
42
42
51
45
45
51
45
45
51
45
45
51
45
45
51
45
45
51
45
45
51
45
45
51
46
43
52
49
44
52
49
44
52
49
44
51
48
43
51
48
43
51
48
43
51
48
43
51
48
43
52
49
44
52
49
44
53
50
45
54
51
46
55
52
47
55
52
47
56
53
48
56
53
46
58
51
43
59
52
44
60
53
45
61
54
46
63
54
47
63
54
45
63
54
45
63
55
44
66
56
46
66
56
46
69
56
47
69
57
45
71
59
47
72
60
48
74
62
50
72
64
51
72
69
54
71
70
52
75
69
53
81
68
52
88
67
50
95
66
50
102
63
46
110
61
44
118
61
44
124
58
42
133
57
41
140
59
42
147
61
46
155
66
50
162
70
55
172
70
56
187
70
60
194
70
62
201
70
62
208
69
62
217
68
64
226
67
63
234
64
64
240
62
62
242
58
58
241
59
58
236
60
60
228
65
60
214
67
59
198
68
54
181
64
47
170
61
41
170
63
45
169
64
45
169
64
45
168
63
44
168
62
46
168
62
46
167
61
45
165
62
45
163
61
46
160
63
46
158
63
45
154
63
45
150
63
44
147
62
42
145
63
42
143
62
41
143
62
43
141
62
45
141
62
45
140
61
44
140
61
46
140
61
46
139
60
45
139
60
45
139
60
45
138
61
45
138
59
44
137
60
44
138
60
47
139
62
46
139
61
48
140
63
47
146
67
50
152
66
49
161
65
49
174
66
54
188
67
58
194
67
60
196
67
61
192
67
61
185
70
65
178
81
74
165
88
82
143
88
81
115
82
73
87
73
62
67
67
55
60
63
54
62
57
51
64
55
50
62
55
49
61
54
48
58
53
47
57
52
46
56
51
47
54
51
46
52
51
46
52
51
46
51
52
47
49
51
46
49
51
46
46
51
45
46
51
45
46
51
45
45
51
47
45
51
47
45
51
47
45
51
47
46
51
47
46
51
47
48
50
47
48
50
47
48
48
46
49
49
47
50
50
48
50
49
47
48
47
45
47
46
44
50
46
45
52
47
44
63
48
43
68
48
41
75
51
41
84
56
45
90
60
52
100
70
70
117
93
106
131
115
141
137
128
171
133
132
190
126
131
199
116
122
198
109
112
189
99
101
175
86
83
154
76
67
130
75
63
113
73
60
103
71
59
99
69
62
95
70
66
93
68
66
87
63
65
78
59
63
72
58
66
69
62
72
71
68
77
72
71
81
73
71
81
72
74
82
71
78
86
75
85
88
77
94
90
79
97
89
78
97
88
79
43
43
43
43
43
43
44
44
44
44
44
44
46
44
45
47
46
44
48
47
45
48
47
45
49
45
42
49
45
42
49
45
42
49
45
42
50
45
41
50
45
41
50
45
41
50
45
42
50
44
44
49
43
43
49
43
43
48
42
42
48
42
42
49
43
43
49
43
43
50
44
44
48
42
42
48
42
42
48
42
42
48
42
42
48
42
42
48
42
42
48
42
42
48
42
42
50
44
44
50
44
44
50
44
44
49
43
43
49
43
43
48
42
42
48
42
42
48
42
42
50
44
44
50
44
44
50
44
44
50
44
44
50
44
44
50
44
44
50
44
44
50
44
44
51
47
44
51
48
43
51
48
43
51
48
43
51
48
43
51
48
43
51
48
43
51
48
43
52
49
44
52
49
44
53
50
45
54
51
46
54
51
46
55
52
47
56
53
48
56
53
48
57
50
44
58
51
43
59
52
44
60
53
45
62
53
46
62
53
46
62
53
46
62
53
44
66
56
47
66
56
46
69
56
47
69
56
47
71
58
49
73
61
49
74
62
50
73
63
51
70
67
52
70
67
52
74
66
53
79
65
52
84
65
50
92
64
50
98
62
48
106
60
45
113
60
46
119
57
42
125
56
41
133
56
40
140
58
44
149
61
47
155
66
52
164
66
53
178
70
58
186
69
59
193
70
62
202
71
63
212
71
64
222
70
65
231
67
65
238
66
64
237
59
59
235
59
59
231
62
59
223
66
59
209
68
58
191
68
52
175
64
45
164
62
40
166
65
45
166
65
45
165
64
44
165
64
46
165
64
46
164
63
45
164
63
45
164
62
47
162
62
46
159
64
46
156
63
46
152
63
45
149
62
43
145
63
42
143
62
41
143
62
43
141
62
45
141
62
45
141
62
45
140
61
44
140
61
46
139
60
45
139
60
45
138
61
45
138
61
45
136
60
46
136
60
46
136
60
46
136
60
47
135
62
47
136
62
49
139
63
49
145
68
50
150
67
49
162
66
52
173
67
54
186
67
59
193
68
62
193
68
62
187
69
65
185
80
76
178
91
84
165
101
92
145
102
93
117
95
84
91
85
73
71
77
65
67
73
63
60
57
50
63
56
50
62
55
49
59
54
48
57
52
46
55
52
45
54
51
46
52
51
46
52
51
46
51
52
46
50
52
47
49
51
46
47
52
46
46
51
45
46
51
45
46
51
45
45
51
47
45
51
47
45
51
47
45
51
47
46
51
47
46
51
47
48
50
47
48
50
47
47
47
45
48
48
46
50
50
48
50
50
48
49
48
46
48
47
45
48
47
45
51
47
44
56
44
44
59
44
41
67
48
42
75
55
48
83
60
54
94
73
72
116
98
110
135
122
148
144
138
182
140
141
198
132
136
207
122
128
204
114
120
198
105
107
182
87
87
159
73
68
132
69
61
110
71
60
100
68
60
97
67
62
94
67
65
89
64
67
86
64
68
80
62
69
75
71
81
82
75
87
85
81
94
87
85
96
88
83
95
85
82
94
84
84
94
83
89
95
83
94
91
76
97
90
74
97
90
74
44
45
47
44
45
47
44
45
47
44
46
45
45
45
45
45
45
45
46
45
43
46
45
43
47
46
44
47
46
44
49
45
42
48
44
41
48
44
41
48
44
41
47
43
40
47
43
40
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
48
44
43
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
48
44
43
48
44
43
49
45
44
49
45
44
50
46
45
49
45
44
49
45
44
49
45
44
49
45
44
49
45
44
49
45
44
49
45
44
49
45
44
48
44
41
49
45
42
50
46
43
51
47
44
51
47
44
51
47
44
51
47
44
51
47
44
51
47
44
51
47
44
51
47
44
51
47
44
52
48
45
53
49
46
54
50
47
55
52
47
56
48
45
57
50
44
58
51
45
59
52
46
61
52
47
62
53
46
62
53
46
62
53
46
63
52
46
63
53
44
66
53
45
67
54
46
68
55
47
70
57
48
71
58
49
70
60
50
71
63
52
72
64
53
74
64
54
80
63
53
84
62
51
88
60
49
94
57
48
98
54
43
109
57
46
116
56
45
124
56
43
131
57
44
141
59
47
147
61
48
152
63
49
158
62
48
167
65
51
173
65
52
181
67
56
190
69
58
202
71
61
212
71
62
222
69
63
229
67
64
232
64
63
234
66
65
230
68
65
218
69
62
200
68
56
183
66
49
170
65
46
162
66
44
162
65
46
161
64
45
162
63
44
162
62
46
161
61
45
161
61
45
162
62
46
160
63
47
156
60
44
154
61
44
152
60
45
149
62
45
147
61
44
144
61
43
142
61
42
140
62
42
139
60
43
139
60
43
138
61
45
137
60
44
137
60
44
137
60
44
136
58
45
135
59
45
137
61
47
134
60
47
134
60
47
135
61
48
136
62
51
136
64
50
137
65
53
139
65
52
145
69
53
151
68
52
163
69
57
177
73
62
186
71
64
187
68
62
187
69
65
184
77
71
180
90
82
175
102
95
163
113
102
145
115
104
126
114
100
106
106
94
86
94
83
73
81
70
76
76
66
74
70
61
66
62
53
60
56
47
57
53
44
55
52
43
54
51
44
51
51
43
52
51
46
51
52
46
49
52
45
49
52
45
48
50
45
47
49
44
44
49
43
44
49
43
45
50
46
45
50
46
45
50
46
45
50
46
47
49
46
47
49
46
47
49
46
47
49
46
48
50
47
48
50
47
49
49
47
49
49
47
49
49
47
49
49
47
49
49
47
52
48
47
52
43
48
59
47
51
65
51
50
67
52
47
74
59
54
93
79
79
121
109
121
139
133
159
147
144
187
148
150
207
144
149
217
131
137
211
116
122
198
106
110
183
95
98
167
90
91
148
76
73
116
72
66
100
65
62
93
62
62
88
63
66
85
65
73
86
76
85
92
85
96
98
98
113
110
110
125
120
118
134
124
114
127
117
108
121
111
103
117
104
95
107
95
87
95
80
91
90
70
96
90
68
99
93
71
44
45
47
44
45
47
44
45
47
44
45
47
45
45
45
45
45
45
46
44
45
46
45
43
47
46
44
46
45
43
48
44
41
48
44
41
47
43
40
47
43
40
47
43
40
47
43
40
46
42
41
46
42
41
46
42
41
46
42
41
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
48
44
43
48
44
43
48
44
43
49
45
44
49
45
44
49
45
44
49
45
44
49
45
44
49
45
44
49
45
44
49
45
44
49
45
44
49
45
44
48
44
43
48
44
41
49
45
42
50
46
43
51
47
44
51
47
44
50
46
43
50
46
43
50
46
43
50
46
43
50
46
43
50
46
43
51
47
44
52
48
45
53
49
46
54
50
47
56
48
46
57
49
46
58
50
47
59
52
46
61
52
47
62
53
48
62
53
48
61
52
45
63
52
46
63
52
46
65
52
46
66
53
45
67
54
46
69
56
48
71
58
50
72
59
51
72
62
53
72
62
53
76
61
54
79
62
54
81
61
52
86
59
50
90
57
48
95
55
47
104
55
48
112
55
46
119
55
45
129
57
45
137
57
46
144
60
49
150
60
49
155
61
49
162
64
51
167
65
51
172
66
53
180
68
56
190
69
58
201
70
60
212
69
61
219
68
61
223
68
64
224
69
64
222
70
65
212
71
62
196
70
56
178
67
50
165
66
45
158
66
45
159
67
46
158
66
45
159
64
46
158
63
45
158
63
45
158
63
45
158
62
46
158
62
46
154
61
44
153
62
44
150
61
45
148
61
44
144
61
43
143
62
43
140
62
42
140
61
44
139
60
43
138
61
43
137
60
44
137
60
44
137
60
44
135
59
43
135
59
45
133
60
45
134
60
47
133
61
47
133
61
49
131
61
49
132
62
50
134
64
54
135
65
55
136
66
54
147
71
57
151
69
55
163
71
60
175
74
64
184
73
66
184
71
65
183
76
70
179
84
78
177
100
92
170
113
102
159
126
111
144
127
111
127
124
109
109
115
101
90
102
90
78
88
77
76
78
65
75
73
61
69
67
55
64
61
52
61
58
49
56
56
46
52
52
44
49
49
41
50
51
45
50
51
45
48
51
44
47
50
43
47
49
44
46
48
43
46
48
43
46
48
43
47
49
46
45
50
46
47
49
46
47
49
46
47
49
46
47
49
46
47
49
46
47
49
46
48
50
47
48
50
47
49
49
47
49
49
47
49
49
47
49
49
47
49
49
47
50
48
49
53
43
52
57
45
55
60
50
51
63
53
51
72
63
58
94
86
84
120
114
124
138
136
158
147
147
185
148
151
202
144
150
212
132
139
207
121
129
194
113
121
184
107
116
173
104
110
158
92
95
128
88
90
113
83
85
106
82
87
106
84
94
104
90
103
109
102
118
118
114
130
127
116
136
127
126
146
135
131
149
137
126
142
129
118
134
121
114
128
113
102
116
101
90
101
84
89
88
67
94
88
62
97
91
67
44
45
47
44
45
47
44
45
47
44
45
47
44
45
47
44
46
45
45
45
45
45
45
45
46
45
43
46
45
43
46
45
43
45
44
42
47
43
40
46
42
39
46
42
39
46
42
39
44
43
41
44
42
43
44
42
43
44
42
43
43
41
42
43
41
42
43
41
42
43
41
42
45
43
44
45
43
44
45
43
44
45
43
44
45
43
44
45
43
44
45
43
44
45
43
44
44
42
43
44
42
43
45
43
44
45
43
44
45
43
44
46
44
45
46
44
45
46
44
45
46
44
45
46
44
45
46
44
45
47
45
46
47
45
46
47
45
46
47
45
46
47
46
44
48
44
43
48
44
43
49
45
44
50
46
45
50
46
45
50
46
45
50
46
45
49
45
44
49
45
44
49
45
44
49
45
44
49
45
44
50
46
45
51
47
46
52
48
47
53
49
46
56
48
46
57
49
46
58
50
47
59
51
48
61
51
49
61
52
47
61
52
47
61
52
47
63
52
48
63
52
46
65
52
46
65
52
46
67
54
48
68
55
49
70
57
51
71
58
52
72
59
53
74
59
54
75
60
55
78
61
54
80
59
54
83
58
53
86
55
50
91
54
46
99
54
48
107
54
46
116
53
44
125
55
47
135
57
47
141
59
48
148
60
48
150
60
49
157
63
51
160
64
52
162
64
51
168
64
51
177
67
54
186
68
56
197
69
58
205
68
58
213
70
64
215
72
64
212
73
66
204
73
63
190
69
58
174
67
51
161
66
46
156
65
46
157
66
47
156
65
46
156
63
46
155
62
45
155
62
45
155
62
45
155
62
47
155
63
48
152
60
45
150
61
45
148
60
46
147
61
46
143
62
45
140
61
44
139
60
43
138
61
43
137
60
42
137
60
42
137
60
44
135
59
43
135
59
45
133
60
45
132
58
45
131
59
45
132
60
48
130
60
48
130
60
50
130
62
51
131
63
52
133
65
56
135
67
58
136
68
57
148
74
61
151
72
59
160
72
62
172
75
68
178
75
70
179
76
71
178
83
77
174
96
86
173
115
103
166
129
113
157
140
122
145
142
123
130
137
119
115
128
111
99
113
98
88
99
85
78
82
68
77
78
64
72
73
59
68
68
56
64
64
52
57
59
46
52
54
43
47
49
38
48
49
41
48
49
41
47
48
42
47
48
42
46
47
42
46
47
42
47
47
45
47
47
45
47
47
45
46
48
45
47
47
45
47
47
45
47
47
45
47
47
45
47
47
45
47
47
45
48
48
46
48
48
46
48
48
46
48
48
46
48
48
46
48
48
46
48
48
46
49
47
50
54
43
57
55
43
57
56
47
52
59
54
51
72
69
62
95
94
89
122
122
124
139
141
154
144
148
177
146
151
191
143
149
199
132
142
195
125
137
187
123
137
182
124
139
178
125
140
171
125
136
154
119
131
143
113
126
135
110
125
130
110
126
126
111
130
126
121
142
133
131
153
141
139
161
148
145
168
152
144
165
148
136
157
138
131
150
131
124
141
123
106
123
105
92
104
84
86
86
62
91
87
58
93
89
62
45
46
48
45
46
48
45
46
48
45
46
48
45
46
48
45
46
48
46
46
46
46
46
46
46
45
43
46
45
43
45
44
42
45
44
42
46
42
39
46
42
39
46
42
39
46
42
41
44
42
43
43
41
42
43
41
42
43
41
42
43
41
42
42
40
41
42
40
41
42
40
41
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
45
43
44
45
43
44
45
43
44
45
43
44
46
44
45
46
44
45
46
44
45
46
44
45
47
45
46
47
45
46
47
45
46
47
43
42
48
44
43
49
45
44
49
45
44
49
45
44
49
45
44
48
44
43
48
44
43
48
44
43
48
44
43
48
44
43
48
44
43
49
45
44
50
46
45
51
47
46
52
48
47
57
49
47
58
50
48
59
51
49
60
52
49
61
51
49
61
51
49
61
51
49
61
52
47
62
51
47
62
51
47
64
50
47
64
51
45
65
52
46
67
54
48
68
55
49
69
56
50
70
57
51
72
57
52
73
58
53
74
59
54
76
58
54
78
57
52
81
56
51
84
53
48
94
55
50
101
54
48
111
54
47
120
56
47
130
57
48
138
58
47
144
60
49
148
62
49
153
63
52
155
63
52
155
63
50
159
63
49
165
65
50
174
66
53
184
68
55
191
69
58
199
71
62
203
72
64
202
74
65
195
73
62
182
70
58
170
66
53
158
65
48
153
64
46
153
65
45
153
65
45
152
63
45
151
62
44
151
62
44
153
62
44
153
61
46
151
62
46
149
60
44
148
61
44
147
61
46
144
61
45
142
61
44
138
61
43
138
61
45
137
60
44
136
59
41
136
59
41
135
59
43
135
59
43
132
59
44
132
59
44
131
59
45
128
59
44
129
59
47
128
60
49
128
60
49
129
62
53
131
64
55
131
67
58
135
68
60
136
69
60
148
76
64
150
72
60
158
71
62
167
74
67
174
76
73
174
81
76
173
93
86
170
106
94
167
128
111
161
143
121
154
154
130
143
155
131
134
149
128
122
139
120
109
123
106
98
109
93
83
90
74
81
85
70
75
79
64
69
73
58
64
68
54
59
61
48
53
55
44
49
51
40
48
49
41
47
48
40
46
47
41
46
45
40
46
45
41
46
45
41
47
46
44
48
47
45
47
46
44
47
46
44
47
46
44
47
46
44
47
46
44
46
46
44
47
46
44
46
46
44
48
47
45
47
47
45
48
47
45
47
47
45
47
47
45
47
47
45
47
47
45
50
45
49
55
44
58
54
42
56
51
45
49
57
54
49
72
74
63
98
102
88
123
128
122
139
144
147
141
147
163
143
150
176
140
149
182
131
144
178
129
144
175
133
153
178
141
164
182
146
168
179
147
168
171
144
164
163
140
160
158
135
158
152
130
153
143
125
152
137
129
156
139
136
163
144
148
176
154
152
177
155
148
173
151
139
163
141
133
155
132
127
146
124
107
124
105
88
102
79
83
86
59
89
85
56
90
86
57
43
47
50
43
47
50
43
47
50
43
47
50
45
46
48
45
46
48
46
46
46
46
46
46
45
45
45
45
45
45
46
45
43
45
44
42
45
44
42
44
43
41
44
43
41
44
43
41
44
42
43
44
42
43
44
42
43
43
41
42
43
41
42
42
40
41
42
40
41
42
40
41
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
44
42
43
43
41
42
43
41
42
43
41
42
44
42
43
45
43
44
45
43
44
46
44
45
46
44
45
47
45
46
47
45
46
47
45
46
47
43
42
48
44
43
48
44
43
48
44
43
48
44
43
48
44
43
47
43
42
46
42
41
47
43
42
47
43
42
47
43
42
47
43
42
48
44
43
49
45
44
51
47
46
52
48
47
57
48
49
58
50
48
59
51
49
60
52
50
61
51
50
61
51
49
60
50
48
60
50
48
61
50
48
61
50
46
63
49
46
63
49
46
64
50
47
65
51
48
66
53
47
67
54
48
68
54
51
67
56
52
68
57
53
69
58
54
71
57
54
72
57
52
75
56
52
78
53
48
88
55
50
95
54
48
105
55
48
114
55
47
125
58
49
134
60
49
140
62
50
144
62
50
151
64
54
151
63
53
151
63
53
151
63
51
156
64
51
163
67
53
171
67
54
179
69
56
186
69
59
189
71
61
190
72
62
185
71
61
176
68
56
166
65
53
155
63
48
151
64
47
151
64
45
151
64
45
150
63
46
149
62
45
148
61
44
149
60
44
150
61
47
150
61
47
149
60
46
147
59
45
146
60
47
142
60
46
139
60
45
137
60
44
137
60
44
136
60
44
135
59
43
134
58
42
134
58
42
134
58
42
131
58
43
131
58
43
130
58
44
128
58
46
128
58
48
128
60
49
128
61
52
127
63
54
130
66
57
131
68
61
135
70
64
136
72
63
145
75
65
146
72
61
154
71
63
163
76
69
168
80
76
168
88
81
169
102
93
166
118
104
162
140
119
155
153
128
149
164
135
140
163
135
134
157
131
126
145
125
116
130
113
106
117
101
92
101
82
86
95
76
78
85
67
69
76
58
63
70
54
60
64
50
55
58
47
54
56
45
48
49
41
48
48
40
46
45
40
47
44
39
47
43
40
47
43
42
49
45
44
50
46
45
49
45
44
49
45
44
49
45
44
49
45
44
49
45
44
47
46
44
49
45
44
47
46
44
50
46
45
48
47
45
50
46
45
48
47
45
48
47
45
48
47
45
48
47
45
50
45
49
56
44
58
53
41
53
49
43
43
54
54
44
71
76
56
96
104
81
122
131
112
136
146
135
141
151
150
143
154
160
141
153
165
134
150
163
135
155
162
144
170
169
155
185
177
160
192
179
152
183
168
154
182
167
155
183
168
152
183
165
145
177
156
137
171
146
137
171
144
144
177
148
146
179
150
149
180
149
144
173
143
136
162
133
132
155
129
123
146
120
103
122
100
84
98
73
81
84
57
85
84
56
86
85
57
44
48
51
44
48
51
44
48
51
44
48
51
46
47
49
46
47
49
47
47
47
47
47
47
46
46
46
46
46
46
47
46
44
46
45
43
46
45
43
46
45
43
45
44
42
45
44
42
45
43
44
45
43
44
45
43
44
44
42
43
43
41
42
42
40
41
42
40
41
42
40
41
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
44
42
43
44
42
43
43
41
42
43
41
42
43
41
42
42
40
41
42
40
41
42
40
41
44
42
43
44
42
43
44
42
43
45
43
44
46
44
45
46
44
45
47
45
46
47
45
46
47
43
42
47
43
42
48
44
43
48
44
43
47
43
42
47
43
42
46
42
41
45
41
40
48
44
43
48
44
43
47
43
42
48
44
43
49
45
44
50
46
45
51
47
46
52
48
47
58
49
50
58
49
50
59
51
49
60
52
50
61
51
50
60
50
49
60
50
48
59
49
47
61
50
48
60
49
47
62
48
45
62
48
45
63
49
46
64
50
47
65
52
46
63
52
46
64
53
49
64
55
50
64
57
51
65
58
52
66
57
52
67
56
52
70
55
52
72
53
47
81
53
49
88
53
47
97
54
47
108
56
45
117
57
46
128
60
47
134
62
48
139
61
49
147
63
53
149
62
53
147
63
53
147
63
52
149
66
52
154
66
52
160
67
52
165
68
52
172
66
53
176
68
56
180
69
58
177
69
59
170
66
55
161
63
52
154
62
49
150
62
48
149
63
46
148
63
43
147
61
44
147
61
44
147
60
43
147
60
43
148
60
46
148
60
46
147
59
45
146
60
45
143
60
46
141
59
45
138
59
44
137
60
44
135
59
43
135
59
43
134
58
42
134
58
42
133
57
41
131
58
41
131
58
43
129
57
42
127
57
45
127
57
45
128
60
49
127
60
51
127
63
54
128
65
56
130
67
60
133
70
63
136
73
66
138
73
67
142
74
65
144
71
62
151
72
67
159
78
74
167
86
82
167
96
90
169
112
101
164
131
114
156
148
125
150
161
131
143
168
138
136
167
136
131
160
132
126
150
126
121
136
117
112
123
106
100
112
92
93
105
83
82
94
74
74
83
64
66
75
58
62
69
53
58
64
50
57
60
49
51
52
44
50
50
42
49
46
41
47
44
39
47
41
41
48
42
42
49
43
43
50
44
44
49
43
43
49
43
43
49
43
43
49
43
43
49
43
43
48
44
43
48
44
43
48
44
43
49
45
44
49
45
44
49
45
44
49
45
44
47
46
44
47
46
44
47
46
44
50
44
46
56
42
55
53
40
49
51
43
40
53
54
38
69
75
49
92
103
71
117
130
100
134
147
121
142
155
138
145
158
148
144
159
154
139
158
152
141
165
151
151
180
158
162
197
165
169
206
173
165
201
173
166
202
176
167
203
177
164
200
172
151
190
159
139
179
145
136
176
141
141
178
144
142
178
142
144
178
143
141
172
138
132
161
130
128
153
123
119
142
114
100
120
93
81
95
69
77
81
54
81
81
53
82
81
53
44
48
51
44
48
51
44
48
51
44
48
51
46
47
49
46
47
49
46
47
49
46
47
49
47
47
47
47
47
47
47
47
47
47
47
47
47
46
44
47
46
44
46
45
43
46
45
43
47
45
46
46
44
45
46
44
45
45
43
44
44
42
43
43
41
42
43
41
42
42
40
41
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
42
40
41
42
40
41
42
40
41
41
39
40
41
39
40
43
41
42
43
41
42
44
42
43
45
43
44
46
44
45
46
44
45
47
45
46
47
45
46
47
43
42
47
43
42
47
43
42
47
43
42
47
43
42
46
42
41
45
41
40
44
40
39
48
44
43
48
44
43
48
44
43
48
44
43
49
45
44
50
46
45
52
48
47
52
48
47
58
49
50
59
50
51
59
50
51
60
51
52
61
51
50
60
50
49
60
50
49
59
49
48
60
49
47
60
49
47
62
48
47
61
47
46
62
48
45
62
48
45
63
49
46
62
51
47
61
54
48
59
54
48
59
56
51
60
57
52
61
58
53
62
57
51
64
55
50
67
54
48
71
52
46
79
52
43
89
52
43
100
53
43
110
56
44
120
58
45
127
59
46
133
59
46
143
60
52
145
60
53
144
62
51
143
64
51
144
65
50
147
66
49
151
65
50
153
64
48
160
62
49
166
64
52
170
66
55
170
66
57
165
64
54
159
62
53
151
61
50
149
63
50
146
63
45
146
63
45
145
62
44
144
61
43
145
59
42
145
59
42
146
60
45
146
60
45
145
59
46
145
59
46
142
59
45
141
59
45
138
59
46
136
58
45
134
58
44
132
59
44
133
57
41
133
57
41
131
58
41
130
57
40
130
57
42
129
57
42
126
56
44
126
56
44
128
60
49
128
61
52
128
64
55
129
66
57
132
69
62
133
73
65
136
75
70
140
77
70
140
73
65
142
72
64
152
74
70
161
83
79
168
93
88
169
104
98
169
123
110
164
142
121
153
156
129
146
167
134
137
172
139
130
170
136
128
161
132
127
153
128
126
139
119
120
129
110
106
120
97
100
114
91
90
104
81
81
93
73
73
84
67
67
76
59
62
68
54
60
63
52
54
55
47
52
52
44
51
48
43
48
43
39
49
40
41
49
40
41
50
41
42
52
43
44
50
41
42
50
41
42
50
41
42
50
41
42
50
41
42
48
42
42
48
42
42
48
42
42
49
43
43
49
43
43
48
44
43
48
44
43
48
44
43
48
44
43
48
44
43
49
43
45
54
41
50
53
41
45
52
45
37
52
54
33
65
72
38
87
99
59
113
128
87
130
146
109
141
156
125
144
160
134
144
162
140
139
161
138
141
168
137
149
184
142
160
200
148
164
208
157
169
211
171
168
210
174
167
209
173
161
203
165
147
189
151
133
175
135
130
172
132
135
176
134
136
174
133
140
177
136
139
171
134
129
159
123
123
148
116
113
136
107
95
115
87
77
93
66
73
79
51
74
77
48
76
76
48
46
47
49
44
48
49
46
47
49
46
47
49
46
48
47
46
48
47
46
48
47
47
47
47
48
48
46
48
48
46
48
48
46
48
47
45
48
47
45
48
47
43
47
46
44
49
45
44
50
46
45
49
45
44
49
45
44
48
44
43
47
43
44
44
42
43
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
43
41
42
43
41
44
43
41
42
43
41
44
43
41
42
45
40
44
42
40
41
44
40
41
41
39
40
41
39
40
40
38
39
42
40
41
43
41
42
44
42
43
44
43
41
45
43
44
46
45
43
47
46
44
47
46
44
45
44
42
45
44
42
47
43
42
47
43
42
46
42
41
45
41
40
44
40
39
44
40
39
49
45
44
50
45
42
50
45
42
50
45
42
51
46
43
52
47
44
53
48
45
54
49
46
59
51
49
59
51
49
60
52
50
60
52
50
61
51
49
60
50
48
59
49
47
59
49
47
60
49
47
60
49
47
61
47
46
61
47
46
61
47
44
62
48
45
63
49
46
61
50
46
62
51
45
61
54
46
60
55
49
57
57
49
57
58
50
58
58
50
59
56
49
63
54
47
67
50
43
74
49
44
84
50
41
94
51
42
104
54
43
111
57
45
118
60
46
123
59
47
135
61
50
139
61
51
139
63
50
140
64
50
142
64
51
143
64
49
146
63
49
149
61
47
152
60
47
156
62
50
162
66
54
161
67
55
158
66
55
153
63
52
150
62
50
148
62
49
148
62
47
147
61
46
144
61
45
142
61
44
142
61
44
140
61
44
140
61
46
141
62
47
142
60
46
142
60
46
139
60
47
138
59
46
136
58
45
135
59
45
132
59
44
132
59
44
130
58
43
128
59
43
127
58
42
126
59
42
126
59
43
124
58
42
124
58
44
124
58
46
128
62
50
129
62
53
128
64
54
130
67
58
133
70
61
133
74
66
136
77
69
139
79
71
140
73
67
142
73
68
147
80
74
152
92
84
157
104
96
157
117
105
158
135
117
157
153
128
147
162
133
143
170
137
137
172
139
129
169
135
127
162
132
125
155
127
122
144
121
116
136
111
103
126
98
98
121
93
92
112
85
85
103
79
78
93
72
71
82
65
64
73
56
61
65
51
56
56
46
56
53
46
53
48
44
51
43
41
50
40
41
50
40
41
51
41
42
52
42
43
50
41
44
48
42
42
48
42
44
50
41
42
50
41
44
50
41
44
50
41
44
50
41
44
51
42
45
51
42
45
49
43
45
49
43
45
48
44
45
48
44
45
48
44
45
49
43
47
52
41
47
52
42
43
52
45
37
52
54
33
63
70
37
84
96
56
109
124
81
129
144
103
137
153
116
141
158
122
141
161
126
136
161
122
138
167
123
145
182
130
155
196
136
159
203
144
159
201
153
158
199
157
157
198
154
152
193
149
140
183
138
130
173
128
133
174
130
140
181
137
132
171
127
137
174
133
137
169
130
126
156
120
118
143
111
108
131
102
90
110
83
74
90
63
70
76
50
70
74
47
70
72
48
47
48
43
46
48
43
47
48
43
48
49
44
48
49
44
48
49
44
49
50
45
50
49
45
51
50
46
52
51
47
52
51
47
54
50
47
53
49
46
52
49
44
51
47
44
52
47
44
53
48
45
53
48
45
52
47
44
51
46
43
50
44
44
49
45
44
48
44
43
48
44
43
46
44
45
46
44
45
45
45
47
45
45
47
44
44
46
43
43
45
42
41
46
41
41
43
46
41
47
46
41
45
47
40
47
46
41
45
47
41
45
46
41
45
46
42
43
46
42
43
47
43
42
47
43
42
45
44
42
46
45
41
45
45
43
45
46
41
46
47
42
46
47
42
45
45
45
45
45
45
46
44
45
47
45
46
47
46
44
48
47
45
50
46
43
50
46
43
50
45
42
52
44
41
53
45
42
54
46
43
56
47
42
58
49
44
58
49
42
59
50
43
61
52
45
61
52
45
62
53
48
62
53
48
62
53
48
61
52
47
60
51
46
59
50
45
61
50
46
60
49
45
60
49
47
59
48
46
61
47
46
62
48
47
62
48
47
64
49
46
66
47
41
66
49
41
60
51
42
57
54
45
53
56
45
52
55
46
54
54
46
59
50
45
64
45
41
71
41
39
79
39
39
86
41
38
91
46
40
97
53
44
102
60
48
109
63
48
121
65
48
127
64
47
134
65
49
139
66
51
143
64
51
147
61
48
147
59
47
146
56
45
147
59
47
147
61
46
148
65
49
149
66
50
148
67
50
146
65
48
145
62
46
145
59
44
151
59
48
151
58
50
146
59
49
140
61
48
136
63
48
132
63
47
130
63
46
130
63
46
136
64
49
136
63
48
138
62
49
137
61
48
136
60
47
135
59
46
132
58
45
131
59
45
123
57
43
126
64
51
118
60
46
112
56
41
118
62
45
118
62
45
118
59
45
125
63
50
125
59
47
131
63
52
136
68
57
139
72
63
138
76
65
135
77
66
133
79
67
135
78
69
144
76
73
149
84
80
147
99
89
142
113
97
138
125
106
134
138
113
135
151
124
136
159
130
139
166
135
138
167
137
139
168
138
138
168
140
132
165
136
124
161
130
114
155
123
110
151
117
104
142
105
97
133
95
91
122
88
89
116
85
90
110
83
86
101
78
79
88
69
73
75
61
66
63
54
64
57
51
58
48
46
53
43
42
51
39
41
51
39
43
51
39
43
49
38
42
47
41
45
45
41
42
45
40
44
45
39
41
47
38
43
49
38
44
51
38
45
51
38
45
52
39
46
52
39
46
51
40
48
50
40
48
48
41
49
47
42
49
47
42
49
47
42
48
47
41
45
49
43
43
48
45
40
47
47
35
60
63
42
86
92
64
112
122
88
127
139
103
141
156
117
139
157
115
140
161
118
140
166
119
138
168
118
137
170
117
142
180
123
153
191
134
159
193
143
157
190
143
150
186
138
143
179
131
136
174
125
133
171
124
131
170
123
133
170
126
131
168
125
131
166
126
128
158
124
121
148
117
114
137
109
104
124
99
89
107
85
77
90
70
74
82
61
68
73
51
62
65
46
47
48
42
48
49
43
48
49
43
48
49
43
49
50
44
49
50
44
50
49
44
51
50
45
52
51
46
52
51
46
55
52
47
55
52
47
54
51
46
53
50
45
53
48
44
53
48
44
55
47
44
55
47
44
54
46
44
54
46
44
51
46
43
50
45
42
48
44
43
48
44
43
48
44
45
47
45
46
47
45
48
45
45
47
45
44
49
43
42
47
41
42
46
41
40
45
46
41
47
47
40
47
47
40
47
47
40
47
47
41
45
47
41
45
46
42
43
46
42
43
47
43
42
47
43
42
46
45
41
46
45
41
46
47
42
46
47
42
46
47
41
46
47
42
46
46
46
46
46
48
46
46
46
46
46
46
47
46
44
47
46
44
49
45
42
51
46
43
53
45
42
53
46
40
55
46
41
56
47
40
58
47
41
59
48
42
60
50
41
61
51
42
62
53
44
62
53
44
62
53
46
62
53
46
62
53
46
61
52
45
60
51
46
60
51
46
61
50
46
60
49
45
59
48
46
59
48
46
61
47
46
61
47
46
62
48
47
64
49
46
68
47
42
70
47
41
67
50
43
64
51
43
61
52
45
60
51
44
59
50
45
62
48
45
65
44
43
70
42
41
75
41
40
80
42
39
86
45
41
92
51
45
97
57
49
103
61
47
112
65
47
120
65
45
128
65
48
136
64
50
143
63
52
145
61
51
147
58
50
147
59
49
141
57
46
139
60
47
139
62
46
140
64
48
141
65
49
141
65
49
142
65
47
145
63
49
149
59
50
149
59
50
144
60
50
139
61
48
134
62
48
131
64
47
129
63
47
129
64
46
133
64
49
134
62
47
135
61
48
136
60
46
133
59
46
132
59
44
130
58
44
127
60
44
119
57
44
119
61
49
112
59
45
109
57
43
114
63
46
114
63
46
113
57
42
117
58
44
127
61
49
132
64
53
138
68
58
140
72
61
138
74
64
135
77
65
133
79
67
136
77
69
144
74
72
149
84
82
145
102
93
138
119
102
131
135
112
127
147
119
128
159
127
131
166
134
134
167
136
139
168
138
143
167
141
143
167
141
136
165
137
127
160
131
117
156
125
109
153
118
100
147
105
94
139
96
89
130
90
89
123
88
90
119
88
88
111
83
82
97
74
77
86
67
69
73
59
65
65
55
59
56
51
53
48
45
51
42
43
50
41
42
49
40
43
46
40
42
49
40
43
48
39
44
48
39
44
48
39
44
49
38
44
51
38
45
52
39
48
52
39
48
52
39
48
51
40
48
50
40
49
48
41
49
47
42
49
45
42
51
44
43
51
44
43
49
50
45
49
50
46
45
48
44
41
45
45
37
56
58
44
83
88
66
111
120
93
128
139
105
135
150
111
134
153
108
134
158
110
136
163
112
134
164
112
132
165
112
138
173
119
147
182
128
153
185
135
150
182
132
147
179
129
140
174
123
134
171
120
130
168
119
130
168
121
131
168
124
131
166
124
130
164
127
127
157
123
120
146
117
113
136
110
104
123
101
89
106
87
77
90
72
70
77
59
64
68
51
59
62
45
49
50
44
49
50
44
49
50
44
49
50
44
50
51
45
50
51
45
52
51
46
52
51
46
53
52
47
53
52
47
56
53
48
56
53
48
56
53
48
55
52
47
55
50
46
54
49
45
56
48
45
56
48
45
55
47
44
54
46
43
52
47
44
51
46
43
50
44
44
49
45
44
49
45
46
47
45
46
48
46
49
47
45
48
45
45
47
44
44
46
43
42
47
43
41
46
47
40
47
49
39
47
47
41
45
47
41
45
47
41
45
47
41
45
46
42
43
46
42
43
48
44
43
48
44
43
47
46
42
47
46
42
47
46
41
48
47
42
48
47
42
48
47
43
47
47
45
47
47
47
48
46
47
48
46
47
48
47
45
47
46
44
49
45
42
50
45
42
54
46
43
55
48
42
57
48
43
58
49
42
60
49
43
61
51
42
62
52
43
63
53
44
63
54
45
63
54
45
64
55
46
64
55
46
63
54
47
62
53
46
61
52
45
61
52
45
61
50
46
60
49
45
59
48
46
59
48
46
59
48
46
59
48
46
60
49
47
63
49
46
68
49
43
71
47
43
74
46
43
76
44
45
79
43
47
78
42
46
74
42
47
71
42
46
68
44
44
65
46
42
66
45
40
70
45
40
79
46
41
89
48
42
99
50
45
105
52
44
107
58
44
111
59
45
119
59
48
129
61
52
135
60
54
140
60
53
141
58
52
141
58
52
138
60
50
135
61
50
134
62
50
133
63
51
134
65
50
137
65
51
140
66
53
143
67
53
142
63
50
143
61
47
140
62
49
138
62
46
135
62
47
133
62
44
134
61
46
134
61
44
137
59
46
137
60
44
136
58
45
134
58
42
132
59
44
128
60
41
126
61
43
122
61
43
124
62
49
120
60
49
117
59
47
116
60
47
118
65
49
118
65
49
117
61
46
118
59
45
127
63
53
132
65
56
138
70
61
140
72
63
138
74
65
137
74
65
136
78
67
140
77
70
147
73
72
153
83
81
151
101
92
146
117
101
138
132
110
131
144
116
129
156
123
130
164
131
134
168
135
138
169
137
143
169
140
145
168
140
144
164
137
138
158
131
130
153
125
122
151
120
107
149
109
97
146
101
92
139
97
92
134
96
93
130
96
91
124
93
85
114
86
80
104
80
70
89
69
66
80
63
58
68
57
51
58
50
46
51
44
42
47
41
40
45
41
43
42
40
50
38
40
53
36
42
53
36
42
53
36
42
52
37
42
53
38
43
51
40
46
51
42
47
48
42
46
47
42
46
44
43
48
43
44
48
41
45
48
40
45
49
38
46
49
40
45
48
49
47
50
51
47
46
46
42
39
42
42
34
53
55
42
80
85
65
109
118
91
124
138
105
128
144
107
126
147
106
127
153
106
131
159
110
128
160
110
126
160
109
128
165
113
135
172
120
141
175
124
139
173
122
136
170
120
132
169
118
129
167
118
127
167
117
127
166
119
127
166
121
127
164
121
128
162
125
125
155
121
119
145
116
112
135
109
104
124
99
89
107
85
77
90
70
69
73
58
62
64
50
57
58
44
50
51
45
50
51
45
51
52
46
51
52
46
51
52
46
52
53
47
53
52
47
53
52
47
54
53
48
55
54
49
57
54
49
58
55
50
58
55
50
57
54
49
57
52
48
57
52
48
58
50
47
57
49
46
57
49
46
56
48
45
53
48
45
52
47
44
52
46
46
52
46
46
51
47
48
51
47
48
49
47
48
49
47
50
48
46
49
46
46
48
45
45
47
45
43
46
48
41
48
50
41
46
48
42
46
48
42
46
48
42
46
48
42
44
47
43
44
47
43
42
49
45
44
49
45
44
48
47
43
48
47
43
49
48
43
49
48
43
49
48
43
49
48
44
49
49
47
49
49
49
49
47
48
49
48
46
49
48
46
50
46
43
51
46
43
51
46
42
57
50
44
57
50
44
59
50
43
60
51
44
62
52
43
64
54
45
64
54
45
65
55
46
66
56
47
65
56
47
66
57
48
65
56
47
65
56
49
64
55
48
62
53
46
62
53
46
61
50
46
61
50
46
60
49
47
59
48
46
59
48
46
60
49
47
61
50
48
63
49
46
66
51
46
70
49
46
77
45
46
84
41
48
88
38
47
87
37
46
81
38
47
75
40
46
68
47
46
62
49
43
60
50
41
64
50
41
72
47
40
84
47
41
96
45
42
101
46
41
105
57
47
107
59
47
117
60
51
125
60
54
133
59
56
137
59
57
138
59
55
136
58
54
137
62
56
134
64
56
134
66
57
133
66
57
133
67
55
134
66
55
135
65
55
137
65
53
137
64
49
137
64
47
136
63
46
136
63
46
135
62
45
136
60
44
136
59
43
137
58
43
138
56
42
138
56
42
136
57
42
133
57
41
130
59
41
125
60
40
122
61
40
120
62
42
127
64
49
122
55
46
120
56
46
120
60
49
117
59
45
118
62
47
123
65
51
123
64
50
128
66
55
132
68
58
135
71
61
137
73
63
136
73
64
138
75
66
138
78
68
140
80
72
144
81
74
147
90
81
148
101
91
143
114
98
137
125
103
132
136
109
131
147
118
131
156
124
135
165
131
136
167
135
139
168
137
143
168
138
143
163
135
139
158
130
136
152
126
127
150
121
114
148
111
108
146
105
104
142
103
102
140
101
103
139
103
100
134
101
96
125
95
92
118
91
81
103
80
76
93
74
66
80
63
57
68
54
50
58
47
46
52
42
44
47
40
45
42
37
53
37
38
56
35
40
56
35
40
55
36
40
52
37
40
51
39
43
50
41
44
48
44
45
44
42
43
43
43
45
42
43
45
40
44
45
39
44
47
39
44
47
37
45
47
40
44
45
46
42
43
49
43
43
48
43
39
46
43
34
56
56
44
80
85
65
105
113
89
118
132
99
123
140
104
121
143
104
124
150
105
127
156
108
125
159
109
121
158
107
122
160
109
127
165
114
130
167
116
128
165
113
125
163
112
125
163
112
125
165
115
126
165
118
126
165
120
125
164
120
124
161
120
125
159
122
122
153
119
117
144
113
110
135
106
101
124
98
87
107
82
78
91
71
69
71
57
63
61
48
57
55
42
52
53
47
52
53
47
52
53
47
53
54
48
53
54
48
53
54
48
55
54
49
55
54
49
56
55
50
56
55
50
59
56
51
60
57
52
60
57
52
60
57
52
60
55
51
60
55
51
60
52
49
60
52
49
59
51
48
59
51
48
56
51
48
55
50
47
54
49
46
54
49
46
53
49
48
54
50
49
54
50
49
51
49
50
50
48
49
49
47
48
48
46
47
49
45
46
50
44
48
52
43
46
52
43
46
52
43
46
50
44
46
50
44
44
50
44
44
50
44
44
51
47
46
51
47
44
51
47
44
51
47
44
50
49
44
50
49
44
51
50
45
51
50
46
51
50
48
51
50
48
51
50
48
51
50
48
53
49
48
53
49
46
54
49
46
54
49
45
59
52
46
60
53
47
61
52
45
63
54
45
65
55
46
66
56
47
69
56
47
67
57
47
68
58
49
68
58
49
69
59
50
68
58
49
67
57
48
66
56
47
64
53
47
64
53
47
62
51
47
62
51
47
61
50
46
61
50
46
61
50
48
61
50
48
62
51
49
62
51
49
66
52
49
70
50
49
78
48
50
85
44
52
90
40
51
89
39
50
83
40
49
76
44
49
68
50
48
60
53
45
57
55
43
60
54
42
68
51
41
78
49
41
90
47
41
94
47
41
102
58
49
106
59
51
115
60
55
122
61
58
130
60
58
134
60
59
135
59
59
134
58
58
130
59
55
129
62
56
129
65
56
129
66
57
131
67
58
131
67
58
133
64
57
132
64
55
132
66
50
132
67
49
133
64
48
134
62
47
136
60
46
137
59
46
139
57
45
140
56
45
138
54
43
137
55
43
135
56
43
131
58
43
127
60
43
122
61
42
119
62
42
120
62
42
127
57
45
122
48
39
129
56
47
131
63
52
117
53
43
113
53
42
124
66
54
125
67
55
127
69
58
129
71
60
131
73
62
132
74
63
133
74
66
135
76
68
138
79
71
137
85
74
130
94
78
129
102
81
134
109
89
135
114
93
136
121
98
137
131
105
141
144
117
141
154
124
138
159
126
136
163
130
136
167
133
137
168
134
135
166
132
132
162
128
129
156
125
127
151
119
125
144
112
125
142
108
123
140
108
120
139
107
120
139
109
117
136
108
112
131
103
108
126
100
99
113
88
92
104
82
83
90
72
73
75
61
66
64
52
61
54
44
57
47
38
55
42
36
57
39
39
57
37
39
54
38
39
52
38
38
49
39
38
46
40
40
44
43
41
44
44
42
41
43
40
41
43
42
41
43
42
41
43
42
42
42
44
42
42
44
43
41
44
45
41
42
44
35
36
50
42
39
53
48
42
56
54
42
66
67
53
85
90
70
102
112
87
110
125
94
116
136
101
113
138
98
117
144
101
120
152
105
119
155
107
115
153
104
116
154
105
119
159
107
122
162
110
120
160
108
119
159
109
120
160
110
121
163
115
123
165
117
123
165
119
124
163
119
121
158
117
122
156
119
119
150
116
115
142
111
109
134
104
100
123
95
87
107
80
78
92
69
68
69
55
62
58
47
56
52
41
53
54
48
53
54
48
54
55
49
54
55
49
55
56
50
55
56
50
56
55
50
56
55
50
57
56
51
58
57
52
61
58
53
62
59
54
62
59
54
62
59
54
62
57
53
62
57
53
63
55
52
63
55
52
62
54
51
61
53
50
59
54
51
58
53
50
57
52
49
57
52
49
56
52
49
56
52
51
57
53
52
56
52
51
55
51
50
52
51
49
53
49
50
52
48
47
54
45
48
54
45
46
54
45
48
54
45
46
52
46
46
52
46
46
52
46
46
52
47
44
52
48
45
52
48
45
52
48
45
53
49
46
51
50
45
51
50
45
52
51
46
52
51
46
52
51
47
52
51
49
53
52
50
53
52
48
56
52
49
56
53
48
57
52
48
58
53
47
61
54
48
62
55
47
64
55
46
65
56
47
67
57
48
68
58
48
71
58
49
72
59
50
70
60
51
70
60
51
70
60
51
70
60
51
69
59
50
67
57
48
66
55
49
65
54
48
64
53
49
64
53
49
63
52
48
62
51
47
62
51
49
63
52
50
64
53
51
64
53
51
65
54
50
68
53
50
75
50
53
81
49
54
86
46
54
85
45
53
82
47
53
75
49
50
68
53
50
63
56
48
61
57
45
63
57
43
67
55
41
73
54
40
83
50
41
88
50
41
93
50
43
100
51
44
108
53
48
118
54
52
125
55
55
130
56
55
131
55
55
130
54
54
126
55
53
125
58
52
125
60
54
125
62
53
128
63
57
131
64
56
134
63
57
133
65
56
131
65
51
131
65
49
132
65
49
132
63
48
136
60
47
136
58
46
137
57
46
138
56
45
135
55
44
134
56
44
131
57
44
128
59
44
126
60
44
124
61
44
121
62
44
126
60
44
133
53
42
138
51
44
160
77
69
166
89
81
138
65
56
124
57
48
130
70
59
128
75
61
123
71
58
122
74
60
124
76
62
125
77
63
127
77
66
130
80
69
137
85
74
134
92
76
119
105
79
117
112
82
130
114
88
141
117
93
150
117
98
159
124
105
166
135
115
167
145
122
157
147
122
150
153
124
146
160
127
143
165
129
139
165
130
134
161
126
130
156
121
131
150
118
134
143
112
136
141
111
134
140
112
131
142
112
129
142
112
126
141
112
122
138
109
118
134
107
109
125
98
104
116
92
96
104
83
87
90
71
80
77
62
72
64
51
66
54
42
64
47
39
61
43
39
60
42
40
56
42
39
53
42
40
50
42
39
47
44
39
44
45
39
43
45
40
43
45
40
44
45
40
45
44
42
47
43
42
51
41
42
52
40
42
54
39
42
54
40
40
51
40
36
60
51
44
69
62
52
73
71
56
81
82
64
93
98
75
103
113
86
106
121
90
110
130
93
107
132
92
108
137
93
112
145
98
113
149
101
111
149
100
111
151
101
114
154
102
117
159
109
114
156
106
113
155
105
116
158
110
120
162
114
122
164
118
121
162
118
121
159
118
118
154
116
119
153
118
117
148
114
111
141
107
106
133
102
99
124
94
85
108
79
78
92
69
67
68
54
64
57
47
58
51
41
54
55
49
55
56
50
55
56
50
55
56
50
56
57
51
56
57
51
57
56
51
58
57
52
58
57
52
59
58
53
62
59
54
63
60
55
63
60
55
63
60
55
64
59
55
64
59
55
66
58
55
66
58
55
65
57
54
64
56
53
61
56
52
61
56
52
60
55
51
60
55
51
60
55
51
60
55
52
60
55
52
60
55
52
59
54
51
57
53
50
57
51
51
56
51
48
56
47
48
57
47
46
56
47
48
56
48
46
56
48
46
56
48
46
54
49
46
54
49
45
54
49
45
54
49
45
53
50
45
53
50
45
54
51
46
54
51
46
55
52
47
55
52
47
53
52
48
53
52
48
56
52
49
57
53
50
58
54
51
60
57
52
61
56
52
64
57
51
64
55
48
65
56
47
67
57
48
68
58
48
71
58
49
72
60
48
73
61
49
73
61
49
74
61
52
74
61
52
74
61
52
73
60
51
70
60
51
68
58
49
66
56
47
65
55
46
65
56
49
64
55
48
64
55
50
63
54
49
63
54
49
64
55
50
64
55
50
65
56
51
64
54
52
65
55
53
67
56
54
70
56
55
72
57
54
74
56
54
74
56
54
72
57
52
69
56
48
69
56
47
69
57
45
70
56
43
73
57
44
73
57
42
76
57
43
82
54
42
90
47
38
100
47
39
110
51
45
122
54
51
131
57
56
137
59
57
139
59
58
139
61
59
139
65
62
135
64
58
130
61
54
127
60
51
127
58
51
130
60
52
136
61
55
138
64
55
133
63
53
132
64
51
132
62
50
134
62
50
134
60
49
133
59
48
132
58
47
131
59
47
128
58
46
127
59
46
126
60
46
127
61
47
127
61
47
128
60
47
129
59
47
135
57
47
144
49
43
160
61
56
203
108
102
216
127
121
171
91
84
138
68
60
136
76
65
127
78
64
117
73
60
114
77
61
113
80
63
114
81
64
118
83
64
124
85
68
132
90
74
128
100
79
113
113
79
116
120
85
138
118
91
156
115
95
173
109
99
186
107
102
194
111
107
195
116
111
191
128
119
183
137
122
173
148
126
163
156
128
154
159
129
145
159
126
139
154
121
137
151
118
140
143
114
142
142
114
139
144
114
135
146
114
132
147
114
128
147
115
122
146
112
119
144
112
112
137
105
107
130
101
103
119
92
94
107
81
88
93
71
80
79
59
72
67
48
67
58
43
65
50
43
64
49
44
59
48
42
56
47
42
52
47
41
50
47
40
46
47
39
46
47
41
48
47
42
51
46
42
54
44
43
58
42
43
62
40
43
65
38
43
69
36
43
67
38
40
70
53
45
76
67
52
86
81
62
91
89
68
96
98
74
103
109
81
105
119
86
106
123
87
108
130
91
103
130
87
102
134
87
108
142
92
111
148
97
110
148
97
110
150
98
114
154
102
115
157
107
111
155
106
111
153
105
114
156
110
119
160
116
121
162
120
120
158
119
117
155
118
115
151
115
117
151
118
115
146
114
110
139
108
105
132
99
98
123
91
84
109
77
77
93
67
72
73
59
69
60
51
63
54
45
55
56
50
55
56
50
56
57
51
56
57
51
56
57
51
57
58
52
58
57
52
58
57
52
59
58
53
60
59
54
63
60
55
64
61
56
64
61
56
64
61
56
65
60
56
65
60
56
67
59
56
67
59
56
67
59
56
66
58
55
63
58
54
62
57
53
62
57
53
61
56
52
62
57
53
62
57
53
62
57
53
62
57
53
61
56
53
60
55
51
58
53
49
59
51
48
58
48
46
58
48
46
57
49
46
57
49
46
57
49
47
57
49
46
57
49
46
55
50
46
54
49
45
54
49
45
54
51
46
54
51
46
54
51
46
55
52
47
55
52
47
55
52
47
53
52
48
54
53
49
57
53
50
59
56
51
60
57
52
63
58
52
66
59
53
66
59
51
65
56
47
66
58
47
67
57
47
69
59
47
72
60
48
73
61
49
74
62
50
74
62
50
75
62
53
75
62
53
74
61
52
74
61
52
70
60
51
69
59
50
67
57
48
66
56
47
66
57
50
66
57
50
65
56
51
64
55
50
64
55
50
65
56
51
66
57
52
66
57
52
63
56
50
64
57
51
66
59
53
69
60
55
71
60
54
73
60
54
77
60
53
78
59
52
77
54
46
78
54
44
81
54
43
81
54
43
81
55
42
82
56
43
80
57
41
86
54
39
103
53
42
114
54
44
126
59
51
137
64
57
149
68
64
155
72
68
159
74
71
159
76
70
160
81
74
153
76
68
142
68
59
133
59
48
132
55
45
133
55
45
140
57
49
143
60
52
139
60
53
138
61
55
137
60
54
136
59
53
136
59
53
132
59
52
128
60
51
125
61
51
122
62
51
122
62
51
122
62
51
125
61
51
129
59
51
134
57
51
139
54
51
146
51
49
150
41
38
176
63
59
234
125
122
252
150
146
195
104
99
149
72
64
137
75
64
120
72
58
112
76
60
106
81
61
104
83
62
106
86
62
111
86
64
118
90
69
128
95
76
130
102
80
124
113
81
134
118
85
160
115
92
183
107
94
203
95
95
214
84
92
218
81
91
218
82
92
230
111
117
219
121
120
207
133
124
195
142
126
184
147
128
174
148
125
165
145
120
162
142
118
159
137
114
159
137
116
156
140
117
152
142
117
148
144
117
141
144
115
137
143
115
133
144
114
127
137
110
124
133
106
119
121
97
112
110
87
105
97
78
98
82
66
87
68
53
83
59
47
79
52
45
77
49
45
73
48
44
68
47
42
64
47
40
59
46
38
55
46
39
54
47
39
57
48
41
58
47
41
61
46
43
65
44
43
70
41
43
72
40
43
75
39
43
72
41
39
84
66
52
89
81
60
100
94
70
103
101
76
105
108
79
110
117
86
109
123
88
108
126
88
107
132
90
103
132
86
103
135
86
109
143
93
112
149
98
113
150
98
114
153
100
117
157
105
116
155
108
111
153
107
111
150
106
114
153
109
119
157
118
121
159
120
119
154
121
116
150
117
115
148
117
117
148
117
115
144
113
111
138
107
105
132
101
97
124
91
84
109
77
77
93
66
78
79
63
76
67
58
69
60
51
57
58
52
57
58
52
57
58
52
57
58
52
57
58
52
58
59
53
61
60
55
61
60
55
61
60
55
61
60
55
64
61
56
64
61
56
65
62
57
66
63
58
68
63
59
68
63
59
69
64
60
69
64
60
69
64
58
68
63
57
68
63
57
67
62
56
69
62
56
68
61
55
66
59
53
66
59
53
66
59
53
66
59
53
66
58
55
66
59
53
65
58
52
65
58
52
65
56
51
65
56
51
65
56
51
64
55
50
64
54
52
62
55
49
62
55
49
62
55
49
60
55
49
60
55
49
59
54
48
58
53
47
56
53
46
56
53
46
55
52
47
55
52
47
57
54
49
57
54
49
57
54
49
58
55
48
60
55
49
61
56
50
65
58
50
66
59
49
67
58
49
68
60
49
70
60
50
74
62
50
75
63
51
76
64
50
78
64
51
78
64
53
78
64
55
78
64
55
76
63
54
75
62
53
73
60
51
72
59
50
69
59
50
69
59
50
66
57
48
65
56
47
64
57
49
63
56
48
63
56
48
63
56
48
64
57
51
64
57
51
60
53
47
77
68
61
77
64
58
75
58
51
84
64
57
82
54
50
81
48
43
95
60
54
91
52
45
94
54
46
95
52
43
92
48
39
94
47
37
101
54
44
109
61
49
114
60
48
137
73
61
143
71
59
151
73
63
159
75
65
159
70
62
155
62
55
154
61
54
158
68
59
166
78
68
164
80
69
168
86
74
171
89
75
165
79
66
153
65
53
151
59
48
156
62
54
148
57
56
146
56
56
143
53
53
137
51
50
136
55
52
135
62
56
127
63
54
117
60
49
117
65
52
115
63
50
116
59
48
121
57
48
133
54
50
142
50
51
149
44
49
155
38
44
198
72
73
229
99
97
194
70
68
202
89
85
224
122
118
171
86
79
134
70
60
132
86
70
109
82
61
105
91
65
105
98
70
107
100
72
107
95
69
111
94
68
125
99
76
141
106
84
145
99
76
177
110
93
208
111
104
218
89
93
222
59
76
229
45
69
241
44
72
243
49
76
243
65
89
233
74
92
227
91
101
226
111
114
224
124
124
218
129
125
216
133
127
215
138
130
196
126
118
192
128
119
189
131
120
185
133
120
181
135
120
175
136
119
171
138
121
167
138
120
157
128
112
157
125
110
154
121
106
150
110
98
145
98
90
135
85
78
127
72
67
123
65
63
117
58
60
103
47
50
100
48
50
97
49
49
79
39
39
71
37
35
76
48
45
77
53
49
72
51
46
69
50
44
68
49
45
70
49
46
70
46
44
71
43
42
75
45
45
81
54
47
89
72
52
96
87
58
102
95
66
103
100
69
107
111
78
117
124
90
117
132
93
111
129
89
108
131
87
108
135
90
112
141
93
114
146
97
115
149
98
117
151
100
119
153
102
118
155
104
114
150
106
115
152
111
115
151
113
113
149
111
115
149
116
119
153
120
122
152
124
121
150
122
120
149
121
115
141
114
109
135
106
107
132
102
105
130
100
97
122
90
84
109
77
79
95
68
79
80
64
84
75
66
82
73
64
57
58
52
57
58
52
57
58
52
57
58
52
58
59
53
59
60
54
62
61
56
63
62
57
62
61
56
62
61
56
65
62
57
65
62
57
66
63
58
67
64
59
68
63
59
68
63
59
70
65
61
69
64
60
69
64
58
69
64
58
68
63
57
68
63
57
69
62
56
69
62
56
69
62
56
69
62
56
69
62
56
68
61
53
67
60
54
67
60
52
68
59
52
67
58
49
66
57
50
66
57
50
66
57
50
66
57
50
65
56
51
65
56
51
64
57
51
64
57
51
62
55
49
60
55
49
59
54
48
59
54
48
57
54
47
56
53
46
55
52
47
55
52
47
57
54
49
57
54
47
57
54
47
58
55
48
60
55
49
62
58
49
65
58
48
66
60
48
67
59
48
69
59
47
73
61
49
74
62
48
76
64
50
76
64
50
78
64
51
78
64
51
78
64
53
78
64
55
77
63
54
74
61
52
73
60
51
72
59
50
69
59
50
69
59
50
67
58
49
66
57
48
64
57
49
64
57
49
64
57
49
64
57
49
65
58
52
65
58
50
67
58
51
73
56
49
75
47
43
86
46
44
106
55
54
116
58
57
123
59
59
133
69
67
129
68
63
131
72
64
132
73
65
132
72
62
141
74
66
152
79
72
161
80
76
162
79
71
147
68
55
148
66
52
154
66
54
159
67
56
160
62
51
156
55
45
156
55
45
159
61
48
163
67
53
155
63
48
153
64
48
160
69
51
164
71
54
167
70
54
174
72
58
182
78
69
181
75
77
176
71
76
167
64
68
155
56
59
147
55
56
142
61
58
133
62
56
121
61
51
114
60
48
124
72
59
135
75
65
133
62
56
133
43
43
149
40
46
181
57
68
211
75
87
219
76
78
221
79
77
199
64
61
190
65
63
208
96
92
179
89
81
132
64
53
130
84
68
121
97
73
101
91
64
96
92
63
107
104
73
117
108
79
119
102
74
127
99
75
150
99
78
185
102
88
210
97
91
224
88
90
231
67
78
238
46
69
249
39
68
255
40
72
255
42
73
255
48
78
243
53
78
236
63
82
234
76
90
230
82
94
224
84
93
222
85
93
221
90
98
219
98
105
216
101
108
214
105
110
213
108
113
208
112
114
206
116
116
204
118
119
201
120
119
204
124
125
203
121
123
203
118
121
202
113
117
202
107
115
201
101
111
199
97
108
197
95
108
195
97
110
177
84
95
163
76
85
153
72
81
141
68
75
132
70
73
107
55
57
72
28
27
84
49
45
77
48
42
72
49
43
74
54
47
75
56
49
75
58
50
79
62
54
85
69
56
91
79
53
99
90
57
103
97
65
105
102
69
110
114
79
122
130
93
122
137
98
116
134
94
114
137
95
112
137
95
112
139
96
113
142
96
117
146
100
119
148
100
119
148
100
119
148
102
115
146
105
120
150
114
122
151
120
121
150
120
126
152
125
131
157
132
133
157
135
131
155
133
129
153
131
122
146
122
114
137
111
109
132
104
104
129
100
96
121
91
84
109
77
78
97
69
82
83
67
86
80
68
86
79
69
56
57
51
56
57
51
56
57
51
57
58
52
59
60
54
60
61
55
63
62
57
64
63
58
64
63
58
64
63
58
67
64
59
67
64
59
67
64
59
67
64
59
68
63
59
68
63
59
70
65
59
70
65
59
70
65
59
69
64
58
69
64
58
69
64
58
71
64
56
70
63
55
74
65
58
73
64
57
72
63
56
71
62
53
70
61
54
69
60
51
70
60
51
69
59
49
68
58
49
68
58
49
68
58
49
68
58
49
68
59
52
68
59
52
68
59
52
67
60
52
63
56
48
63
56
48
60
55
49
59
54
48
58
53
47
58
53
47
57
52
46
57
52
46
57
54
47
57
54
47
58
53
47
59
55
46
62
55
47
64
57
47
66
57
48
67
59
48
69
59
49
70
60
48
73
61
49
75
63
49
77
63
50
77
64
48
78
65
49
77
64
48
80
63
53
77
63
52
76
62
51
75
61
50
73
60
51
72
59
50
69
59
49
68
58
48
67
58
49
67
58
49
65
58
48
65
58
48
63
59
50
64
60
51
64
60
51
68
59
52
83
69
60
82
51
46
105
51
51
142
65
71
173
76
87
193
86
96
203
93
102
196
96
98
154
66
62
150
73
63
149
77
65
155
79
66
171
82
74
190
85
82
203
80
83
196
76
75
163
65
52
157
66
48
160
64
48
165
63
49
165
59
46
163
55
43
164
56
43
167
61
47
171
69
54
161
62
43
152
55
36
157
58
39
165
62
45
172
65
47
178
67
50
184
67
58
167
48
50
177
57
66
185
68
76
190
75
82
186
77
82
173
75
76
150
65
62
130
55
49
123
56
47
118
54
44
122
49
42
133
48
45
156
51
55
186
60
71
215
70
87
232
77
91
225
71
73
206
54
51
205
60
57
187
52
49
198
79
75
198
99
93
148
74
63
140
91
74
125
99
76
106
95
67
104
99
69
117
111
79
128
111
81
130
103
74
143
103
78
177
104
89
224
100
98
239
84
90
238
71
81
239
58
73
248
52
74
255
52
78
255
50
77
254
46
72
255
53
79
245
51
75
240
53
74
239
56
76
237
55
77
233
54
75
236
54
76
239
57
80
241
65
88
239
67
91
239
70
93
237
73
97
236
77
99
234
81
101
233
84
104
232
86
107
228
84
107
229
83
106
228
80
106
228
78
105
233
78
109
238
81
112
242
83
115
244
87
118
252
105
133
237
99
124
225
93
116
210
87
108
203
90
108
199
101
114
166
83
93
114
45
50
93
39
39
88
49
44
87
58
52
81
63
53
66
58
45
58
56
41
63
66
49
76
78
57
86
79
51
97
86
54
101
95
63
104
101
68
113
117
84
127
134
101
126
140
105
119
136
100
120
140
105
117
139
101
114
139
100
114
139
99
118
140
101
121
143
104
122
144
105
122
144
106
128
149
118
132
155
126
138
158
133
141
160
138
146
163
144
151
168
150
152
169
153
150
166
153
145
162
146
135
152
134
122
141
121
112
131
109
103
126
98
94
119
89
84
109
77
80
99
71
82
87
67
88
85
70
89
85
73
55
56
50
55
56
50
56
57
51
57
58
52
59
60
54
61
62
56
64
63
58
65
64
59
66
65
60
66
65
60
68
65
60
68
65
60
68
65
60
68
65
60
69
64
60
69
64
60
71
66
60
71
66
60
71
66
60
70
65
59
70
65
59
70
66
57
72
65
57
72
65
57
76
67
60
75
66
57
74
65
56
73
64
55
72
62
53
71
61
51
70
60
50
69
59
49
68
58
48
68
58
48
69
59
49
69
59
50
68
59
50
69
60
51
69
60
53
69
60
53
64
57
49
63
56
48
61
56
50
60
55
49
59
54
48
58
53
47
58
53
47
58
53
47
57
54
47
57
54
45
59
55
46
59
55
44
63
56
46
64
58
46
67
59
48
68
60
47
70
60
48
70
61
46
74
62
48
75
63
47
77
64
48
77
64
48
77
64
47
77
64
48
79
63
50
79
62
52
76
62
51
75
61
50
74
60
51
71
58
49
69
59
49
68
58
48
68
59
50
68
59
50
66
59
49
64
60
49
64
60
51
65
61
52
66
62
53
70
60
51
81
56
49
88
43
38
135
59
63
189
85
96
216
85
101
228
86
102
226
86
99
201
75
79
157
52
48
148
58
47
144
62
48
152
66
51
173
70
61
196
74
71
212
67
72
203
63
64
177
66
55
167
70
53
169
66
51
172
64
51
175
63
51
176
62
51
179
66
52
180
68
54
178
71
53
169
66
47
163
60
41
164
59
40
169
60
40
171
58
40
174
54
38
173
51
40
182
57
55
186
57
61
187
56
62
186
55
60
190
61
66
197
74
76
196
84
83
190
88
83
153
63
55
139
52
42
135
45
37
157
54
49
195
72
75
219
80
87
224
66
81
218
52
64
221
59
57
204
47
42
209
57
52
198
56
52
197
70
64
200
93
85
167
88
75
144
87
70
119
87
64
120
100
73
126
112
83
130
111
81
133
102
74
145
99
75
172
109
91
213
117
105
234
85
87
246
70
80
242
62
74
242
60
73
249
64
78
253
67
81
249
61
76
243
55
72
253
66
83
248
60
77
244
55
75
246
54
75
248
52
74
249
50
73
255
51
78
255
56
83
255
51
84
252
53
86
252
54
87
249
56
87
247
58
90
248
61
92
246
63
94
245
64
97
247
67
102
245
67
103
244
64
101
245
63
103
247
65
105
252
67
109
255
71
115
255
74
115
255
81
117
253
84
117
250
87
118
239
85
113
228
86
110
226
99
120
213
102
118
180
89
98
124
49
53
104
48
47
88
49
42
80
56
44
71
62
47
63
64
46
59
68
47
65
70
47
81
74
46
95
84
56
103
96
67
111
107
78
124
124
96
136
142
114
135
145
118
125
139
113
122
141
113
124
143
115
124
143
115
122
141
111
122
138
109
125
140
111
131
146
117
136
151
122
149
163
138
156
169
149
163
174
157
166
177
161
172
180
167
177
185
174
176
183
176
171
181
173
161
171
162
148
160
148
131
145
130
115
132
113
104
124
99
93
116
88
83
108
78
80
100
72
80
88
67
86
87
69
87
88
72
54
55
49
54
55
49
55
56
50
56
57
51
58
59
53
60
61
55
63
62
57
64
63
58
66
65
60
66
65
60
68
65
60
68
65
60
68
65
60
69
66
61
70
65
61
70
65
61
70
67
60
70
67
60
71
66
60
72
67
61
72
68
59
72
68
59
74
67
59
74
67
57
76
67
58
75
66
57
75
65
56
74
64
54
74
61
52
73
61
49
72
60
48
71
59
47
69
59
47
69
59
47
69
59
47
69
59
49
69
59
49
69
59
49
68
59
50
68
59
50
64
57
47
64
57
47
64
57
49
63
56
48
60
55
49
59
54
48
59
54
48
58
53
47
59
55
46
59
55
46
59
55
46
60
56
45
63
56
46
65
59
47
67
59
48
68
60
47
71
61
49
71
62
47
75
63
49
77
64
48
77
64
48
78
65
48
79
63
47
79
63
48
79
63
50
78
61
51
78
61
51
74
60
49
73
59
48
71
59
47
68
58
48
68
58
48
68
60
49
68
60
49
66
59
49
64
60
49
65
61
50
65
63
51
66
63
54
75
61
52
91
54
46
116
52
50
172
77
83
218
94
105
229
78
95
226
65
81
213
55
69
182
39
43
167
50
41
156
56
40
145
58
38
149
61
41
171
65
51
193
69
61
211
63
63
204
60
59
177
63
53
168
66
52
170
64
50
173
63
50
178
64
53
184
67
57
188
72
59
187
74
60
172
62
47
170
63
47
168
61
43
166
59
41
170
57
41
176
58
44
183
59
47
187
61
49
200
71
65
204
69
65
206
64
63
205
57
57
211
56
60
220
67
69
223
77
78
218
83
79
193
68
62
195
77
67
203
84
76
209
85
77
215
75
74
218
64
66
223
53
62
224
51
55
217
52
46
215
57
46
205
51
43
206
59
52
190
57
52
178
65
57
181
91
80
150
85
67
123
78
57
130
98
73
136
106
80
133
96
70
147
90
70
176
100
84
206
112
100
232
109
104
231
70
75
237
60
68
230
59
67
227
62
68
227
68
72
227
73
75
228
74
76
229
73
77
240
76
83
239
68
77
239
58
73
242
55
72
245
52
71
248
49
70
254
52
76
255
58
82
255
57
86
253
58
88
252
59
88
250
59
90
251
60
93
252
63
95
253
65
98
252
67
101
253
69
105
251
69
107
250
68
108
250
68
109
250
67
111
249
68
111
249
68
113
250
67
111
255
72
112
255
66
104
255
74
109
253
79
112
238
75
104
230
80
107
226
93
114
216
99
115
186
91
99
140
64
68
96
38
36
81
43
34
86
64
51
86
79
61
74
76
55
65
67
45
82
74
51
101
89
67
117
106
86
128
123
103
142
141
121
153
156
139
149
156
140
136
147
131
130
142
128
137
151
136
143
155
141
140
153
136
135
146
130
138
147
130
151
158
142
164
168
153
178
182
167
184
188
174
191
193
182
192
193
185
194
195
189
197
198
193
195
195
195
190
190
190
176
181
177
162
169
162
141
151
140
121
135
118
105
123
101
92
115
89
84
109
79
82
102
74
80
90
66
84
89
67
86
91
71
53
54
48
53
54
48
54
55
49
55
56
50
56
57
51
58
59
53
61
60
55
62
61
56
64
63
58
64
63
58
67
64
59
68
65
60
69
66
61
69
66
61
71
66
62
71
66
62
71
68
61
71
68
61
72
67
61
73
68
62
73
69
60
73
69
60
75
68
58
76
69
59
76
67
58
75
67
56
76
66
56
75
65
53
76
64
52
75
63
51
74
62
50
73
61
49
71
61
49
71
61
49
70
60
48
70
60
48
69
59
49
69
59
49
68
59
50
67
58
49
65
58
48
65
58
48
64
57
49
64
57
49
61
56
50
60
55
49
59
54
48
59
54
48
59
55
46
59
55
46
59
55
44
60
56
45
63
57
45
65
59
47
68
60
47
69
61
48
72
63
48
74
62
48
76
63
47
77
64
48
78
65
48
78
65
48
79
63
47
79
63
47
79
63
50
78
61
51
77
60
50
76
59
49
73
59
48
71
59
47
70
57
48
67
57
47
67
59
48
66
60
48
66
59
49
64
60
49
64
62
50
65
63
51
66
63
54
78
59
52
106
57
52
149
71
69
195
90
95
218
88
98
220
68
81
219
57
70
209
52
59
188
44
43
172
50
39
159
56
37
150
58
33
148
58
32
161
61
38
179
64
46
193
59
50
189
56
49
167
55
44
162
58
45
165
57
45
167
55
44
175
58
49
184
66
56
186
69
59
183
69
58
174
62
50
173
63
48
167
60
44
164
54
39
170
54
41
182
62
48
195
68
59
202
71
61
188
58
45
203
65
54
221
70
63
232
68
66
239
63
65
239
59
62
231
52
55
218
48
48
216
57
53
222
73
66
229
85
76
224
80
71
216
63
57
214
50
48
224
48
50
231
55
55
216
49
40
221
61
47
202
46
34
205
56
49
189
52
44
174
55
47
200
104
92
184
108
92
142
87
67
135
90
67
132
86
63
139
81
61
170
91
76
209
106
97
226
103
98
229
83
84
240
69
75
239
62
70
226
60
64
216
62
62
209
66
62
209
72
66
219
80
77
233
87
87
235
79
82
238
68
77
239
59
71
245
54
70
246
50
70
246
47
68
249
50
71
253
55
78
251
57
83
248
58
84
248
58
86
249
59
87
254
61
92
255
63
97
255
67
101
255
68
107
251
60
101
251
62
104
250
63
106
247
64
108
247
64
110
244
62
110
241
61
108
241
60
105
255
74
114
251
61
97
251
66
100
255
78
110
251
77
110
243
80
109
236
86
111
224
91
110
231
114
130
194
97
106
150
74
76
116
58
56
93
53
45
82
56
43
82
66
50
87
76
58
94
81
65
114
101
85
136
124
110
149
141
130
159
157
145
166
167
159
161
166
159
150
156
152
149
158
155
158
167
164
166
175
172
166
172
168
162
167
163
166
168
163
181
180
176
194
193
188
204
201
192
209
206
197
213
208
204
213
208
205
212
206
206
213
207
209
209
203
207
203
198
202
191
189
192
176
178
175
152
159
152
128
140
126
109
126
107
96
116
91
87
110
82
86
106
78
83
95
71
86
96
72
87
95
72
53
54
48
53
54
48
53
54
48
53
54
48
54
55
49
56
57
51
58
57
52
59
58
53
61
60
55
62
61
56
65
62
57
67
64
59
68
65
60
70
67
62
72
67
63
72
67
63
71
68
61
72
69
62
73
68
62
73
68
62
74
70
61
74
70
61
77
70
60
77
70
60
77
69
58
76
68
57
77
67
57
77
67
55
78
66
54
77
65
51
78
64
51
77
65
51
74
65
50
74
65
50
73
64
49
72
63
48
71
61
49
70
60
48
68
60
49
68
60
49
67
58
49
67
58
49
65
58
50
64
57
49
63
56
50
63
56
50
60
55
49
60
55
49
60
56
47
60
56
45
60
56
45
60
56
45
64
58
46
65
59
47
68
60
47
69
61
48
72
63
48
75
63
49
77
64
48
78
65
48
80
64
48
80
64
48
79
64
45
79
63
47
79
61
49
79
61
49
77
61
48
76
60
47
73
59
48
70
58
46
69
56
47
67
57
47
66
58
47
65
59
47
65
58
48
63
59
48
63
61
49
64
62
50
64
64
52
78
58
49
111
50
47
165
75
75
190
84
86
186
63
68
188
49
54
193
48
51
191
49
47
183
50
41
167
50
33
162
57
35
155
61
33
150
61
31
152
61
32
158
63
35
164
59
37
162
55
37
158
54
41
160
56
45
162
54
44
165
52
44
173
56
49
183
64
58
184
67
60
178
64
54
177
66
55
173
65
52
168
60
47
166
56
43
175
58
48
188
67
56
195
68
61
196
65
55
193
64
45
203
64
45
213
55
43
221
44
38
232
36
37
245
39
43
253
44
49
252
49
52
233
44
42
227
48
43
220
53
44
217
55
44
220
53
44
223
52
44
229
50
45
228
51
43
215
52
37
212
55
38
207
51
38
206
55
44
196
55
46
187
63
55
199
97
85
210
127
113
166
101
83
143
86
66
139
76
58
164
86
73
203
100
91
224
99
95
232
82
84
233
66
73
246
69
77
238
63
68
225
62
63
213
64
60
202
63
56
200
63
55
211
69
65
229
77
76
240
74
78
245
63
75
251
56
72
255
54
72
254
53
72
249
50
69
249
54
71
250
59
77
247
58
80
246
58
82
246
56
82
249
56
85
251
56
88
254
59
93
255
61
97
255
63
103
255
64
106
255
64
108
255
65
112
255
67
115
251
68
116
249
67
116
247
67
115
244
67
111
251
73
109
245
68
97
249
72
101
252
77
106
250
77
107
255
88
116
255
97
122
242
94
116
240
107
126
229
113
126
211
111
119
175
93
97
132
67
65
104
54
47
105
65
55
115
85
74
121
101
92
140
125
118
163
150
144
173
163
161
178
174
171
182
182
182
178
182
185
170
175
179
176
183
189
182
189
197
189
193
202
192
195
202
195
194
202
201
198
205
213
206
214
221
215
219
226
218
216
231
221
219
233
221
221
230
218
220
229
216
223
228
215
224
224
212
222
217
207
216
202
197
203
186
186
186
162
167
163
136
146
135
116
130
113
101
120
98
92
115
87
91
111
83
92
106
80
92
105
79
94
104
79
53
54
49
53
54
49
53
54
49
53
54
48
53
54
49
54
55
49
56
55
50
57
56
51
60
59
54
61
60
55
64
61
56
66
63
58
68
65
60
70
67
60
71
68
61
72
69
62
72
69
62
72
69
62
72
69
60
74
70
61
74
70
59
75
71
59
75
71
59
78
72
58
77
71
59
77
71
59
77
69
58
77
68
59
77
68
59
77
68
61
78
67
63
78
67
61
77
67
55
77
68
53
75
66
51
74
65
50
72
62
50
71
61
49
69
61
50
68
59
50
67
58
49
67
58
51
65
58
50
65
58
52
64
57
51
63
55
52
60
55
51
60
55
49
62
55
47
62
55
47
60
56
47
61
57
48
62
58
49
65
58
50
67
60
50
68
61
51
72
64
53
73
63
51
76
64
50
77
65
49
78
65
48
80
64
48
79
64
45
79
63
47
76
62
49
76
62
51
75
61
50
76
59
51
74
57
49
73
56
49
70
55
48
67
56
50
65
56
47
62
58
47
61
59
47
60
61
47
63
61
48
64
62
47
66
63
48
83
57
44
134
61
55
193
93
91
200
90
89
174
56
54
170
47
42
177
50
43
171
47
37
171
51
35
170
58
38
171
67
42
169
71
44
163
70
39
160
66
38
161
66
38
161
61
35
158
58
35
160
57
42
163
59
48
166
58
48
166
55
46
173
60
52
182
67
60
183
69
59
176
64
53
171
61
48
168
60
47
169
59
46
172
60
48
187
69
57
198
76
65
197
69
60
188
56
44
188
55
36
201
60
40
218
61
46
227
53
44
238
44
42
246
42
43
249
40
43
247
39
39
246
44
42
241
48
43
236
52
44
232
54
44
227
53
42
226
52
41
225
51
40
222
52
37
222
59
42
207
50
31
219
63
48
205
55
41
199
57
45
194
63
53
179
65
54
209
108
96
191
103
89
165
82
66
170
78
65
205
98
90
232
103
98
231
81
82
235
64
70
246
65
74
237
60
68
227
58
61
215
59
60
207
62
57
195
61
52
185
54
44
187
52
46
199
56
52
229
71
72
234
63
69
242
58
68
249
57
72
252
57
73
250
55
71
250
58
73
251
63
78
252
67
85
251
65
86
250
63
84
249
59
84
251
58
85
253
58
90
255
59
94
255
60
99
255
64
104
255
63
107
255
63
110
251
63
111
247
64
112
245
65
113
245
64
115
243
66
112
244
68
106
252
79
109
255
86
115
243
76
104
231
66
96
249
86
115
255
105
133
255
102
130
246
94
119
238
96
120
231
104
123
220
112
125
199
112
120
170
102
103
143
92
88
126
89
81
153
126
119
171
152
146
190
176
173
197
187
186
198
194
195
200
199
204
199
199
207
195
195
207
203
202
216
204
202
216
209
202
218
215
206
223
221
213
228
230
219
233
232
224
235
237
227
235
239
229
230
242
232
230
244
232
232
240
228
230
238
225
232
237
224
233
234
221
231
228
216
226
209
202
209
193
191
194
171
171
169
145
150
143
122
134
120
108
123
104
98
117
95
96
114
90
100
114
89
100
112
88
100
110
86
50
49
47
51
50
48
52
51
49
54
53
49
56
55
53
57
56
52
57
56
52
57
56
52
59
58
53
60
59
54
61
61
53
63
63
55
65
65
57
67
67
57
69
69
59
69
69
59
72
72
64
72
72
62
72
72
60
74
72
59
75
74
56
76
75
55
77
76
56
78
75
56
77
74
57
74
70
58
71
68
59
74
70
67
78
73
77
78
73
80
71
68
79
68
63
69
78
69
62
76
67
52
74
64
52
75
65
53
77
67
57
76
66
56
70
61
52
66
57
48
69
60
53
67
58
53
64
57
51
63
55
52
64
56
53
63
55
53
60
55
52
60
52
50
67
58
53
66
57
52
65
57
54
62
57
53
63
58
55
62
58
55
63
59
56
64
60
57
66
63
58
68
64
55
71
64
54
74
64
52
76
64
48
79
64
45
79
64
43
76
63
44
70
64
50
69
62
52
72
59
51
74
56
52
75
54
53
77
52
55
75
55
57
72
58
58
67
62
58
55
56
48
55
59
45
62
66
49
62
64
43
58
56
35
67
59
36
101
66
46
189
106
92
183
75
65
166
55
44
164
54
41
167
55
43
164
54
39
163
56
38
168
61
43
162
57
36
165
60
38
169
64
42
171
65
41
172
61
41
172
57
38
174
54
37
170
55
37
164
54
39
161
55
41
162
56
42
166
60
46
170
64
50
172
66
52
172
66
50
172
65
49
169
62
44
171
61
44
173
60
44
179
61
47
189
67
52
196
70
56
192
62
49
184
50
38
199
57
43
203
57
42
207
57
43
213
57
45
219
55
45
225
54
46
231
52
47
237
50
45
240
46
44
242
47
45
242
49
44
239
50
44
234
53
42
227
55
41
221
58
41
218
59
40
219
58
38
212
51
31
207
52
32
206
56
39
197
56
39
190
54
40
194
66
53
207
83
73
200
78
67
207
83
75
225
92
87
238
93
90
240
80
82
237
62
67
243
56
63
248
63
71
235
64
70
225
67
68
214
64
65
201
62
57
187
60
51
179
58
47
177
59
47
180
59
48
202
76
64
212
75
67
222
70
67
226
62
63
235
60
65
248
63
71
254
62
73
249
59
71
253
69
81
250
68
81
253
68
82
255
68
85
253
64
86
250
60
85
254
61
90
255
66
100
255
70
107
255
65
106
249
59
103
245
58
103
244
61
109
248
68
115
253
73
121
255
75
122
255
78
121
255
78
117
249
81
116
239
82
113
229
79
106
229
79
106
247
90
119
255
103
135
255
93
128
240
69
105
245
86
118
232
94
120
198
91
109
194
117
125
140
92
92
145
114
109
171
143
132
189
167
156
212
195
187
222
213
208
221
220
218
220
219
224
222
220
231
226
219
235
231
216
237
235
216
238
241
217
241
244
220
242
245
227
243
245
231
244
243
236
244
244
239
243
248
243
240
248
243
239
248
240
238
245
236
237
242
231
235
237
226
232
232
221
227
229
220
225
214
208
212
205
201
202
187
186
184
163
165
160
139
144
137
121
131
120
114
125
111
112
123
107
123
132
115
119
128
109
118
125
107
48
46
47
49
47
48
51
50
48
53
52
50
54
53
51
55
54
50
56
55
51
56
55
50
58
57
52
59
58
53
61
61
53
63
63
55
65
65
55
67
67
57
68
68
56
69
69
59
72
72
62
72
72
62
72
72
60
74
72
57
75
74
54
76
75
54
77
77
53
78
77
56
75
74
56
75
73
61
75
74
69
80
80
82
87
85
96
89
88
106
86
84
106
82
79
96
79
73
75
77
68
63
72
63
58
70
61
56
71
62
57
72
63
58
70
61
56
66
59
53
68
58
56
65
57
54
63
55
52
63
55
52
64
56
53
64
56
53
61
56
53
59
54
50
61
52
47
61
52
47
61
53
50
60
55
52
61
57
54
62
58
57
61
60
58
62
61
57
66
63
58
68
63
57
71
64
54
74
64
52
77
64
48
79
64
45
80
63
43
76
63
44
65
64
46
63
64
50
69
60
53
72
57
54
74
54
55
74
52
55
70
51
55
67
52
55
65
60
57
60
59
54
61
61
49
69
63
49
74
58
42
80
54
37
100
65
46
133
74
58
176
80
66
171
59
45
160
50
35
161
54
38
163
58
39
158
55
36
158
57
37
163
62
42
161
59
37
167
60
40
174
63
43
179
64
45
182
61
44
183
57
42
184
54
40
181
55
41
170
57
41
165
58
42
164
57
41
165
59
43
168
62
46
171
65
49
171
66
47
171
64
46
174
63
44
171
58
40
177
60
43
191
69
54
198
72
57
193
63
49
188
56
43
192
54
41
203
55
43
207
55
44
208
57
46
209
59
45
212
60
47
216
58
46
224
56
47
231
52
45
238
49
45
244
46
43
244
46
43
241
48
43
234
52
41
225
55
40
215
58
39
213
58
36
222
60
39
217
54
35
213
56
37
212
61
42
204
59
42
193
53
38
196
58
47
207
70
60
209
70
63
218
73
68
233
79
77
243
80
81
244
71
73
240
59
64
245
58
65
248
67
72
229
66
69
218
68
67
210
66
65
197
62
56
183
59
49
174
58
45
170
60
45
171
61
46
182
69
53
198
76
63
212
78
69
220
71
67
227
63
62
239
63
66
250
63
70
254
64
74
248
67
74
247
67
76
250
68
80
255
70
84
253
66
83
249
62
83
252
62
88
255
66
97
255
70
104
255
67
105
252
63
105
249
64
106
249
66
112
252
70
118
254
74
122
255
75
123
255
72
120
255
71
115
244
74
111
242
82
116
243
93
122
246
96
123
247
88
118
247
79
112
250
70
107
248
68
107
238
68
104
220
73
102
203
92
111
182
106
116
115
71
72
150
121
115
180
148
137
206
176
165
229
209
200
239
226
220
235
231
230
233
232
237
236
230
242
237
226
243
246
227
249
251
225
250
255
227
253
255
230
253
255
236
254
255
242
253
254
247
254
252
250
251
253
249
246
252
249
244
251
246
243
249
244
241
247
238
239
243
234
237
240
231
236
238
229
234
230
221
224
220
214
216
202
198
197
179
178
174
158
161
154
142
147
140
137
143
133
136
144
131
142
148
134
137
144
128
134
140
126
47
46
44
48
47
45
49
48
46
50
49
47
52
51
47
53
52
48
55
54
50
55
54
49
58
57
52
58
58
50
60
60
52
62
62
54
64
64
54
66
66
56
67
67
57
68
68
58
73
70
61
73
71
59
74
72
59
76
73
56
77
74
55
78
76
55
78
76
55
79
76
57
75
73
58
78
75
66
84
82
83
95
94
102
105
104
120
109
109
133
108
107
138
107
104
131
96
89
105
89
81
92
79
71
82
72
64
75
72
62
71
71
61
69
69
60
65
64
58
60
67
58
59
63
58
55
60
55
52
60
55
51
61
56
50
62
57
51
62
58
49
61
57
48
56
51
45
57
52
46
59
54
48
60
57
50
61
58
51
62
59
52
63
60
53
64
60
51
69
62
52
71
63
52
73
63
51
76
64
48
79
63
48
80
63
47
80
63
45
76
63
44
60
59
39
58
61
42
62
60
47
65
58
48
66
55
51
67
53
52
63
52
50
60
50
49
63
55
52
68
57
53
75
56
50
82
49
44
92
43
39
111
46
44
139
58
57
164
69
67
165
58
48
162
50
36
159
49
32
164
57
39
165
60
41
158
55
36
158
56
34
162
60
38
164
59
38
170
61
41
177
65
45
182
65
47
185
62
46
187
60
45
190
58
45
190
60
47
178
60
46
172
60
46
169
57
43
169
57
43
171
60
43
173
62
45
173
62
45
173
60
44
178
63
45
176
56
40
185
62
46
203
76
61
203
71
58
188
54
42
186
50
38
201
57
46
208
54
44
212
54
43
212
56
44
214
58
46
216
58
47
220
56
46
227
54
47
232
51
44
240
48
45
244
46
43
244
46
43
240
49
41
231
50
39
221
54
38
212
56
34
210
55
33
221
56
36
220
53
35
218
57
39
218
62
47
211
61
46
200
54
41
197
57
44
203
65
55
201
60
53
205
62
56
217
69
65
229
75
73
234
74
74
233
67
67
236
63
65
235
67
67
222
67
65
213
68
63
205
66
61
194
61
52
182
58
46
174
56
42
171
58
42
172
59
43
172
56
41
192
68
56
212
78
69
221
73
69
224
64
64
232
62
65
247
66
73
255
72
80
244
65
71
244
64
73
249
67
79
253
71
84
254
69
85
250
64
85
251
63
88
254
65
95
255
68
102
255
67
105
255
67
107
255
70
112
255
73
119
255
75
123
255
76
124
255
74
122
255
66
116
255
68
116
254
72
113
250
78
116
252
91
124
255
98
128
250
87
118
236
69
99
248
77
109
254
86
119
218
64
92
203
71
94
218
121
132
187
121
123
103
64
59
151
123
112
178
140
129
215
177
166
245
216
208
253
234
228
250
240
239
250
245
249
249
243
253
244
236
251
251
233
255
255
233
255
255
234
255
255
237
255
255
242
255
255
246
255
255
251
255
255
254
252
255
253
250
255
252
248
255
249
247
255
247
245
254
244
245
252
242
243
250
239
243
248
239
242
240
231
234
229
223
225
211
207
206
190
189
185
173
174
168
162
165
158
157
163
153
157
163
151
163
171
158
157
165
150
153
161
148
47
46
44
47
46
44
48
47
45
49
48
44
50
49
45
51
50
46
53
52
47
54
53
48
56
56
48
57
57
49
59
59
51
61
61
53
63
63
53
65
65
55
66
66
56
67
67
57
72
69
60
74
70
59
75
72
57
77
74
57
80
75
56
79
77
56
81
76
56
79
76
59
76
74
62
81
80
76
94
93
98
109
108
122
122
122
146
129
130
161
128
130
168
128
128
166
123
117
151
114
107
138
101
95
123
89
83
109
84
76
100
77
69
90
71
64
80
66
60
72
68
63
70
64
59
63
60
56
55
59
54
51
59
54
48
60
56
47
61
57
46
59
56
47
58
55
48
57
57
49
59
59
51
62
59
50
63
61
49
64
60
49
63
59
47
65
59
45
70
63
47
72
63
46
75
63
47
77
64
47
79
63
48
79
63
47
80
62
48
76
63
47
66
61
42
63
62
44
64
61
46
65
58
48
65
58
50
64
57
51
62
57
53
60
57
52
61
56
52
70
56
53
80
52
49
88
44
43
107
41
43
135
48
54
161
55
65
174
61
65
167
54
46
165
55
40
165
58
40
170
63
45
168
63
44
160
57
38
161
56
35
164
59
38
165
58
38
171
60
41
179
62
44
185
64
47
189
62
47
193
61
48
197
61
49
196
62
50
189
63
51
182
61
50
178
57
46
174
56
42
175
57
43
176
58
44
178
58
44
179
57
42
182
59
44
185
59
45
196
66
52
203
69
57
200
62
51
190
49
39
192
50
40
205
57
47
214
53
43
216
52
42
218
54
45
219
55
45
222
55
46
225
54
44
230
51
44
236
49
42
243
48
44
244
46
43
244
47
41
239
48
40
228
50
38
219
52
36
210
54
32
208
53
31
215
50
30
218
48
31
218
52
36
220
60
46
214
61
47
208
57
46
205
61
50
210
68
58
196
53
45
197
54
48
202
59
53
216
68
64
226
77
73
229
75
73
222
67
63
214
61
56
211
64
57
206
65
56
199
62
52
190
60
47
181
58
43
176
56
42
175
57
43
177
59
45
172
52
38
189
63
51
209
72
64
217
72
67
221
66
64
229
64
68
243
70
76
253
78
85
240
63
69
240
63
69
246
66
77
254
72
85
254
72
87
252
66
87
250
64
88
252
65
94
255
65
99
255
67
102
255
70
109
255
73
114
255
77
121
255
79
124
255
76
123
255
72
121
255
63
113
255
74
122
255
79
122
252
73
112
246
77
110
252
87
117
252
87
117
243
79
106
239
75
102
236
78
103
206
62
85
220
98
113
251
156
164
211
141
141
104
57
51
113
75
64
166
119
109
210
163
155
247
210
202
255
231
226
255
242
241
255
251
254
255
250
255
249
241
254
250
237
255
252
237
255
255
237
255
255
240
255
255
242
253
255
246
254
254
250
251
255
251
250
255
252
249
255
251
247
255
249
247
255
248
246
255
245
246
254
244
245
254
243
247
253
242
246
246
235
239
236
227
228
218
213
210
201
198
193
186
185
180
179
180
172
177
180
169
176
182
170
183
191
180
175
183
172
169
177
166
49
48
44
49
48
44
48
47
43
48
47
43
49
48
44
50
49
44
52
51
46
53
52
47
55
55
47
56
56
48
57
57
49
59
59
51
61
61
51
63
63
53
65
65
55
68
65
56
73
69
58
76
70
58
78
72
58
80
74
58
82
75
57
82
77
58
83
76
58
81
75
61
79
74
68
86
84
85
102
100
113
119
120
141
134
134
168
141
144
185
143
147
195
143
144
198
142
139
192
135
131
182
123
119
169
112
109
154
105
100
141
95
91
126
85
80
110
78
74
97
75
71
88
69
66
77
63
61
66
59
55
56
57
53
50
57
54
47
56
53
44
56
53
44
54
55
49
55
56
51
56
57
49
59
59
49
62
60
47
63
60
43
66
61
42
67
60
41
71
63
44
74
63
43
75
64
44
76
65
47
77
64
48
77
63
50
76
64
52
76
62
51
77
63
52
76
62
53
73
58
51
67
56
50
63
56
50
60
57
52
56
59
52
55
60
53
57
60
51
64
57
49
72
51
46
90
49
47
119
55
56
149
62
68
167
61
71
169
56
60
170
59
48
170
60
43
171
64
46
172
65
47
169
64
45
165
60
41
166
59
39
171
62
42
167
56
36
173
58
39
181
60
43
188
61
46
193
60
45
197
59
46
202
60
48
202
61
51
197
63
54
190
62
51
186
58
47
183
55
44
183
55
44
184
56
43
186
56
43
186
56
43
187
55
43
199
65
53
205
67
56
197
56
46
193
49
38
198
51
41
203
55
45
207
53
43
219
52
43
223
51
41
225
52
45
226
54
44
227
53
44
231
53
43
236
49
42
240
47
42
245
47
44
246
47
42
242
47
41
236
48
39
226
50
35
217
52
33
208
53
31
207
52
30
214
49
30
217
47
30
216
50
34
216
54
41
215
57
45
212
58
48
214
63
54
216
69
61
214
70
62
205
64
55
201
60
53
203
64
57
214
75
68
217
78
71
210
69
62
197
59
49
197
61
49
194
60
48
192
60
47
186
59
44
183
57
42
180
58
43
182
60
45
185
63
48
181
57
45
191
60
50
201
64
58
210
67
63
219
67
66
227
68
72
237
74
79
246
79
86
236
63
69
237
62
67
242
66
76
251
73
85
255
74
89
253
70
88
250
67
89
251
67
93
251
64
95
251
66
100
255
69
108
255
74
115
255
78
122
255
79
124
255
76
121
255
72
118
255
68
114
255
78
124
255
80
122
251
72
111
245
74
108
253
86
116
254
87
115
243
81
105
231
71
95
224
72
93
226
88
104
255
147
158
255
178
184
239
155
155
140
72
69
108
47
42
166
106
98
206
149
142
243
198
192
255
225
220
255
240
239
255
251
253
255
253
255
251
246
253
252
246
255
253
245
255
255
245
255
255
246
255
255
248
255
255
250
254
255
252
251
255
252
249
255
251
247
255
250
247
255
248
247
255
247
246
255
245
247
255
244
246
255
243
247
255
243
247
253
241
243
243
233
234
228
220
218
214
209
205
205
202
195
201
201
191
202
202
192
202
205
194
201
208
200
190
200
191
182
192
183
54
53
49
52
51
46
51
50
46
49
48
43
49
48
43
50
49
44
51
50
45
52
51
46
54
54
46
55
55
47
56
56
48
58
58
50
60
60
50
62
62
52
64
64
54
66
64
52
74
68
56
77
69
56
79
72
56
82
75
59
84
75
58
84
77
59
83
76
60
82
75
65
81
76
73
90
87
94
108
105
124
127
126
157
140
143
184
150
153
204
153
158
216
154
158
222
153
152
218
148
145
212
140
138
201
134
133
191
129
126
181
120
118
167
110
107
150
102
101
135
93
90
119
86
84
105
77
75
89
68
65
74
62
60
63
59
58
56
59
55
52
55
54
50
53
53
53
52
53
55
54
54
52
56
55
50
60
58
46
63
60
43
67
62
42
70
64
42
71
63
42
72
64
43
73
65
44
73
66
48
73
65
52
72
66
54
72
65
57
72
63
58
76
61
58
76
58
58
71
55
55
64
54
53
58
54
53
53
55
52
49
56
49
47
57
48
55
60
53
55
52
43
67
48
41
93
56
50
126
66
65
149
71
71
157
63
64
158
52
52
167
59
47
170
60
43
170
63
45
170
63
45
169
62
44
169
62
42
172
63
43
176
63
45
171
56
37
178
57
40
185
58
43
191
57
45
195
57
44
200
58
46
205
58
48
207
60
52
203
62
53
198
59
52
194
57
49
193
56
46
193
56
46
194
57
47
194
56
46
195
54
44
197
55
45
209
67
57
208
64
55
195
48
38
195
44
35
208
55
47
214
60
52
211
50
42
224
51
44
228
50
40
231
50
43
232
51
42
234
51
43
237
50
43
240
47
42
243
45
42
246
47
42
245
46
41
241
48
41
236
49
40
225
52
36
218
53
34
209
54
32
208
53
31
216
55
35
217
51
35
214
51
36
213
51
38
213
53
41
212
56
44
212
60
49
214
63
54
236
89
81
225
82
74
211
72
65
204
67
59
202
69
60
204
73
63
200
69
59
192
62
49
188
58
42
188
59
40
188
58
42
185
58
41
183
58
40
184
58
43
188
62
48
191
65
51
193
65
54
193
60
51
197
60
54
207
65
61
215
69
69
221
71
72
228
73
77
235
76
80
232
66
70
234
64
67
240
67
73
249
75
84
254
78
89
252
74
90
252
70
92
252
70
95
248
65
95
249
66
97
253
69
105
255
74
112
255
78
119
255
79
122
255
78
120
255
76
118
254
77
119
255
79
118
252
77
116
250
79
115
255
87
121
255
95
123
252
85
111
233
71
94
243
86
107
231
83
99
240
100
113
255
144
154
254
137
145
236
130
134
194
95
98
165
77
76
179
104
101
206
141
137
240
187
183
255
219
215
255
238
237
255
247
247
255
250
253
251
249
254
253
250
255
252
251
255
253
250
255
255
250
255
255
250
254
255
251
252
253
252
250
255
251
248
255
251
247
255
251
248
255
250
249
255
250
249
255
247
249
255
245
247
254
242
246
252
240
242
249
237
239
242
230
230
231
221
219
223
216
210
219
215
206
221
219
207
224
222
210
223
225
214
221
228
220
207
216
211
197
206
201
60
57
52
59
56
49
56
53
48
53
50
43
52
49
42
52
49
42
53
50
43
54
51
44
55
52
45
56
53
46
57
54
47
59
56
49
61
58
49
63
60
51
65
62
53
67
63
52
74
66
55
77
67
55
80
71
56
83
74
59
87
75
59
85
76
61
85
75
63
83
74
67
83
74
75
93
87
99
110
108
130
130
130
166
145
149
197
154
160
218
159
166
234
162
169
241
161
165
239
158
160
235
153
155
229
149
152
221
148
149
214
144
145
202
137
137
189
130
131
177
122
121
161
114
113
145
102
99
126
88
86
107
80
78
92
73
71
82
70
67
76
67
66
74
63
63
75
61
61
71
59
58
64
59
57
58
60
57
52
63
59
47
67
61
45
69
64
44
70
64
42
70
65
43
69
66
47
69
67
52
67
67
55
66
67
59
65
67
62
65
65
65
69
59
67
71
61
70
71
64
72
71
68
75
70
71
76
67
72
75
63
72
71
62
71
66
67
72
65
65
61
52
75
58
48
102
70
59
128
80
70
138
74
64
140
63
53
146
56
45
164
62
47
169
62
44
171
62
42
172
63
43
173
64
44
175
64
45
176
63
45
177
60
42
177
56
39
182
56
41
189
57
42
195
57
44
200
56
45
204
56
46
210
57
49
212
59
53
207
58
52
205
58
51
203
56
49
203
56
49
204
57
50
205
58
50
204
55
48
203
54
47
211
60
53
213
62
53
210
57
49
204
50
40
206
52
42
216
59
50
218
60
51
215
51
42
228
49
42
233
49
39
236
49
42
238
50
41
240
49
41
242
47
41
244
45
40
246
44
40
247
45
41
244
47
41
240
49
41
233
51
40
226
53
39
217
54
37
210
55
33
207
55
32
212
57
37
213
56
37
211
54
37
209
52
37
210
52
40
211
55
43
210
56
46
208
55
47
227
78
71
232
88
80
231
90
83
218
81
73
204
71
62
196
68
57
191
64
55
185
62
47
185
60
42
185
60
38
185
60
40
185
58
39
184
57
40
186
59
42
192
62
48
195
65
52
201
68
59
198
64
55
201
64
58
209
70
67
213
73
72
214
72
71
216
71
74
223
73
75
230
70
72
232
66
68
237
68
73
245
76
83
252
81
90
251
79
93
251
76
93
251
75
96
246
68
94
247
68
97
249
70
102
253
73
108
255
78
117
255
79
120
255
79
120
255
80
119
246
85
119
242
84
117
244
84
118
250
89
120
255
93
123
255
92
119
247
83
107
236
74
95
246
89
106
239
87
102
227
79
93
239
94
107
218
78
89
223
86
96
222
86
96
210
89
96
191
99
100
199
128
124
230
169
166
255
208
207
255
233
231
255
241
240
255
246
247
254
252
253
251
252
255
249
253
255
248
252
255
250
251
253
250
250
252
249
249
249
250
249
247
252
248
245
255
250
246
255
250
247
255
251
250
255
250
249
255
247
247
254
242
242
252
237
240
249
234
237
240
225
228
233
221
221
228
217
213
226
217
210
230
223
213
235
231
219
241
237
225
241
241
229
236
243
236
220
231
227
208
219
215
64
61
54
61
58
51
58
55
48
55
52
45
53
50
43
53
50
43
53
50
43
54
51
44
55
52
45
55
52
45
57
54
47
59
56
49
61
58
49
63
60
51
64
61
52
66
62
51
74
66
55
77
67
55
82
70
56
86
74
58
87
75
59
88
76
60
87
75
63
84
73
67
83
74
77
94
88
102
112
109
136
130
132
171
145
151
203
156
164
227
163
173
245
167
176
253
170
177
255
166
172
250
161
167
243
160
164
238
159
163
234
157
161
225
153
155
214
149
150
204
146
146
196
137
136
180
121
121
159
106
105
139
96
93
124
88
85
112
82
80
104
79
77
101
78
74
99
75
71
94
70
66
83
64
61
72
63
58
62
63
58
54
64
60
49
65
62
47
67
64
47
67
66
48
66
67
51
66
68
55
63
68
61
62
68
64
60
69
68
59
66
72
65
67
82
71
72
92
80
81
99
89
91
106
95
97
110
99
99
109
99
98
104
101
95
97
102
88
87
95
74
69
106
72
63
130
83
73
146
88
76
143
76
60
142
65
49
150
63
46
167
66
48
174
64
47
174
65
45
175
64
45
176
65
46
180
65
47
178
61
43
176
55
38
181
55
40
188
56
43
194
56
43
200
56
45
205
54
45
209
55
47
213
56
49
216
59
52
210
55
50
208
55
50
208
55
50
209
56
50
212
57
52
212
57
52
211
56
51
211
54
47
222
65
58
213
56
47
209
51
42
214
56
47
222
61
53
222
61
53
218
57
49
220
53
44
231
50
41
235
48
39
239
48
40
241
48
41
244
47
41
247
45
41
247
44
40
247
44
40
247
45
41
244
47
41
238
50
41
233
52
41
225
53
39
218
55
38
212
55
36
207
56
35
206
55
34
207
56
37
206
55
36
207
54
38
208
55
41
210
57
43
209
55
43
206
52
42
203
52
43
227
78
71
243
100
92
235
96
89
213
79
70
196
65
55
186
59
50
181
57
45
184
62
41
184
62
39
185
60
38
185
59
37
186
57
38
189
57
42
192
60
47
196
62
50
202
68
57
202
65
57
206
69
63
213
75
72
211
76
73
206
71
68
205
69
69
214
72
71
227
73
73
230
67
68
232
69
72
242
77
83
248
83
90
249
82
92
247
79
94
250
79
97
244
72
94
245
71
96
248
71
100
251
73
105
255
78
114
255
81
120
255
82
121
255
84
122
239
84
116
239
90
119
249
96
126
253
96
125
249
86
113
242
78
103
245
81
105
253
90
109
251
93
108
255
102
116
235
80
94
243
88
102
238
83
97
246
90
104
237
79
94
223
83
94
190
92
93
188
114
111
216
152
150
249
198
195
255
226
225
255
235
233
254
242
242
255
255
253
252
255
255
251
255
255
248
255
255
249
255
253
249
253
252
250
252
249
252
251
247
253
249
246
253
248
244
255
248
245
255
249
248
255
249
248
255
245
245
251
239
239
247
232
235
243
229
229
235
221
221
232
218
217
228
217
211
232
222
213
240
232
221
249
243
229
255
250
236
254
254
242
244
250
246
227
237
236
214
224
223
66
62
53
65
61
52
64
60
51
62
58
49
60
56
47
58
54
45
56
52
43
55
51
42
54
50
41
54
50
41
55
51
42
56
52
43
59
54
48
62
57
51
65
60
54
69
62
54
73
64
55
77
67
55
82
70
56
85
73
57
87
74
58
88
75
59
90
76
65
91
78
72
84
73
77
88
82
96
103
100
129
124
126
167
141
146
202
151
160
227
158
170
246
165
176
255
170
179
255
171
178
255
172
180
255
172
178
252
170
177
249
170
175
243
168
172
236
169
171
232
159
161
218
153
154
210
141
141
193
129
127
177
116
114
161
104
101
146
93
90
135
88
82
126
88
81
122
89
81
118
86
79
110
77
72
95
67
63
77
63
58
64
63
59
58
66
63
56
62
62
52
64
66
53
66
69
58
63
68
61
60
66
62
58
67
66
62
70
72
65
75
84
70
85
106
85
101
126
101
113
137
108
114
136
119
117
139
131
119
139
132
111
126
128
95
106
137
91
101
144
88
91
157
87
87
164
87
81
162
78
68
154
65
51
151
58
40
158
59
40
167
60
42
172
61
42
174
63
44
177
64
46
179
64
45
180
63
45
182
61
44
185
59
44
186
54
39
190
54
40
196
54
42
203
55
45
209
55
45
214
56
47
218
57
49
218
56
51
215
56
52
216
57
53
217
58
54
216
57
51
216
54
49
216
54
49
218
56
51
221
58
51
230
67
60
224
61
54
218
55
48
227
64
55
254
91
82
255
108
99
255
93
84
231
63
54
229
51
41
237
50
41
241
50
42
244
47
41
244
42
38
245
40
37
248
40
38
248
43
40
247
45
43
243
48
44
237
50
43
227
49
39
218
48
35
212
49
34
212
52
36
211
58
40
204
59
38
202
60
40
205
60
41
207
60
42
210
58
44
210
57
43
210
56
44
209
55
45
218
64
56
207
56
49
220
71
65
244
99
94
229
88
81
203
66
60
197
64
59
183
52
42
183
58
38
182
60
36
185
60
38
186
60
38
189
58
40
191
58
43
196
60
48
198
62
50
199
62
52
204
67
59
217
80
74
223
88
82
212
79
74
195
63
58
192
63
58
207
72
68
222
72
71
226
71
69
230
74
75
239
80
84
241
82
87
239
78
86
237
75
88
241
76
92
249
82
102
248
79
102
247
75
101
247
74
102
252
77
108
255
81
115
255
82
118
253
82
118
245
88
119
239
89
118
243
89
117
251
92
120
255
93
120
255
91
115
254
88
110
252
86
106
255
101
117
247
89
103
253
95
109
252
94
108
234
73
88
235
73
88
251
84
101
233
88
101
198
102
103
173
103
101
203
142
139
232
181
178
249
209
207
255
234
231
254
238
238
252
248
247
246
250
249
247
255
253
247
255
254
246
255
252
248
254
252
253
255
252
253
252
248
251
247
244
255
250
246
255
250
246
255
247
244
253
243
241
255
244
244
255
243
243
246
232
232
231
217
217
228
214
213
223
209
206
225
214
208
241
231
222
252
244
231
253
247
231
255
251
232
255
255
243
249
254
248
231
240
239
218
227
226
67
63
54
66
62
51
65
61
52
63
59
50
61
57
48
59
55
46
58
54
45
57
53
44
55
51
42
55
51
42
55
51
42
56
52
43
58
53
47
61
56
50
64
59
53
68
61
53
72
63
54
76
66
54
81
69
55
84
72
56
86
73
56
87
74
57
89
75
62
90
77
69
85
74
78
87
81
95
101
98
127
121
123
164
138
143
201
148
157
226
155
166
245
163
174
254
171
180
255
172
181
255
174
182
255
175
183
255
176
183
255
175
182
252
175
180
248
175
178
245
169
170
235
162
163
227
152
152
216
140
138
201
128
127
187
117
113
174
105
101
160
99
94
152
96
87
142
96
86
136
93
84
127
86
80
116
77
73
98
70
67
84
66
65
73
65
65
67
60
60
58
65
66
61
70
69
65
67
68
63
65
63
64
68
66
69
79
77
82
88
86
99
102
103
131
116
116
150
130
125
157
138
125
155
149
124
154
160
124
150
165
113
136
164
96
117
172
89
107
180
87
98
185
81
88
181
73
71
173
64
57
168
60
47
168
61
41
169
63
41
167
56
37
168
55
37
171
56
38
174
57
40
176
56
39
178
57
40
181
55
40
184
54
40
187
51
37
194
52
40
202
55
45
211
58
50
217
60
51
222
61
53
223
60
53
223
60
53
225
62
57
223
60
55
223
60
55
225
62
57
229
66
61
232
67
61
231
66
60
230
65
59
229
64
58
230
66
57
228
64
55
220
56
47
213
49
40
216
54
43
235
73
62
255
91
79
241
64
54
238
54
44
234
43
35
238
41
35
248
45
41
255
47
45
254
44
43
246
40
40
250
48
46
245
52
47
239
54
49
231
54
46
222
51
41
214
50
38
212
50
37
207
54
38
203
58
39
200
59
39
202
60
40
205
58
40
207
57
42
208
55
41
209
53
41
208
52
40
211
57
47
207
53
45
212
61
54
229
80
74
229
84
81
219
76
72
205
66
63
183
49
40
188
59
40
187
61
38
189
60
39
190
59
39
190
57
40
192
56
40
194
58
44
196
58
47
200
62
52
203
66
56
213
79
70
220
87
80
210
82
73
195
68
61
191
67
59
204
72
67
219
76
72
223
74
70
229
77
76
235
83
82
237
84
87
235
80
86
233
78
86
236
79
90
247
86
102
248
85
104
249
83
105
250
82
107
252
82
109
255
83
113
255
83
116
254
83
117
241
76
108
245
84
115
255
92
121
255
96
123
255
91
117
253
85
110
250
82
105
249
84
101
249
87
102
242
84
98
249
93
104
249
93
104
236
82
94
241
87
99
247
93
105
217
86
94
182
96
95
153
92
87
185
127
125
218
169
165
239
199
197
254
224
222
251
233
233
253
248
245
248
250
247
247
255
253
247
255
254
245
255
252
248
254
252
253
255
252
254
253
251
252
248
245
253
250
245
254
249
245
252
247
243
252
244
241
255
245
244
255
244
244
245
233
233
234
220
219
229
215
212
222
209
203
223
213
204
239
229
219
252
245
229
255
249
231
255
252
232
255
255
239
248
253
246
232
241
238
221
227
225
69
65
54
69
65
53
67
63
52
65
61
50
63
59
48
61
57
46
60
56
45
59
55
44
56
52
43
56
52
43
55
51
42
56
52
43
58
53
47
61
56
50
63
58
52
67
60
52
71
62
53
74
66
55
78
68
56
82
70
54
84
71
54
87
71
55
89
73
60
89
74
67
87
75
75
88
80
93
98
94
121
115
117
156
133
138
194
144
153
220
152
164
240
160
171
250
170
179
254
173
181
254
175
183
255
177
185
255
179
186
255
180
187
255
181
185
255
181
185
255
175
178
249
172
172
244
163
163
235
153
151
224
142
141
211
131
128
199
120
117
188
115
109
179
107
96
162
105
93
155
99
90
145
93
87
133
86
82
119
78
75
104
69
68
86
64
64
76
56
53
60
65
60
64
75
65
66
76
64
66
81
62
66
90
69
74
112
87
93
130
99
115
152
115
149
165
122
165
175
127
167
178
123
162
184
118
154
192
112
147
194
99
131
192
84
110
193
71
94
207
76
92
207
73
82
195
61
62
183
54
48
180
58
43
178
61
41
173
61
39
175
60
41
176
59
41
178
58
41
180
59
42
183
60
44
187
60
45
192
60
45
196
60
46
196
54
42
203
56
46
211
60
49
217
63
53
223
65
56
225
62
55
224
60
51
223
58
52
220
55
49
217
52
46
216
51
45
222
57
51
230
65
59
236
69
63
233
66
60
228
61
53
227
60
52
219
52
44
216
52
43
219
55
45
212
50
39
204
44
32
214
54
42
233
71
58
245
75
62
242
64
54
239
52
45
240
45
41
246
44
42
250
44
44
252
44
44
248
42
44
240
42
41
236
47
43
232
50
46
227
53
46
220
52
43
216
49
40
213
49
39
209
51
39
205
55
40
203
58
41
205
58
40
208
58
41
210
59
42
211
58
42
212
56
43
212
56
44
209
52
43
214
60
52
213
58
53
213
60
55
229
79
78
237
89
87
217
69
69
194
51
47
192
59
44
191
60
40
192
59
42
191
58
41
193
57
41
192
56
40
192
56
42
192
56
44
198
61
51
199
65
54
207
74
65
216
85
75
210
82
71
198
71
62
192
68
58
199
71
62
214
75
68
216
73
67
221
76
73
228
83
80
229
83
84
226
80
83
226
77
83
229
78
87
242
88
100
246
90
104
253
91
112
254
90
114
252
86
110
250
82
108
251
81
108
255
81
112
248
70
106
255
79
116
255
89
123
255
90
121
255
86
113
252
84
107
254
87
107
255
92
109
250
89
104
247
91
104
248
98
109
242
98
107
231
96
103
234
105
110
229
106
109
197
94
95
160
93
87
126
77
70
155
108
102
195
151
148
223
185
182
241
211
209
244
224
223
255
245
244
249
249
247
249
255
253
247
255
255
245
255
252
248
254
252
253
255
254
255
252
253
252
248
247
252
249
244
252
249
242
250
247
242
251
246
242
255
247
244
255
246
244
249
238
236
239
228
226
230
219
215
220
209
203
221
211
201
238
228
216
253
246
228
255
251
232
255
253
231
255
255
236
248
251
242
233
239
235
224
229
225
73
69
57
72
68
56
70
66
54
68
64
52
66
62
51
64
60
49
63
59
48
62
58
47
58
54
45
57
53
44
57
53
44
57
53
44
58
53
47
60
55
49
62
57
51
64
59
53
69
62
54
73
64
55
76
68
55
79
70
53
82
69
52
85
69
53
87
71
56
87
73
62
88
77
75
88
78
87
94
90
113
112
112
150
128
134
186
140
150
212
149
161
233
157
169
245
168
176
249
169
177
249
173
179
253
176
183
255
177
183
255
178
185
255
180
184
255
180
184
255
175
177
252
171
173
248
165
167
242
159
158
234
151
150
226
140
139
215
130
129
205
125
121
197
117
109
182
112
103
170
102
96
158
95
90
144
89
88
132
81
81
117
71
71
99
65
62
81
61
53
68
72
56
66
84
61
67
96
64
69
110
67
74
128
79
85
150
93
100
170
103
120
189
108
141
201
110
153
203
111
152
201
105
143
198
97
131
199
88
120
198
73
103
196
59
85
197
51
72
216
65
80
223
69
79
208
60
60
195
56
49
194
64
50
186
65
44
175
59
36
173
54
34
173
53
36
174
53
36
179
53
38
182
55
40
188
56
43
195
59
45
200
60
47
211
64
54
215
64
55
218
64
54
220
63
54
222
61
53
222
57
51
221
54
46
219
52
44
211
46
40
210
45
39
211
46
40
216
51
45
225
58
52
227
60
52
225
56
49
220
53
45
220
53
45
208
44
34
205
41
31
217
55
44
226
66
54
220
63
48
208
52
37
203
46
31
233
70
55
243
73
60
253
72
63
252
60
55
244
45
42
241
35
37
245
39
41
251
47
50
241
45
46
235
47
46
228
50
46
222
51
44
218
49
42
215
48
42
214
47
41
211
48
39
211
55
42
210
57
43
214
58
43
216
60
45
218
62
47
219
63
48
220
64
51
220
64
51
210
54
42
230
76
66
229
74
69
206
53
48
223
71
70
243
90
92
223
73
75
213
65
63
200
60
47
196
59
43
195
58
42
195
58
42
194
58
42
194
58
42
193
57
43
190
57
42
194
60
48
194
62
50
200
70
57
207
79
66
206
80
68
198
72
60
191
67
55
193
67
55
206
72
63
209
70
63
213
74
69
219
80
75
221
82
79
217
77
76
218
75
77
220
75
80
235
88
96
243
91
103
252
97
113
255
97
117
250
88
109
245
79
103
245
77
103
253
78
107
255
76
112
255
79
116
255
83
115
255
82
110
250
84
108
252
90
111
255
99
117
255
105
120
255
102
117
250
98
111
244
98
109
236
101
108
224
103
108
212
106
106
200
106
104
177
104
97
141
94
84
104
70
60
130
93
85
172
135
129
206
171
169
229
199
197
239
218
217
255
243
243
251
247
246
249
253
252
248
255
255
245
255
254
248
254
254
255
255
255
255
251
255
254
248
248
251
250
245
250
250
242
249
248
243
253
250
245
255
251
247
255
250
247
253
243
241
247
236
232
235
224
218
223
213
204
222
212
202
237
230
214
254
247
228
255
254
232
255
254
229
255
254
233
247
249
238
234
239
232
228
231
224
78
72
58
77
71
57
76
70
56
74
68
54
72
66
54
70
64
52
68
61
51
67
60
50
63
56
48
62
55
47
61
54
46
61
54
46
61
54
48
62
55
49
64
57
51
63
58
52
67
62
56
69
65
56
75
67
56
78
69
54
81
68
51
82
69
50
86
70
54
86
72
59
90
77
71
86
77
82
91
87
104
108
107
139
126
130
177
138
147
206
146
159
227
154
166
238
166
174
247
169
175
249
173
176
253
175
179
253
176
179
255
176
180
254
176
179
255
175
179
253
171
174
251
168
171
248
164
167
244
160
162
239
154
156
233
147
149
226
138
140
217
134
133
209
131
127
201
123
117
187
109
107
172
100
101
158
95
97
146
89
91
132
82
81
115
78
72
98
87
70
89
95
67
81
109
66
76
129
69
77
152
77
84
172
85
93
188
91
98
202
90
104
214
84
112
223
83
118
222
84
117
215
80
110
208
74
99
203
67
89
198
57
74
194
47
63
211
57
69
225
68
77
228
72
75
215
63
60
205
62
54
203
71
58
194
73
52
182
63
41
183
62
43
184
61
45
186
60
45
189
62
47
195
63
48
202
66
52
208
68
55
213
69
58
217
69
57
218
66
55
219
62
53
217
59
50
217
54
45
217
53
44
218
51
43
215
51
42
212
49
42
215
52
45
218
55
46
221
57
48
223
56
48
222
55
47
221
54
46
220
53
44
214
47
38
217
53
43
215
55
43
207
50
35
205
49
34
210
57
41
213
62
45
212
59
41
216
60
45
230
68
55
248
74
65
253
68
63
249
54
52
245
42
45
246
42
45
247
47
50
247
54
55
237
53
53
226
51
48
218
49
44
215
48
42
215
47
44
217
48
45
217
48
43
227
56
49
228
57
49
226
58
49
226
59
50
224
60
48
223
61
48
220
63
48
218
62
49
209
55
43
239
87
76
248
97
90
211
59
54
209
59
58
230
80
81
220
70
72
230
80
81
207
60
52
202
58
47
198
56
44
197
57
44
196
58
45
196
60
46
195
59
45
192
59
44
190
58
45
190
60
46
195
65
51
199
72
57
199
73
59
194
68
54
190
64
50
188
62
48
201
70
60
202
69
60
206
73
66
213
80
73
215
82
77
213
78
75
211
75
75
215
76
79
233
90
96
240
94
104
252
102
114
255
104
120
252
93
113
242
80
101
243
77
101
252
79
107
255
83
117
255
82
116
254
81
111
244
82
106
243
92
111
250
103
119
253
105
119
249
101
115
253
101
116
245
94
109
239
94
107
236
106
114
221
115
117
192
107
102
168
101
92
156
110
97
127
101
86
89
71
57
110
86
74
150
123
114
190
159
156
223
193
193
238
214
214
254
238
239
254
245
246
252
252
252
249
255
255
246
255
254
249
253
254
255
255
255
255
251
255
254
248
250
253
254
248
250
253
244
250
251
245
254
253
248
255
255
250
255
253
249
255
250
244
255
246
241
242
233
226
229
221
210
225
217
204
240
233
215
255
250
228
255
255
230
255
254
229
255
254
233
246
248
234
237
240
231
232
233
225
81
75
61
80
74
60
79
73
59
77
71
57
75
69
57
73
67
55
71
64
54
71
64
54
66
59
51
65
58
50
64
57
49
63
56
48
63
56
50
63
56
50
65
58
52
64
59
53
66
63
56
69
66
57
75
69
57
77
70
54
80
69
51
82
69
50
85
70
51
85
72
55
89
76
67
85
75
76
90
83
99
105
105
133
124
129
171
137
147
200
147
158
222
154
164
235
166
172
248
169
172
251
172
173
253
173
176
255
174
177
255
174
177
254
173
176
253
173
176
253
169
172
249
167
170
247
164
168
242
162
166
240
159
163
237
154
158
232
149
153
227
146
148
222
146
146
218
137
138
205
124
128
192
115
120
178
111
117
169
109
112
157
105
103
142
106
95
125
121
95
120
130
87
106
142
80
95
163
79
92
187
83
92
205
84
93
214
80
87
218
72
83
224
65
83
228
65
84
224
69
85
217
69
81
210
69
77
204
65
70
199
59
60
198
54
54
227
74
76
229
73
74
224
66
65
211
56
52
206
57
50
206
65
55
200
68
53
190
63
46
195
69
54
195
69
54
197
70
55
201
69
56
206
70
56
209
71
58
215
71
60
218
71
61
214
62
51
213
59
49
214
56
47
213
52
44
214
51
42
216
52
43
219
52
44
218
54
45
219
56
49
221
60
52
224
61
52
223
60
51
221
57
48
220
53
45
220
53
44
219
55
45
210
46
36
221
59
48
218
61
46
205
49
34
198
45
29
206
55
38
210
61
41
205
58
38
204
57
39
212
56
41
226
58
47
243
61
57
255
64
64
255
60
62
254
51
55
243
44
47
234
44
46
225
43
42
216
42
41
212
43
40
215
47
44
222
52
52
228
56
56
233
57
57
239
55
53
240
55
52
238
55
51
232
55
49
227
55
45
220
54
42
215
53
40
209
53
38
205
53
40
231
80
69
255
109
99
219
72
65
200
52
50
215
67
67
211
62
66
234
86
86
217
65
60
210
62
52
203
56
46
198
54
43
198
58
45
198
60
47
195
62
47
193
61
46
187
57
43
187
60
45
190
63
48
192
65
50
191
65
50
189
64
46
188
61
46
187
60
45
195
67
54
196
68
57
200
72
63
208
80
71
211
82
76
208
79
73
210
78
76
213
79
78
233
94
99
238
97
103
251
105
116
255
110
122
255
100
117
245
86
106
245
81
105
254
86
111
255
86
115
255
86
113
245
87
110
238
93
110
241
109
120
247
117
127
240
106
115
227
87
98
243
95
109
245
94
109
241
96
111
241
114
125
238
135
138
206
128
124
169
117
104
151
122
106
117
106
88
82
77
58
97
85
71
132
112
103
177
149
146
221
191
191
239
213
216
253
233
235
254
244
245
252
250
251
250
254
255
248
254
254
252
253
255
255
254
255
255
250
255
255
249
253
255
255
251
248
254
244
250
253
246
255
255
250
255
255
250
255
255
248
255
252
246
255
253
245
250
241
232
238
230
219
234
226
213
245
240
221
255
251
229
255
254
229
255
254
226
255
255
231
249
250
234
242
244
231
238
240
229
83
77
61
82
76
60
81
75
61
79
73
59
77
71
57
75
69
55
74
68
56
73
67
55
69
62
52
68
61
51
66
59
51
65
58
50
64
57
51
65
58
52
65
57
54
64
59
55
65
64
59
67
67
59
73
69
57
77
71
55
79
71
50
82
70
48
85
70
49
85
72
53
87
75
63
82
72
70
88
82
94
106
103
130
125
129
167
137
147
196
146
158
218
152
162
231
164
167
246
166
167
250
171
169
252
171
172
254
172
173
255
173
174
254
172
174
251
172
174
251
168
172
246
166
170
244
164
168
241
163
167
240
161
168
238
160
167
237
157
164
232
155
162
230
153
158
226
146
154
219
139
147
210
134
143
202
131
141
194
132
136
183
131
129
168
136
121
154
143
109
136
154
99
122
169
88
105
187
81
95
207
77
89
224
73
82
232
65
73
232
62
65
228
62
62
222
65
60
216
69
61
210
74
60
202
75
60
194
71
53
191
68
50
195
65
49
226
86
73
223
72
63
217
60
55
216
54
51
218
56
53
216
59
54
210
57
52
200
56
48
192
58
46
190
60
46
191
59
46
192
58
46
194
56
43
195
55
42
198
52
39
198
50
38
206
52
42
208
51
42
209
51
40
212
51
41
214
51
42
217
53
44
221
54
46
220
56
47
222
61
51
219
61
50
220
59
49
219
56
47
219
55
45
218
54
44
216
52
42
215
51
41
213
51
38
211
51
37
208
52
37
207
54
38
210
61
41
209
62
42
202
57
36
192
50
28
200
58
38
202
52
37
213
49
39
231
53
49
250
60
60
255
61
64
255
55
61
247
51
55
234
45
49
225
45
46
217
45
45
215
47
46
220
52
51
227
57
58
233
59
61
240
57
61
246
50
54
248
48
51
244
48
49
237
49
47
228
49
42
222
50
40
213
49
37
207
51
36
201
49
36
209
61
49
250
106
95
228
84
76
200
57
53
208
64
63
203
58
61
228
82
83
228
74
72
220
67
61
209
58
51
201
54
46
198
56
46
197
59
48
194
60
48
191
61
47
186
59
42
188
63
45
189
64
46
188
63
45
186
59
42
186
59
40
189
59
43
190
60
44
188
62
48
188
62
50
192
68
56
200
76
66
204
80
72
202
78
70
204
76
73
208
78
76
232
97
101
236
97
102
248
104
114
255
111
123
255
104
120
247
90
107
247
85
106
253
91
114
249
91
114
244
95
115
233
98
113
227
107
116
234
125
128
242
133
136
234
118
121
222
92
100
246
101
116
255
107
124
247
98
117
238
106
119
249
144
149
239
161
159
196
148
138
163
140
124
106
105
85
73
81
60
88
85
70
117
103
92
165
140
136
220
190
190
242
213
217
249
227
230
255
242
245
255
249
251
252
253
255
249
253
255
252
253
255
255
253
255
255
251
255
255
248
253
254
255
251
247
253
243
247
253
243
253
255
247
255
255
248
255
253
246
255
251
245
255
255
246
255
250
240
246
240
226
242
236
220
251
246
226
255
252
230
254
252
227
254
253
225
255
255
230
253
254
236
248
249
235
246
246
234
86
79
63
84
78
62
83
75
62
80
74
60
79
71
58
76
70
56
76
68
55
74
68
56
72
64
53
70
64
52
69
60
51
66
59
49
66
57
50
65
58
50
67
58
51
65
60
54
67
64
57
68
68
58
74
70
58
77
71
55
79
71
50
81
70
48
84
72
50
85
72
53
85
73
61
81
71
69
87
81
93
106
104
128
126
130
168
140
148
197
147
157
216
153
162
231
159
160
242
164
160
247
166
163
250
169
166
253
169
169
255
169
171
254
169
172
253
169
172
251
165
171
245
163
170
242
163
167
238
162
167
235
164
169
237
165
170
238
165
168
237
163
167
238
152
161
230
150
161
227
148
159
221
148
159
213
149
155
203
152
150
190
158
141
175
168
132
160
164
102
127
178
93
114
193
82
99
206
74
88
218
66
78
228
62
72
235
60
67
235
61
62
231
64
58
223
67
55
216
70
55
209
74
55
199
74
52
191
71
47
189
67
44
193
64
45
225
83
69
221
68
60
221
57
55
230
62
61
236
68
67
230
64
64
219
56
57
207
53
51
205
67
57
200
68
56
201
67
56
199
65
53
198
62
50
195
57
44
194
54
41
194
50
39
198
50
40
203
51
40
206
52
42
211
53
44
216
53
46
217
52
46
219
52
46
219
52
44
228
66
55
222
60
49
218
54
44
218
54
44
223
57
45
223
57
45
219
53
41
213
49
37
216
56
40
206
49
32
202
49
33
208
57
40
206
56
39
199
52
34
202
55
37
212
67
48
200
55
38
206
56
42
215
54
44
222
53
46
230
51
47
238
53
51
246
58
57
251
63
62
255
76
75
250
72
70
242
66
66
236
62
61
234
60
61
231
55
58
229
50
54
230
44
49
244
43
49
248
41
47
244
44
46
239
47
46
232
49
43
224
52
42
217
53
41
211
55
40
198
48
33
192
46
31
244
100
89
235
93
83
207
64
58
212
68
67
203
59
59
223
77
77
234
82
81
224
72
69
211
59
54
201
54
47
198
56
46
198
57
47
195
59
45
190
58
43
190
60
44
193
64
45
192
65
46
187
60
41
183
56
37
184
57
38
189
60
41
190
63
46
181
55
41
179
55
43
184
62
49
193
71
58
197
75
64
197
73
65
200
73
66
203
74
69
229
97
95
230
94
96
241
100
106
254
110
119
254
107
117
243
93
105
240
88
101
246
94
109
248
97
116
241
100
116
229
104
112
223
110
114
233
127
127
249
141
139
247
131
132
239
110
115
253
112
121
255
120
134
245
97
113
223
88
102
246
136
145
255
180
180
211
173
162
164
153
135
96
104
80
69
81
57
82
84
63
107
97
85
156
136
129
220
192
191
244
215
217
246
224
227
255
241
244
255
248
251
253
253
255
251
252
255
252
253
255
255
253
255
255
252
255
254
249
253
250
255
249
242
254
240
242
252
241
249
255
244
253
255
247
250
252
241
254
251
242
255
255
244
255
255
243
252
246
232
248
242
226
254
249
229
255
253
231
253
251
226
253
252
224
255
255
233
255
255
241
252
252
240
251
251
239
88
79
64
87
80
64
87
78
63
85
78
62
84
75
60
82
75
59
82
73
58
80
72
59
77
67
55
75
67
54
75
65
53
72
64
51
72
62
50
71
63
50
72
62
50
70
64
52
71
67
58
71
69
57
74
71
56
78
73
54
82
74
53
85
74
52
85
74
52
86
75
57
88
78
68
85
76
77
90
84
98
107
104
131
123
126
167
137
143
195
145
153
215
152
159
231
166
162
246
167
161
249
165
160
252
164
161
252
162
163
255
163
167
255
164
170
255
164
173
252
161
170
245
161
169
241
161
168
236
164
169
237
167
170
239
172
170
243
174
170
246
171
170
246
161
168
246
156
166
238
149
160
222
152
159
211
166
164
203
180
160
188
180
135
155
179
105
120
186
78
93
205
70
84
219
65
77
227
61
73
228
61
71
227
62
69
223
64
69
223
64
68
226
61
65
221
61
61
219
70
66
200
59
52
202
68
59
202
69
60
186
49
41
219
74
69
230
75
73
233
67
69
235
62
66
232
59
63
230
60
63
225
61
62
219
59
59
212
60
57
207
66
59
200
63
55
202
68
59
209
77
65
205
75
62
191
61
48
184
54
41
189
57
44
191
55
43
195
54
44
200
53
45
203
50
42
209
47
42
212
47
43
219
50
47
224
53
46
231
65
53
224
56
43
219
49
36
222
50
38
230
57
43
231
59
45
225
55
38
215
50
31
209
50
31
206
51
31
203
52
33
201
51
34
201
51
34
203
51
37
206
53
39
207
53
41
202
48
38
207
50
41
209
53
41
210
54
42
210
52
40
213
51
38
215
51
39
220
52
41
224
50
41
230
51
46
235
53
50
235
51
51
233
47
50
231
45
50
232
45
52
237
46
53
246
45
51
246
43
47
239
43
44
236
47
45
234
53
46
227
57
44
217
55
40
208
52
37
195
45
28
196
51
34
231
88
72
233
89
78
203
59
51
223
78
73
197
52
49
218
72
72
225
79
80
220
74
74
212
67
64
204
59
54
200
56
48
198
56
44
199
57
43
200
60
43
197
60
42
194
59
40
190
57
38
188
59
40
187
60
41
186
61
41
186
61
43
183
60
44
180
56
44
181
59
48
185
61
49
187
63
51
187
63
51
190
64
52
192
64
53
196
65
55
239
106
97
245
112
105
234
99
95
244
109
106
251
113
113
236
97
100
244
103
109
255
112
121
255
107
122
251
104
120
234
103
111
226
101
105
229
103
106
237
109
110
243
110
111
245
109
111
255
119
123
255
126
133
241
95
108
241
105
119
214
103
112
248
174
175
189
167
154
133
141
117
97
112
81
74
89
58
77
81
56
115
110
90
165
149
136
205
182
176
237
213
211
255
238
240
255
244
245
255
245
249
252
247
251
251
251
253
253
254
255
254
255
255
255
254
255
251
255
254
239
255
243
229
252
232
231
249
233
240
254
237
248
255
245
252
255
244
253
255
242
255
253
240
255
253
241
255
251
238
255
250
234
255
250
230
254
249
227
252
250
225
252
250
225
251
250
229
253
253
241
255
255
250
255
255
250
89
80
65
88
79
64
88
79
64
86
77
62
85
76
61
84
75
60
83
74
59
82
73
58
78
69
54
77
68
53
76
67
52
75
66
51
74
65
50
74
65
50
74
65
50
73
65
52
71
67
56
72
68
57
76
70
56
79
72
54
82
74
53
83
75
52
86
75
55
84
75
58
88
78
69
85
76
77
89
85
99
107
104
131
123
125
166
134
140
192
141
149
211
150
154
225
161
156
236
163
155
241
161
157
244
161
158
249
160
161
253
160
165
255
162
170
255
161
172
251
160
172
244
160
171
237
158
167
232
161
166
232
164
164
234
166
164
237
166
162
238
163
160
237
163
166
243
160
164
235
160
160
220
165
155
205
177
153
189
188
143
166
189
115
130
191
87
98
204
70
77
220
63
70
230
59
67
233
58
65
230
59
67
225
62
67
219
64
68
218
65
70
224
60
67
218
57
63
218
65
68
203
59
59
204
66
63
202
64
61
193
51
49
229
79
80
230
67
70
232
63
68
234
59
66
231
56
63
226
56
59
219
56
57
213
58
56
207
59
55
212
69
63
198
61
53
197
60
52
204
71
62
204
76
65
193
67
53
184
58
44
183
57
43
185
55
42
191
55
43
198
54
45
203
52
45
209
50
46
216
51
47
225
53
51
228
56
52
229
61
50
226
58
45
225
53
41
225
52
38
228
52
39
226
53
39
221
51
34
213
50
31
211
56
36
205
56
34
201
56
37
200
58
38
204
58
43
206
58
44
207
55
44
208
51
42
213
51
46
214
53
45
211
55
43
207
55
41
201
54
38
199
52
34
203
52
33
207
52
34
216
50
36
222
50
38
227
48
43
230
46
44
230
44
45
230
44
47
230
45
50
232
46
51
236
43
46
237
44
45
236
48
46
232
50
46
224
52
42
219
53
39
213
56
39
210
57
39
213
63
46
187
42
25
214
68
53
208
62
49
217
70
62
235
86
80
218
66
63
216
66
65
231
85
86
226
82
82
220
75
72
212
67
62
205
61
53
201
57
46
200
57
43
199
56
40
200
59
42
196
59
40
191
58
39
189
60
41
189
62
43
186
63
45
185
64
47
182
62
46
178
58
44
180
59
48
183
61
48
184
62
49
186
63
48
189
63
49
193
66
51
197
67
53
235
105
92
242
109
100
231
98
91
240
107
102
245
111
108
232
98
97
241
107
108
254
115
120
255
111
126
255
110
124
242
107
114
228
97
102
228
92
96
239
99
102
253
110
112
255
119
120
253
117
119
255
121
126
235
89
100
237
99
112
215
99
110
242
168
169
178
162
147
118
134
107
94
114
79
78
95
59
83
90
59
117
115
92
169
157
141
214
196
184
246
225
220
255
245
241
255
248
248
255
248
249
253
249
250
251
251
251
250
254
253
251
255
254
250
254
255
246
255
250
237
255
242
227
255
232
227
251
229
235
254
234
244
255
240
249
255
241
252
255
241
255
255
240
255
253
239
255
253
237
255
251
233
254
247
229
249
244
224
248
243
221
250
245
223
250
247
228
249
249
239
251
252
246
254
253
249
92
80
66
91
79
65
91
79
65
89
77
63
88
76
62
87
75
61
86
74
60
86
74
60
83
71
57
82
70
56
81
69
55
79
67
53
79
67
53
79
67
53
79
67
53
77
67
55
74
67
57
74
70
59
77
71
57
80
73
55
83
75
54
86
75
53
86
75
55
84
75
58
87
77
67
86
77
78
90
87
98
107
105
129
121
123
161
133
137
185
139
145
203
146
150
214
153
152
222
155
151
227
155
152
233
155
156
238
157
159
244
158
163
247
160
167
248
160
169
244
160
170
239
157
168
232
155
165
227
156
162
224
156
160
224
157
158
225
156
154
227
155
153
226
153
154
221
158
152
214
168
147
204
178
140
189
191
131
169
198
115
145
197
91
111
194
70
81
209
68
76
218
65
68
223
63
65
225
62
63
225
62
65
225
62
67
223
62
68
223
62
70
224
60
67
215
56
61
213
63
65
207
65
64
202
67
63
195
60
56
196
57
54
233
87
87
224
66
67
228
62
66
232
59
63
227
57
60
220
54
56
214
54
54
209
57
54
204
59
54
213
72
65
196
59
49
190
53
43
197
65
53
205
75
62
198
72
58
187
61
47
183
56
41
186
54
41
191
55
41
199
55
44
207
54
46
215
53
48
222
55
49
230
57
53
233
61
57
217
50
42
218
51
42
221
53
42
223
52
42
223
52
42
222
54
41
220
57
42
217
62
44
198
51
33
192
51
31
187
52
33
186
53
34
191
55
41
191
55
41
191
50
40
193
44
37
227
64
59
229
62
56
222
61
51
213
60
46
204
57
41
200
55
36
200
55
34
204
55
35
210
55
37
213
51
36
215
47
38
217
46
39
219
46
42
220
46
45
219
47
47
220
46
47
219
44
41
223
48
43
224
53
45
220
53
44
213
51
38
208
52
37
208
57
38
209
62
42
212
65
47
194
49
32
231
83
69
210
59
48
218
65
59
222
67
63
232
74
73
240
84
85
237
85
84
231
83
81
227
79
77
221
74
67
214
67
59
207
60
50
202
56
43
197
54
38
196
55
38
192
55
37
188
55
38
185
58
39
185
60
42
183
62
45
181
61
44
178
61
44
176
58
44
177
59
47
180
60
46
181
61
47
183
61
46
185
63
48
190
67
52
195
69
55
220
92
81
234
103
93
228
97
89
240
108
103
246
112
109
231
97
96
236
102
103
245
106
113
228
80
94
240
94
107
240
106
113
235
104
109
235
99
103
242
102
105
254
111
113
255
117
119
253
114
117
252
111
117
231
81
93
232
90
104
217
97
109
237
158
161
170
152
138
108
122
96
89
109
72
79
100
61
87
96
65
119
119
93
173
165
146
225
209
194
255
238
230
255
248
241
255
251
247
255
252
248
252
253
248
250
255
251
251
255
253
251
255
253
249
255
251
244
255
246
233
255
237
222
252
226
220
246
221
225
247
224
233
251
229
239
251
231
245
252
234
252
255
236
253
250
233
255
250
234
255
248
230
249
242
224
241
234
216
239
232
213
243
236
217
247
242
223
251
248
239
252
252
244
255
254
249
92
80
66
92
80
66
91
79
65
90
78
64
89
77
63
89
77
63
88
76
62
88
76
62
85
73
59
84
72
58
83
71
57
82
70
56
82
70
56
82
70
56
82
70
56
80
70
58
76
69
59
75
71
60
79
73
59
82
75
57
84
76
55
87
76
54
87
76
56
87
75
59
86
76
66
85
77
75
92
86
96
106
104
126
121
121
155
130
133
178
138
141
194
143
148
206
148
148
210
149
148
214
149
149
219
150
152
227
152
155
232
152
158
236
155
161
237
155
163
235
156
165
232
152
162
223
150
159
218
149
155
213
147
153
213
147
151
214
146
147
214
145
144
210
136
131
189
148
130
182
168
124
175
184
115
162
199
102
143
207
88
120
210
72
97
208
62
75
217
68
74
218
68
67
218
69
65
220
68
65
222
66
67
225
64
69
229
62
70
229
62
70
227
63
70
214
58
62
209
63
64
209
69
68
198
65
60
186
54
49
197
62
58
234
90
89
221
65
66
227
64
67
230
61
64
227
58
61
219
56
57
211
57
55
208
60
56
206
65
56
207
69
59
194
58
46
187
50
40
192
60
47
199
69
55
196
69
54
189
62
47
184
57
40
189
56
41
194
57
41
203
56
46
210
58
47
217
56
48
224
57
49
232
59
55
235
62
56
217
48
43
218
49
42
222
51
44
224
51
44
225
52
45
225
54
46
220
56
46
217
59
47
202
54
40
199
58
41
195
59
43
192
59
44
192
58
46
195
58
48
198
59
54
208
56
53
236
67
64
237
64
60
228
64
55
217
61
49
207
57
42
199
57
37
200
58
38
201
59
37
207
60
40
208
55
39
207
49
37
209
49
37
212
49
40
214
51
44
212
49
44
209
47
42
211
50
42
211
53
44
211
55
43
208
55
41
205
53
39
202
55
37
204
59
40
204
62
42
204
59
40
207
60
44
244
94
80
224
70
60
210
51
47
210
46
44
230
64
64
251
87
88
255
117
115
255
114
111
255
106
103
246
94
89
231
80
71
213
65
55
199
53
40
189
46
32
196
56
39
192
56
40
187
56
38
184
59
41
183
60
44
181
61
45
178
61
44
176
60
45
175
59
46
173
60
46
176
60
47
176
60
45
177
59
45
182
62
46
188
66
51
192
70
55
203
77
65
223
96
87
226
97
91
244
112
108
248
116
114
230
96
97
228
93
97
232
93
100
218
70
84
234
88
101
239
105
114
236
107
111
240
104
108
249
109
112
255
113
118
255
115
118
254
111
115
248
103
110
229
74
88
229
82
98
218
93
107
230
148
150
168
145
131
106
116
89
85
106
67
83
104
63
92
103
69
121
124
95
176
170
148
231
220
202
255
245
232
255
249
240
255
249
239
252
252
242
251
254
245
250
255
249
249
255
250
246
255
248
242
252
243
234
251
235
219
247
222
208
241
210
205
234
204
209
235
206
217
237
210
223
239
213
233
243
219
242
247
225
244
246
225
249
246
227
251
246
227
244
237
219
234
227
209
231
222
205
235
226
209
240
233
217
251
244
234
251
246
240
254
249
243
92
80
66
92
80
66
91
79
65
91
79
65
90
78
64
89
77
63
89
77
63
89
77
63
87
75
61
86
74
60
85
73
59
85
73
59
84
72
58
85
73
59
85
73
59
83
73
61
79
71
60
79
73
61
81
73
60
83
76
58
85
77
56
88
77
55
89
76
57
88
76
60
85
75
65
85
75
73
92
85
93
104
101
120
117
116
147
124
126
165
133
135
183
140
143
194
141
144
195
142
145
200
142
146
207
144
148
212
146
149
220
147
151
224
148
152
223
147
154
222
149
157
220
146
155
214
142
148
206
137
144
199
133
139
197
129
135
195
126
130
193
129
125
184
128
115
159
145
110
150
170
102
143
193
92
134
214
78
118
225
65
101
229
59
85
230
59
75
226
67
71
220
71
67
216
73
65
216
73
65
220
71
67
227
66
71
236
62
72
237
61
72
230
66
73
215
62
64
208
64
63
208
73
69
191
62
56
179
50
44
200
67
60
224
85
80
220
68
67
225
65
67
227
63
64
225
62
63
218
63
61
212
64
60
209
68
59
207
70
60
199
61
50
192
56
42
189
53
41
189
57
42
191
61
45
191
61
45
189
59
43
189
60
41
194
58
42
199
59
42
207
59
47
213
59
47
219
56
47
224
56
47
231
56
51
235
57
53
231
56
53
230
52
50
229
49
48
232
50
49
233
51
50
231
52
48
222
49
43
214
47
39
205
48
39
205
54
45
201
57
48
197
54
46
194
51
45
196
51
48
206
57
59
222
62
64
235
56
59
237
55
54
228
55
49
216
53
44
204
52
38
198
53
34
196
55
35
197
58
37
203
61
41
203
56
38
204
52
38
206
53
39
212
56
44
212
58
48
211
57
47
208
56
45
208
62
49
202
59
45
197
56
39
196
55
37
198
57
39
199
60
41
199
60
41
199
58
40
205
62
45
205
58
42
216
62
50
225
67
58
212
47
45
228
58
58
224
50
52
223
53
54
215
53
51
214
59
54
222
67
62
229
75
67
234
81
73
234
83
72
232
84
72
228
85
71
203
63
48
196
63
46
190
60
44
184
61
45
183
62
45
180
63
46
176
63
47
174
62
48
172
60
46
172
60
46
172
60
46
172
61
44
173
60
44
177
61
46
183
65
51
186
68
54
198
76
65
222
98
88
226
99
93
240
110
108
244
114
114
229
96
99
229
96
101
232
95
103
251
105
118
254
109
122
241
110
118
229
100
104
234
98
102
252
109
113
255
117
123
255
118
124
254
107
115
245
95
106
232
71
87
228
75
93
221
90
104
226
137
141
172
142
131
115
121
93
93
114
73
94
117
73
104
119
80
129
136
102
179
178
150
232
226
204
255
250
233
255
253
237
254
252
237
251
255
240
249
255
242
247
255
243
244
255
241
237
251
234
227
241
224
218
237
215
201
232
200
191
227
189
187
221
186
192
222
188
200
224
192
207
226
196
217
230
204
227
235
211
235
239
216
243
242
222
247
242
223
242
235
217
234
225
210
231
219
205
233
221
207
238
226
214
241
231
222
240
231
224
242
233
226
91
79
65
91
79
65
91
79
65
90
78
64
90
78
64
89
77
63
89
77
63
89
77
63
88
76
62
88
76
62
87
75
61
86
74
60
86
74
60
87
75
61
87
75
61
88
76
62
81
73
62
81
73
62
83
75
62
85
76
59
88
77
57
88
77
55
89
76
57
88
77
59
87
75
63
85
76
71
89
83
87
101
94
110
110
106
131
117
116
150
126
125
165
131
134
177
133
138
180
132
139
185
135
141
193
137
142
200
139
143
207
140
143
212
141
144
213
141
144
211
144
148
211
139
144
202
131
136
192
126
129
184
120
123
178
114
116
173
108
110
171
111
104
156
131
105
140
148
98
123
175
88
120
203
79
113
227
65
102
239
56
87
243
53
78
244
58
72
235
66
69
225
72
66
217
76
66
215
77
66
222
73
69
229
69
71
240
63
73
240
62
74
228
67
72
215
67
67
204
65
62
206
73
68
186
59
52
176
52
44
201
73
64
213
76
70
216
66
65
218
63
61
219
59
59
218
60
59
216
64
61
212
69
63
205
71
60
203
71
58
192
56
42
193
56
40
192
56
42
190
57
40
188
57
39
187
56
38
189
58
40
192
59
40
196
59
41
202
61
44
210
60
46
215
59
46
220
56
46
225
54
46
228
54
47
232
53
48
234
50
50
234
46
47
234
43
48
240
47
50
246
51
55
247
54
55
240
52
53
230
50
49
219
46
42
216
51
47
215
53
50
210
50
50
209
46
49
211
46
52
220
53
61
235
58
68
237
48
54
238
48
50
230
51
47
220
51
44
209
53
41
202
55
39
200
57
40
201
60
40
198
56
36
201
54
36
206
54
40
211
58
44
218
62
50
221
65
53
221
65
53
216
65
54
211
71
56
201
65
49
194
59
40
192
57
38
195
60
41
198
61
42
197
60
42
197
56
38
204
58
43
205
55
41
205
48
39
230
67
60
219
50
47
239
63
65
227
47
50
218
42
44
213
49
47
210
53
46
213
56
49
214
57
48
210
56
46
204
53
42
198
50
38
192
49
35
206
69
53
198
65
50
188
61
46
181
58
42
177
57
41
175
57
43
171
58
44
170
58
44
171
59
47
171
61
48
172
62
47
172
60
46
171
60
43
173
60
44
178
62
47
181
65
50
201
83
71
224
103
94
224
99
95
234
106
105
238
110
111
230
99
104
240
106
113
246
110
120
255
110
123
255
114
126
241
112
117
231
102
107
240
103
110
255
116
122
255
117
126
255
110
117
253
103
112
244
90
102
237
72
89
229
72
91
222
87
102
220
128
131
180
146
134
133
136
107
113
134
93
111
137
90
121
138
96
141
151
114
182
183
152
223
222
194
249
244
222
252
250
229
255
255
239
250
255
238
244
255
238
238
255
234
230
249
227
220
239
217
208
227
205
197
222
193
178
213
173
172
211
167
172
209
168
178
210
171
185
213
175
190
214
178
201
218
186
210
223
193
224
230
204
232
234
212
240
237
218
241
234
216
236
227
212
233
221
209
232
218
207
234
220
211
232
219
211
230
217
211
229
216
210
91
78
62
91
78
62
91
78
62
91
78
62
90
77
61
90
77
61
90
77
61
90
77
61
89
76
60
89
76
60
88
75
59
88
75
59
88
75
59
89
76
60
89
76
60
89
77
63
83
73
63
84
74
62
85
76
61
88
76
60
89
78
58
90
77
58
90
77
58
89
78
60
89
77
63
85
76
69
88
79
82
95
87
98
100
95
115
106
102
129
115
111
144
121
121
157
122
128
164
122
130
169
127
131
178
130
133
188
133
134
198
135
136
203
136
137
204
136
137
202
136
136
200
131
131
191
125
124
181
121
118
173
115
112
167
110
107
164
105
101
160
111
95
142
132
94
117
151
85
97
179
78
96
210
74
98
236
64
90
246
54
79
251
52
71
252
58
69
240
64
64
231
70
62
222
74
64
219
75
66
224
72
69
231
68
71
240
63
73
239
63
74
221
65
68
212
70
68
200
65
61
201
72
66
180
58
47
177
55
44
206
79
70
201
66
60
206
61
56
208
54
52
207
49
48
208
53
49
211
62
56
207
69
59
200
70
57
195
68
53
190
57
40
195
58
40
194
58
42
191
58
39
189
56
37
190
57
38
191
58
39
193
58
38
196
57
38
203
58
41
211
58
44
217
57
43
221
55
43
225
53
43
228
51
43
234
51
45
235
42
43
241
42
45
248
45
51
253
46
52
255
49
55
255
50
55
255
51
56
253
53
56
250
55
59
246
55
60
243
57
62
243
56
65
246
56
68
247
55
70
252
53
72
254
53
69
246
42
53
244
45
50
237
49
50
227
52
47
217
54
45
208
56
42
206
59
43
206
61
44
203
53
38
209
56
42
217
59
47
223
61
50
226
62
52
228
61
52
229
62
53
223
65
54
217
74
60
207
71
55
201
65
49
196
60
44
194
57
41
194
57
39
198
56
42
200
57
41
201
50
39
211
57
47
225
64
56
235
68
62
222
48
47
224
46
46
235
52
54
232
54
54
216
51
45
213
55
46
215
57
48
215
58
49
213
59
49
210
59
48
205
58
48
201
59
47
214
76
63
204
70
58
191
64
49
180
57
42
176
54
41
172
54
40
171
55
42
169
56
42
172
59
45
173
61
47
174
62
48
173
61
47
171
60
43
171
60
43
173
60
46
177
61
48
192
75
65
218
100
90
222
99
94
232
107
105
240
111
115
234
105
110
245
114
122
253
117
127
241
101
112
248
111
121
245
116
121
237
111
115
248
111
118
255
120
127
255
116
125
255
101
111
252
98
110
242
84
98
241
72
91
231
68
89
222
81
97
217
118
123
189
149
137
152
153
122
129
150
107
123
151
102
130
149
104
145
157
117
172
178
142
203
204
173
224
224
196
233
236
209
236
244
220
227
241
215
215
235
207
205
228
199
196
221
191
187
212
182
180
203
174
172
199
164
156
193
149
152
194
144
156
195
148
163
201
154
170
203
158
175
203
162
184
206
168
194
211
177
206
217
187
216
222
196
228
227
206
235
229
213
236
226
214
233
219
210
229
212
205
226
209
202
223
205
201
218
203
198
213
198
193
91
78
62
90
77
61
90
77
61
90
77
61
90
77
61
90
77
61
89
76
60
89
76
60
90
77
61
89
76
60
89
76
60
88
75
59
88
75
59
89
76
60
90
77
61
90
76
63
86
74
62
86
74
62
88
76
62
89
78
60
90
77
60
90
77
58
90
77
58
90
77
60
91
79
65
87
77
68
87
77
76
92
82
90
94
88
102
98
93
113
106
102
127
113
113
141
116
119
150
116
122
156
120
125
167
125
128
181
129
129
191
132
131
197
133
132
200
135
132
199
130
125
191
127
121
183
125
116
173
122
111
167
122
109
165
120
105
162
116
101
160
124
95
141
133
79
93
152
72
75
178
69
75
211
71
82
237
66
82
246
58
73
252
54
67
253
58
64
244
62
61
235
66
59
228
69
63
225
70
65
227
69
66
230
67
70
237
63
72
234
64
73
214
64
65
209
71
68
197
65
60
195
71
63
178
57
46
179
58
47
209
85
75
195
62
53
201
56
51
199
47
44
197
42
38
200
47
42
205
58
51
204
67
57
195
67
54
188
62
47
192
59
42
196
59
41
195
60
41
193
58
39
194
59
40
195
60
41
194
59
39
195
56
37
196
53
36
202
55
37
212
56
41
217
55
42
221
53
42
225
51
42
229
51
41
234
50
42
245
51
51
254
54
56
255
56
60
255
52
58
255
45
50
255
40
45
255
40
45
254
43
49
254
45
51
246
41
48
242
38
49
247
41
54
255
44
63
255
42
65
252
32
57
244
25
47
246
34
48
243
40
46
237
44
49
230
48
47
218
49
44
212
51
43
209
53
41
208
54
42
212
54
43
221
57
48
230
61
54
235
62
55
235
58
52
232
55
49
232
53
48
226
57
50
223
72
61
215
75
60
211
69
55
203
61
47
196
54
40
194
51
37
200
54
41
207
56
45
206
52
42
212
54
45
230
67
60
223
56
50
224
50
49
210
32
32
236
56
57
229
54
51
213
49
40
210
52
41
210
52
41
207
53
41
206
54
43
202
54
42
200
53
43
197
55
43
227
89
78
216
82
70
202
72
59
190
64
50
183
59
47
181
59
46
181
60
49
181
60
49
175
57
45
177
59
47
177
61
48
177
61
46
175
59
44
173
57
42
173
57
44
174
58
45
177
60
50
208
89
81
220
98
93
236
112
112
244
118
121
238
109
114
245
113
124
248
114
125
255
121
131
255
127
137
250
124
128
235
109
113
241
104
112
255
114
124
255
115
127
255
105
116
250
92
106
243
80
97
245
71
94
232
64
87
220
78
94
215
112
116
196
152
139
164
163
132
134
156
110
126
154
103
129
151
104
141
156
113
161
169
130
180
186
150
200
205
173
213
220
189
202
215
185
191
210
178
176
202
167
166
193
158
157
187
151
154
182
144
150
178
140
145
176
134
140
180
130
138
181
127
143
186
133
152
192
140
158
195
144
162
196
146
170
197
154
179
201
162
190
206
170
201
210
181
216
218
196
227
224
205
232
222
210
230
216
207
223
205
201
218
199
195
211
193
189
206
188
184
200
182
180
90
77
61
90
77
61
90
77
61
90
77
61
90
77
61
90
77
61
90
77
61
90
77
61
88
75
59
88
75
59
88
75
59
89
76
60
89
76
60
90
77
61
90
77
61
90
77
61
90
76
63
90
76
63
90
77
61
91
78
61
93
77
61
94
79
60
92
79
60
92
79
62
89
77
61
87
77
67
87
76
72
88
78
79
90
80
88
93
85
98
96
89
105
96
93
114
103
103
129
106
109
142
112
114
155
114
116
167
120
119
179
129
126
193
137
131
201
138
131
199
137
126
192
140
126
188
138
120
178
135
112
168
139
112
167
145
114
171
139
106
163
137
90
132
143
71
82
161
66
60
179
65
64
202
68
67
220
64
67
231
58
62
243
60
62
254
68
69
246
62
60
241
62
58
236
62
61
233
65
64
233
67
67
233
68
72
233
66
73
226
67
72
207
63
62
197
64
57
190
63
54
183
62
51
176
60
47
178
60
48
192
68
56
210
77
68
199
54
49
212
59
54
205
50
46
204
51
46
214
67
59
199
63
51
181
54
39
187
62
44
186
55
37
193
58
39
195
60
41
194
59
40
192
57
37
194
57
38
197
58
39
199
58
40
203
58
39
208
57
40
214
54
40
216
50
36
218
46
34
224
47
37
236
55
46
245
61
53
246
57
51
247
54
49
248
49
46
249
43
43
250
40
41
252
38
40
252
38
40
252
38
40
251
38
42
247
33
41
248
34
46
255
39
55
255
41
62
255
33
59
255
24
54
255
23
49
254
37
54
238
34
43
229
34
42
227
43
45
221
45
45
211
42
39
209
44
40
218
50
47
226
54
50
232
54
52
238
54
54
242
54
53
242
49
50
239
46
47
238
44
44
229
47
44
220
57
50
209
58
49
210
57
49
236
83
75
187
34
26
196
43
35
223
70
62
200
46
38
205
46
40
222
60
55
232
69
64
226
58
55
216
47
42
218
46
42
226
53
49
229
60
55
223
62
52
213
60
46
207
53
41
203
51
38
204
53
42
205
59
46
203
59
48
197
56
46
236
98
88
224
90
79
200
67
58
185
54
44
186
58
47
184
56
45
177
51
39
180
54
42
176
48
37
176
50
38
174
50
38
175
52
37
176
54
39
178
58
42
180
60
46
179
61
49
176
58
48
186
67
59
214
92
87
236
112
112
241
115
118
246
120
124
251
121
131
245
113
124
255
122
132
253
119
128
242
117
123
244
118
122
253
116
124
255
114
124
255
107
120
255
100
115
255
94
110
241
74
92
245
69
92
239
70
91
217
70
86
208
103
107
190
144
128
158
155
122
134
156
109
125
153
102
126
148
99
135
151
106
148
159
119
158
166
129
160
168
131
156
168
132
146
163
127
138
163
124
128
159
118
122
154
113
121
154
111
124
157
114
129
160
118
131
164
117
132
172
119
134
177
121
140
183
127
147
188
132
149
190
134
152
189
135
157
189
140
162
189
144
167
188
147
175
189
154
189
195
167
206
205
185
220
212
199
224
211
202
218
200
196
211
192
188
203
185
183
192
177
174
184
168
168
90
77
61
90
77
61
90
77
61
90
77
61
90
77
61
90
77
61
90
77
61
90
77
61
88
75
59
88
75
59
88
75
59
89
76
60
89
76
60
90
77
61
90
77
61
90
77
61
90
76
63
90
77
61
92
76
61
93
77
61
93
78
59
94
79
60
92
79
60
92
79
62
93
80
64
91
79
67
91
78
70
89
78
74
90
80
81
92
83
88
95
85
94
94
88
102
97
93
116
101
100
131
107
105
145
110
108
157
117
113
172
127
122
188
135
128
196
139
128
196
143
128
193
145
125
186
143
116
171
143
108
162
154
111
164
165
115
168
158
103
158
154
86
125
156
61
69
166
55
46
175
54
46
192
61
53
208
65
59
218
63
58
231
64
58
243
65
61
245
61
61
245
59
60
244
60
62
241
62
65
236
66
69
231
66
70
225
66
70
216
66
67
199
60
55
189
61
52
182
60
49
177
59
47
171
58
44
174
58
43
189
65
53
208
74
65
218
71
64
220
65
61
212
54
51
208
53
49
213
64
57
205
67
56
187
57
43
178
53
35
189
56
39
194
59
40
196
61
42
196
61
42
196
59
40
198
59
40
199
58
40
202
57
40
207
57
40
207
54
38
212
50
37
218
50
37
227
53
42
234
56
46
239
56
48
240
57
49
231
50
39
231
49
38
235
47
38
240
45
39
246
44
40
250
45
42
254
46
44
255
46
47
251
42
45
248
37
43
250
36
46
255
39
55
255
37
59
255
29
55
255
20
51
252
19
48
244
29
47
245
45
56
244
50
59
228
41
48
218
39
43
219
45
47
221
47
49
218
42
45
241
58
62
244
55
61
247
52
58
246
50
54
246
47
52
248
48
51
249
50
53
244
56
55
219
50
45
218
59
53
215
56
50
228
69
63
228
69
63
206
47
43
209
50
46
193
34
30
212
53
49
225
63
58
232
69
64
224
61
56
216
51
45
217
50
44
221
54
48
221
58
49
212
58
46
207
57
42
201
53
39
199
53
40
200
57
43
202
60
48
202
60
50
198
57
47
237
99
89
225
88
78
201
64
56
186
52
43
194
57
49
195
58
48
191
54
44
194
57
47
191
53
43
190
52
42
189
52
42
187
53
41
184
54
41
181
53
40
180
54
40
176
54
41
180
59
48
187
66
57
211
89
84
232
108
106
239
113
116
246
120
124
254
124
132
249
117
128
255
122
132
253
119
128
244
119
125
245
118
125
254
117
125
255
111
123
255
104
118
255
97
113
255
93
110
242
75
93
246
68
92
242
70
92
221
73
89
208
102
104
183
138
119
149
146
111
127
147
98
117
146
92
120
142
93
127
144
99
138
149
107
141
152
112
139
149
112
133
148
109
112
135
93
108
137
93
104
137
92
103
141
94
107
143
95
113
149
101
121
155
105
124
158
107
132
171
116
133
177
118
139
183
124
142
186
125
146
187
127
148
187
130
151
188
134
155
187
137
155
181
134
161
179
139
172
183
151
188
190
168
205
199
183
213
200
191
212
195
188
206
188
186
197
181
181
186
172
172
177
163
163
92
76
60
92
76
60
92
76
60
92
76
60
92
76
60
92
76
60
92
76
60
92
76
60
90
74
58
90
74
58
90
74
58
91
75
59
91
75
59
92
76
60
92
76
60
92
76
60
92
76
61
92
76
61
93
75
61
93
77
61
93
78
59
94
79
60
94
79
60
92
79
60
95
82
65
93
81
65
92
80
68
91
78
70
90
79
75
91
80
78
93
81
83
92
82
90
93
84
103
98
89
118
101
95
133
105
99
145
113
108
164
125
119
181
132
126
190
137
125
189
152
132
193
155
128
183
156
118
169
160
111
158
177
115
162
189
117
165
183
104
152
177
81
118
181
61
70
186
54
49
182
54
45
189
61
50
200
68
56
210
69
59
219
66
58
228
63
57
241
61
62
247
58
62
249
58
63
247
61
66
239
64
69
229
66
69
215
65
66
205
66
63
192
59
52
184
57
48
177
57
43
173
57
42
169
56
40
173
57
42
187
63
51
207
70
60
217
68
62
215
57
54
217
55
52
218
59
55
220
67
61
221
79
69
206
72
60
181
51
35
190
57
40
193
58
39
196
59
41
198
61
43
201
61
44
200
60
43
201
58
41
204
57
41
207
55
41
207
50
35
211
47
35
223
52
42
238
61
51
244
63
54
241
57
49
233
52
43
224
54
39
221
54
38
226
50
37
230
48
37
234
46
37
238
45
38
239
44
40
242
44
41
242
44
45
239
40
43
240
39
47
247
41
54
253
37
58
252
30
55
251
22
51
248
24
51
244
37
55
241
47
58
242
52
64
239
53
64
238
57
66
238
58
67
233
53
62
229
45
55
240
50
60
243
47
59
246
46
57
246
45
55
245
46
51
245
46
49
243
47
49
237
52
50
220
48
44
226
61
57
218
54
52
223
59
57
255
109
106
229
67
65
209
47
45
206
47
44
217
58
55
223
64
60
224
65
59
219
60
54
213
55
46
214
53
45
214
53
43
210
54
42
200
54
39
195
54
37
194
54
39
196
56
41
197
59
46
198
60
49
198
60
49
197
59
49
235
96
89
225
84
77
201
60
51
191
48
40
203
59
51
209
62
54
206
59
51
210
61
54
216
61
56
215
60
55
212
61
54
207
60
50
201
59
49
195
57
46
188
54
42
182
54
41
185
59
47
188
64
54
207
84
77
227
103
101
237
111
114
248
122
126
255
129
136
253
123
131
255
122
131
252
121
129
245
122
127
246
121
127
254
117
127
255
110
124
255
99
117
255
92
111
255
89
109
242
73
94
245
65
90
242
70
92
224
76
90
203
97
97
167
122
101
130
128
90
110
130
81
104
133
79
109
131
82
116
133
88
123
136
93
125
136
94
120
132
92
113
130
88
96
122
77
95
127
80
97
133
85
101
141
89
110
149
96
117
156
103
125
160
106
128
163
109
132
171
114
135
176
116
139
180
120
142
184
121
140
184
121
143
185
122
146
186
126
148
186
129
147
177
127
149
172
128
154
170
134
168
174
146
184
181
162
196
186
174
200
185
178
198
183
180
183
171
173
169
160
165
159
150
155
91
75
59
91
75
59
91
75
59
91
75
59
91
75
59
91
75
59
91
75
59
91
75
59
90
74
58
90
74
58
90
74
58
91
75
59
91
75
59
92
76
60
92
76
60
92
76
60
93
75
61
93
75
61
93
76
60
94
77
61
94
77
59
94
79
60
94
79
60
94
79
60
94
81
64
94
81
64
93
79
66
91
79
67
92
78
69
91
78
70
92
79
73
91
79
79
94
80
95
98
84
109
100
89
123
103
94
137
111
104
156
122
117
173
132
124
183
136
123
179
155
132
186
166
133
180
175
128
170
184
122
161
200
120
159
211
116
156
204
99
140
198
77
108
216
72
82
217
67
66
209
67
63
203
70
61
201
73
60
204
72
59
211
69
59
222
65
60
238
64
65
245
60
65
251
60
68
249
62
69
240
65
72
226
66
68
209
65
64
197
64
59
191
63
54
182
60
49
175
57
43
171
58
42
170
57
41
174
56
42
188
62
48
206
68
58
213
60
55
213
51
49
224
59
57
229
65
63
225
68
63
232
83
76
223
85
74
191
58
43
193
57
41
190
55
36
192
55
37
196
59
41
201
61
44
202
61
44
203
57
42
206
56
41
205
52
38
210
50
38
217
50
41
229
57
47
241
62
55
245
62
54
240
53
46
228
47
38
222
54
41
221
55
41
224
52
42
228
50
40
231
46
41
233
46
41
236
44
41
236
44
43
232
42
44
230
39
44
234
40
49
240
42
57
245
39
60
246
34
59
250
31
61
251
37
65
253
56
74
228
40
54
229
44
58
255
77
89
255
93
105
255
78
90
242
60
73
244
58
72
232
41
56
237
42
56
241
45
57
244
48
58
241
50
55
234
48
49
225
43
42
215
40
37
216
48
45
218
55
50
212
50
47
229
67
64
255
115
112
245
86
83
221
62
59
229
71
68
219
61
58
218
60
57
213
58
53
210
56
48
209
55
45
208
54
42
205
52
38
198
51
35
190
53
35
185
54
36
187
56
38
191
59
44
192
60
47
191
57
45
192
58
47
197
60
50
229
88
81
220
77
71
202
55
48
199
48
41
215
60
55
222
65
58
218
59
53
223
60
53
231
62
57
233
61
57
230
63
57
225
64
56
217
63
53
208
60
50
200
58
46
193
57
45
187
57
44
187
61
49
203
78
72
224
99
95
236
110
111
250
124
127
255
132
136
255
125
133
252
118
127
250
119
127
245
122
127
247
122
128
255
118
128
255
109
123
255
98
116
254
91
110
250
84
104
240
71
92
241
61
86
240
68
90
225
79
92
195
91
90
146
104
82
111
109
71
95
115
66
92
121
67
100
122
73
108
125
80
116
129
86
118
131
88
114
129
88
111
129
87
109
135
90
108
142
92
111
149
98
117
157
104
123
164
108
129
168
111
132
170
113
135
170
112
134
172
113
136
176
116
139
179
117
140
182
118
138
183
118
138
183
118
139
184
119
144
184
122
145
180
124
143
171
123
144
165
124
154
165
133
170
172
150
187
180
164
197
184
175
198
184
181
177
171
175
161
158
165
151
146
153
91
75
59
91
75
59
91
75
59
91
75
59
91
75
59
91
75
59
91
75
59
91
75
59
90
74
58
90
74
58
90
74
58
91
75
59
91
75
59
92
76
60
92
76
60
92
76
60
93
76
60
94
75
60
94
75
60
94
77
61
94
77
59
95
78
60
94
79
60
94
79
60
94
79
60
94
78
62
94
78
62
92
79
63
94
78
65
93
79
66
94
80
69
94
79
74
98
78
89
99
80
100
99
84
113
101
89
125
108
101
145
120
114
162
127
119
168
131
118
164
148
125
167
172
134
171
194
137
169
206
130
158
218
119
148
223
108
137
218
89
119
216
69
95
236
70
84
242
73
78
233
77
78
219
76
72
205
71
62
202
69
60
210
69
62
219
67
62
234
66
66
242
63
67
248
63
71
246
65
74
237
68
73
222
68
70
205
66
63
192
63
57
189
65
55
181
61
47
174
58
43
171
58
42
170
57
41
174
56
42
187
59
46
204
63
54
225
70
65
226
61
59
238
66
66
235
65
65
222
59
54
227
74
66
230
88
76
213
75
62
200
64
50
192
56
40
188
51
35
193
56
40
200
60
45
201
58
42
203
55
41
208
56
43
206
48
37
215
51
41
224
56
47
233
59
52
237
58
53
237
54
48
235
48
43
229
44
39
223
48
43
223
50
44
229
49
48
235
50
48
241
51
53
244
51
54
246
53
56
244
53
58
235
46
53
232
45
54
235
47
61
240
49
67
242
46
70
245
42
71
252
44
78
255
52
82
245
58
77
229
51
63
238
62
75
255
92
105
255
104
118
255
85
100
245
67
83
247
64
82
238
51
68
237
51
65
237
51
62
233
54
60
227
57
57
219
56
51
206
52
42
199
47
36
203
50
42
199
46
40
208
55
50
242
89
84
246
92
90
242
88
86
219
65
63
226
72
70
220
66
64
212
60
55
206
55
48
205
54
45
207
56
45
207
57
43
202
55
39
194
53
35
185
56
35
179
57
36
181
58
40
184
61
45
185
59
44
182
54
41
188
55
46
198
61
53
219
78
71
217
69
65
206
53
48
208
49
45
225
62
57
231
64
58
227
56
49
231
53
49
235
50
48
238
50
49
235
53
50
231
56
51
225
58
50
216
58
47
208
57
46
200
58
46
190
54
42
186
56
43
200
73
66
220
95
89
235
110
108
251
125
128
255
133
137
255
126
131
247
116
124
248
118
126
245
122
127
248
123
129
254
116
129
255
108
124
255
97
117
255
90
112
247
81
101
241
72
93
240
62
86
238
69
90
227
83
93
188
86
82
131
91
66
98
98
60
92
112
63
91
118
67
102
121
75
110
126
81
116
129
86
120
133
90
121
136
95
119
140
97
121
149
101
120
154
104
122
160
109
124
167
113
129
170
114
132
171
114
133
171
112
134
169
111
137
172
114
139
174
116
138
178
116
138
180
116
136
181
116
135
182
114
137
184
116
139
184
119
143
183
123
140
173
120
141
164
120
151
165
130
170
174
149
191
186
167
203
193
181
207
198
193
188
186
191
168
170
182
156
156
168
90
74
58
90
74
58
90
74
58
90
74
58
90
74
58
90
74
58
90
74
58
90
74
58
90
74
58
90
74
58
90
74
58
91
75
59
91
75
59
92
76
60
92
76
60
93
76
60
94
75
60
94
75
60
94
75
60
95
76
61
94
77
59
95
78
60
94
79
60
94
79
60
93
78
59
93
78
59
94
79
60
95
79
63
95
79
63
96
80
64
97
81
65
98
80
70
101
76
80
101
77
93
97
79
101
98
85
115
104
96
133
116
111
151
123
116
157
125
113
151
143
118
150
174
131
159
203
140
161
218
131
150
227
116
133
232
101
117
230
81
100
228
63
80
242
55
72
252
64
78
244
73
81
227
71
74
209
65
64
203
66
60
209
67
63
217
67
66
230
67
70
235
64
70
239
64
71
239
66
72
231
68
73
218
68
69
201
66
62
190
63
54
186
64
53
177
59
45
169
56
40
168
57
40
170
57
41
176
56
42
188
58
45
202
59
51
232
73
69
233
63
63
235
61
62
229
55
56
218
50
47
222
65
58
238
90
80
244
104
91
218
80
67
201
65
51
190
53
37
194
54
39
199
57
43
200
57
43
203
55
43
210
56
44
209
48
38
218
54
45
230
59
52
233
59
52
234
52
48
232
47
44
234
45
43
235
45
45
241
52
56
243
52
59
247
51
61
250
50
60
251
49
61
253
49
61
250
48
60
247
49
62
244
49
65
241
50
66
242
53
75
243
53
78
242
49
78
244
46
79
254
49
88
255
62
95
230
51
72
248
78
91
255
95
108
255
92
107
252
84
99
250
79
95
244
68
88
233
56
74
243
65
81
234
58
71
221
50
58
210
47
50
202
50
45
194
56
45
189
59
43
188
58
42
196
58
47
192
50
40
217
74
66
250
107
99
221
76
71
225
80
75
207
59
57
207
59
57
224
76
74
212
64
60
202
55
48
202
55
45
206
60
47
205
60
43
203
58
41
196
59
40
180
59
38
173
58
37
175
60
41
179
62
45
177
57
41
175
51
39
186
58
47
202
68
59
214
71
65
216
67
61
212
55
50
215
50
46
231
59
55
236
58
54
232
50
46
237
50
45
242
44
45
244
44
44
242
46
47
239
51
49
232
55
49
226
58
49
218
60
49
210
62
50
194
54
41
186
52
41
197
66
58
215
88
82
231
106
104
250
124
125
255
134
137
255
127
132
248
117
125
249
119
127
246
123
128
247
122
128
252
114
127
253
105
121
255
94
115
255
89
111
245
79
99
244
75
96
242
64
88
239
70
91
229
85
95
182
83
78
121
84
58
94
96
56
96
115
69
97
124
73
110
129
83
118
134
89
123
136
93
126
139
96
129
144
103
130
151
108
128
156
108
125
159
109
124
164
111
125
168
112
128
169
111
130
170
110
131
169
108
133
169
108
137
170
113
139
173
113
137
175
114
136
178
114
133
178
111
133
180
112
135
182
114
137
184
116
139
181
118
135
173
116
140
166
119
153
169
132
175
181
153
198
196
175
213
206
190
217
210
204
194
194
202
172
178
192
155
161
175
90
75
56
90
75
56
90
75
56
90
75
56
90
75
56
90
75
56
90
75
56
90
75
56
90
75
56
90
75
56
90
75
56
91
76
57
91
76
57
92
77
58
92
77
58
93
76
58
94
75
60
96
74
60
94
75
60
95
76
61
94
77
59
95
78
60
94
79
60
94
79
60
94
79
58
95
80
59
96
79
59
97
80
62
97
80
62
97
80
62
97
80
60
100
78
65
104
75
77
103
75
87
99
78
95
98
84
109
106
97
128
118
112
146
124
118
152
128
115
145
143
117
142
173
128
148
205
134
148
222
127
135
237
113
121
246
101
108
246
82
91
244
62
75
248
47
66
255
55
75
251
65
79
235
64
73
219
63
67
214
66
66
213
67
67
215
65
66
224
65
69
227
64
69
230
63
70
228
65
70
223
67
70
212
66
66
199
64
60
189
62
53
184
62
51
175
57
43
168
55
39
169
58
41
173
60
42
180
60
44
193
61
49
207
63
55
223
61
58
229
57
57
227
48
51
226
47
50
224
52
50
219
57
52
230
77
69
252
108
97
242
104
91
217
81
67
198
60
47
198
58
45
200
58
46
201
55
42
205
54
43
213
56
47
213
52
44
222
55
47
230
57
51
232
54
50
233
49
47
232
47
44
238
49
47
245
49
53
255
50
66
255
48
68
255
45
69
255
43
65
255
38
63
255
35
60
250
32
56
244
32
55
246
39
65
243
43
69
242
47
77
242
49
80
240
44
80
243
42
84
254
51
96
255
66
104
240
67
87
255
95
108
255
103
116
247
85
100
236
74
89
239
74
90
234
65
84
217
49
66
235
64
82
221
55
67
201
45
49
186
40
40
177
46
36
171
56
38
165
63
38
168
64
39
184
63
46
188
58
44
221
89
77
225
93
81
205
71
62
205
68
62
193
54
51
201
59
55
228
85
81
213
70
64
201
57
49
200
56
45
201
58
44
200
57
40
200
57
40
196
61
41
177
63
39
166
60
38
168
59
38
172
61
42
171
55
40
172
51
40
188
61
52
211
76
70
214
71
65
221
69
64
219
57
54
220
51
48
233
55
51
236
53
49
235
48
43
244
49
45
253
44
47
255
43
45
250
46
47
244
49
47
237
52
47
230
56
49
223
59
49
216
62
50
201
57
46
189
51
40
192
59
50
207
78
72
225
97
94
247
121
122
255
136
138
255
131
135
252
121
129
252
122
130
248
124
132
247
122
130
249
111
124
249
101
117
252
90
111
252
86
108
238
75
96
242
74
97
240
64
87
236
69
89
225
84
93
174
77
71
112
77
49
92
94
54
95
114
68
102
126
78
117
136
91
127
142
99
132
144
104
134
146
106
136
151
110
137
158
115
136
164
116
132
166
116
127
167
114
125
168
112
127
168
110
129
169
109
131
169
108
134
168
108
136
167
110
138
169
110
136
172
111
134
174
111
130
175
108
130
177
107
131
181
110
133
183
114
135
179
116
134
173
116
140
168
119
155
173
133
176
185
154
199
199
175
214
207
189
215
210
204
188
192
201
165
174
189
147
156
171
91
74
56
90
75
56
91
74
56
90
75
56
91
74
56
90
75
56
91
74
56
90
75
56
91
74
56
90
75
56
91
74
56
91
76
57
92
75
57
92
77
58
93
76
58
93
76
58
94
75
60
94
75
60
94
75
60
94
77
61
94
77
61
94
79
60
94
79
60
92
79
60
96
81
62
97
82
61
97
82
63
98
81
63
98
81
63
97
80
62
97
79
59
98
76
63
107
77
75
106
76
84
101
79
92
100
87
107
107
101
127
120
117
148
125
124
156
127
120
151
145
122
148
171
129
149
198
132
144
217
126
133
238
117
122
252
109
111
253
90
91
249
70
76
247
52
69
255
58
78
251
64
81
238
63
76
229
65
74
225
69
73
221
66
70
215
60
64
222
63
68
223
62
67
225
62
67
222
64
65
218
66
65
208
65
61
197
62
56
188
61
52
181
63
49
172
59
43
169
56
40
173
60
44
180
63
46
187
64
49
202
65
55
216
67
61
217
55
52
225
56
53
222
46
46
226
51
48
230
58
54
211
48
39
207
53
41
235
85
71
255
119
106
235
91
80
209
67
55
202
60
48
203
59
48
201
55
42
205
54
43
212
58
48
218
55
48
223
56
50
229
54
51
231
51
50
234
48
49
237
49
50
244
51
54
252
53
60
252
37
53
255
35
56
255
34
56
255
33
56
255
32
55
255
31
53
251
29
52
246
30
53
237
28
50
236
32
57
237
39
66
238
41
71
237
39
74
239
41
77
253
53
92
255
70
104
255
96
115
254
94
106
247
85
98
242
80
93
242
80
93
240
75
89
227
62
76
210
48
59
218
61
70
205
54
59
187
47
46
175
46
40
168
52
39
162
59
40
155
63
38
157
61
37
170
57
39
178
56
43
210
86
74
187
59
50
196
63
56
190
52
49
189
50
47
206
67
64
229
87
83
213
72
65
200
57
49
196
54
42
197
54
40
196
53
37
198
53
36
195
58
40
177
60
40
167
58
38
168
55
37
173
57
42
172
52
38
175
49
37
194
63
53
219
82
74
217
73
65
225
72
66
222
59
54
222
50
46
234
52
49
237
49
47
239
45
43
252
50
50
254
44
47
253
43
46
249
43
45
243
43
43
237
45
42
230
49
42
223
52
44
214
56
44
209
61
49
191
50
40
188
55
46
200
73
66
217
92
88
243
119
119
255
137
139
255
134
138
253
126
133
255
125
133
252
125
134
253
119
130
252
107
122
249
96
114
249
87
108
249
83
105
234
68
92
240
71
94
236
59
85
228
65
86
215
81
90
162
73
67
102
70
45
88
93
53
90
109
63
98
125
74
116
140
92
128
150
103
135
152
108
136
155
110
138
160
114
139
165
118
140
172
122
134
171
119
129
170
114
126
167
109
124
165
105
125
165
103
129
165
103
131
165
105
131
164
107
133
166
109
132
170
109
130
172
108
128
173
104
128
175
105
129
179
106
132
182
111
137
182
115
136
176
114
140
173
120
153
176
132
172
188
152
189
198
169
201
203
181
203
205
194
182
191
196
159
171
183
140
152
164
93
75
55
92
75
55
93
75
55
92
75
55
93
75
55
92
75
55
93
75
55
92
75
55
94
76
56
93
76
56
94
76
56
93
76
56
94
76
56
93
76
56
94
76
56
93
76
58
93
74
60
93
75
61
94
76
62
92
76
60
92
76
60
91
78
61
93
80
63
94
83
65
96
83
66
95
82
63
96
80
64
98
81
65
99
82
66
100
81
66
100
78
65
100
76
66
107
75
76
104
75
80
101
79
91
102
91
108
110
107
134
120
123
156
129
133
170
137
137
173
143
132
166
165
136
166
190
136
159
208
130
144
225
121
128
240
114
115
249
106
102
255
97
96
244
77
87
237
65
79
231
61
74
233
66
76
235
69
79
231
65
75
228
61
69
229
62
70
226
59
66
224
59
63
221
58
61
217
59
58
212
60
57
204
59
54
196
58
48
185
57
44
167
54
38
175
65
48
173
60
44
172
54
40
191
65
53
204
70
61
204
61
53
208
55
50
218
56
53
220
55
49
222
53
46
222
54
45
222
56
42
221
60
42
215
58
39
211
54
37
217
59
48
240
83
76
242
90
79
217
69
57
199
56
42
205
62
46
212
66
51
210
58
45
211
50
42
240
71
66
225
47
47
239
54
59
244
55
62
239
45
56
255
66
77
240
40
51
250
38
50
255
41
50
255
41
50
249
29
39
253
34
42
255
39
47
254
39
47
240
29
38
238
31
41
248
44
55
233
33
46
235
38
55
240
44
64
227
33
57
255
86
112
229
43
66
255
92
108
255
94
105
255
89
99
251
84
92
246
79
87
236
71
77
222
62
64
208
54
52
204
61
55
197
64
55
184
62
49
167
54
38
156
49
33
154
53
35
157
58
39
160
59
41
167
56
45
174
55
47
194
69
63
191
59
55
189
51
49
196
54
52
197
53
52
212
68
67
219
76
72
218
75
69
209
68
59
200
58
48
196
52
41
199
53
40
203
52
41
197
53
42
188
60
47
179
57
44
173
47
35
171
40
30
179
45
36
195
57
47
210
67
59
216
72
63
228
80
70
223
69
61
218
55
48
221
48
42
231
48
44
242
50
49
249
49
51
253
49
52
247
47
49
246
48
49
246
48
49
243
48
46
242
48
46
237
50
45
230
53
45
221
57
47
213
61
48
190
50
37
192
62
49
185
61
51
214
95
89
233
115
113
255
143
143
251
132
134
255
135
140
255
132
141
255
120
135
253
104
123
252
95
116
254
92
115
250
86
110
243
77
101
242
72
99
235
60
89
237
62
91
224
66
89
216
94
105
141
63
59
92
67
45
83
91
54
92
116
66
103
134
77
119
150
93
125
156
99
127
157
103
128
161
106
131
166
110
130
168
111
128
167
110
127
166
109
126
165
108
125
165
105
124
164
104
125
163
102
127
163
101
127
163
102
127
165
108
127
166
109
129
171
108
127
172
107
126
173
103
127
174
102
129
177
103
131
179
105
135
181
109
139
180
114
139
177
118
142
174
124
153
180
137
167
189
153
175
194
164
178
193
174
163
175
173
141
152
158
124
135
141
93
75
55
93
75
55
93
75
55
93
75
55
93
75
55
93
75
55
93
75
55
93
75
55
94
76
56
94
76
56
94
76
56
94
76
56
94
76
56
94
76
56
94
76
56
94
75
58
92
75
59
94
76
62
95
77
63
94
78
62
91
78
61
91
78
61
92
81
63
94
82
66
95
83
67
93
81
65
93
79
66
95
79
66
96
78
68
98
80
70
99
79
72
101
77
73
105
76
78
103
78
84
101
83
97
103
95
116
110
111
142
121
126
166
128
136
183
135
139
187
158
150
199
163
140
182
171
127
160
185
120
142
211
121
133
234
124
125
244
117
111
244
105
100
245
94
99
238
81
90
233
71
82
238
74
83
243
76
86
242
71
80
238
64
74
234
60
69
228
57
63
224
58
60
220
57
58
215
60
56
210
61
55
203
60
52
196
58
47
186
59
44
171
60
43
170
63
45
169
56
40
178
58
44
193
65
54
197
60
52
199
54
49
212
60
55
210
53
48
214
53
45
215
53
42
216
52
40
217
56
38
219
58
38
219
59
37
218
57
39
221
54
46
230
65
59
235
77
68
225
73
60
208
62
47
199
56
40
202
56
41
211
59
46
221
58
51
245
73
69
228
48
49
241
55
60
246
54
65
241
46
60
255
63
77
237
37
50
243
36
44
247
37
40
251
41
44
253
43
44
252
42
43
248
40
40
246
40
40
247
43
44
239
36
39
237
37
40
234
37
44
253
59
68
216
26
38
238
50
65
250
63
80
213
35
51
255
104
117
255
97
106
255
90
97
253
86
93
247
78
83
231
65
67
218
59
56
210
61
54
193
55
44
194
66
53
188
71
54
172
61
44
154
47
31
147
39
26
146
38
25
147
39
27
158
45
37
176
58
54
209
84
80
206
72
71
195
53
52
198
52
53
209
61
61
235
87
85
216
71
66
214
71
63
208
65
57
199
57
47
198
51
41
202
54
44
208
55
47
206
55
48
199
61
51
194
57
49
194
53
46
195
50
45
201
52
46
209
56
51
216
63
57
222
69
61
228
74
64
222
65
56
218
54
45
221
48
42
231
48
44
242
50
49
251
48
51
251
48
51
244
48
50
243
49
50
244
48
49
243
47
48
243
48
46
238
49
45
231
52
45
223
57
45
215
61
49
191
51
36
190
60
47
183
61
50
211
94
87
233
118
115
255
144
144
249
133
136
253
134
140
255
128
138
255
115
132
255
100
121
255
92
116
255
89
114
252
84
110
246
76
103
243
68
97
236
61
90
236
66
95
223
74
96
194
83
92
128
61
55
88
67
46
82
92
57
91
118
67
101
135
75
115
149
89
120
156
95
119
157
98
123
161
102
125
165
105
125
166
106
124
165
105
124
165
105
124
164
104
123
163
101
123
161
100
123
161
100
125
161
99
126
162
101
124
163
106
125
166
108
125
169
108
125
170
105
124
171
101
125
173
99
127
175
99
130
177
99
133
176
104
135
178
107
136
175
112
136
171
115
139
171
124
144
172
132
144
170
135
140
162
139
118
133
126
99
111
111
83
95
95
93
76
56
93
76
56
93
76
56
93
76
56
93
76
56
93
76
56
93
76
56
93
76
56
93
76
56
93
76
56
93
76
56
93
76
56
93
76
56
93
76
56
93
76
56
93
76
56
94
75
58
95
78
62
97
80
64
96
80
64
93
80
63
92
79
62
91
79
63
91
82
67
91
81
69
91
81
71
92
79
71
92
79
73
95
80
77
99
81
79
102
84
84
103
84
86
103
84
88
103
86
96
104
92
112
108
105
134
115
118
161
124
131
185
130
138
201
138
142
206
152
146
210
160
139
196
170
131
178
185
125
159
207
128
147
228
131
138
238
126
122
242
115
109
248
102
105
242
85
92
234
73
81
237
72
79
243
73
82
245
71
81
242
65
75
238
61
69
228
55
61
224
55
58
219
56
57
213
60
55
207
63
55
201
63
52
193
61
48
184
61
45
174
61
45
170
60
43
171
58
44
184
66
54
199
72
63
192
59
50
187
48
41
202
58
50
205
54
47
208
54
46
213
55
44
216
56
44
217
55
40
217
56
38
220
57
38
224
58
42
221
52
45
219
52
46
226
65
57
233
79
67
222
72
58
200
53
37
200
50
35
217
64
50
226
62
53
245
72
68
231
48
50
242
53
59
248
54
63
245
49
61
255
66
80
244
46
59
240
41
48
235
35
38
236
36
39
249
49
51
245
45
47
239
39
39
236
38
37
245
49
50
234
40
41
245
55
57
231
42
48
225
40
48
234
50
60
255
81
93
237
59
73
255
89
101
255
94
102
242
85
92
239
78
84
241
76
82
235
69
73
222
58
59
214
55
52
214
62
57
194
53
44
194
62
50
189
65
53
181
60
49
173
55
45
168
49
41
162
43
35
157
38
32
155
36
30
169
47
42
199
69
67
194
58
58
188
44
44
200
51
53
215
65
66
244
94
93
208
63
58
210
67
59
208
65
57
203
61
51
204
57
47
212
61
52
218
64
56
219
64
59
219
64
60
216
61
59
221
62
59
227
65
63
228
62
62
224
58
58
227
63
61
235
73
68
224
66
57
221
60
52
217
53
44
221
50
43
231
49
45
240
51
49
245
49
50
248
48
50
244
48
50
244
48
50
244
48
49
243
47
48
243
48
46
238
49
45
231
52
45
223
57
45
216
62
50
192
52
37
186
56
43
181
59
48
208
91
84
237
122
119
255
147
146
251
135
138
253
129
137
255
124
137
255
113
129
253
100
120
254
90
114
255
85
111
252
79
107
250
73
102
244
65
95
238
63
92
234
71
98
223
84
103
165
64
72
117
56
51
88
70
50
85
95
60
95
119
69
102
135
78
114
147
90
117
152
94
116
154
95
120
158
99
123
163
103
123
163
103
121
162
102
121
162
102
121
161
99
120
160
98
121
159
98
121
160
97
124
160
98
122
160
99
122
162
102
123
164
104
123
167
104
123
168
101
123
169
97
123
169
96
125
171
96
127
174
96
135
178
106
138
181
110
139
178
115
137
172
116
134
166
119
130
158
117
121
147
110
111
134
108
81
99
87
64
79
76
50
65
62
94
77
57
94
77
57
94
77
57
94
77
57
94
77
57
94
77
57
94
77
57
94
77
57
94
77
57
94
77
57
94
77
57
94
77
57
94
77
57
94
77
57
94
77
57
94
77
57
94
76
56
96
78
58
97
80
60
96
81
62
93
80
63
90
78
62
89
79
67
88
80
69
88
79
72
91
81
79
95
85
84
98
88
89
101
88
95
105
89
99
108
92
102
107
94
104
108
96
108
108
99
116
111
107
134
114
116
155
121
126
181
127
135
200
133
141
214
140
143
220
142
135
212
159
139
208
173
138
194
179
129
167
189
121
144
204
123
130
222
129
124
241
130
123
254
116
116
253
100
103
244
88
92
242
79
84
242
73
80
242
68
77
240
63
71
239
60
66
229
54
59
225
55
56
219
57
55
213
60
55
206
64
54
198
64
52
191
64
49
184
63
46
173
57
42
173
62
45
177
61
48
188
70
58
204
82
71
199
72
63
186
53
46
186
49
41
200
57
49
205
56
49
211
59
48
217
61
49
219
59
45
217
54
39
220
54
38
224
56
43
224
53
45
219
50
43
224
61
52
236
78
66
227
75
61
210
58
44
209
56
42
224
66
54
222
54
45
240
62
58
233
47
48
239
48
53
245
50
58
248
52
62
255
74
86
255
70
79
253
62
69
242
51
56
226
36
38
237
47
49
234
44
44
240
50
50
232
42
42
235
47
46
241
57
57
209
26
28
229
50
54
220
43
49
231
56
63
239
68
76
255
87
98
241
75
85
217
60
67
219
64
68
225
66
70
225
62
65
222
57
61
220
56
57
218
55
56
212
57
55
201
56
51
195
56
49
190
57
50
193
64
58
206
77
72
215
85
83
210
77
78
196
66
64
178
55
50
182
57
53
207
75
73
217
77
76
227
81
82
239
89
90
234
82
81
241
89
86
201
53
49
206
62
54
210
66
57
208
64
55
211
63
53
217
66
57
224
67
60
226
64
59
233
63
64
231
56
61
236
57
61
242
63
67
239
58
63
232
53
56
236
62
61
246
77
74
222
57
51
219
54
48
218
51
43
222
51
44
229
52
46
236
51
48
242
50
49
244
48
49
244
48
50
246
47
50
246
48
49
245
47
48
245
47
46
240
48
45
233
52
45
224
56
45
217
63
51
194
54
39
183
53
40
179
57
46
203
86
79
239
124
121
255
147
146
254
135
139
255
128
137
255
121
135
252
110
126
250
99
118
250
88
111
252
80
106
251
72
102
250
67
98
242
61
92
239
66
94
229
77
100
215
90
104
136
50
53
107
57
48
92
77
56
91
101
67
99
123
75
106
137
80
115
146
89
116
149
92
117
152
94
119
157
98
121
161
101
122
162
102
119
160
100
118
160
97
119
159
97
118
158
96
120
159
96
120
159
96
123
159
97
122
160
99
122
162
102
122
163
103
122
167
102
122
167
98
122
168
96
122
168
93
124
171
93
126
173
95
134
178
103
139
180
110
140
179
116
138
173
115
134
166
117
128
157
113
116
142
105
105
128
100
80
98
84
67
82
75
56
71
64
94
77
59
94
77
59
94
77
59
94
77
59
94
77
59
94
77
59
94
77
59
94
77
59
94
77
59
94
77
59
94
77
59
94
77
59
94
77
59
94
77
59
94
77
59
94
77
57
93
75
53
95
77
53
96
80
57
95
80
59
92
79
62
90
78
64
88
79
70
88
81
75
89
80
81
96
87
92
103
96
104
110
102
115
115
104
121
115
103
123
115
103
125
113
103
127
111
108
129
112
112
138
114
118
153
118
124
172
122
130
193
127
135
208
132
139
220
138
140
225
147
142
224
157
141
214
160
132
190
158
117
159
160
108
130
172
110
115
188
117
111
210
118
107
240
115
109
253
109
108
254
104
105
251
95
96
249
83
87
244
71
77
237
60
66
234
55
61
232
53
59
227
54
56
221
57
56
215
60
55
208
64
55
199
65
53
192
65
48
185
64
47
173
55
41
180
64
51
175
58
48
176
59
49
200
82
72
213
92
81
201
77
67
187
59
48
190
57
48
193
55
45
204
57
47
214
62
51
219
61
49
218
54
42
220
52
39
226
54
42
224
51
44
226
55
48
228
61
53
229
69
57
226
70
57
222
66
51
222
65
50
226
62
50
219
48
38
234
55
50
236
48
47
238
45
48
239
44
50
246
51
59
255
76
84
255
87
94
255
90
95
255
79
83
225
46
49
227
48
51
224
44
45
247
69
69
232
54
54
220
44
44
218
44
45
234
61
63
219
49
52
216
50
54
255
98
104
224
61
66
202
41
49
202
45
52
193
38
42
213
59
61
224
65
69
218
55
58
217
51
55
223
57
61
220
55
59
205
47
48
204
52
51
205
59
59
211
71
70
221
83
83
235
96
99
245
105
108
244
101
107
234
94
97
196
64
62
198
69
64
229
94
91
248
104
103
255
111
112
255
106
106
229
75
75
215
61
59
201
52
48
209
62
55
215
68
60
214
67
59
213
62
53
213
60
52
216
58
49
220
52
49
238
53
59
241
45
55
240
44
54
242
46
56
239
44
52
235
44
51
243
57
60
254
74
75
223
51
47
220
51
46
218
51
43
220
51
44
227
53
46
234
52
48
239
52
47
242
50
49
244
48
50
246
47
50
247
47
49
246
46
48
245
47
46
240
48
45
234
51
45
226
55
45
217
63
51
198
56
42
180
50
37
178
56
45
195
78
71
239
124
121
255
146
145
255
135
140
255
126
137
255
117
132
249
108
124
246
97
116
245
85
109
247
74
102
249
64
96
249
59
93
243
58
92
237
68
97
219
80
101
193
87
97
114
46
43
98
62
48
96
84
62
99
106
73
102
124
77
107
137
83
116
146
92
117
150
95
118
153
95
120
158
99
121
161
99
120
160
98
117
159
96
117
159
96
117
157
95
118
158
95
119
158
95
120
159
96
123
159
97
122
161
98
122
162
100
123
165
101
124
166
100
124
167
96
121
167
94
121
168
90
123
170
92
125
172
94
128
172
97
133
175
103
135
174
109
135
170
112
133
165
115
131
160
114
123
149
110
114
138
106
100
119
100
91
107
94
81
97
84
95
78
60
95
78
60
95
78
60
95
78
60
95
78
60
95
78
60
95
78
60
95
78
60
95
78
60
95
78
60
95
78
60
95
78
60
95
78
60
95
78
60
95
78
60
95
78
58
94
76
52
95
78
52
96
78
56
94
79
58
91
78
62
89
79
69
90
83
77
90
84
84
95
90
97
104
98
112
114
109
129
122
117
140
123
117
145
123
113
147
122
112
147
118
112
148
113
116
149
114
119
157
116
124
170
119
128
183
122
131
196
126
134
207
132
138
216
139
141
218
150
144
218
153
139
201
155
133
180
162
131
162
170
132
145
171
126
123
159
107
93
153
82
64
186
78
66
210
82
73
225
88
82
233
88
85
239
84
82
241
75
77
236
63
67
229
52
58
235
56
60
231
57
59
226
58
57
218
61
56
211
64
56
203
65
54
195
63
48
188
62
47
179
57
44
186
65
54
171
53
43
163
46
36
190
73
63
217
103
92
217
100
90
203
85
73
184
60
50
184
54
41
192
51
41
206
58
48
217
59
48
218
54
44
222
51
41
228
54
45
223
49
42
228
55
49
227
59
50
221
57
47
222
62
48
227
70
53
228
66
51
223
57
43
222
48
39
233
52
45
240
51
49
239
45
46
236
41
45
242
47
53
255
66
71
255
82
87
255
93
98
255
92
97
226
57
60
227
57
60
213
43
44
244
74
75
226
56
57
215
47
47
217
51
53
203
39
40
202
37
41
247
87
89
220
61
65
213
57
61
209
54
58
186
31
35
196
40
43
219
61
62
229
66
69
219
53
57
215
44
50
220
49
55
217
48
53
204
41
46
208
52
55
226
76
78
244
98
101
250
105
110
247
100
108
246
96
105
248
96
108
248
98
107
214
76
76
213
78
74
234
92
90
234
86
84
231
77
77
227
72
70
207
52
50
206
51
47
217
65
60
224
75
68
228
79
72
223
74
67
217
64
56
215
58
51
215
54
46
220
46
45
245
50
58
251
45
58
248
42
55
245
39
52
245
41
52
248
48
58
255
62
67
255
74
74
224
49
46
222
51
44
220
51
44
221
53
44
225
52
45
231
52
47
237
52
47
240
51
47
244
48
50
247
47
50
249
46
49
248
45
48
246
46
46
241
47
45
235
50
45
226
55
45
217
61
49
201
59
45
179
49
36
177
55
44
186
67
61
236
121
118
255
141
141
255
136
141
255
123
135
255
113
129
246
104
120
243
96
114
243
83
107
244
69
98
249
58
92
249
55
90
243
58
92
235
72
99
205
80
96
160
74
77
102
52
43
89
66
48
95
87
64
99
106
73
102
124
78
107
136
82
116
145
91
119
149
95
120
153
96
123
158
100
122
160
99
118
158
96
115
155
93
114
156
92
115
155
92
116
156
93
118
157
94
119
158
95
123
159
97
122
161
98
121
161
98
122
164
98
124
167
98
123
166
95
120
166
91
120
167
89
122
169
89
123
170
90
126
170
95
131
173
101
133
172
107
135
169
109
137
167
115
137
165
117
132
157
115
124
148
114
117
137
112
111
128
109
103
120
102
95
80
61
95
80
61
95
80
61
95
80
61
95
80
61
95
80
61
95
80
61
95
80
61
94
79
60
94
79
60
94
79
60
94
79
60
94
79
60
94
79
60
94
79
60
95
78
58
97
80
54
97
80
54
97
79
57
94
79
60
91
79
65
91
82
75
92
86
86
95
90
97
107
103
118
115
111
134
125
122
151
130
126
161
129
124
165
125
119
163
123
117
165
120
118
167
115
121
171
115
125
178
120
129
184
123
132
191
126
134
196
132
138
200
141
142
206
148
147
205
154
147
198
160
150
187
171
156
179
188
167
176
201
173
169
197
166
148
170
136
109
154
103
76
155
70
50
173
65
52
181
65
52
194
66
57
214
72
68
232
78
78
238
74
75
236
63
67
235
61
63
233
59
61
229
59
59
224
60
58
217
64
56
210
63
53
202
62
49
194
60
48
189
58
48
190
63
54
175
52
44
167
49
39
187
73
63
209
99
86
214
104
91
213
101
87
195
79
66
186
62
50
186
52
41
199
55
44
214
57
48
217
53
44
223
50
43
231
54
48
226
48
44
227
52
47
226
55
47
222
55
46
224
62
49
229
67
52
228
62
48
222
52
37
229
51
41
232
47
42
240
51
47
240
46
46
239
45
46
241
48
51
242
54
55
244
64
67
246
77
82
252
89
94
233
70
75
246
81
87
216
51
55
235
70
74
221
56
60
230
65
69
216
53
56
204
44
46
207
46
51
226
70
73
210
55
59
208
55
58
185
35
37
213
59
61
217
57
59
226
60
62
232
62
65
230
55
60
223
46
54
219
42
50
224
50
59
228
61
68
238
77
83
250
95
101
255
109
115
255
106
115
248
97
106
245
91
103
246
90
104
246
92
102
244
100
100
232
90
86
232
87
84
216
64
61
207
49
48
212
52
52
211
51
51
225
67
64
227
74
69
233
82
75
234
83
76
225
74
65
217
63
55
216
57
51
219
56
49
227
51
51
243
46
55
255
48
62
255
48
61
250
42
55
252
46
58
255
59
69
255
67
72
251
65
66
227
49
47
223
52
45
221
52
45
221
53
44
226
52
45
230
51
44
237
52
47
241
52
48
244
48
50
247
47
50
249
46
49
248
45
48
246
46
46
241
47
45
235
50
45
227
55
45
216
58
47
205
62
48
181
49
37
177
55
44
178
59
53
233
115
113
255
138
138
255
135
141
255
116
132
253
104
124
243
97
116
241
92
112
244
82
106
245
68
97
251
55
93
252
56
94
245
61
95
232
78
102
188
81
91
125
57
54
94
60
48
81
68
49
91
86
64
93
100
69
101
119
77
106
133
82
114
142
91
118
148
94
121
154
97
123
158
100
120
158
97
115
155
93
112
152
89
112
153
87
113
154
88
113
154
88
116
155
92
117
156
93
121
157
95
120
159
96
120
161
95
121
162
94
122
165
94
121
164
92
120
164
89
120
164
87
121
165
86
123
167
88
127
169
95
130
172
100
132
171
104
134
168
108
135
165
111
134
162
113
128
154
109
120
145
106
112
132
104
108
128
101
101
121
96
95
80
61
95
80
61
95
80
61
95
80
61
95
80
61
95
80
61
95
80
61
95
80
61
94
79
60
94
79
60
94
79
60
94
79
60
94
79
60
94
79
60
94
79
60
95
78
58
99
81
57
99
81
57
97
80
60
93
80
64
90
80
71
92
84
82
95
90
97
97
95
109
116
114
138
121
120
152
129
128
168
131
129
176
126
126
176
121
121
175
121
117
176
119
119
181
119
124
190
120
127
195
125
133
196
130
136
196
133
138
193
141
143
191
151
150
190
160
157
188
172
167
187
182
172
180
190
179
175
200
184
169
208
190
166
214
193
162
209
187
150
208
172
136
175
111
84
174
92
71
164
71
53
168
62
48
194
71
63
224
87
81
239
87
86
241
78
79
233
64
67
234
61
63
230
60
61
226
61
59
223
61
58
215
61
53
207
59
49
201
57
48
193
56
48
193
60
53
185
58
51
184
61
53
193
79
69
199
88
77
199
93
79
209
101
88
211
99
85
194
76
62
188
58
45
197
56
46
210
57
49
215
52
45
223
50
44
231
54
48
230
52
48
225
50
45
224
53
46
227
60
51
231
67
55
230
67
52
226
58
45
223
51
37
233
52
43
229
45
37
240
48
45
240
48
45
241
52
50
242
54
53
232
46
47
226
50
52
231
66
70
247
88
93
246
87
92
255
111
117
229
68
74
235
72
77
224
60
67
254
91
96
255
109
115
220
59
64
255
123
128
255
108
112
234
81
84
211
61
63
198
48
50
215
59
62
241
72
75
235
59
62
237
56
63
246
62
70
240
56
66
230
46
56
241
64
74
255
95
103
255
110
119
255
107
114
254
100
110
245
94
103
247
95
107
253
99
111
253
95
110
246
90
101
241
93
93
227
80
73
226
74
71
213
55
52
210
47
48
217
53
52
210
48
46
216
57
53
223
68
63
227
74
68
226
73
65
217
64
56
211
54
47
214
52
47
220
55
49
232
54
54
234
39
47
252
48
60
254
50
61
247
43
54
249
48
58
255
62
69
253
63
65
236
52
52
226
51
46
224
53
46
222
54
45
223
52
44
225
51
42
232
51
44
239
52
47
244
52
49
246
48
49
247
47
50
249
46
49
248
45
48
248
46
46
242
47
45
235
50
45
227
55
45
215
57
46
209
63
50
182
48
37
178
54
44
176
54
49
232
112
111
255
135
136
255
134
142
255
110
128
252
99
120
241
94
113
242
90
111
245
81
106
246
67
97
253
57
95
253
59
96
242
67
98
225
84
103
174
82
87
98
48
39
89
68
51
74
69
47
85
83
60
88
93
63
99
115
76
105
129
81
113
139
91
118
146
95
122
152
98
124
157
100
120
156
95
114
153
90
110
151
85
110
151
85
110
151
85
111
152
86
114
153
90
116
155
92
119
155
93
119
158
93
119
158
91
120
161
91
121
163
91
119
163
88
118
162
87
118
162
85
119
163
84
121
165
88
122
164
90
126
166
95
129
166
99
127
161
100
126
156
102
123
151
102
114
140
93
105
130
88
95
117
81
92
113
82
86
107
76
92
76
60
94
78
62
96
80
64
96
80
64
95
79
63
95
79
63
95
79
63
97
81
65
98
82
66
95
79
63
93
77
61
93
77
61
97
81
65
99
83
67
98
82
66
96
81
62
100
83
63
97
80
62
93
80
64
94
81
73
91
81
80
90
83
90
101
97
112
115
113
137
121
120
154
122
124
165
126
128
177
128
129
185
125
127
186
120
122
183
114
115
180
108
111
182
116
119
198
118
124
202
128
131
202
139
141
198
150
151
195
164
164
190
178
176
187
188
185
180
203
197
181
211
204
176
217
207
172
219
207
165
220
207
163
221
209
161
217
203
156
217
194
152
209
168
136
184
128
103
161
93
70
203
122
103
216
116
101
189
72
63
216
81
78
247
99
99
233
75
76
229
63
65
225
57
57
227
59
58
226
58
57
219
55
53
216
54
49
214
59
54
203
56
49
192
53
46
199
66
59
187
60
51
191
73
63
198
88
75
185
79
65
206
100
86
209
99
84
204
88
73
197
71
57
196
58
47
205
54
45
216
55
47
225
54
47
227
52
47
223
48
45
223
50
44
224
55
48
228
61
52
224
60
48
221
55
41
223
53
40
229
55
44
228
47
36
234
47
38
236
47
41
238
49
45
238
50
48
235
52
48
232
53
49
226
54
52
215
55
57
230
74
78
245
89
93
237
78
83
215
56
61
215
54
60
249
88
96
255
126
132
255
120
128
255
107
113
255
99
107
255
102
109
255
101
107
245
92
97
242
89
92
252
91
96
255
87
90
255
70
75
249
58
66
248
57
65
243
51
62
235
45
57
234
50
62
238
62
72
252
85
95
255
97
105
255
104
114
254
103
112
251
100
109
254
100
110
255
97
111
251
94
103
237
83
83
223
70
64
214
56
53
214
52
49
215
49
49
215
47
46
216
51
49
219
57
52
225
68
61
220
66
58
217
63
55
214
60
52
215
57
48
219
56
49
224
57
51
230
54
54
240
55
60
243
52
59
243
52
59
246
52
60
245
54
59
242
53
57
235
51
51
227
49
47
222
51
44
222
53
46
223
55
46
224
53
43
228
51
43
233
50
44
239
50
44
245
50
48
247
49
50
248
48
51
250
47
50
249
46
49
249
47
47
243
48
46
236
51
46
229
55
46
224
63
53
204
56
44
191
54
44
175
47
38
184
59
55
210
88
87
255
152
154
254
123
131
255
109
128
255
102
124
246
93
114
239
81
104
241
72
101
249
68
101
253
61
100
245
59
96
242
79
108
209
84
100
137
61
63
80
44
32
69
58
38
68
70
46
77
77
53
90
94
67
91
105
69
99
120
77
113
136
90
120
146
98
120
150
96
116
149
92
114
150
89
113
152
89
108
149
83
109
150
82
110
151
83
111
152
84
114
153
88
116
155
90
117
156
93
117
156
91
116
155
88
116
157
87
118
160
88
119
161
87
118
162
87
118
162
85
117
161
84
116
160
83
123
165
91
123
163
93
122
158
94
117
151
91
109
139
85
98
126
77
88
114
67
83
106
62
63
85
46
63
85
47
62
84
46
91
75
59
94
78
62
96
80
64
96
80
64
96
80
64
95
79
63
96
80
64
97
81
65
98
82
66
96
80
64
95
79
63
96
80
64
98
82
66
99
83
67
98
82
66
96
80
64
102
86
71
95
81
70
92
79
71
94
84
83
97
90
97
100
96
111
109
107
131
118
119
150
119
121
162
123
125
174
125
127
184
120
124
185
113
117
181
107
112
180
108
111
182
110
113
190
117
119
206
124
126
211
137
137
209
152
152
206
168
167
199
184
182
193
201
197
186
211
206
177
219
214
172
226
219
167
231
222
167
230
221
164
228
219
164
226
216
163
219
209
158
212
199
154
212
188
154
189
155
128
170
124
100
199
138
119
202
125
109
182
86
74
211
96
91
246
114
112
236
90
91
231
75
76
226
63
64
228
62
62
233
63
64
229
59
59
224
54
54
219
54
52
214
61
56
201
56
51
202
63
56
185
54
46
184
62
51
189
75
64
182
72
59
200
93
77
221
111
96
212
96
81
198
72
58
189
53
41
196
48
38
212
54
45
220
53
45
219
48
41
226
53
49
229
57
53
228
61
55
224
60
51
220
56
46
219
53
41
223
52
42
227
53
42
236
55
46
237
53
43
237
50
43
235
48
43
231
48
44
227
50
44
225
52
46
221
53
50
214
56
55
216
62
64
228
74
76
243
87
91
250
94
98
252
92
100
254
93
101
255
97
105
240
79
87
235
74
82
238
78
86
252
92
100
255
104
111
255
101
107
244
89
95
242
79
84
236
53
58
239
45
53
241
44
54
244
47
57
238
40
53
228
34
45
227
41
54
234
57
67
251
81
90
253
92
100
255
100
108
255
101
109
255
102
110
255
101
109
252
92
104
242
82
90
229
69
69
222
60
55
219
54
52
222
54
51
223
53
53
220
51
48
218
50
47
219
54
50
224
62
57
219
62
55
216
59
52
215
58
51
219
56
49
223
56
50
229
57
53
231
58
54
233
57
59
234
55
58
235
55
58
238
55
57
238
55
57
236
54
53
229
51
49
223
50
44
220
51
44
221
54
45
223
55
44
226
54
44
229
50
43
234
49
44
242
49
44
245
50
48
247
49
50
247
48
51
250
47
50
249
46
49
247
47
47
243
48
46
236
51
46
229
55
46
223
60
51
212
61
50
194
56
46
182
49
42
183
55
52
216
90
91
255
144
147
251
115
125
254
103
122
254
94
118
247
87
111
244
80
105
246
71
102
246
65
98
250
61
99
241
66
99
227
81
104
183
76
86
119
60
56
76
50
35
62
57
35
61
65
40
71
71
47
80
83
56
88
99
65
96
114
74
110
131
88
117
143
95
118
147
93
116
147
88
113
147
86
112
148
84
107
146
79
107
148
80
108
149
81
109
150
82
112
151
86
114
153
88
115
154
91
115
154
89
116
155
88
117
157
87
117
159
87
117
159
87
116
158
84
114
158
81
113
157
82
112
156
81
110
152
80
108
148
78
104
140
76
98
129
70
88
117
63
78
105
54
70
93
47
64
87
43
64
87
45
64
87
45
64
87
45
89
76
60
91
78
62
94
81
65
94
81
65
94
81
65
94
81
65
95
82
66
96
83
67
97
84
68
96
83
67
95
82
66
96
83
67
97
84
68
97
84
68
96
83
67
94
80
67
98
85
77
92
81
77
91
81
82
99
89
98
107
100
116
113
109
132
118
117
149
121
123
164
121
123
172
121
126
182
120
124
187
110
115
181
101
105
176
96
103
175
106
110
184
114
117
196
125
125
213
135
133
217
151
148
217
169
167
216
187
185
209
203
201
202
220
215
193
229
224
184
233
228
173
237
231
169
240
232
167
237
229
166
231
225
167
226
219
165
215
207
160
203
196
154
185
174
144
170
154
129
156
130
107
171
130
112
168
110
96
158
81
71
187
93
85
222
109
105
247
117
117
238
96
95
227
74
76
226
66
68
234
65
68
235
62
64
231
57
58
224
54
54
223
61
58
212
59
54
212
65
58
194
55
48
183
55
44
183
62
51
174
61
47
182
70
56
219
106
92
223
105
91
217
91
77
202
66
54
197
50
40
209
52
43
217
54
45
217
50
44
222
53
50
233
65
62
236
71
67
224
61
54
213
50
41
213
49
39
218
50
39
222
48
37
230
52
42
232
49
41
231
46
41
230
47
41
227
50
44
226
55
47
225
61
52
224
66
57
198
45
40
202
52
51
220
68
67
240
86
88
250
94
98
247
90
97
243
83
91
241
81
91
231
69
80
232
70
81
237
77
87
251
91
101
255
107
116
255
109
115
242
87
93
225
60
66
232
47
53
242
45
52
250
50
60
251
51
62
244
44
57
236
40
52
243
55
69
255
76
87
245
74
83
244
80
87
247
87
95
252
95
102
255
100
107
255
97
105
246
82
89
232
67
73
222
57
55
222
55
49
227
55
53
233
60
56
235
59
59
230
57
53
225
53
49
221
54
48
221
58
51
217
59
50
215
57
48
215
57
48
220
55
49
225
56
51
231
58
54
233
60
56
227
57
57
227
57
57
229
57
55
231
57
56
232
56
56
230
55
52
225
52
48
219
50
43
218
54
45
219
55
45
222
56
44
226
54
44
231
50
41
235
48
41
243
48
44
247
49
48
245
49
50
245
49
51
248
48
50
247
47
49
247
47
47
242
48
46
238
51
46
231
54
46
221
57
48
220
68
57
199
56
48
187
52
46
181
47
46
228
95
98
255
131
136
248
108
119
252
99
120
251
88
115
248
81
109
251
78
108
250
69
102
243
59
93
239
61
95
233
74
102
210
85
103
155
68
74
107
60
52
79
62
44
62
60
39
59
63
40
70
68
47
72
72
48
81
92
60
89
107
69
103
124
81
114
137
91
116
143
90
113
144
85
109
143
82
107
143
79
105
144
77
105
145
75
105
146
76
107
148
80
109
150
84
110
151
85
112
151
88
113
152
87
117
156
89
117
157
87
116
156
85
114
156
84
112
154
82
109
151
77
107
149
77
106
148
76
101
141
71
97
136
69
91
127
65
85
116
59
76
105
51
69
96
45
65
88
42
62
85
39
67
93
48
68
94
49
69
95
50
87
74
58
90
77
61
92
79
63
94
81
65
93
80
64
94
81
65
95
82
66
97
84
68
96
83
67
96
83
67
96
83
67
97
84
68
97
84
68
96
83
67
95
82
66
92
80
68
90
79
77
90
81
86
98
88
99
106
99
115
113
109
134
118
117
149
120
122
163
122
124
173
119
124
180
115
121
183
109
114
180
98
105
175
93
100
172
97
103
177
108
114
190
120
123
202
137
138
221
149
146
225
164
162
225
182
181
225
199
196
217
213
209
208
225
220
198
231
227
189
235
229
179
238
233
175
238
232
174
233
226
171
227
219
170
218
211
167
203
195
158
188
182
150
166
164
139
157
152
132
150
134
118
155
127
113
155
113
99
156
98
87
181
104
96
208
113
109
251
139
137
243
118
116
232
92
93
229
76
78
229
69
71
234
65
68
234
61
63
232
60
60
223
58
56
219
60
56
222
69
63
211
67
59
195
58
48
186
58
47
176
54
41
167
49
35
191
73
59
219
97
82
234
107
92
219
83
69
201
58
44
204
52
39
212
54
43
215
52
45
217
52
50
241
77
75
254
90
88
237
75
70
217
56
48
214
51
42
219
52
43
222
50
40
223
46
38
225
44
37
225
44
37
222
45
37
220
49
41
218
54
44
217
59
48
215
63
52
194
47
39
196
52
44
206
58
54
217
67
66
223
70
72
224
69
73
228
71
78
234
77
86
241
81
93
246
86
98
247
87
97
248
91
100
255
102
111
255
107
115
241
86
94
219
56
61
233
50
55
240
47
52
241
46
54
240
43
52
235
37
50
235
41
52
249
62
73
255
83
95
237
63
73
232
65
73
234
71
76
242
81
86
255
90
96
255
88
93
244
71
77
227
54
56
225
53
51
228
55
51
234
59
56
242
64
62
244
64
63
239
61
59
230
57
53
224
55
50
221
56
50
218
57
49
217
56
48
217
56
48
220
55
49
227
55
51
232
57
54
233
58
55
226
58
55
224
59
57
227
58
55
230
58
56
231
58
54
229
56
52
224
53
46
218
51
43
218
54
44
219
55
45
222
56
44
226
54
44
231
50
41
235
48
41
242
47
43
246
48
47
244
50
50
244
50
51
247
49
50
246
48
49
246
48
47
242
48
46
238
51
46
231
54
46
220
56
47
224
70
60
201
57
49
190
51
46
182
44
44
238
102
106
255
121
128
248
103
116
253
100
121
250
86
113
251
76
107
255
72
106
252
64
99
239
55
89
232
63
94
224
82
104
195
89
101
135
67
66
101
67
55
90
79
59
72
70
49
65
67
45
76
71
51
70
68
45
76
85
54
83
99
62
98
116
76
108
131
85
112
139
88
110
141
82
107
141
80
105
142
75
103
143
73
104
144
74
104
145
75
106
147
77
107
148
80
109
150
84
111
150
87
112
151
86
116
155
88
115
155
85
113
153
83
111
151
80
106
147
77
103
144
74
101
142
72
100
141
71
96
135
68
93
129
65
87
121
61
82
113
56
77
105
54
76
102
54
78
101
55
79
102
56
77
103
56
78
104
56
79
105
58
85
72
56
87
74
58
91
78
62
92
79
63
93
80
64
93
80
64
95
82
66
97
84
68
95
82
66
96
83
67
97
84
68
97
84
68
96
83
67
95
82
66
95
82
66
94
81
73
86
77
82
95
88
104
110
102
123
116
112
137
120
116
151
120
119
161
120
122
171
121
123
180
115
121
183
106
114
179
96
103
173
91
99
171
95
103
176
106
114
187
118
124
198
128
132
206
147
149
223
158
159
224
173
174
228
187
189
228
203
201
225
212
211
217
221
218
209
225
223
202
229
224
194
230
226
191
228
222
188
224
215
182
218
208
181
206
198
175
190
182
163
174
167
151
155
153
140
143
141
128
140
132
121
141
123
113
147
117
107
157
114
105
169
109
101
188
111
105
230
137
132
237
129
126
240
116
114
240
102
100
238
88
89
237
79
78
235
69
69
233
65
64
227
58
55
224
59
55
228
66
61
222
69
63
204
60
51
198
61
51
196
66
53
175
49
35
174
48
34
199
73
59
220
88
75
214
78
64
204
61
47
206
56
42
208
54
42
209
51
42
214
52
50
245
85
85
255
107
104
250
93
88
227
68
62
220
59
51
223
59
50
227
56
48
226
52
45
227
50
44
226
49
43
221
48
41
212
48
39
205
47
36
198
47
36
194
48
35
197
57
44
192
52
39
193
51
41
203
58
53
215
67
65
219
69
71
219
66
71
218
64
72
225
69
80
236
80
91
242
86
97
242
86
97
249
93
104
255
101
109
241
87
95
222
63
68
233
59
60
237
53
55
237
51
56
239
50
56
242
50
61
247
57
67
255
72
83
255
86
95
229
53
63
222
51
59
223
54
59
234
65
68
249
74
79
253
72
77
244
59
65
233
49
51
232
53
49
235
57
53
239
60
56
242
60
57
243
59
57
240
58
55
233
55
51
226
55
48
223
59
50
220
59
49
219
58
48
220
57
48
223
56
50
228
55
51
232
54
52
233
55
53
230
58
56
228
59
56
230
58
56
232
58
57
232
59
55
230
57
53
225
54
47
220
51
44
219
55
45
220
56
46
223
57
45
225
55
42
229
51
41
233
49
41
241
48
41
244
49
45
243
51
50
241
51
51
244
50
50
244
48
49
244
49
47
242
48
46
238
51
46
231
54
46
222
55
47
224
67
58
202
55
48
192
49
45
192
49
51
247
106
112
255
113
122
249
103
116
248
95
116
246
82
109
252
68
102
255
63
100
255
59
99
242
58
94
228
71
100
216
91
109
171
86
91
119
69
62
100
74
61
97
89
70
84
79
59
75
73
52
80
73
54
71
69
46
71
80
51
76
92
56
90
108
70
102
124
78
109
133
83
109
138
82
108
140
77
104
141
74
102
142
72
103
143
72
103
145
73
105
146
76
106
147
79
108
149
83
110
149
86
111
150
85
116
153
86
115
152
85
110
149
82
106
146
76
102
143
75
99
140
72
97
138
70
95
136
70
90
129
66
88
124
63
83
116
59
80
110
56
79
107
58
82
108
61
87
110
64
90
113
67
85
111
63
85
112
61
85
111
63
82
69
53
85
72
56
88
75
59
90
77
61
91
78
62
92
79
63
94
81
65
96
83
67
96
83
67
97
84
68
97
84
68
97
84
68
96
83
67
96
83
67
98
85
69
97
86
80
94
86
99
106
101
124
121
117
144
125
121
154
122
121
161
120
121
169
118
121
176
115
119
180
106
114
179
99
106
176
91
99
171
93
101
174
104
112
185
118
126
199
129
135
209
136
141
209
153
157
220
163
166
221
175
179
226
187
191
228
198
199
227
207
206
224
212
211
219
215
213
214
217
214
209
217
213
204
215
208
198
210
201
192
204
195
190
197
186
184
180
168
168
163
155
153
141
137
134
124
121
116
124
116
113
120
107
101
130
107
101
143
110
103
144
97
91
157
96
91
192
115
109
215
122
117
237
128
123
249
126
121
253
115
112
250
100
99
243
85
84
237
73
71
237
68
63
234
62
58
225
60
54
224
65
59
209
58
49
211
67
58
221
84
74
194
62
50
184
54
41
189
57
44
194
61
46
200
63
47
207
66
49
215
67
53
214
64
49
209
55
45
208
50
47
234
76
77
250
95
93
242
87
83
223
68
63
217
58
52
219
56
49
224
55
48
230
57
51
231
57
50
231
57
50
226
58
49
218
57
47
209
57
44
202
56
43
196
56
41
176
40
24
184
51
34
200
64
50
211
73
62
210
68
64
203
59
58
204
55
59
207
58
64
211
57
67
227
73
83
237
86
95
240
89
98
244
93
102
246
95
104
232
83
89
218
63
67
229
64
62
236
63
59
243
64
67
253
70
74
255
76
84
255
81
89
255
84
93
255
85
94
226
51
58
221
46
53
221
46
51
231
55
58
245
60
65
249
58
63
248
52
56
244
48
50
237
52
50
239
57
53
240
56
54
237
52
50
235
50
48
234
50
48
231
52
48
225
52
46
227
60
52
224
61
52
223
60
51
222
59
50
225
56
51
228
55
51
232
52
51
232
52
51
232
56
56
231
57
56
232
56
56
235
57
57
235
57
55
233
55
53
226
53
47
222
51
44
219
55
45
220
56
44
221
58
43
223
55
42
227
50
40
231
48
40
239
48
40
242
49
44
241
52
50
240
52
51
243
51
50
243
49
49
243
49
47
241
49
46
236
51
46
231
54
46
224
57
49
217
60
51
204
55
49
193
48
45
209
64
67
255
110
117
253
107
117
247
99
113
237
84
105
241
74
102
250
62
97
255
55
94
255
55
98
248
64
100
227
79
105
200
92
105
139
71
70
106
70
58
93
76
60
93
86
67
88
81
62
81
74
55
79
71
52
73
68
46
68
74
46
71
85
50
83
99
62
94
115
72
103
127
77
106
135
79
106
138
75
103
140
71
103
140
70
102
142
71
102
144
72
103
144
74
105
146
78
107
148
82
109
148
85
109
148
85
113
149
85
111
148
81
106
145
78
103
142
75
99
140
72
96
137
69
95
136
70
94
134
71
92
130
69
91
126
68
87
120
65
86
114
63
85
111
64
89
112
66
93
114
71
94
117
71
90
114
66
88
115
64
88
115
64
78
66
50
81
69
53
85
73
57
87
75
59
88
76
60
90
78
62
92
80
64
94
82
66
97
85
69
98
86
70
98
86
70
97
85
69
95
83
67
97
85
69
102
90
74
104
93
89
110
103
119
117
112
142
124
120
153
123
121
160
119
120
166
118
119
173
112
117
175
106
111
175
97
104
172
94
102
174
95
103
176
101
109
182
112
120
192
125
133
205
137
144
216
143
151
214
153
161
210
161
168
210
170
178
217
180
186
222
186
191
223
192
193
224
195
195
223
199
197
221
198
194
217
197
192
212
193
186
204
188
179
196
187
176
193
181
168
186
166
153
170
150
139
153
139
132
139
117
111
113
119
110
111
115
101
101
122
102
101
137
109
106
133
94
89
150
99
95
155
90
84
183
103
96
214
117
110
237
124
118
252
125
119
255
120
115
254
106
102
249
92
87
252
83
78
245
72
66
230
61
54
231
68
61
216
59
52
219
71
61
234
92
82
197
59
48
196
60
48
189
55
43
190
54
40
199
62
46
209
68
51
212
66
51
212
65
49
215
65
51
210
56
54
216
62
62
221
67
67
220
66
64
214
61
55
213
56
49
216
55
47
222
55
47
229
58
51
230
57
51
229
58
51
226
59
51
219
62
53
213
65
53
209
69
56
204
71
54
174
45
24
188
59
37
205
74
56
209
76
61
198
61
53
187
48
43
192
48
48
202
57
62
215
65
74
226
76
85
236
86
97
240
90
101
239
92
100
236
89
97
223
76
82
212
62
64
209
52
47
218
53
47
227
57
57
237
63
64
245
66
72
247
68
74
244
64
73
239
62
70
228
51
59
225
48
54
230
49
54
240
55
60
247
54
57
249
49
52
254
45
50
253
49
52
239
50
48
240
57
53
240
55
52
236
48
46
232
44
42
231
48
44
230
51
46
224
51
44
228
61
53
224
61
52
224
61
52
224
60
51
227
58
53
230
57
53
234
54
53
236
54
53
236
53
55
236
53
55
237
53
55
238
54
56
238
54
54
235
53
52
228
50
46
222
49
43
222
55
46
221
57
45
221
58
43
223
55
42
225
52
38
230
49
38
237
49
40
241
49
44
240
52
50
238
53
51
241
52
50
242
50
49
243
49
47
241
49
46
236
51
46
231
54
46
226
59
51
210
52
43
210
58
53
198
50
48
231
85
88
255
112
120
249
99
110
241
90
105
227
75
96
239
72
102
253
58
98
255
49
93
255
54
98
253
67
104
219
77
101
177
80
89
106
53
47
94
71
55
88
75
58
85
78
59
89
81
62
85
74
56
76
64
48
73
68
48
64
70
44
65
79
46
75
91
54
88
106
64
98
120
71
102
129
74
104
136
73
102
139
70
101
138
68
99
139
66
99
141
69
101
142
72
103
144
76
104
145
79
105
145
82
107
146
83
110
146
82
108
144
80
104
143
78
100
139
74
97
138
72
95
136
70
95
135
72
95
135
72
95
133
72
95
130
72
93
126
71
93
121
70
93
119
72
96
119
75
99
120
79
99
122
78
92
116
66
90
117
64
90
117
64
74
67
51
77
70
54
81
72
57
84
75
60
87
75
61
89
77
63
92
79
63
94
81
65
100
87
70
101
88
71
100
88
72
98
86
70
95
85
73
97
87
77
103
94
85
108
99
102
119
114
137
121
117
154
120
118
158
116
115
159
114
114
164
116
117
173
108
112
175
98
103
169
90
97
169
95
101
175
102
108
184
109
115
191
118
124
200
129
135
211
143
147
221
151
156
222
155
161
211
160
167
209
169
174
216
174
179
221
177
180
223
178
179
223
180
179
223
182
179
222
181
176
217
179
172
213
174
166
203
171
161
196
170
159
193
164
153
185
153
140
170
136
126
150
126
120
132
102
97
103
108
99
102
100
88
90
108
90
88
124
99
95
123
86
80
147
98
93
140
77
70
162
85
77
187
97
88
212
108
99
238
121
114
255
129
122
255
124
119
255
112
106
254
97
92
247
84
79
233
71
66
238
79
73
221
66
61
222
71
64
234
87
79
186
44
34
190
49
39
190
52
39
199
62
46
209
69
54
203
62
45
193
50
34
202
55
39
219
69
55
222
69
63
211
57
55
204
51
46
205
53
48
212
59
53
215
61
53
221
60
52
225
61
52
228
59
52
228
57
50
222
53
46
215
52
43
207
53
41
202
56
43
197
60
44
194
63
45
212
83
62
200
71
50
189
60
41
190
58
45
195
61
50
196
59
53
194
55
52
194
52
51
220
75
78
221
76
81
225
80
85
229
84
89
232
89
93
229
86
88
220
77
79
212
67
64
203
52
43
212
54
43
218
53
47
224
52
50
228
49
52
232
49
54
233
48
56
231
46
54
238
51
62
237
50
59
244
52
63
253
56
66
255
51
62
253
41
53
255
40
51
255
46
57
248
45
51
249
53
55
249
53
57
240
46
47
235
42
43
233
48
46
232
53
49
226
53
47
227
60
52
224
61
52
224
61
52
225
61
52
228
59
54
231
58
54
235
55
54
238
54
54
236
52
54
235
51
53
236
50
53
237
51
52
237
53
53
234
52
49
227
49
45
219
48
40
222
56
44
221
58
43
221
58
43
222
56
40
224
52
38
227
51
36
233
51
38
236
52
42
238
53
48
238
53
50
240
52
50
240
51
47
242
50
47
239
50
44
235
52
44
231
54
46
228
61
52
205
47
38
215
62
57
202
52
53
249
98
103
255
112
121
246
91
105
237
80
99
228
70
93
243
72
104
255
61
101
255
50
93
255
55
97
245
71
104
201
74
93
148
66
70
83
43
35
88
72
56
84
75
58
79
72
54
89
80
63
83
74
57
69
64
45
71
70
49
61
67
39
64
74
40
73
85
49
83
100
58
95
114
69
101
125
73
103
132
74
101
136
72
99
136
67
97
137
66
98
138
67
99
141
69
101
142
72
102
143
77
104
143
80
105
144
81
108
144
80
107
144
77
102
141
74
99
138
73
97
136
71
96
135
70
96
135
72
96
134
73
94
129
71
94
127
70
95
125
71
96
124
73
97
123
75
100
123
77
102
124
78
102
126
78
93
120
67
92
121
63
93
122
66
68
66
54
70
68
56
74
70
59
80
71
62
82
72
62
87
73
62
93
77
62
95
79
63
96
81
62
97
82
63
96
83
66
95
86
71
98
89
82
103
98
95
111
106
110
115
111
126
119
113
147
122
116
160
124
121
168
121
119
169
112
112
166
103
103
163
96
97
162
92
95
164
96
98
173
102
104
181
111
112
192
121
122
204
132
130
214
141
139
223
150
146
231
156
153
232
160
158
223
161
160
218
163
162
219
164
163
220
166
163
220
165
162
219
165
160
216
164
159
213
165
158
210
163
156
207
162
154
201
162
153
198
159
150
193
153
142
184
142
131
171
131
124
155
113
111
124
104
103
108
99
95
96
102
90
90
108
89
85
115
86
80
128
85
78
143
86
79
150
82
73
169
89
80
181
88
80
192
91
83
216
107
100
235
122
114
246
131
124
255
139
134
249
117
113
255
117
115
237
95
93
223
80
76
217
69
65
239
90
84
204
55
49
197
50
42
196
54
42
195
55
40
192
55
39
195
55
38
198
57
40
202
59
43
208
60
46
213
63
49
210
56
46
214
60
50
215
63
52
211
59
48
205
53
40
205
51
39
216
58
47
226
64
53
224
57
48
225
57
48
222
55
46
215
55
43
207
55
41
201
58
42
198
63
44
196
65
47
194
61
46
198
64
52
203
69
58
205
71
60
204
67
59
197
60
52
187
50
44
182
43
38
199
60
55
208
69
66
217
79
76
224
86
83
227
89
86
225
88
82
216
79
73
208
67
58
205
58
42
213
58
40
220
56
44
227
53
46
232
46
47
237
42
50
240
40
51
244
39
54
251
44
62
250
43
63
251
42
63
253
41
63
255
39
63
255
38
63
255
36
60
255
35
60
255
35
55
255
38
56
255
42
59
254
44
57
244
44
54
237
46
53
233
53
54
232
60
58
221
56
52
221
58
51
222
59
52
224
59
53
227
58
53
230
57
53
233
55
51
235
53
52
235
51
53
236
52
54
235
53
52
233
53
52
231
53
51
226
53
47
221
53
44
219
53
41
220
56
44
219
57
42
220
57
40
222
57
38
223
56
38
226
55
37
227
54
37
230
52
38
236
53
45
237
52
47
240
53
48
241
52
46
241
53
44
237
53
43
232
54
42
228
56
44
222
58
48
212
55
46
211
58
53
215
62
64
255
109
117
255
98
113
248
85
106
241
75
99
247
79
105
236
58
90
249
57
94
255
63
103
245
64
99
230
80
107
178
73
87
97
36
35
71
43
32
65
56
39
75
68
52
82
76
60
79
73
57
66
67
49
59
68
47
60
73
47
65
72
38
69
75
37
75
83
46
83
93
56
91
107
68
97
118
75
100
126
78
100
131
74
98
134
70
96
136
66
97
137
64
99
139
66
101
141
70
104
144
74
108
144
80
109
145
81
108
145
76
105
142
72
103
140
71
102
139
72
102
138
74
101
137
73
99
133
72
97
131
71
100
131
74
100
129
73
98
127
71
99
126
71
100
127
74
100
127
72
101
126
71
97
125
67
91
123
60
92
127
61
95
130
66
61
63
52
63
65
54
70
67
60
74
69
63
80
69
63
84
71
62
91
75
62
93
77
61
96
79
59
96
81
60
94
83
65
94
86
73
99
91
88
104
99
105
110
108
121
115
111
134
116
112
149
118
112
158
116
112
162
112
109
162
103
102
159
97
97
159
93
94
161
92
92
164
101
100
176
107
106
186
116
114
197
125
123
207
134
130
217
141
137
224
148
142
230
152
147
229
155
149
221
156
151
217
157
152
218
158
153
219
158
151
218
157
150
217
156
150
214
155
149
211
158
150
210
156
148
205
156
147
200
155
147
198
154
145
192
147
138
183
136
127
170
128
121
154
115
109
123
107
101
105
100
94
94
101
91
89
107
90
83
113
86
77
124
84
74
141
84
75
157
84
75
178
91
82
187
88
82
195
88
82
211
102
97
221
114
106
224
120
111
235
126
119
251
136
133
255
135
133
247
117
115
238
103
100
227
85
83
235
90
85
199
50
44
194
47
37
197
55
41
195
55
38
192
55
37
193
56
38
196
56
39
200
59
42
207
59
45
211
61
47
213
57
44
213
57
44
211
58
44
210
58
44
209
57
43
209
57
43
212
56
43
216
56
44
223
59
49
224
57
48
220
56
46
214
56
44
207
57
42
200
57
40
193
58
38
190
57
38
190
54
42
193
56
48
198
61
53
201
64
56
202
65
57
200
63
55
197
60
52
194
57
49
191
52
45
199
62
54
209
72
64
216
79
71
218
81
73
215
78
70
204
67
59
195
55
42
202
55
37
211
56
36
221
55
41
229
52
44
237
47
47
244
43
51
249
41
55
253
40
58
255
43
64
255
42
65
255
41
65
255
38
65
255
35
64
255
33
60
255
30
61
255
29
58
255
30
57
255
31
56
255
36
58
255
41
59
247
45
57
241
50
58
234
55
58
230
60
60
220
57
52
220
58
53
222
59
52
224
59
53
227
58
53
230
57
53
233
55
51
235
53
50
235
51
53
236
52
54
235
53
52
232
54
52
228
55
49
223
54
47
220
53
44
217
53
41
219
57
42
219
57
42
220
57
40
220
57
38
222
57
38
223
56
37
225
56
37
227
54
37
231
53
43
234
51
43
235
52
44
235
53
42
235
53
42
232
54
40
228
55
41
223
55
42
221
61
49
209
52
43
207
53
51
239
84
88
255
102
114
255
91
111
241
73
98
249
76
104
248
69
99
247
64
95
244
56
91
243
62
97
239
78
109
205
76
98
139
56
64
89
41
37
69
51
39
60
54
40
62
59
44
67
64
49
67
66
48
61
64
45
56
68
46
58
73
44
60
68
29
75
80
39
92
98
60
101
111
74
104
118
82
101
122
81
100
126
79
100
130
76
97
131
70
97
134
65
97
137
64
99
140
64
103
141
68
105
142
72
107
144
77
108
145
78
108
145
75
106
144
71
104
141
72
102
139
70
103
138
74
102
137
73
102
133
74
100
131
72
102
131
75
100
129
73
100
127
72
100
128
70
101
129
71
101
129
71
102
127
69
98
126
65
93
128
62
95
132
63
98
135
66
54
57
46
58
60
49
63
63
55
68
65
58
75
66
59
79
69
60
86
72
61
89
76
60
94
79
60
92
79
60
92
83
66
94
87
77
99
93
93
104
101
108
111
109
123
115
113
137
115
111
148
113
107
153
106
102
152
99
96
149
92
91
148
90
90
152
92
93
160
92
95
166
104
106
181
110
111
191
119
120
202
127
128
210
134
132
216
138
136
220
143
139
224
145
142
221
147
144
215
147
144
211
147
144
211
147
144
211
148
142
212
147
141
211
147
140
208
146
139
206
149
140
205
147
139
199
148
139
194
148
140
191
147
139
186
140
133
175
131
123
162
126
115
145
121
103
119
115
96
102
103
91
91
100
91
86
100
91
82
104
88
75
117
85
72
135
83
72
166
89
81
190
95
89
203
91
89
208
88
87
218
100
98
222
109
105
220
113
107
226
119
113
251
138
132
255
138
134
255
133
129
255
125
121
247
110
104
238
97
88
196
52
43
192
50
38
196
56
41
193
58
39
192
57
38
192
57
37
194
57
38
198
59
40
204
58
43
208
58
44
214
58
45
211
53
41
206
53
39
208
56
42
212
62
47
212
62
47
206
54
40
203
47
34
218
58
46
218
56
45
217
55
44
214
56
44
208
58
43
201
58
41
194
57
38
190
55
36
189
53
41
192
54
44
194
56
46
196
58
48
199
61
51
202
64
54
203
65
55
204
66
56
199
58
49
202
64
54
207
69
59
208
70
60
210
72
62
209
71
61
202
64
54
197
55
43
204
54
37
214
54
38
223
55
44
231
52
47
241
48
51
246
45
53
251
43
57
255
42
60
252
39
59
252
39
61
251
37
61
249
35
59
251
33
58
250
30
55
252
28
55
254
27
54
255
29
55
255
30
53
254
35
55
253
42
59
247
50
60
242
55
62
236
57
60
229
59
59
220
57
52
220
58
53
222
59
52
224
59
53
229
57
53
231
56
53
233
55
51
235
53
50
237
51
54
238
52
55
237
53
53
234
54
53
229
54
49
225
54
47
220
53
44
216
52
40
218
56
41
217
57
41
218
57
39
219
56
37
221
56
37
222
55
36
224
55
36
225
54
37
225
53
39
226
52
41
227
53
42
227
54
40
227
54
40
225
55
40
221
55
41
218
56
43
219
61
50
204
50
42
211
56
54
255
108
112
255
93
108
249
81
104
237
62
91
255
75
108
247
59
92
255
66
100
240
56
90
233
65
98
227
87
113
171
64
82
101
34
41
81
46
42
65
52
43
52
53
39
53
54
38
58
59
41
63
62
42
62
61
40
60
64
41
62
69
38
62
70
29
88
97
52
115
126
84
128
140
100
119
135
96
104
127
83
97
125
76
97
128
71
96
131
67
96
133
63
97
137
64
99
140
64
104
142
69
106
143
74
107
144
77
108
145
78
110
147
77
107
145
72
104
141
72
103
140
71
104
139
75
104
139
75
102
136
76
100
134
74
101
132
75
100
131
74
99
128
70
100
129
71
102
130
72
102
130
71
101
129
70
98
128
66
99
134
68
101
138
69
104
141
72
50
53
42
53
56
45
59
60
52
65
62
55
71
64
58
75
66
59
82
69
60
86
74
60
88
76
60
89
77
61
89
81
68
94
87
79
99
94
98
105
103
114
111
110
128
114
111
140
114
109
149
106
103
148
96
94
143
87
87
139
84
85
142
87
88
152
92
95
162
97
99
173
108
110
187
114
115
195
123
124
206
130
131
213
136
134
217
138
136
219
140
136
220
141
138
217
143
142
212
143
140
209
143
140
209
142
139
210
143
137
211
142
136
210
142
134
209
141
133
206
143
133
202
142
133
196
143
134
191
144
136
187
143
136
180
137
131
169
128
121
154
126
112
137
127
96
112
121
90
96
107
89
89
99
92
84
96
94
81
98
91
73
109
86
70
128
82
69
161
84
76
193
91
89
209
89
91
217
88
92
228
99
103
228
108
107
224
112
108
228
121
115
236
125
118
237
122
115
254
131
126
255
134
129
255
129
123
245
107
97
201
60
50
196
56
41
194
59
40
191
58
39
191
59
38
190
58
37
193
58
38
197
58
39
200
57
41
206
56
42
216
58
46
212
54
42
206
53
39
206
56
41
209
62
46
209
62
46
203
56
40
199
47
33
210
54
41
211
53
41
210
52
40
210
54
41
209
57
43
205
60
43
200
61
42
196
59
41
196
58
45
196
58
47
198
57
48
198
57
47
199
58
49
200
59
49
201
60
51
203
61
51
211
68
60
211
69
59
208
65
57
203
61
51
203
60
52
207
65
55
207
64
56
206
59
49
206
53
39
217
53
41
226
53
46
235
51
49
242
47
53
247
43
54
249
41
55
252
41
58
249
38
57
247
38
59
247
38
59
248
39
60
251
38
60
253
37
58
255
36
58
255
35
58
255
33
54
253
34
54
250
37
55
249
44
59
247
53
62
242
57
63
233
57
59
225
57
56
220
57
52
220
58
53
222
59
52
224
59
53
229
57
53
231
56
53
233
55
51
235
53
52
238
52
55
238
52
55
238
54
54
234
54
53
230
55
50
225
54
47
220
53
44
215
53
42
216
56
42
216
56
40
216
56
40
217
56
38
218
55
38
220
55
36
221
54
36
221
54
36
219
53
37
220
54
40
221
55
39
220
57
40
220
57
40
218
59
40
215
58
41
214
58
43
213
59
49
206
51
46
231
73
74
255
111
119
254
85
104
243
69
94
243
62
95
255
67
102
251
55
91
252
60
97
240
66
99
228
79
108
196
80
101
129
47
61
78
29
33
72
48
46
54
47
39
50
52
39
56
58
44
62
65
46
65
64
43
63
61
36
65
64
36
69
70
36
84
93
48
112
126
77
143
159
114
151
168
124
131
152
109
106
132
87
95
123
74
93
126
69
95
131
67
97
134
64
98
138
65
102
143
67
107
144
74
108
145
76
109
146
79
109
146
79
111
148
78
108
146
73
105
142
73
104
141
72
105
140
76
105
140
76
104
138
78
103
137
77
102
133
76
100
131
74
100
129
71
101
130
72
103
131
72
104
132
73
102
130
71
100
130
68
103
138
72
105
142
73
108
145
76
45
51
39
51
54
43
56
57
49
60
59
54
66
61
57
71
64
58
77
68
61
81
71
61
83
73
61
83
75
62
85
78
68
90
85
81
97
94
101
105
103
117
109
107
129
108
107
138
106
104
144
98
96
143
88
86
136
80
80
134
81
81
141
88
89
154
96
99
170
102
106
180
111
114
193
117
120
201
124
127
208
130
133
214
134
135
217
135
136
216
138
137
217
139
138
214
139
139
209
139
138
206
138
136
209
137
135
210
137
133
210
136
131
211
138
131
211
137
130
207
140
132
205
139
132
199
141
133
192
141
134
185
141
134
175
135
130
162
126
120
146
127
110
129
131
90
104
125
84
90
108
87
84
99
92
82
92
96
79
91
95
72
102
90
68
122
83
66
149
74
68
187
83
82
209
83
87
219
82
89
230
93
100
231
102
106
224
108
108
227
118
113
228
117
110
226
111
104
247
126
118
255
130
123
255
135
126
244
108
96
201
63
50
193
56
40
192
59
40
191
61
39
191
61
39
192
60
39
194
59
39
197
58
39
199
56
40
205
55
41
216
58
47
217
56
46
210
56
44
207
57
43
204
58
43
201
58
42
200
57
41
201
55
40
207
57
43
209
55
43
210
56
44
209
55
43
207
55
42
204
56
42
203
60
44
203
62
45
202
60
48
201
61
48
202
60
50
201
59
47
200
58
48
198
56
44
197
55
45
197
53
42
208
64
55
208
64
53
205
61
52
201
57
46
203
56
48
206
59
49
205
58
50
204
53
44
208
47
37
217
48
41
228
50
48
236
50
53
244
47
56
249
45
57
251
43
59
252
44
60
252
45
63
251
46
63
252
47
64
252
47
64
254
47
65
255
47
63
255
45
63
255
46
64
255
40
56
251
40
55
247
43
55
245
48
58
241
54
61
237
56
61
228
56
56
221
53
52
220
57
52
220
58
53
222
59
54
226
58
55
229
57
53
231
56
53
234
54
53
236
52
52
241
52
56
241
52
56
239
53
54
237
55
54
230
55
50
225
54
47
218
54
44
215
53
42
216
56
42
214
57
40
214
57
40
216
57
38
216
57
38
217
56
38
218
55
38
217
56
38
213
56
37
213
58
38
214
59
39
213
61
40
212
61
40
211
62
42
211
61
44
211
61
46
208
55
47
215
60
56
255
93
100
255
96
111
255
80
103
240
60
89
255
66
104
250
55
95
255
55
95
242
52
88
237
78
108
215
92
113
148
64
79
91
38
46
71
40
45
63
49
48
49
46
41
52
54
43
58
62
47
60
65
43
58
61
34
62
61
30
80
74
40
94
92
53
121
133
83
144
163
108
170
190
139
168
190
141
142
165
119
111
139
91
97
127
75
94
127
70
96
132
68
99
136
66
101
141
68
106
147
71
111
148
78
112
149
80
111
147
83
111
147
83
111
148
78
108
146
73
105
142
73
103
140
71
105
140
76
105
140
76
105
139
79
104
138
78
102
133
74
101
132
73
100
131
72
101
132
73
103
133
73
104
134
72
103
133
71
101
133
68
105
142
73
105
145
75
108
148
78
43
49
37
46
52
42
52
55
48
56
57
51
62
58
55
66
61
58
72
65
59
75
68
60
78
72
60
78
71
61
79
74
68
87
83
82
95
94
102
102
101
117
103
103
127
103
102
134
98
97
137
91
89
138
81
81
133
78
79
135
82
83
147
91
94
163
101
105
179
108
111
190
117
119
202
122
124
209
127
129
214
131
133
216
134
135
217
134
135
215
136
135
213
137
136
212
135
135
205
134
134
204
134
132
205
133
130
207
133
128
210
133
128
210
135
127
212
135
128
208
141
133
210
140
133
201
141
133
192
140
134
182
139
133
169
132
128
155
123
118
138
124
106
120
129
87
97
126
81
86
109
85
81
100
94
80
92
100
77
89
97
73
99
93
69
118
87
67
146
78
69
183
85
82
209
84
90
219
82
90
231
92
99
231
98
103
228
104
106
231
113
111
236
121
114
230
113
104
247
124
116
243
115
106
254
120
109
230
92
81
196
59
43
190
53
35
194
59
39
193
61
40
193
61
40
195
60
40
196
59
40
200
59
41
203
57
42
207
55
42
214
53
43
216
55
45
212
58
46
206
58
44
199
58
41
196
56
39
198
58
41
202
61
44
207
61
46
214
64
50
217
65
52
214
62
49
206
56
42
200
52
38
202
56
41
203
60
44
201
58
44
200
58
44
201
59
47
201
59
45
202
58
47
200
57
43
200
53
43
199
53
40
202
54
44
207
59
47
210
62
52
211
63
51
214
63
54
213
62
51
206
55
46
201
44
35
210
43
37
220
45
42
231
48
50
240
51
57
246
49
59
250
48
60
253
48
63
253
48
63
249
48
64
247
49
64
247
49
64
246
48
61
247
47
60
247
45
57
250
44
57
250
42
55
252
46
59
248
46
58
245
48
58
240
51
58
237
54
59
231
55
58
224
54
54
218
53
51
220
57
52
221
58
53
223
58
54
226
58
55
229
57
53
231
56
53
234
54
53
236
52
52
242
53
57
242
53
57
240
54
55
237
55
54
230
55
52
223
54
47
218
54
45
214
54
42
213
55
43
212
56
41
212
56
41
212
57
39
213
56
39
213
56
37
215
55
39
212
57
37
208
57
36
206
60
37
207
61
38
207
63
39
206
64
40
206
64
42
206
63
46
208
60
48
208
55
49
232
72
72
255
103
113
254
78
98
255
72
100
246
56
90
255
66
108
246
49
92
255
57
96
234
55
87
221
83
106
185
87
102
107
50
57
71
41
43
71
55
58
59
50
51
56
51
48
58
55
48
57
59
45
53
58
35
51
56
24
66
68
29
102
99
58
129
131
84
159
174
119
169
193
135
181
206
151
175
199
147
146
174
125
118
148
98
101
134
81
96
130
70
96
133
66
99
136
66
103
143
70
109
150
74
114
152
79
114
151
82
113
149
85
112
148
84
111
148
78
108
145
75
104
141
72
102
139
70
104
139
75
105
140
76
105
139
78
105
139
79
103
134
75
102
133
74
101
132
73
102
133
73
104
136
73
105
137
72
105
135
71
102
134
67
106
143
74
107
147
77
110
150
80
40
48
35
43
49
39
47
52
45
51
53
48
55
54
52
60
56
53
64
61
56
67
62
56
73
69
60
72
69
60
75
72
67
82
80
81
91
91
101
97
98
116
98
98
124
96
96
130
95
94
136
90
88
138
84
84
138
84
84
144
92
93
160
103
105
179
112
115
196
118
120
205
123
127
216
126
130
219
131
132
222
132
134
221
132
133
216
131
132
214
133
132
212
134
133
209
131
134
203
130
133
202
131
131
205
130
129
207
131
127
211
133
128
212
136
128
214
136
128
211
141
133
210
139
132
200
139
131
188
139
132
176
137
130
163
129
124
146
119
113
127
117
102
109
125
86
91
121
81
81
107
87
78
101
95
79
93
101
77
91
100
73
100
96
71
116
91
69
140
82
70
176
91
84
202
89
91
214
85
90
226
91
97
230
93
100
231
98
101
237
109
108
245
122
115
237
114
106
247
120
111
222
89
80
229
91
80
210
68
56
195
53
39
199
58
41
197
58
39
196
60
38
197
61
39
200
61
40
202
59
42
205
58
42
208
56
42
212
54
42
211
48
39
212
51
41
210
56
44
205
59
44
199
59
42
194
59
40
192
59
40
195
60
41
200
59
42
213
67
52
225
75
61
222
72
58
211
61
47
202
52
38
201
53
39
205
59
44
200
57
41
200
57
41
200
57
43
200
57
41
202
56
43
202
56
41
203
55
43
203
55
41
205
54
43
210
60
46
215
63
52
216
64
51
218
64
54
219
65
53
215
58
49
210
49
41
220
48
46
229
49
52
238
52
57
245
54
62
249
52
62
249
49
62
250
48
64
248
47
63
242
46
60
239
47
60
237
47
57
237
48
55
239
45
53
239
44
50
242
43
50
241
42
49
244
49
57
244
50
58
242
53
59
236
53
57
229
53
55
225
53
53
223
55
54
221
56
54
220
56
54
221
57
55
223
58
54
227
58
55
230
56
55
233
55
55
234
54
55
236
52
54
244
53
58
244
53
58
241
55
58
238
56
55
230
55
52
224
55
48
218
54
45
212
54
42
211
55
42
209
56
42
208
56
42
208
57
40
208
57
40
208
57
38
209
56
40
208
57
38
203
57
34
200
58
34
200
62
36
199
63
37
200
64
40
200
64
42
203
62
45
206
59
49
213
58
54
250
85
91
255
94
111
255
71
97
253
62
95
254
58
96
255
56
102
250
53
98
244
58
95
230
71
99
192
80
96
135
64
72
84
50
51
67
53
52
68
57
61
62
51
55
66
51
54
64
53
49
58
56
43
54
60
34
57
67
30
80
92
46
125
132
80
159
168
113
176
197
138
175
203
142
176
204
146
166
195
141
146
174
125
123
153
101
103
136
81
95
129
69
94
131
64
99
137
64
105
143
68
111
149
74
115
153
80
115
152
83
113
149
85
111
147
83
110
147
78
107
144
74
103
140
71
101
138
69
103
138
74
104
139
75
105
139
78
105
139
78
102
136
75
100
134
73
100
134
73
101
136
72
103
138
74
104
139
73
105
137
72
102
137
69
106
146
76
108
150
78
111
153
81
38
46
33
40
48
37
45
50
43
49
51
46
51
51
49
55
54
52
58
57
53
60
59
54
71
68
61
68
68
60
71
70
66
79
79
81
90
90
100
95
97
112
94
96
121
92
92
126
96
95
137
92
90
140
89
88
145
92
92
154
103
103
173
113
115
192
122
124
209
127
128
220
129
132
225
130
133
228
133
134
227
132
133
225
131
131
219
129
130
213
131
130
210
131
130
206
132
135
204
131
134
201
132
132
204
131
130
206
133
130
211
135
130
214
138
130
215
140
131
212
141
133
208
140
131
196
139
130
183
138
130
171
133
127
155
127
120
136
115
110
117
111
99
99
117
87
85
114
84
76
105
87
75
102
95
77
99
101
77
97
100
73
102
96
70
115
93
70
125
79
63
159
89
77
184
89
83
197
83
82
215
86
90
226
87
92
233
90
96
240
100
101
247
114
109
241
110
102
247
113
104
209
68
59
212
65
55
197
49
37
200
52
38
212
67
50
199
57
37
199
58
38
201
59
39
205
60
41
208
58
43
211
58
44
216
56
44
217
55
44
207
44
35
206
48
37
206
54
41
203
60
44
199
64
45
193
62
42
190
59
39
188
55
36
192
52
35
209
66
50
227
79
65
228
78
64
216
66
52
203
55
41
203
55
41
208
62
47
202
59
43
201
58
42
199
56
40
198
55
39
200
54
39
202
54
40
206
56
42
208
56
43
211
59
46
213
59
47
213
57
45
210
54
42
213
55
44
218
60
49
221
60
50
220
55
49
232
56
56
241
56
61
247
58
65
251
57
66
250
52
65
245
47
60
242
44
59
239
44
58
237
47
59
234
49
57
234
51
56
235
52
56
237
53
55
241
53
54
244
51
54
243
53
55
239
50
54
238
53
58
236
56
59
233
54
57
226
52
53
222
52
52
223
55
54
225
60
58
221
56
54
222
57
55
225
57
54
227
58
55
230
56
55
233
55
55
234
54
55
236
52
54
244
53
58
244
53
58
241
55
58
236
56
57
230
57
53
223
56
50
216
55
47
211
55
43
208
56
43
205
58
42
205
57
43
204
59
42
205
58
42
205
58
40
205
58
42
204
59
40
197
57
34
194
58
34
195
61
36
194
62
37
195
63
40
198
63
43
201
61
46
205
56
49
220
60
62
255
93
102
255
81
103
255
72
100
250
56
91
255
63
103
246
46
92
251
62
104
228
61
91
224
93
111
162
76
85
91
45
45
71
56
51
70
66
63
64
55
58
67
52
57
71
48
54
70
50
49
64
57
41
63
68
38
69
86
42
96
116
63
139
157
97
170
190
127
177
203
140
171
201
139
166
195
139
157
186
132
142
172
122
123
153
101
102
135
80
90
124
64
94
129
63
97
135
62
104
142
67
111
149
74
114
152
79
114
151
82
112
148
84
110
146
82
110
147
78
106
143
73
102
139
70
100
137
68
102
137
73
104
139
75
105
139
78
105
139
78
102
136
75
101
135
74
100
135
71
101
136
70
103
138
72
104
139
71
104
139
71
101
138
68
107
147
76
109
151
79
112
154
82
40
48
33
39
47
34
40
45
38
41
46
40
46
48
45
51
51
49
55
56
51
57
58
52
61
61
53
63
64
56
68
69
64
74
76
75
79
82
89
86
88
101
90
92
115
93
94
125
89
88
130
91
89
139
94
93
150
101
101
163
110
110
182
119
120
200
127
129
216
132
133
226
140
140
238
140
140
240
138
138
236
134
135
228
130
130
220
128
128
214
130
128
211
131
130
206
128
131
198
129
133
197
131
131
201
132
132
206
134
131
208
136
131
211
139
132
210
140
132
207
138
128
197
138
128
188
137
128
175
135
125
160
128
120
143
119
111
122
107
101
103
102
93
88
106
92
81
108
92
77
104
92
76
102
94
75
101
95
73
102
96
72
106
96
71
114
94
70
114
79
59
135
84
65
166
92
79
193
96
89
209
89
88
220
81
84
237
83
91
247
92
96
249
104
101
230
89
80
212
68
60
206
55
48
209
55
47
215
58
49
214
61
47
212
59
43
207
57
40
205
58
38
207
57
40
209
58
41
212
56
41
216
56
42
220
53
44
220
53
44
208
47
37
205
51
41
204
56
44
202
62
47
197
64
47
193
64
45
190
61
42
188
57
39
194
57
41
190
48
34
233
87
74
223
75
63
195
47
35
209
61
49
206
60
47
199
56
40
202
59
43
201
58
41
200
57
40
199
56
39
200
55
38
201
54
38
203
53
38
204
52
38
208
55
41
215
59
46
220
62
50
220
60
48
217
55
44
218
56
45
226
62
52
236
67
60
245
66
69
242
57
63
240
50
60
246
52
63
246
50
64
241
45
59
237
45
58
237
50
61
236
55
62
230
53
59
228
54
56
229
55
56
235
57
57
240
58
57
241
55
56
239
55
55
242
59
61
238
60
60
234
58
58
230
56
55
225
56
53
223
55
52
220
55
53
220
55
53
222
57
55
225
57
56
227
57
57
230
58
58
233
57
59
236
57
60
237
57
60
239
56
60
244
55
59
243
54
58
240
56
58
237
57
58
231
59
57
224
59
53
215
57
48
207
55
44
202
56
43
200
57
41
199
57
43
198
58
41
199
58
41
199
58
40
200
59
42
199
60
41
193
57
35
196
61
39
190
58
35
182
52
30
188
57
37
201
68
51
206
64
54
201
49
44
255
95
101
255
87
103
255
75
98
252
61
92
245
51
88
242
50
89
247
56
99
240
69
105
228
91
111
171
71
81
110
52
51
78
51
44
67
63
54
65
66
60
66
54
56
66
43
49
77
46
51
77
49
46
67
56
38
68
74
40
88
110
61
125
155
95
155
186
119
167
198
130
171
203
140
159
190
130
152
182
128
148
178
126
130
160
110
102
132
82
88
118
66
89
123
63
93
128
62
99
135
65
105
143
68
110
148
73
109
149
76
108
148
78
107
146
79
108
145
78
105
142
73
102
138
68
99
134
66
99
134
66
102
137
73
104
139
75
102
138
76
100
136
74
103
139
77
104
140
78
103
139
75
102
139
72
101
138
69
102
139
69
106
143
73
107
147
74
113
155
81
113
157
82
114
158
83
42
48
34
41
47
35
41
46
39
41
46
40
45
47
44
49
51
48
53
54
49
55
56
50
59
60
52
62
63
55
65
68
61
72
74
71
79
80
84
83
85
97
88
89
107
90
92
117
92
90
127
94
93
137
100
98
148
107
106
164
117
116
186
127
126
206
135
135
223
139
140
233
141
141
239
140
140
240
137
137
235
133
132
226
128
128
216
128
126
210
128
127
207
130
130
204
133
134
199
134
135
199
135
136
201
137
136
204
139
136
207
140
134
206
141
134
204
142
133
200
138
128
188
137
127
177
134
124
161
130
120
147
123
115
130
116
107
112
108
98
96
100
93
83
99
96
79
98
96
75
101
94
75
102
94
75
105
92
73
108
93
72
110
94
71
113
93
69
121
93
71
130
89
67
149
88
70
173
90
76
199
86
80
221
83
83
243
82
88
254
89
95
240
85
83
227
76
69
217
62
57
213
54
48
217
54
47
220
57
48
219
57
44
214
57
40
211
56
38
209
56
38
209
56
40
212
56
41
215
55
41
220
54
42
223
52
44
221
53
44
206
48
39
201
53
41
200
57
43
198
61
45
196
65
47
193
64
45
191
60
42
190
57
40
196
56
41
203
60
46
228
82
69
220
72
60
200
52
40
204
58
45
200
58
44
194
52
38
202
61
44
203
60
43
202
59
42
202
59
42
203
58
41
204
57
41
206
56
41
207
54
40
211
55
42
214
56
44
217
55
44
215
51
41
213
49
39
216
49
40
222
55
46
231
60
53
248
68
71
241
56
62
237
47
57
239
47
58
240
48
61
238
46
59
235
49
60
235
54
63
228
53
58
220
51
54
216
50
50
223
58
56
238
69
66
248
73
70
248
68
67
243
63
62
233
58
55
231
57
56
229
57
55
226
57
54
225
56
53
224
56
53
225
57
56
225
57
56
224
56
55
226
56
56
229
57
57
231
57
58
233
57
59
236
57
60
237
57
60
238
55
59
243
54
58
242
53
57
238
54
56
234
56
56
228
59
56
220
58
53
211
57
47
203
55
43
198
56
42
195
58
42
194
58
42
192
59
42
192
59
42
193
60
41
193
60
43
193
60
41
189
57
36
193
63
41
187
58
37
179
53
31
185
58
39
192
60
47
201
58
50
215
61
61
255
95
103
255
81
100
253
67
91
246
57
87
242
57
91
242
61
96
240
64
100
224
74
101
170
64
76
131
63
62
95
58
50
72
58
47
59
61
48
59
61
50
73
59
58
85
61
61
81
47
46
84
55
47
87
75
53
100
104
69
119
143
91
140
176
112
154
195
125
159
199
129
152
186
125
154
185
128
154
182
131
141
169
120
114
142
94
89
117
69
84
112
61
92
123
66
95
127
64
98
134
64
105
141
67
109
147
72
108
148
75
107
147
76
105
144
77
105
144
77
103
140
71
101
137
67
99
134
66
99
134
68
103
138
74
105
140
76
104
140
78
103
139
77
105
141
79
105
141
77
105
142
75
103
140
71
103
140
70
102
142
69
108
146
73
109
149
76
112
156
81
111
157
82
114
158
83
45
49
34
45
49
35
44
47
38
44
46
41
47
47
45
49
49
47
51
52
47
52
53
45
57
59
48
60
62
49
64
66
55
69
72
65
75
77
76
81
82
87
85
85
97
86
87
105
92
89
116
94
93
127
102
100
140
112
110
160
122
120
183
133
131
206
141
139
223
144
144
234
144
142
239
142
140
237
138
137
231
135
132
223
129
127
211
127
124
203
127
125
200
128
127
195
134
134
196
136
136
196
138
138
198
140
139
199
140
138
201
140
136
197
140
132
191
139
130
183
137
128
175
134
124
161
128
118
145
123
112
129
117
106
114
110
100
99
105
94
88
99
93
81
93
98
76
93
98
75
97
95
74
103
92
74
109
90
73
112
91
72
114
92
71
114
92
69
122
96
73
123
87
63
137
82
61
164
85
68
199
88
79
224
86
83
245
80
84
252
79
83
226
62
60
222
60
55
221
56
52
222
55
49
225
54
47
225
54
46
221
55
43
218
55
40
212
55
38
211
56
36
211
56
38
213
56
39
216
54
41
221
53
42
224
51
44
221
53
44
203
51
40
196
54
40
194
57
41
193
60
43
194
63
45
194
63
45
193
60
43
192
56
40
195
53
39
220
74
61
218
70
58
213
65
53
206
60
47
196
54
40
196
59
43
190
54
38
198
58
41
199
58
41
199
58
41
201
58
42
202
56
41
203
55
41
204
54
40
205
51
39
213
57
45
213
55
44
214
51
42
215
51
42
218
51
43
223
54
47
229
58
51
233
60
56
240
60
61
237
52
57
235
48
55
236
49
56
238
51
60
237
52
60
235
55
64
234
61
67
221
55
59
225
65
65
236
78
75
248
89
85
253
90
85
250
81
76
239
66
62
229
56
52
226
54
50
224
55
50
224
55
50
224
55
50
225
56
53
226
57
54
228
59
56
228
59
56
225
55
55
228
56
56
230
56
58
232
56
59
235
56
60
235
56
60
235
56
60
236
56
59
239
53
56
238
52
55
236
53
55
231
55
55
226
58
55
217
58
52
207
56
47
199
55
44
194
56
43
191
58
43
189
59
43
187
60
43
187
60
43
187
60
41
187
60
43
187
60
41
184
57
38
189
64
42
179
57
36
177
54
36
182
59
43
180
49
39
194
52
48
235
80
84
255
92
105
254
75
96
243
59
85
237
57
86
238
65
93
236
73
102
227
75
100
202
79
97
130
56
57
102
62
54
85
63
50
71
64
48
59
60
44
56
54
41
65
52
43
78
55
47
72
43
35
83
60
44
103
93
66
129
136
95
146
171
116
149
188
123
148
194
122
149
192
123
146
180
120
156
184
133
154
180
133
128
154
109
92
117
75
72
98
53
78
104
56
91
120
66
95
125
63
99
131
64
104
140
68
108
146
71
106
146
73
104
146
74
103
144
76
102
143
75
100
137
68
100
135
67
99
134
68
101
136
70
104
139
75
107
142
78
107
143
79
106
142
78
105
144
79
105
144
79
105
144
77
104
144
74
103
143
72
104
146
72
108
148
75
111
153
77
115
161
86
115
163
87
117
163
88
50
52
38
48
52
38
48
49
41
48
49
43
49
50
45
50
51
46
51
52
46
52
54
43
57
59
45
59
62
45
63
65
51
68
70
57
73
74
68
77
79
78
82
81
87
83
83
93
88
86
100
92
90
111
100
97
126
110
108
148
122
119
172
132
130
195
140
137
214
145
143
227
148
145
234
147
144
235
142
139
226
137
133
217
130
127
204
127
124
195
127
124
191
128
126
189
132
131
189
134
133
190
137
136
193
141
138
193
139
136
189
137
133
183
135
127
174
133
125
166
133
123
157
128
118
143
121
110
127
114
102
112
109
97
99
107
93
90
105
90
83
99
91
78
92
97
75
90
98
74
97
95
74
103
92
74
111
89
75
115
89
74
117
90
73
119
91
70
118
86
65
123
81
59
139
81
61
171
88
70
205
91
80
226
84
80
237
71
73
239
63
65
219
50
47
220
53
47
225
53
49
227
54
48
227
53
46
225
53
43
224
54
41
219
56
41
214
55
36
211
56
36
209
56
38
212
57
39
216
54
41
220
54
42
224
51
44
218
54
45
200
54
41
189
56
39
188
55
38
189
58
40
191
62
43
194
63
45
195
59
43
196
56
41
198
52
39
233
85
73
211
60
49
204
56
44
209
66
52
190
53
37
192
61
43
192
61
43
192
56
40
196
56
39
197
57
40
199
58
41
200
57
41
203
55
41
204
54
40
205
51
39
210
54
42
212
51
41
213
50
41
219
52
44
227
58
51
233
62
55
236
63
57
236
61
58
232
52
53
236
53
57
239
54
59
238
53
59
234
50
58
230
51
57
228
55
61
226
60
64
243
85
86
252
98
98
255
109
106
255
105
100
243
86
81
228
65
60
223
54
51
221
52
47
223
54
49
222
55
49
222
55
49
223
56
50
225
56
53
226
57
54
229
57
55
230
58
56
229
55
56
230
56
57
232
56
59
233
57
60
235
56
60
235
56
60
235
56
60
236
55
60
238
53
58
238
54
56
234
54
55
229
57
55
223
60
55
214
60
52
204
57
47
197
57
44
190
57
42
186
59
42
182
59
41
181
60
41
181
60
43
180
61
41
179
59
42
179
60
40
179
58
39
183
64
44
173
56
38
173
56
39
179
59
45
171
43
34
193
53
52
252
97
103
253
85
100
249
72
92
238
62
85
232
62
88
229
73
95
220
81
100
206
81
95
177
85
90
119
70
65
92
70
57
79
66
50
74
67
49
70
67
48
64
61
42
61
54
35
61
50
30
65
50
29
81
70
42
110
107
72
138
147
102
149
172
116
145
180
116
141
184
115
144
186
120
157
187
133
161
184
138
144
167
125
108
130
91
75
95
58
65
87
48
76
99
55
87
114
63
95
123
64
100
131
64
105
138
67
107
145
70
105
145
72
103
145
71
100
143
72
101
142
74
98
135
66
99
134
66
100
135
69
103
138
72
106
141
77
108
143
79
109
145
81
110
146
82
106
145
80
106
145
78
106
146
76
104
146
74
103
145
71
105
147
71
109
151
75
112
156
79
119
166
88
118
166
90
118
166
90
55
56
40
54
56
42
54
54
44
53
54
46
53
54
48
53
54
48
54
55
47
54
56
43
59
62
45
61
64
45
64
67
46
68
71
52
73
75
62
77
78
70
81
80
78
82
82
82
86
85
83
89
88
93
97
95
108
107
105
129
117
115
155
128
125
178
136
134
199
140
138
211
149
146
225
148
145
226
144
141
220
139
135
211
132
129
196
129
125
186
127
124
181
127
124
179
129
126
181
132
129
182
135
133
183
137
134
181
138
132
176
134
128
166
129
122
155
128
119
146
127
115
135
122
110
124
114
101
108
109
95
95
106
91
88
105
90
83
106
89
79
102
90
76
97
96
76
95
97
76
100
95
76
105
92
75
110
91
76
116
90
75
122
90
75
125
90
71
127
84
65
136
81
61
157
81
65
182
86
70
205
83
72
216
72
64
223
58
56
226
52
51
221
49
45
223
51
47
226
53
47
227
53
46
225
51
44
223
51
41
221
53
40
220
57
40
211
56
36
208
57
36
207
58
38
209
58
39
212
56
41
217
55
42
221
53
44
216
55
45
196
59
43
185
58
41
181
54
37
183
56
39
189
59
43
194
61
46
199
59
46
199
55
44
208
57
48
245
92
84
214
61
53
201
53
43
207
65
53
184
51
36
183
58
40
188
63
43
191
58
41
195
58
42
198
58
43
200
58
44
202
59
45
204
58
45
207
56
45
209
55
45
208
51
42
210
49
41
214
49
43
222
55
49
229
60
55
234
62
58
232
59
55
230
55
52
233
55
53
239
59
60
240
60
63
234
54
57
228
49
53
228
53
58
232
66
70
237
77
79
255
120
119
255
113
110
241
96
91
221
74
67
209
56
51
209
50
46
220
55
51
231
64
58
224
57
51
224
57
49
224
55
50
224
55
50
226
54
50
227
55
51
229
55
54
229
55
54
230
54
56
231
55
57
234
55
59
235
56
60
235
56
62
235
56
62
232
55
61
234
55
59
238
55
59
238
55
57
234
55
58
228
58
58
223
61
58
214
61
55
203
59
50
195
59
47
186
56
42
181
58
42
179
59
42
175
60
41
175
60
42
172
60
40
172
59
41
172
59
41
170
59
40
172
61
42
166
56
39
167
57
42
170
56
45
171
48
41
203
64
67
255
105
114
243
78
94
241
72
91
235
69
89
226
73
91
215
80
94
199
85
93
178
86
87
153
90
85
103
75
63
83
74
57
80
68
52
78
67
49
73
67
45
66
64
39
63
66
35
65
71
35
77
85
46
94
103
60
120
129
84
139
153
102
145
164
109
141
169
110
143
178
114
151
185
125
157
183
136
146
167
128
117
134
100
82
99
67
63
80
48
66
83
49
79
97
57
86
108
61
98
123
66
101
130
66
105
138
67
106
144
69
105
146
70
101
145
70
98
144
72
99
142
71
98
135
66
102
134
67
103
138
72
105
140
74
107
142
78
109
144
80
110
146
82
111
147
83
106
145
78
107
146
79
106
147
77
105
147
75
105
147
73
106
150
73
110
154
75
114
158
79
120
167
89
118
167
88
118
167
88
63
61
46
61
62
46
61
61
49
61
61
51
61
61
53
59
60
52
59
61
50
59
61
47
63
66
47
64
68
45
67
71
46
70
74
51
75
76
58
78
79
65
81
81
73
82
82
72
89
87
74
90
90
78
95
94
92
103
101
114
113
110
137
123
121
161
131
128
183
135
133
196
143
140
209
143
140
211
141
138
207
138
133
199
131
127
186
126
121
175
122
119
166
121
118
165
123
119
169
124
122
171
128
125
170
131
126
166
131
125
161
127
121
149
124
116
137
122
114
129
118
107
115
114
102
104
109
95
94
106
91
86
107
90
82
108
90
80
108
90
78
107
91
78
103
94
77
102
95
77
103
94
77
106
93
77
110
93
77
118
92
77
124
92
77
131
89
73
150
94
79
157
85
71
169
77
64
185
73
61
197
64
55
204
56
46
213
50
43
221
50
43
225
53
49
225
54
47
226
53
46
226
52
45
224
50
41
222
52
39
220
54
38
218
59
40
209
57
36
205
59
36
204
59
38
206
59
39
209
58
41
213
57
42
217
55
44
211
57
45
193
62
44
180
59
40
176
53
35
178
53
35
187
57
41
196
60
46
201
59
47
203
55
45
217
63
55
248
91
84
227
73
65
206
58
48
204
64
51
184
54
38
172
51
32
181
60
41
188
57
39
191
55
39
192
55
39
195
55
40
197
55
41
201
55
42
206
55
44
209
55
45
217
60
51
219
58
50
222
57
51
229
60
55
234
62
58
234
61
57
231
56
53
227
52
49
232
57
54
235
60
57
236
58
58
232
56
58
236
62
64
248
80
80
255
98
99
255
112
112
249
104
101
226
84
80
202
61
54
192
49
43
201
52
48
214
59
55
223
61
58
224
59
55
226
59
53
225
58
50
226
57
52
225
56
51
227
55
51
228
55
51
229
53
53
230
54
54
233
54
57
233
54
57
234
55
59
235
56
60
235
56
62
234
55
61
232
55
61
233
54
58
239
56
60
236
56
57
232
56
58
227
59
58
221
62
58
211
62
55
201
60
50
192
60
47
182
56
41
178
58
41
174
59
41
170
59
40
169
60
40
168
59
39
165
58
38
165
58
40
165
60
41
160
57
40
160
59
41
160
57
42
162
54
42
181
62
56
219
84
88
249
102
112
237
76
92
237
74
93
231
76
92
216
78
91
198
83
90
177
87
86
156
92
82
136
97
82
84
71
55
77
74
57
83
70
53
77
62
43
63
51
27
57
54
23
65
77
37
78
102
54
108
138
84
119
148
92
132
155
101
137
154
100
141
155
102
144
163
107
148
173
116
152
176
124
131
149
109
110
125
92
81
96
67
60
74
48
57
70
44
65
80
49
79
95
59
87
106
61
99
122
68
101
130
66
106
139
68
107
145
70
105
146
70
101
145
70
97
143
70
99
142
71
99
136
67
105
137
70
106
141
75
108
143
77
108
143
79
108
143
79
109
145
81
110
146
82
109
148
81
109
149
79
109
150
80
108
150
76
107
151
74
109
153
74
114
159
78
115
162
81
124
173
94
123
172
93
122
171
92
68
65
48
68
66
51
68
66
54
67
67
57
67
67
59
67
67
59
66
66
54
66
67
51
68
70
48
69
72
45
71
74
45
74
77
50
79
78
57
82
81
63
84
82
69
85
83
68
92
90
67
92
92
68
92
93
79
97
97
97
105
104
120
114
113
144
123
122
166
129
126
181
134
130
191
136
131
195
136
132
193
132
129
186
127
123
174
121
115
161
116
111
151
111
109
148
112
109
154
114
111
158
118
113
154
119
115
150
119
115
142
116
111
131
115
107
120
114
105
110
109
97
97
108
95
89
106
92
83
107
90
80
109
91
79
111
91
80
111
92
78
111
92
78
110
92
78
109
93
78
107
94
78
108
95
79
111
95
79
117
96
79
125
93
78
137
89
77
161
95
83
167
79
69
174
66
56
186
59
50
197
55
45
204
50
40
212
51
41
220
53
44
224
55
48
225
54
47
225
52
45
224
51
44
224
52
42
221
53
40
216
55
37
212
57
37
207
58
34
202
61
34
201
61
36
202
62
39
205
60
41
210
59
42
213
57
44
207
59
45
191
64
45
177
60
40
172
53
33
174
51
33
186
56
40
196
60
46
202
58
47
206
53
45
216
57
51
240
78
73
241
84
77
217
66
57
204
66
53
190
63
46
166
49
29
179
60
40
190
60
44
193
57
43
193
55
42
194
54
41
197
55
43
201
57
46
207
59
49
212
59
51
225
68
61
227
65
60
228
63
59
231
62
59
233
61
59
233
59
58
232
56
56
231
56
53
231
56
53
230
57
53
230
55
52
233
59
58
244
74
74
255
92
91
255
100
99
248
99
95
204
61
57
193
56
50
189
52
46
195
56
49
213
65
61
222
69
64
223
61
59
217
52
50
226
59
53
226
59
51
226
57
52
226
57
52
228
56
52
229
56
52
231
55
55
233
55
55
234
54
57
235
55
58
235
54
59
235
56
60
235
56
62
232
55
61
232
55
61
233
54
58
235
55
58
233
54
57
230
56
57
224
59
57
219
62
57
209
62
54
198
60
49
190
60
46
179
56
41
174
59
41
170
59
40
166
59
39
164
59
40
163
58
39
163
58
39
160
57
38
161
62
43
150
55
37
156
61
43
154
57
41
154
50
39
195
80
75
237
107
109
237
93
102
235
80
94
232
75
92
220
74
87
202
77
83
182
84
83
161
93
82
142
101
81
127
106
85
78
75
56
75
72
55
80
63
47
76
55
36
69
53
27
71
71
35
87
111
61
105
146
86
127
178
111
130
179
113
131
165
104
125
146
89
131
141
89
142
150
99
137
151
100
124
140
95
88
102
67
71
84
56
57
69
45
54
66
42
58
68
44
61
74
48
74
88
55
87
104
62
101
124
70
104
130
67
108
139
69
108
144
70
105
147
71
99
145
70
97
145
71
97
143
71
102
139
70
108
140
73
109
144
78
110
145
79
108
143
79
107
142
78
107
143
79
109
145
81
112
151
84
113
153
83
112
153
83
112
154
80
111
155
78
113
157
78
116
163
82
119
166
85
126
175
96
123
174
95
121
172
93
73
67
51
74
68
54
73
69
57
73
69
58
73
71
59
73
71
59
72
70
57
72
71
53
73
72
51
74
74
50
76
76
50
78
78
52
82
80
59
84
81
62
88
82
68
89
84
65
93
89
62
91
90
62
92
89
72
94
91
84
100
97
104
110
107
126
120
115
147
125
120
160
129
123
167
131
125
171
133
127
173
131
124
166
125
119
157
117
110
143
111
105
133
107
103
130
105
102
133
106
103
134
109
104
134
109
105
128
109
105
120
108
103
110
107
98
101
106
96
94
103
93
84
104
92
80
104
90
77
107
91
76
110
93
77
113
94
79
113
94
79
112
93
78
112
93
79
111
93
79
112
93
78
115
93
79
119
93
78
126
93
78
137
91
76
147
87
76
162
84
72
166
69
60
171
57
47
186
55
45
200
56
47
211
54
45
217
54
45
221
54
45
223
54
47
223
52
45
223
52
44
223
52
44
223
55
44
221
55
41
216
54
39
209
54
36
207
58
36
204
60
36
202
60
38
203
61
39
206
59
41
209
59
42
211
58
44
206
60
45
190
65
47
175
60
39
168
51
33
172
51
32
182
57
39
193
61
46
201
59
47
204
56
46
204
50
42
224
65
59
248
91
84
224
73
64
208
68
55
202
70
55
171
50
31
184
63
44
202
70
55
204
66
53
202
64
51
203
63
50
205
63
51
210
66
55
220
69
60
225
71
63
226
64
59
226
61
57
227
58
55
226
54
52
226
54
52
227
55
53
229
57
55
229
60
57
228
63
57
224
61
54
221
58
53
225
63
58
234
77
72
234
81
76
211
64
57
184
41
35
184
45
38
190
53
45
201
62
55
208
65
59
214
62
57
217
59
56
225
59
59
231
61
61
226
57
54
226
57
54
226
57
54
228
56
54
229
57
53
231
58
54
233
57
57
234
58
58
234
56
56
234
56
56
235
56
59
235
56
59
233
57
60
232
56
59
231
55
58
231
55
58
233
54
58
230
54
56
226
56
56
222
58
56
214
61
56
204
61
53
194
60
49
185
59
45
174
57
40
170
59
42
166
59
41
162
59
40
160
59
39
158
59
38
159
58
40
155
58
39
152
63
45
140
53
34
153
61
46
152
56
42
151
47
38
205
92
88
246
122
124
222
88
95
229
89
100
217
81
91
200
77
82
183
77
79
167
87
80
155
97
85
141
104
85
128
110
86
84
84
56
67
72
42
66
57
28
72
59
27
88
80
43
110
116
68
132
162
102
148
193
126
135
188
118
134
184
115
124
158
97
110
130
77
117
126
81
130
135
94
117
125
86
91
98
64
59
68
41
52
60
37
53
61
40
61
68
50
60
69
50
57
67
43
67
80
50
85
101
62
103
123
72
105
131
70
108
139
71
109
145
71
106
147
71
102
146
71
98
144
71
99
142
71
104
141
74
108
143
77
111
146
80
110
147
80
108
143
77
105
142
75
106
143
74
106
146
76
115
155
84
115
157
85
115
157
83
114
158
81
114
158
79
115
162
82
119
166
85
122
171
90
119
167
91
117
165
91
116
164
90
82
75
59
82
75
59
82
74
61
82
74
61
82
74
61
82
74
61
82
74
61
82
75
59
85
78
60
86
79
61
87
80
62
88
81
63
89
82
64
91
84
68
92
85
69
92
85
69
98
91
72
97
90
72
97
89
76
98
89
82
101
93
91
107
97
105
113
102
116
117
106
123
122
110
132
124
112
136
127
115
139
127
115
137
124
112
132
117
106
123
109
98
114
101
93
106
99
96
105
96
95
101
95
93
98
95
90
94
94
90
89
94
89
85
97
90
82
98
90
79
101
91
79
103
91
75
105
92
75
107
92
73
107
92
71
109
92
72
109
93
70
106
94
72
106
98
79
114
101
84
113
90
74
117
79
66
139
87
74
156
92
80
162
90
76
170
91
76
164
78
61
165
70
52
166
60
44
176
56
40
194
56
45
210
56
48
221
52
49
222
49
45
223
54
47
222
55
46
222
55
46
222
55
46
222
55
46
219
55
43
216
54
41
215
55
41
213
56
41
212
56
41
210
57
41
210
57
41
210
57
43
210
57
43
212
56
43
205
59
44
189
59
43
182
62
45
174
57
40
169
49
32
171
50
33
183
57
42
192
60
47
195
57
46
200
53
43
206
53
45
212
58
48
219
67
56
221
75
62
212
72
57
194
61
44
183
50
35
221
80
70
216
74
64
212
70
60
211
69
59
215
73
63
219
72
64
221
68
62
223
61
58
223
55
52
230
56
55
234
58
58
234
58
58
229
57
55
224
55
52
222
57
51
220
61
55
220
69
62
209
65
57
199
57
47
191
50
40
185
48
38
186
52
41
191
59
47
195
63
51
195
58
48
199
61
51
208
64
55
217
64
58
224
62
59
229
60
57
233
57
57
236
58
58
230
56
58
229
56
58
229
56
58
229
57
57
229
57
55
229
57
55
229
57
53
229
57
53
224
52
48
225
53
49
228
54
53
229
55
54
230
56
57
231
57
58
231
57
58
231
57
59
235
59
62
230
57
59
223
57
57
220
61
58
214
65
61
205
66
59
193
65
54
183
63
49
169
58
41
163
60
43
160
61
42
155
60
40
153
58
38
150
58
37
151
56
38
145
56
38
144
65
48
139
60
45
135
47
35
145
47
38
188
76
74
228
109
111
229
109
111
205
86
90
189
76
78
181
78
79
170
81
77
162
85
79
155
88
79
150
93
82
149
97
84
138
106
83
88
89
47
61
75
22
56
69
15
86
97
41
127
141
80
155
176
111
161
188
121
156
189
120
146
181
115
126
157
97
104
128
76
92
109
67
89
100
66
84
93
66
72
80
57
63
68
48
58
61
44
64
67
50
60
64
49
58
62
48
60
67
51
58
67
48
65
78
50
82
100
62
93
115
66
103
131
72
112
143
75
111
147
75
108
146
71
106
146
73
105
147
75
103
144
74
108
144
80
109
145
83
111
147
83
108
147
80
109
146
77
107
147
74
109
150
74
109
152
73
113
156
76
113
158
77
114
159
78
114
161
80
116
163
82
118
167
85
121
170
89
122
170
94
123
168
101
111
155
92
101
145
82
87
78
63
87
78
63
87
78
63
87
78
63
87
78
63
87
78
63
87
78
63
87
78
63
88
79
64
89
80
65
89
80
65
91
82
67
92
83
68
93
84
69
94
85
70
95
86
71
100
91
74
99
90
73
99
89
77
99
89
79
102
91
85
106
95
93
110
98
100
112
100
104
114
101
108
116
103
112
118
105
114
118
105
114
115
102
109
109
96
103
103
91
95
98
88
89
94
91
86
90
90
82
89
86
79
87
84
75
88
84
73
88
84
72
92
86
72
94
87
71
99
90
75
99
90
73
102
91
73
103
92
72
105
93
71
106
94
70
107
95
71
106
95
73
102
97
77
111
103
84
119
91
77
129
83
70
153
86
77
168
88
79
175
82
74
180
82
69
173
73
57
170
67
48
171
61
44
177
57
41
190
56
44
204
55
48
216
52
50
220
51
48
221
54
46
220
56
46
220
56
46
220
56
46
219
55
45
218
56
45
217
55
44
215
55
43
215
55
43
213
55
43
213
55
43
213
55
43
212
56
43
212
56
43
212
56
43
206
58
44
190
60
46
182
62
46
174
57
40
166
49
32
167
50
32
178
58
41
185
62
46
190
60
46
198
58
45
199
52
42
203
52
41
210
59
48
220
69
58
225
79
66
223
81
67
221
79
67
219
70
63
214
65
59
209
60
53
208
59
52
211
62
55
215
62
56
217
58
54
218
53
51
230
56
55
234
56
56
236
56
57
234
56
56
230
56
55
226
58
55
225
62
57
219
66
58
205
64
55
194
61
52
187
57
44
180
52
39
178
52
38
181
55
41
186
60
45
191
64
49
198
64
52
205
64
54
213
65
55
220
63
56
226
58
55
230
56
55
235
55
54
239
56
58
233
54
58
231
54
60
231
55
58
230
56
57
230
56
55
229
57
53
229
57
53
227
58
53
226
57
50
227
58
51
227
58
53
228
59
54
228
59
56
228
59
56
229
57
57
229
57
57
228
58
59
227
59
59
223
60
61
217
63
61
208
65
59
196
63
54
183
61
50
172
59
45
165
59
43
160
61
42
156
61
43
151
60
41
150
59
40
146
58
38
147
56
37
140
58
37
128
57
39
128
56
41
137
53
42
157
60
53
191
81
80
215
101
101
209
94
97
183
77
77
172
78
76
164
84
77
157
89
80
152
92
81
151
94
83
151
94
83
152
95
86
140
102
81
96
95
47
83
103
42
94
114
51
122
144
79
153
175
110
165
190
124
153
178
112
136
160
98
106
130
72
97
117
66
84
101
59
74
88
55
70
80
55
63
72
53
57
63
49
53
56
45
58
60
47
65
66
52
61
63
52
58
61
50
61
65
51
58
67
48
64
77
51
82
100
62
94
116
69
103
131
72
113
144
77
112
148
76
110
146
72
107
148
72
107
147
76
104
145
75
109
145
81
110
146
84
109
148
83
109
148
81
107
147
76
107
148
72
108
151
72
109
152
72
112
157
74
112
157
74
112
160
76
115
162
81
117
166
84
120
169
88
120
169
88
120
168
94
110
152
89
100
141
83
90
131
73
93
84
69
93
84
69
93
84
69
93
84
69
93
84
69
93
84
69
93
84
69
93
84
69
91
82
67
92
83
68
93
84
69
94
85
70
95
86
71
96
87
72
97
88
73
97
88
71
101
93
74
101
93
72
100
91
74
100
91
76
101
91
81
102
91
85
104
93
89
104
93
91
105
93
93
106
94
94
107
95
95
107
95
95
105
94
92
102
91
89
98
87
83
95
86
81
90
86
77
87
84
75
85
81
72
82
78
67
81
77
66
83
79
67
87
81
67
88
82
66
94
87
71
94
87
69
97
89
70
99
91
70
103
92
72
105
94
72
107
96
74
105
97
76
101
96
77
113
101
85
123
94
80
136
88
78
162
91
85
175
88
81
177
74
69
182
71
64
182
65
55
183
63
49
182
60
47
184
56
43
189
55
43
197
57
44
204
57
47
211
57
47
217
56
46
219
55
45
219
55
45
219
57
46
218
56
45
217
55
44
217
55
44
215
55
43
215
55
43
213
55
43
213
55
43
212
56
43
212
56
43
212
56
43
210
57
43
205
59
44
192
62
48
182
62
46
173
58
40
163
50
32
163
51
31
170
58
38
179
62
44
185
62
46
196
63
48
196
56
43
197
50
40
200
52
42
208
60
48
216
70
57
220
77
63
225
78
68
216
59
54
215
53
51
210
51
48
211
53
50
216
58
55
222
63
60
227
62
60
232
60
60
236
57
60
237
54
58
234
51
55
231
51
54
230
54
56
227
57
57
224
60
58
217
64
58
193
55
45
184
56
43
179
53
39
175
52
37
175
52
37
178
55
40
182
59
43
187
61
46
200
68
53
205
67
54
213
66
56
217
63
55
223
58
52
229
56
52
234
54
53
238
55
57
234
54
57
233
54
57
233
54
57
231
55
55
231
55
55
230
57
53
229
57
53
229
58
51
227
58
51
228
59
52
228
59
54
227
60
54
227
59
56
226
58
57
225
57
57
222
56
56
221
57
58
220
60
60
218
64
62
211
66
61
200
63
57
185
58
49
173
55
43
165
55
40
161
59
44
157
60
43
153
60
43
148
59
41
147
58
40
144
57
38
144
55
37
138
57
38
122
54
35
121
54
37
136
57
44
160
70
61
185
83
79
192
87
84
184
80
79
171
73
70
162
77
72
156
83
76
153
89
80
151
94
83
151
97
87
151
97
87
152
95
86
139
101
80
115
114
70
122
141
86
147
166
110
164
185
128
171
192
135
158
179
123
124
144
91
91
111
60
63
82
36
65
83
43
65
80
47
63
76
50
58
67
48
54
60
46
54
57
48
53
56
47
57
59
46
64
65
51
61
63
52
57
60
49
61
65
51
57
66
47
64
77
51
81
99
61
96
118
71
105
133
74
115
146
79
114
150
78
112
148
74
109
150
74
109
149
78
107
148
78
112
148
84
113
149
85
111
150
83
110
150
80
108
148
75
108
149
73
108
151
72
110
153
73
112
157
76
112
157
76
112
159
78
117
164
83
120
169
88
121
170
91
117
166
87
113
160
88
95
137
74
86
127
69
79
120
62
97
88
73
97
88
73
97
88
73
97
88
73
97
88
73
97
88
73
97
88
73
97
88
73
95
86
71
96
87
72
96
87
72
97
88
73
98
89
74
99
90
75
100
91
76
100
91
74
101
93
72
102
94
71
102
94
73
103
95
76
103
94
79
102
92
80
102
92
82
102
92
83
104
93
87
104
93
87
104
93
87
104
94
85
103
93
84
101
91
81
100
90
80
99
91
78
93
89
77
90
86
74
86
82
70
82
78
66
80
76
64
80
77
62
84
78
64
85
79
63
87
81
65
89
84
65
91
86
67
93
88
68
97
90
71
100
93
74
102
95
76
102
97
77
103
97
81
111
99
85
119
91
79
138
91
81
169
98
92
182
91
88
180
71
68
183
61
58
193
61
56
195
60
54
195
58
48
193
57
45
189
56
41
189
57
42
193
61
46
202
62
47
213
57
45
217
55
44
218
56
45
217
57
45
218
56
45
216
56
44
216
56
44
213
55
43
213
55
43
212
56
43
212
56
43
212
56
43
212
56
43
212
56
43
210
57
43
205
59
44
193
63
49
182
62
46
172
57
39
162
51
32
157
51
29
162
56
34
172
61
41
181
64
46
188
63
45
191
58
43
195
55
42
197
53
42
203
55
43
206
58
46
206
60
47
211
58
50
220
55
53
223
50
52
218
50
50
218
52
52
222
56
56
226
60
60
232
62
63
235
61
62
235
56
60
235
52
56
232
49
54
229
50
53
228
54
56
224
56
55
217
55
53
204
55
49
185
51
40
177
54
39
175
53
38
174
54
38
174
54
38
176
56
40
179
58
41
181
58
42
195
65
49
202
66
52
210
66
55
217
64
56
223
60
53
227
58
53
234
56
54
236
56
55
237
54
56
235
55
56
235
55
56
234
56
54
234
56
54
232
57
52
231
58
52
230
59
52
226
57
50
225
58
50
223
58
52
224
59
55
222
58
56
220
58
56
219
57
55
217
57
57
214
60
58
213
64
60
210
67
61
202
65
59
187
59
50
175
54
43
164
54
39
158
55
40
156
59
42
152
59
41
148
59
41
146
59
40
143
58
38
141
56
36
140
55
35
135
57
37
128
61
42
118
53
35
129
53
39
153
69
58
170
80
72
172
77
71
169
76
71
166
81
76
157
80
74
152
85
76
148
90
79
148
94
82
149
97
86
149
97
86
151
97
87
141
103
84
137
133
95
153
169
120
173
189
142
166
184
136
144
161
116
115
134
89
78
96
56
47
65
27
45
62
28
51
66
37
58
70
46
61
70
51
59
65
51
58
61
50
58
61
52
60
61
53
57
57
45
63
64
50
60
62
51
57
60
49
60
64
50
57
66
47
63
76
50
80
98
60
97
119
72
106
134
75
116
147
80
115
151
79
114
150
76
111
152
76
111
151
80
109
151
79
112
151
84
115
152
85
112
152
82
111
151
80
109
150
74
109
150
72
110
153
74
112
155
75
113
158
77
114
159
78
116
163
83
121
168
88
123
172
93
119
167
91
109
157
83
101
146
77
90
132
69
82
123
65
77
118
60
99
90
75
99
90
75
99
90
75
99
90
75
99
90
75
99
90
75
99
90
75
99
90
75
99
90
75
99
90
75
100
91
76
100
91
76
101
92
77
102
93
78
102
93
78
102
93
76
102
94
71
103
95
72
104
96
73
105
97
76
105
97
78
105
96
79
104
95
80
103
94
79
106
96
84
105
96
81
105
96
81
104
95
78
104
95
78
104
96
77
104
96
77
104
97
78
100
94
78
96
90
76
91
85
71
86
80
66
80
77
62
78
75
58
78
75
58
78
75
58
80
77
60
82
79
62
84
81
64
86
83
64
88
87
67
91
90
70
93
92
72
95
92
75
99
96
81
105
93
81
112
85
74
134
91
82
172
105
99
189
100
96
191
79
77
197
67
67
205
61
61
210
58
57
209
57
52
203
56
48
194
56
43
187
60
41
184
65
43
190
65
43
208
58
43
215
55
41
216
56
42
215
58
43
217
57
43
214
57
42
214
57
42
212
56
41
212
56
41
212
56
41
212
56
41
210
57
41
210
57
41
210
57
41
210
57
41
205
59
44
193
63
49
182
62
48
171
58
40
161
54
34
154
52
30
155
55
32
163
61
38
175
66
45
179
59
42
186
59
44
194
58
44
199
57
45
203
56
46
206
58
48
208
60
48
216
59
52
232
58
60
234
53
60
228
51
57
226
51
56
223
53
56
223
53
56
225
52
56
225
51
53
233
54
60
232
53
57
231
52
58
230
56
58
226
57
60
218
56
54
206
51
49
191
48
40
181
53
40
174
57
40
174
57
40
173
58
40
173
58
40
173
58
40
173
56
38
175
55
38
184
58
43
193
59
47
205
63
51
215
64
55
221
62
56
225
60
56
231
58
54
235
57
55
237
55
54
238
54
54
238
54
54
237
55
52
234
56
52
232
58
51
231
58
51
228
60
51
225
58
50
223
59
50
223
60
53
220
61
55
219
61
58
217
62
60
215
61
59
212
62
61
207
64
60
203
66
60
197
66
58
185
61
53
173
55
45
163
53
40
158
55
40
154
58
42
150
59
41
147
60
41
145
59
42
141
58
40
138
57
38
137
56
37
136
55
36
131
56
37
131
64
45
117
52
34
123
51
37
147
71
58
164
81
73
161
78
70
158
78
71
161
87
78
152
85
76
146
88
76
144
90
78
141
92
78
143
93
82
148
96
85
151
97
87
143
104
87
141
137
102
152
165
122
157
169
129
128
142
106
93
107
72
68
83
52
50
64
38
36
50
25
45
58
38
48
59
42
53
61
46
57
63
51
60
63
52
61
63
52
58
59
51
58
58
48
57
57
45
62
63
49
59
61
50
56
59
48
59
63
49
56
65
46
62
75
49
80
98
60
98
120
73
107
135
76
117
148
81
116
152
80
115
151
77
113
154
78
113
153
82
111
153
81
114
154
83
114
154
83
114
154
81
112
153
77
109
151
75
109
152
73
112
155
76
115
158
79
116
160
81
118
162
83
120
167
89
123
169
94
122
168
95
113
159
86
100
146
74
89
134
67
92
133
73
85
124
69
81
120
65
101
92
77
101
92
77
101
92
77
101
92
77
101
92
77
101
92
77
101
92
77
101
92
77
101
92
77
102
93
78
102
93
78
102
93
78
103
94
79
103
94
79
104
95
80
104
95
78
103
95
74
104
96
73
105
97
76
106
98
77
106
98
79
106
98
79
106
97
80
106
97
80
106
97
80
105
97
78
104
96
77
104
96
75
104
96
75
104
96
73
105
97
74
106
98
75
102
95
77
98
93
74
94
88
72
88
82
66
83
77
61
77
74
57
75
72
55
73
72
54
75
74
56
76
75
57
76
77
59
78
79
61
80
81
63
81
84
65
83
86
69
84
85
69
92
88
76
95
85
73
100
80
69
124
87
78
161
104
97
181
102
97
193
88
85
211
83
82
214
65
67
220
60
62
219
57
55
210
56
48
200
58
46
189
62
43
178
66
42
183
64
40
204
59
42
212
55
40
213
56
41
213
57
42
214
57
42
213
57
42
213
57
42
212
56
41
212
56
41
210
57
41
210
57
41
210
57
41
210
57
41
210
57
41
210
57
41
205
59
44
193
63
49
181
61
47
171
60
41
162
57
36
153
54
31
149
53
29
155
59
34
167
65
42
178
66
46
185
64
47
191
59
44
195
55
42
197
50
40
203
52
43
210
59
48
223
61
56
233
54
58
237
52
60
234
55
61
232
57
62
229
59
62
228
59
62
225
59
61
224
58
60
223
53
56
224
55
58
226
57
62
224
61
62
217
61
62
207
58
54
194
51
47
181
48
39
177
57
43
172
61
42
172
61
42
171
62
42
170
61
41
169
60
40
169
58
39
171
56
37
175
54
37
183
56
41
197
59
48
206
62
51
214
61
53
222
60
55
227
59
56
232
59
55
236
56
55
239
55
53
239
55
53
238
56
53
235
58
52
233
59
52
231
60
52
228
61
52
226
62
53
224
63
55
222
65
58
218
65
59
217
65
62
213
65
63
209
64
61
205
66
63
195
63
58
187
64
56
179
61
51
170
56
45
161
53
40
155
53
39
152
56
40
149
60
42
145
60
40
141
60
41
138
60
40
136
58
38
134
57
39
132
55
37
131
54
36
127
54
37
125
58
41
116
51
33
126
57
42
151
79
65
163
89
78
156
82
71
149
79
69
152
86
74
149
89
78
144
91
77
142
90
77
140
91
77
142
92
81
147
95
84
153
96
87
145
103
87
126
120
88
121
131
94
114
124
90
85
96
66
59
69
44
47
59
37
48
59
42
48
60
46
51
61
50
52
60
49
54
60
50
60
63
54
64
65
57
63
65
54
60
60
48
55
55
43
56
57
43
62
63
49
58
60
49
55
58
47
58
62
48
55
64
45
61
74
48
79
97
59
98
120
73
107
135
76
117
148
81
117
153
81
116
152
78
113
154
78
114
154
83
112
154
82
114
154
81
115
156
80
114
155
79
112
153
75
110
153
74
112
155
76
115
158
79
118
161
82
121
165
88
123
167
90
122
168
93
120
166
93
113
159
87
103
148
79
93
138
69
88
130
66
95
134
77
87
126
71
82
121
66
103
94
79
103
94
79
103
94
79
103
94
79
103
94
79
103
94
79
103
94
79
103
94
79
103
94
79
103
94
79
103
94
79
104
95
80
104
95
80
104
95
80
104
95
80
105
96
79
106
97
80
106
98
79
106
98
79
106
98
79
105
96
79
105
96
79
105
97
78
105
97
78
105
97
78
104
96
75
104
96
75
104
96
73
104
96
73
104
97
71
104
97
71
104
97
71
102
95
76
101
94
75
98
91
73
92
87
68
87
82
63
80
77
58
76
73
56
73
72
54
73
72
54
71
72
54
71
74
57
71
75
58
72
76
59
73
77
60
72
79
63
74
78
63
82
80
67
89
81
70
92
78
67
105
82
68
129
89
77
149
87
76
174
84
76
209
91
89
217
75
74
223
65
66
223
57
57
217
55
50
207
59
49
195
62
45
183
64
42
180
60
36
201
58
41
211
55
40
212
56
41
211
58
42
213
57
42
211
58
42
211
58
42
211
58
42
210
57
41
209
58
41
209
58
41
209
58
41
209
58
41
209
58
41
209
58
41
205
59
44
193
61
48
181
61
47
172
61
44
163
60
41
152
56
32
144
52
27
149
57
32
161
65
41
173
67
45
182
65
47
191
61
47
192
54
43
194
47
37
199
48
39
208
54
46
222
58
56
232
53
57
238
54
62
236
59
65
233
64
67
229
66
69
224
66
67
220
64
65
217
63
63
204
50
50
207
55
54
211
58
60
207
62
59
199
60
57
189
56
49
180
53
46
171
53
41
170
60
45
167
62
43
167
62
43
166
63
44
165
62
43
166
61
40
166
59
39
169
58
39
170
55
37
177
56
39
188
56
43
195
57
46
203
56
48
211
58
52
220
61
57
228
63
59
233
58
55
236
57
53
236
57
53
235
58
52
233
59
52
232
59
52
229
61
52
226
62
52
224
66
55
221
67
57
218
67
60
213
66
59
207
65
61
202
63
60
199
61
59
191
62
57
177
60
51
169
58
47
162
56
43
156
54
40
152
55
39
148
56
41
145
58
41
141
58
40
139
61
41
137
60
42
135
60
41
131
58
39
130
57
40
128
55
38
127
54
37
125
54
36
120
51
35
120
53
37
133
66
50
152
85
69
157
87
75
148
80
67
144
80
68
149
90
76
147
89
77
144
90
78
144
92
81
144
92
81
146
91
84
149
92
85
152
91
86
142
98
85
107
98
69
86
96
61
74
83
52
61
71
44
54
63
42
50
61
44
52
62
51
53
63
54
54
61
54
56
63
56
61
66
59
67
70
61
69
71
60
67
68
54
64
62
47
59
57
42
55
56
42
61
62
48
58
60
49
55
58
47
58
62
48
54
63
44
61
74
48
78
96
58
97
119
72
107
135
76
117
148
81
117
153
81
116
152
78
114
155
79
114
154
83
112
154
80
113
155
79
114
157
78
113
156
77
112
155
76
111
154
75
114
157
78
117
161
84
121
165
88
127
171
96
126
169
97
122
165
93
113
156
85
102
145
76
95
137
71
93
135
69
94
136
73
97
136
79
89
128
73
82
121
66
104
95
80
104
95
80
104
95
80
104
95
80
104
95
80
104
95
80
104
95
80
104
95
80
103
94
79
104
95
80
104
95
80
104
95
80
104
95
80
104
95
80
105
96
81
105
96
81
109
100
85
108
99
84
106
97
82
105
96
81
104
95
80
103
94
79
103
94
77
104
96
77
105
97
78
105
97
76
105
97
76
105
97
76
104
96
73
104
96
73
104
97
71
104
96
73
103
97
75
102
95
76
101
94
75
98
91
72
92
87
68
85
82
63
80
77
60
77
76
58
69
70
52
68
71
52
68
72
55
68
72
55
66
73
57
66
75
58
66
75
58
67
74
58
74
74
62
84
80
68
85
79
65
89
76
60
101
75
58
115
69
53
146
74
62
192
91
83
215
83
79
222
70
69
223
59
58
220
55
53
213
59
51
204
61
47
189
60
41
184
55
34
203
58
41
207
56
39
208
57
40
210
59
42
210
59
42
210
59
42
210
59
42
210
59
42
209
58
41
209
58
41
209
58
41
209
58
41
209
58
41
209
58
41
209
58
41
205
59
44
193
59
48
182
58
46
173
60
44
167
62
43
153
57
35
143
51
26
147
55
30
159
63
39
163
57
35
176
59
41
192
62
48
198
60
49
204
56
46
208
55
47
217
60
53
227
63
61
236
61
66
238
63
68
233
67
69
226
68
67
214
64
63
202
59
55
192
53
48
187
49
46
183
48
44
187
54
49
190
58
54
187
60
54
179
58
50
169
57
46
164
56
44
160
58
43
161
61
45
159
62
43
160
63
44
161
64
45
161
64
45
163
64
43
163
62
42
164
61
42
169
59
42
173
58
40
178
55
40
184
54
41
191
53
43
201
57
48
212
63
57
224
67
62
229
60
55
232
59
53
232
59
53
231
60
52
229
60
53
228
61
52
225
62
53
222
64
53
220
68
57
216
68
58
210
67
59
202
65
57
196
63
58
188
59
54
184
56
53
176
57
51
162
55
45
153
56
40
149
53
39
147
54
39
145
58
41
143
60
42
139
58
41
133
56
38
136
61
42
133
60
43
131
60
42
128
60
41
126
58
39
124
56
37
123
54
38
123
54
38
121
49
35
128
56
42
140
71
56
150
83
67
146
80
66
139
76
61
144
82
69
153
95
83
145
87
76
146
89
78
148
91
82
150
93
84
150
93
86
151
92
86
150
89
84
140
94
81
97
88
59
69
77
40
54
64
30
56
65
38
62
71
50
60
69
52
51
61
50
45
55
46
49
56
49
55
62
54
66
69
60
71
74
63
72
73
59
66
67
49
63
62
42
59
58
38
55
56
40
61
62
48
57
59
48
54
57
46
58
62
48
54
63
44
60
73
47
78
96
58
97
119
72
106
134
75
117
148
81
116
152
80
116
152
78
114
155
79
114
154
83
112
154
80
113
156
77
113
156
76
113
156
76
112
155
75
112
155
76
115
158
79
119
163
86
123
167
92
130
173
101
128
171
100
120
163
92
106
149
80
93
135
69
89
131
65
94
136
72
101
143
80
99
138
81
90
129
74
83
122
67
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
95
83
106
96
87
106
96
87
106
96
87
106
96
87
106
96
86
106
96
86
106
96
84
106
97
82
106
97
82
106
97
80
106
97
80
106
98
79
106
98
79
106
98
79
106
98
77
106
98
77
103
95
74
103
95
74
102
94
73
101
95
73
98
93
73
90
88
67
83
80
61
77
76
58
72
73
55
69
72
55
65
69
54
64
68
53
63
69
55
63
71
56
63
71
56
63
69
55
72
74
61
74
75
61
67
68
52
75
74
54
92
81
59
96
70
47
128
75
57
189
107
93
217
104
96
224
86
83
226
71
69
229
65
64
226
64
61
215
61
53
203
59
48
201
59
45
199
51
37
203
53
38
204
54
39
205
55
40
205
55
40
206
56
41
209
59
44
211
61
46
210
60
45
210
60
45
210
60
45
210
60
45
209
59
44
209
59
44
209
59
44
206
60
45
187
50
40
182
55
46
177
59
47
168
58
41
154
55
34
148
53
31
149
55
30
156
57
34
174
65
45
179
59
43
186
54
41
193
52
42
205
54
47
215
60
55
226
67
63
234
72
69
232
68
69
232
69
72
221
67
67
201
58
54
187
54
49
180
56
48
176
55
46
169
52
43
171
57
47
169
57
46
164
56
46
160
56
45
155
57
44
150
58
43
147
60
43
146
60
43
150
61
43
152
61
42
153
62
43
154
63
44
155
64
45
156
65
44
158
66
45
161
64
45
162
59
42
165
58
40
171
55
40
176
54
41
182
54
43
190
56
45
197
58
51
204
60
52
218
61
54
224
61
54
226
63
56
226
65
55
226
65
57
222
66
54
218
64
54
215
64
53
216
72
61
210
69
59
199
65
56
187
59
50
178
53
47
170
51
45
166
49
42
159
50
43
153
57
45
146
59
42
145
57
43
141
58
42
139
58
41
136
59
41
135
59
43
133
60
43
128
57
39
125
56
40
124
57
40
124
57
40
124
57
40
123
56
39
121
54
38
120
51
36
119
45
32
139
65
52
151
79
67
147
77
65
142
76
64
145
81
69
147
85
74
142
82
71
152
92
82
152
92
82
145
85
77
144
84
76
151
90
85
149
88
83
148
87
84
147
103
90
93
84
53
73
81
42
65
75
40
66
75
44
62
72
47
55
64
43
50
61
45
52
63
49
52
60
49
72
80
67
90
94
80
87
91
74
77
79
58
68
70
46
65
65
39
61
61
37
58
59
41
62
63
49
59
61
50
58
61
50
59
63
49
52
61
42
59
72
46
80
98
60
99
121
74
108
136
77
117
148
81
115
151
79
114
150
76
112
153
77
114
154
83
113
155
81
112
157
76
113
158
75
114
159
78
114
159
78
115
159
80
118
162
85
123
167
92
127
170
98
130
173
102
120
163
94
106
147
79
96
137
71
94
135
69
97
137
74
102
142
80
104
144
84
100
137
83
91
128
76
84
121
69
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
95
83
106
95
89
106
95
91
106
95
91
106
95
89
106
95
89
106
96
87
106
96
86
106
96
84
106
96
84
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
80
106
97
80
104
96
77
104
96
75
103
95
74
103
95
74
100
93
74
94
89
69
85
82
63
79
78
58
73
74
58
69
72
55
66
68
54
64
68
53
63
69
55
62
70
55
62
70
55
61
69
54
66
70
56
65
67
53
64
69
49
72
76
53
77
73
48
82
65
39
124
82
58
185
117
98
201
100
88
221
96
90
234
86
84
230
72
71
226
64
62
226
64
61
219
62
57
209
56
48
205
57
43
206
61
44
207
61
46
207
62
45
207
59
45
206
59
43
205
57
43
205
58
42
206
58
44
208
58
43
208
58
44
208
58
43
208
58
44
208
58
43
208
58
44
205
59
46
201
63
53
197
66
58
188
66
55
174
61
47
159
54
35
150
49
29
150
49
29
155
50
29
168
53
35
180
54
40
195
59
47
209
65
56
222
69
63
230
73
68
233
74
70
234
75
72
224
66
65
219
69
68
208
66
62
190
59
51
175
57
47
171
61
48
165
63
49
159
61
48
159
63
49
156
62
50
151
62
48
149
61
47
143
61
47
139
62
46
135
64
46
136
63
44
142
61
42
143
60
42
146
60
43
145
62
44
148
63
43
148
63
43
150
63
44
153
62
44
159
64
46
161
61
45
164
58
44
168
56
44
173
55
43
178
56
45
184
57
48
189
56
47
202
60
50
208
60
50
211
63
53
213
65
55
215
67
57
214
67
57
212
68
57
209
69
56
202
66
54
195
65
52
185
61
51
176
58
48
169
54
47
164
53
46
159
52
44
153
55
44
146
58
46
140
61
44
138
59
44
136
59
43
134
58
42
132
59
42
131
60
42
129
60
44
126
59
42
124
59
41
123
57
41
121
58
41
120
57
40
119
56
39
117
54
39
119
52
36
128
52
39
144
68
55
152
78
67
145
75
63
140
74
62
143
81
68
146
84
73
144
82
71
147
83
74
152
88
79
152
85
79
151
86
80
156
93
88
148
87
82
142
83
79
138
99
84
93
87
53
88
99
57
95
106
66
90
102
66
73
84
54
54
67
41
52
64
42
60
72
52
79
88
71
89
98
81
95
102
84
87
92
70
74
78
55
66
69
42
63
63
35
60
60
34
57
58
40
61
62
48
58
60
49
57
60
49
60
64
50
53
62
43
59
72
46
80
98
60
100
122
75
108
136
77
117
148
81
116
152
80
114
150
76
113
154
78
114
154
83
113
155
81
115
160
79
114
159
76
112
157
76
113
158
77
116
160
81
120
164
87
124
167
95
127
170
99
123
166
97
111
154
85
99
140
74
93
134
68
97
137
74
101
141
78
101
141
78
99
139
79
100
137
83
91
128
76
84
121
69
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
95
85
106
95
91
106
94
94
106
94
94
106
95
93
106
95
89
106
96
87
106
96
86
106
96
84
106
96
84
106
96
84
106
96
84
106
96
84
106
96
84
106
96
84
106
96
86
106
96
84
108
97
79
107
96
76
105
97
78
105
97
78
102
95
76
97
92
72
89
86
67
83
82
62
77
75
60
72
73
57
68
70
56
65
69
54
65
69
54
63
70
54
62
68
54
61
67
53
62
69
53
58
62
45
66
71
49
73
75
51
69
63
37
84
64
39
132
92
67
181
116
96
179
87
74
214
100
90
239
101
98
236
84
83
230
70
72
235
71
72
232
66
68
219
57
54
203
55
43
201
58
42
201
58
44
201
58
42
203
57
44
202
56
41
200
54
41
200
54
39
202
56
43
203
55
41
203
55
43
203
55
41
203
55
43
204
56
42
204
56
44
203
56
46
194
51
45
190
53
47
182
54
45
173
51
40
161
48
34
158
47
30
160
49
32
167
51
36
188
65
50
199
67
55
214
71
63
225
76
70
231
76
71
233
74
70
229
70
66
223
68
64
216
66
65
209
67
65
195
63
58
177
59
49
166
60
47
161
65
49
156
69
52
152
69
51
149
68
51
146
67
52
143
66
50
138
65
48
133
64
48
130
65
47
126
65
46
129
64
46
132
59
42
136
59
43
137
58
41
137
60
42
139
60
43
139
60
43
142
61
42
143
60
42
150
63
46
152
60
45
155
59
43
158
56
42
162
56
43
167
56
45
170
58
46
175
57
47
185
57
46
190
57
48
193
59
48
196
62
51
197
63
51
197
65
52
195
65
51
193
65
52
182
59
44
178
58
44
170
56
45
163
55
43
158
54
43
155
54
44
152
55
46
148
58
47
141
59
47
136
60
44
135
59
45
132
59
44
130
58
43
128
59
43
128
59
43
127
60
44
125
59
43
122
59
42
122
59
44
119
57
42
118
56
41
117
55
40
116
54
41
118
52
38
136
59
49
148
70
60
150
78
66
142
74
61
136
72
60
141
79
66
145
83
72
146
82
73
146
79
73
153
84
79
153
83
81
154
86
83
154
93
90
147
89
85
141
91
84
141
108
91
129
125
88
126
137
94
128
139
97
108
120
82
76
88
52
54
68
35
61
74
46
76
90
64
101
113
89
101
111
87
93
101
77
80
85
62
69
73
48
63
67
40
61
64
35
58
60
36
56
57
39
59
60
46
56
58
47
57
60
49
61
65
51
54
63
44
60
73
47
80
98
60
100
122
75
109
137
78
118
149
82
116
152
80
115
151
77
113
154
78
114
154
83
112
154
80
117
162
81
112
160
76
110
157
76
112
159
79
117
164
86
121
167
92
125
168
96
124
167
96
112
153
85
102
143
77
92
132
69
92
132
69
101
140
77
105
144
81
102
141
78
95
133
72
99
136
82
90
127
76
83
120
69
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
96
86
107
96
92
107
95
95
107
96
94
107
96
92
107
96
90
107
97
88
107
97
87
107
97
85
107
97
85
107
97
85
107
97
85
107
97
85
107
97
87
107
97
87
107
97
88
107
97
87
110
98
82
109
98
80
106
98
79
106
98
79
104
97
78
101
94
75
95
90
71
89
86
69
82
79
62
77
75
60
71
72
58
68
70
56
67
69
55
66
68
54
64
68
53
62
66
51
63
70
52
60
65
45
71
73
51
76
70
48
77
59
37
110
78
57
152
100
79
167
96
78
158
66
53
197
84
76
229
100
95
239
95
94
237
83
85
237
74
79
234
68
72
227
64
65
205
56
49
199
57
45
197
55
43
197
55
43
200
56
45
202
58
47
203
59
48
205
58
48
209
62
52
209
62
52
210
62
52
210
62
52
211
63
53
211
63
53
211
63
53
211
62
55
211
63
59
209
66
62
203
68
62
198
70
61
194
70
60
195
73
62
199
77
64
206
80
68
217
83
72
221
80
71
226
77
71
228
73
69
228
71
66
226
67
63
224
65
61
220
65
61
214
69
66
202
67
63
189
62
55
173
59
48
161
61
46
155
66
48
151
70
51
148
73
54
142
69
52
140
69
51
138
66
51
133
66
49
130
64
48
125
64
46
123
64
46
124
63
45
128
59
44
131
57
44
131
58
43
132
59
44
132
59
44
133
60
45
135
59
43
136
59
43
137
58
43
140
57
41
143
55
41
147
55
42
150
56
44
156
58
47
160
59
47
163
59
48
170
58
47
173
56
46
175
57
47
176
58
46
176
58
46
176
58
44
175
57
43
174
56
42
168
55
41
165
55
40
158
54
41
155
54
42
149
55
43
146
56
45
145
57
47
142
60
48
136
60
46
133
60
45
131
59
44
129
57
42
127
58
42
126
57
42
125
58
42
125
59
43
124
58
44
122
59
44
121
59
44
119
57
42
115
56
42
113
54
40
114
55
41
117
53
41
140
68
56
146
72
61
145
75
63
138
72
58
135
73
60
139
79
68
147
84
75
151
84
76
150
81
76
155
84
80
152
81
79
148
83
79
150
92
88
144
95
88
144
104
94
151
126
106
148
148
112
133
146
102
117
130
87
89
104
63
67
82
43
63
79
42
80
96
60
99
114
81
101
116
85
92
105
75
78
88
61
67
76
49
63
69
43
62
66
41
61
65
40
59
61
39
55
56
40
58
59
45
55
57
46
57
60
49
61
65
51
55
64
45
60
73
47
79
97
59
98
120
73
108
136
77
118
149
82
117
153
81
116
152
78
114
155
79
114
154
83
111
155
80
113
160
79
111
158
77
111
158
77
115
162
82
122
168
93
124
170
97
122
165
94
117
160
91
99
140
74
94
135
69
91
131
68
95
135
72
103
142
79
106
145
80
102
141
76
96
135
72
98
135
81
89
126
75
82
119
68
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
96
84
107
96
90
107
96
92
107
96
90
107
97
88
107
97
87
107
97
85
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
97
85
107
97
87
107
97
87
107
97
88
107
97
87
111
99
85
110
98
82
107
98
81
107
98
81
107
98
81
104
97
79
100
93
77
96
90
74
88
82
66
82
79
62
78
75
60
73
71
56
71
69
54
68
69
53
66
67
51
63
66
49
64
72
49
65
70
47
77
71
49
83
62
43
101
62
45
148
90
76
170
96
83
153
63
52
155
52
43
179
64
57
214
89
85
241
106
103
245
101
101
235
82
85
232
73
78
233
77
80
217
73
65
211
70
60
205
64
54
203
62
52
206
64
54
210
68
58
213
71
61
215
71
62
218
74
65
218
74
65
221
74
66
221
74
66
222
75
67
222
75
67
224
75
68
224
75
69
235
83
82
232
84
84
227
83
82
220
82
79
216
83
76
216
83
76
219
85
76
224
85
78
224
76
72
225
73
70
227
69
66
226
67
64
228
66
64
226
67
64
228
69
66
224
70
68
218
73
70
203
68
64
188
61
55
174
60
50
163
62
50
153
66
49
147
68
51
146
71
52
141
68
51
138
66
51
137
65
51
132
63
48
130
62
49
128
62
48
125
62
47
125
62
47
127
59
46
128
58
46
128
58
46
128
58
46
128
61
45
129
60
45
129
60
45
131
59
45
130
57
42
132
56
42
134
56
43
138
56
42
140
57
43
144
58
45
146
58
46
148
58
47
155
59
47
158
57
45
159
57
45
160
58
44
161
57
44
162
56
43
162
56
42
162
56
42
159
56
41
156
56
41
152
56
40
148
56
41
143
57
42
140
58
44
137
58
45
135
59
45
132
58
45
130
58
44
129
57
43
128
56
42
125
56
41
125
55
43
124
56
43
123
57
43
122
56
44
121
57
45
120
58
45
118
56
43
113
53
42
111
53
41
113
55
43
116
57
43
140
72
59
141
73
60
137
73
61
132
73
59
133
75
63
140
82
70
147
87
77
152
88
79
152
81
77
156
85
81
152
82
80
147
86
81
146
96
89
136
98
87
131
103
91
134
122
100
110
114
81
90
105
64
74
89
48
65
82
40
73
90
48
87
105
63
100
118
78
106
124
86
88
105
69
76
91
58
63
76
46
58
68
41
60
68
44
62
67
45
60
65
45
58
61
44
56
57
43
58
59
45
54
56
45
56
59
48
62
66
52
55
64
45
59
72
46
76
94
56
95
117
70
105
133
74
116
147
80
116
152
80
116
152
78
114
155
79
115
155
84
112
156
81
110
157
77
109
158
76
112
161
80
119
168
89
124
170
97
122
168
96
114
157
88
105
147
81
90
131
65
92
132
69
97
136
73
101
140
77
103
142
77
103
142
75
104
141
74
102
138
76
99
134
80
89
123
73
82
116
66
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
97
85
108
98
89
108
98
89
108
98
89
108
98
86
108
98
86
108
99
82
108
99
82
108
100
81
108
100
81
108
100
81
108
99
82
108
99
84
108
98
86
108
98
88
108
98
88
108
98
88
111
99
85
110
98
82
107
98
81
108
99
82
108
99
82
107
98
81
103
96
80
100
93
77
94
87
71
89
83
67
83
77
63
77
74
59
75
72
57
71
69
54
69
66
51
64
65
47
61
71
46
66
70
45
81
66
45
98
61
45
129
69
58
169
89
82
178
79
74
155
46
43
175
60
57
184
62
59
208
84
82
242
112
110
254
118
118
243
100
104
237
88
94
242
93
95
227
85
81
218
80
70
212
71
64
208
67
58
209
68
61
212
71
62
214
71
65
214
71
63
217
72
67
218
74
66
220
72
68
221
74
67
222
74
70
222
75
68
224
75
71
224
75
71
226
72
72
225
71
73
220
72
72
217
71
71
214
72
68
215
72
68
216
71
66
219
70
66
224
66
65
228
65
66
230
66
65
232
66
66
231
67
66
228
66
64
223
65
62
219
65
63
221
73
71
205
66
63
188
59
54
175
61
51
165
63
51
153
64
48
146
65
48
144
67
51
140
64
50
137
64
49
136
62
49
134
60
49
132
60
48
129
59
49
129
59
49
128
60
49
128
60
49
127
61
49
127
60
51
127
61
49
127
61
49
127
61
47
127
61
49
127
61
47
128
60
47
129
60
45
131
59
45
131
57
44
133
57
44
132
56
42
133
55
43
133
54
41
141
57
46
143
56
46
145
57
45
148
58
47
151
59
46
154
61
46
158
60
47
159
62
46
153
57
41
150
57
40
147
58
42
144
58
43
138
59
44
135
59
43
132
59
44
130
58
43
130
58
44
129
57
43
127
55
41
126
54
40
123
53
41
123
53
41
123
53
41
122
54
41
120
54
42
120
56
44
119
57
44
115
56
42
112
52
41
111
53
41
114
58
45
118
60
48
139
77
64
137
75
60
132
74
60
130
77
61
133
81
67
139
86
72
146
88
77
151
87
78
147
78
73
153
84
79
151
86
82
149
92
85
144
101
92
123
94
80
102
86
70
96
94
71
69
79
45
62
78
41
63
79
40
74
92
50
95
114
69
107
126
81
101
120
75
88
106
64
77
95
55
64
81
45
54
69
38
55
68
42
61
70
49
61
68
52
58
62
48
55
59
45
56
58
44
58
59
45
54
56
45
56
59
48
62
66
52
55
64
45
57
70
44
73
91
53
91
113
66
101
129
70
114
145
78
115
151
79
115
151
77
114
155
79
115
155
84
112
156
81
109
156
76
111
160
79
115
164
85
120
168
92
122
168
96
115
160
91
105
147
81
96
138
72
89
129
66
94
134
71
101
140
77
103
142
77
102
141
74
101
141
71
103
140
71
104
140
76
97
132
78
88
122
72
81
115
65
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
108
98
88
108
98
86
108
98
86
108
99
82
108
99
82
108
100
79
108
100
79
108
100
77
108
100
77
108
100
77
108
100
79
108
100
81
108
99
82
108
99
84
108
98
86
108
98
86
111
99
85
109
97
83
107
98
83
108
99
84
109
100
85
108
99
84
105
96
81
102
95
79
98
91
75
94
87
71
88
81
65
82
76
60
79
73
57
74
71
54
73
67
51
65
67
46
60
70
43
66
69
42
86
64
43
122
70
57
163
80
76
178
72
72
180
57
60
183
52
57
197
67
69
193
65
64
200
75
73
222
98
96
241
113
112
242
109
112
233
97
101
228
89
92
219
81
78
213
76
68
207
68
63
205
66
59
207
68
63
208
69
62
208
66
62
205
64
57
210
67
63
210
67
61
212
67
64
213
68
63
215
67
65
216
68
64
217
69
67
218
68
67
221
65
68
221
65
68
222
68
70
221
71
72
223
73
72
224
74
73
227
73
73
229
71
72
231
65
67
234
65
68
236
66
69
236
66
69
231
65
65
226
64
62
219
61
58
214
60
58
219
69
70
204
60
60
188
54
51
177
58
50
169
62
52
155
61
49
147
61
48
145
63
49
139
60
47
139
59
48
138
58
49
137
57
48
136
56
49
135
56
51
136
57
52
133
58
52
128
60
51
125
61
51
125
61
52
125
61
51
125
61
51
125
61
49
125
61
51
124
60
48
124
60
48
124
61
46
126
60
46
126
58
45
125
57
44
125
56
41
126
54
42
126
54
42
131
57
46
131
57
46
135
57
45
137
57
46
141
57
46
145
59
46
149
60
46
150
61
45
146
57
41
145
58
41
141
58
42
138
59
42
135
59
43
131
60
42
128
59
43
126
59
42
128
56
42
128
56
44
126
54
42
125
53
41
124
52
40
122
52
40
122
52
42
121
53
42
117
50
41
118
54
44
118
56
45
114
54
43
111
51
41
111
53
42
115
58
47
120
64
51
138
80
66
134
78
63
129
78
61
130
81
64
133
86
70
137
88
73
142
85
74
145
82
73
146
79
73
151
84
78
147
84
79
145
92
86
138
102
90
110
91
74
80
75
55
69
75
49
65
78
48
75
90
57
87
103
67
96
114
74
103
121
79
100
122
76
86
108
62
71
93
47
65
86
45
55
72
36
49
64
33
54
66
42
61
70
53
60
68
57
56
61
54
55
58
51
57
59
46
59
60
46
54
56
45
57
60
49
63
67
53
55
64
45
55
68
42
70
88
50
86
108
61
97
125
66
111
142
75
113
149
77
115
151
77
114
155
79
116
156
85
113
157
82
111
158
80
113
162
81
117
166
87
118
166
90
115
161
89
106
151
82
98
140
74
91
133
69
94
134
72
96
136
73
99
138
75
101
140
75
102
141
74
102
142
72
102
139
70
101
137
73
96
131
77
87
121
71
80
114
64
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
105
96
81
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
108
98
86
108
99
84
108
99
84
108
99
82
108
100
81
108
100
79
108
100
79
108
100
77
108
100
77
108
100
79
108
100
79
108
100
81
108
99
82
108
99
84
108
99
84
108
99
84
110
98
84
109
97
83
109
97
83
109
97
83
109
100
85
108
99
84
106
97
82
103
96
80
100
93
77
96
89
73
90
83
67
85
78
62
80
75
56
77
72
53
73
68
49
69
67
46
65
69
42
71
67
40
92
65
44
137
83
71
175
92
88
169
63
65
169
49
51
203
74
78
195
64
69
189
61
62
185
59
60
195
71
71
216
90
91
229
101
102
221
88
91
205
71
72
209
74
71
207
69
66
205
66
63
206
67
62
210
71
66
210
71
66
206
67
62
201
62
55
207
68
61
209
68
61
210
68
64
212
69
65
214
69
66
217
69
67
220
67
69
222
68
70
222
63
67
223
64
68
222
66
69
223
69
69
225
71
71
224
70
70
224
66
67
225
62
63
232
66
68
234
64
67
232
62
65
231
61
62
228
62
62
228
64
63
226
67
64
223
69
67
215
65
66
200
56
56
184
52
48
177
58
52
169
62
54
155
61
51
146
60
47
142
63
50
137
59
47
136
58
48
136
58
48
136
56
49
136
55
51
138
55
51
137
56
52
136
59
53
128
58
50
126
59
50
126
59
50
124
60
50
124
60
48
124
60
48
124
60
48
124
60
48
120
56
44
121
57
45
124
58
46
124
58
46
125
59
45
125
59
45
125
57
44
125
57
44
126
58
47
127
57
47
128
55
46
129
55
44
132
54
42
133
53
42
134
55
42
134
55
40
138
56
42
136
57
42
136
59
43
133
60
45
132
60
45
128
61
44
126
60
44
125
59
43
125
57
44
125
55
45
124
54
44
121
53
42
122
52
42
120
52
41
120
52
41
120
53
44
114
50
40
117
53
43
118
56
45
114
54
43
110
52
41
110
53
42
116
59
48
120
66
54
138
85
71
132
80
66
128
79
64
132
83
68
136
87
73
140
86
74
140
82
71
141
78
71
149
84
78
148
85
78
139
82
75
132
88
77
127
100
83
101
90
68
71
75
50
61
74
44
73
88
57
90
107
73
103
120
84
99
120
81
89
110
69
83
104
61
78
99
56
74
95
52
58
76
38
48
63
30
44
57
29
53
62
41
61
67
53
60
66
56
58
60
55
57
60
53
59
61
50
59
61
48
55
57
46
57
60
49
63
67
53
56
63
45
56
66
41
69
85
49
85
104
59
97
122
67
110
138
77
114
147
78
114
150
78
114
154
81
115
157
83
113
157
82
114
160
85
117
163
88
118
164
91
116
162
90
111
154
85
102
144
78
94
136
72
91
131
68
100
140
78
97
137
75
96
135
72
99
138
73
102
141
74
103
142
75
101
138
69
97
133
69
96
131
77
87
121
71
80
114
64
105
96
81
105
96
81
105
96
81
106
97
82
106
97
82
107
98
83
107
98
83
107
98
83
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
107
98
83
107
98
83
107
98
83
107
98
83
107
98
81
107
98
81
107
98
81
107
98
81
108
99
82
108
99
82
108
99
82
108
99
82
108
99
84
108
99
84
108
99
84
110
98
84
107
93
80
112
96
81
116
100
87
114
100
87
110
98
86
106
96
84
104
96
83
104
98
84
103
97
83
103
97
81
99
93
77
91
86
67
86
81
61
83
78
56
78
72
50
71
65
41
76
70
44
74
62
36
98
73
51
145
103
87
182
120
109
187
108
101
183
89
87
185
79
79
182
66
69
189
66
69
194
68
71
200
70
72
205
72
75
208
74
75
211
75
77
211
75
77
205
69
69
205
69
69
208
70
68
211
72
69
212
70
66
207
68
61
206
67
60
207
70
60
200
68
56
201
69
57
203
69
60
207
68
63
212
67
64
216
66
67
222
63
68
222
61
67
225
66
70
223
67
68
223
67
68
224
68
69
224
68
69
225
70
68
225
70
68
227
69
68
227
67
67
228
66
64
228
64
63
227
63
62
227
63
62
227
63
61
227
63
61
224
66
63
211
66
63
197
62
58
184
57
51
170
55
48
160
57
48
151
61
50
142
62
51
134
62
50
131
63
50
129
63
51
129
61
50
129
59
51
132
57
51
135
56
51
136
56
49
136
57
50
133
59
48
132
60
46
132
60
46
129
60
45
129
59
47
128
58
46
128
58
46
128
58
46
126
58
45
126
58
45
126
58
45
126
58
45
126
58
47
126
58
47
126
58
47
126
58
47
125
57
48
125
57
48
126
56
48
126
56
46
128
55
46
128
55
46
128
55
46
126
56
44
128
56
44
126
56
44
125
57
44
124
56
45
123
57
45
122
56
44
120
56
44
120
56
44
120
56
46
120
56
46
119
55
45
118
56
45
118
54
44
116
54
43
115
53
42
113
53
43
109
49
39
114
54
44
110
52
41
109
51
40
111
54
43
109
52
41
113
56
45
127
73
61
133
83
72
133
83
74
134
82
71
136
82
72
139
80
72
144
81
74
148
81
75
149
82
76
147
84
77
141
84
75
137
91
78
130
99
81
111
95
72
89
85
58
77
86
55
79
95
59
106
126
89
98
120
81
89
111
72
85
106
67
82
103
64
77
98
59
73
89
53
67
82
49
55
68
38
52
62
35
50
58
35
54
59
39
60
62
48
64
66
53
64
64
54
60
62
51
53
56
45
51
57
45
52
58
46
55
58
47
57
61
47
59
63
46
61
66
44
60
69
40
83
95
59
97
114
70
110
134
82
115
145
85
114
149
81
114
154
83
115
157
83
114
157
85
117
158
90
126
167
101
126
167
101
112
152
89
98
138
75
94
134
71
94
134
72
94
132
71
96
134
73
98
136
75
100
138
77
102
141
78
101
140
77
99
138
75
96
135
72
94
132
71
95
130
76
84
118
68
75
109
59
105
96
81
105
96
81
105
96
81
106
97
82
106
97
82
107
98
83
107
98
83
107
98
83
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
110
98
84
114
98
83
113
97
82
114
98
85
113
99
86
114
102
90
111
103
90
109
101
88
106
100
86
103
99
87
107
104
89
111
105
89
108
103
84
104
99
79
99
94
72
91
85
61
82
76
52
80
74
48
72
62
37
81
63
41
107
80
61
127
88
71
132
78
66
136
69
61
147
68
63
177
87
86
184
84
84
187
81
81
192
80
79
192
78
77
193
77
77
194
76
74
194
76
74
205
85
84
197
73
71
192
62
62
195
61
58
205
67
64
209
70
65
205
66
59
199
62
52
202
68
57
202
68
57
204
70
61
208
69
62
214
69
66
219
66
68
223
64
69
224
63
69
223
67
70
223
67
68
223
67
68
223
69
69
224
69
67
224
70
68
225
70
68
225
70
68
224
69
65
225
67
64
224
66
63
224
65
62
226
64
61
226
64
61
226
64
61
224
66
63
213
68
63
200
67
60
186
62
54
172
58
48
160
57
48
148
58
47
138
60
48
130
60
48
128
65
50
126
64
51
126
62
52
127
60
51
131
58
51
134
57
49
136
56
49
137
57
48
135
59
46
133
60
45
132
59
44
132
59
44
131
59
45
130
58
44
130
58
44
130
58
44
127
57
45
127
57
45
127
57
45
127
57
45
126
58
47
126
58
47
126
58
47
126
58
47
125
57
48
125
57
48
125
57
48
125
57
48
125
57
48
125
57
48
124
57
48
124
57
48
124
57
48
122
58
48
122
58
48
121
57
47
121
57
47
121
57
47
119
57
46
119
57
46
119
56
47
119
56
47
116
56
46
115
55
45
114
54
44
114
54
44
112
54
43
112
54
43
107
49
38
113
55
44
109
52
41
108
51
40
111
54
43
108
51
40
112
55
44
126
72
62
132
82
73
133
83
76
136
81
74
139
80
74
144
81
76
148
80
77
149
81
78
148
83
79
138
79
71
133
83
72
128
91
75
123
101
80
115
105
80
104
107
76
102
113
81
103
120
84
94
116
77
88
110
71
80
102
63
76
97
58
75
95
58
75
92
58
71
86
55
67
80
52
60
70
45
56
64
41
54
59
39
55
58
41
60
61
47
63
64
50
62
62
50
59
61
48
52
58
48
51
59
48
53
59
49
54
60
48
58
60
47
60
63
46
62
65
44
62
68
42
81
91
57
94
109
68
107
129
82
112
140
82
112
147
83
113
153
82
115
157
83
115
158
86
120
159
94
124
162
101
120
158
97
107
145
84
96
134
73
93
131
70
93
131
70
92
130
69
96
134
73
97
135
74
99
137
76
101
139
78
100
138
77
98
136
75
96
134
73
94
132
73
93
127
76
82
116
66
73
107
57
105
96
81
105
96
81
105
96
81
106
97
82
106
97
82
107
98
83
107
98
83
107
98
83
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
110
98
84
113
100
84
111
95
80
109
93
80
109
95
82
113
101
89
111
103
92
108
100
89
104
98
86
102
98
87
112
108
96
123
120
105
126
123
106
127
121
105
122
117
98
112
105
86
101
96
74
84
84
60
70
70
46
67
62
40
77
66
46
83
64
47
85
56
40
93
55
42
108
61
51
110
55
48
117
57
49
126
59
53
135
64
58
146
73
66
155
80
74
165
88
80
171
91
82
185
94
89
183
84
78
181
72
69
186
67
63
198
68
66
206
71
67
210
68
64
207
64
58
208
65
59
209
66
60
211
68
62
213
70
64
215
70
67
219
69
70
222
68
70
222
68
70
222
68
68
222
68
68
222
68
68
221
69
68
223
69
69
222
70
69
223
69
67
223
69
67
222
68
66
222
68
66
221
68
63
220
67
62
221
66
61
221
66
61
221
66
61
220
67
62
214
69
64
206
69
63
195
66
60
179
62
53
162
58
49
148
56
45
136
57
44
131
59
45
130
64
50
127
64
49
126
62
50
127
61
49
131
58
49
134
57
49
135
56
49
136
58
48
135
59
46
132
59
44
132
59
44
132
59
44
130
58
44
130
58
44
128
58
46
128
58
46
127
57
45
127
57
45
126
58
47
126
58
47
126
58
47
126
58
47
126
58
47
126
58
47
125
57
48
125
57
48
125
57
48
125
57
48
124
57
48
123
56
47
123
56
47
123
56
47
123
59
49
123
59
49
122
58
48
121
59
48
120
58
47
120
58
47
120
58
47
120
58
47
116
56
46
116
56
46
115
55
45
115
55
45
114
54
44
113
53
43
111
53
42
111
53
42
106
49
38
111
54
43
109
52
41
107
50
39
108
54
42
105
51
39
109
55
43
125
71
61
135
80
73
135
80
73
139
80
74
142
81
76
145
82
77
146
83
78
148
85
80
145
86
80
140
87
79
135
91
80
126
94
79
116
98
78
111
105
81
111
114
85
108
119
89
102
119
85
81
101
66
75
95
58
69
89
54
68
85
51
69
86
54
71
86
57
71
84
56
69
82
56
63
73
48
60
68
45
57
62
42
55
59
42
58
60
46
60
62
48
59
61
47
57
59
46
54
60
50
53
61
50
54
60
50
54
60
48
58
60
47
60
62
48
61
64
45
61
67
41
77
87
53
90
105
64
102
124
77
108
136
78
110
145
81
113
153
82
117
159
85
116
159
87
122
161
96
119
157
96
110
148
87
99
137
76
92
130
69
92
130
69
92
130
69
90
128
67
95
133
72
97
135
74
98
136
75
100
138
77
99
137
76
97
135
74
95
133
72
93
131
72
91
125
74
80
113
66
70
103
56
105
96
81
105
96
81
105
96
81
106
97
82
106
97
82
107
98
83
107
98
83
107
98
83
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
106
97
82
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
110
98
84
114
100
87
111
97
84
109
95
82
107
95
83
107
97
87
108
99
90
108
101
91
106
102
93
106
102
93
120
116
105
136
132
120
145
141
129
149
143
129
146
140
126
137
130
114
125
122
103
96
99
78
79
84
62
68
71
50
70
69
49
72
67
48
70
61
44
76
60
44
86
67
52
80
57
43
82
54
42
86
52
42
89
53
41
92
54
43
97
57
47
101
61
49
109
61
49
136
76
66
157
83
74
173
86
79
181
79
75
188
70
68
196
66
66
208
68
67
216
70
70
212
64
62
213
65
63
214
66
62
216
68
64
217
69
67
219
69
68
219
69
68
221
69
68
222
68
68
221
69
68
221
69
68
221
69
68
221
69
68
221
69
68
221
69
66
220
71
67
219
70
66
218
69
65
217
68
62
217
68
62
218
67
60
218
67
60
219
68
61
217
68
62
214
67
60
211
70
63
201
70
62
186
65
56
167
59
49
152
56
44
140
57
43
133
60
45
131
64
48
127
64
49
126
63
48
127
61
47
131
58
49
132
58
47
135
57
47
136
58
46
132
58
45
132
59
44
132
59
44
131
58
43
130
58
44
130
58
44
127
57
45
127
57
45
127
57
45
127
57
45
126
58
47
126
58
47
126
58
47
126
58
47
126
58
47
126
58
47
125
57
48
124
57
48
125
57
48
123
56
47
123
56
47
123
56
47
123
56
47
120
56
46
123
59
49
121
59
48
121
59
48
121
59
48
120
58
47
120
58
47
120
58
47
118
58
47
115
55
45
115
55
45
115
55
45
114
54
44
113
53
43
111
53
42
111
53
42
109
52
41
106
49
38
111
54
43
108
51
40
104
50
38
107
53
41
104
50
38
108
54
42
124
70
60
138
79
71
140
79
74
141
80
75
143
82
77
144
83
78
145
86
80
144
87
80
140
90
81
137
93
82
133
97
85
121
95
80
105
90
71
100
95
73
100
104
79
92
102
75
78
93
64
69
85
56
65
81
52
61
77
48
60
75
46
63
77
51
67
81
55
70
82
58
71
83
59
66
76
52
62
71
50
58
65
47
57
61
44
57
61
46
57
61
46
56
60
45
55
59
45
55
61
51
54
61
53
54
59
52
54
60
50
58
60
49
59
61
47
60
63
44
60
65
42
74
83
52
86
101
62
98
120
73
106
134
76
110
145
81
115
155
84
119
161
87
120
162
90
123
162
97
114
152
91
101
139
78
92
130
69
90
128
67
92
130
69
92
130
69
90
128
67
95
133
72
96
134
73
97
135
74
98
136
75
97
135
74
96
134
73
94
132
71
93
131
74
90
124
74
78
111
64
69
102
55
104
97
81
104
97
81
104
97
81
105
98
82
105
98
82
106
99
83
106
99
83
106
99
83
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
114
102
88
116
104
92
115
103
91
109
99
89
106
97
88
108
101
93
115
110
104
122
119
112
129
126
119
144
141
134
160
157
148
168
165
156
171
167
156
168
164
153
163
157
145
153
150
135
122
123
107
99
102
83
79
82
63
71
74
55
69
72
53
66
69
50
66
69
50
69
72
53
75
76
60
74
72
57
71
68
53
68
62
48
65
57
44
65
55
43
64
54
42
70
52
40
85
53
42
113
66
56
142
79
72
160
79
75
175
73
71
191
71
73
204
69
73
211
66
69
213
64
66
215
65
66
216
66
65
217
67
66
218
69
65
218
69
65
218
69
65
218
69
65
221
69
68
221
68
70
221
68
70
219
69
70
219
69
68
219
69
68
219
69
68
217
69
67
218
70
66
217
69
65
214
70
62
213
69
61
213
69
61
213
69
61
213
69
61
214
70
62
212
65
58
211
67
59
206
69
61
193
66
57
174
62
51
158
57
45
145
59
46
138
60
47
132
63
48
129
63
47
128
62
46
128
61
45
129
59
47
132
58
45
134
58
45
135
59
46
132
58
45
131
59
45
130
58
44
130
58
44
130
58
44
129
57
43
127
57
45
127
57
45
126
58
47
126
58
47
126
58
47
126
58
47
125
58
49
125
58
49
125
58
49
125
58
49
124
57
49
122
58
49
124
57
49
121
57
48
121
57
48
120
56
47
120
56
47
118
55
46
121
58
49
121
58
49
120
57
48
118
58
48
118
58
48
117
57
47
117
57
47
116
58
47
114
55
47
113
54
46
113
54
46
112
53
45
111
52
44
110
51
43
110
51
43
109
52
43
106
49
40
109
55
45
106
52
42
104
50
40
106
52
42
102
50
39
106
54
43
125
68
59
141
78
71
143
78
72
144
81
74
144
84
76
144
85
79
142
87
80
138
89
82
131
91
81
123
89
77
118
92
79
107
90
74
93
85
66
89
88
68
89
94
72
79
89
65
63
75
51
58
72
47
55
69
46
52
66
43
53
65
43
56
68
46
61
73
51
67
76
55
69
78
57
66
75
54
63
72
53
60
69
50
58
65
47
56
63
47
55
62
46
53
60
44
52
58
44
53
60
52
53
60
53
54
59
53
54
59
52
57
59
48
58
60
47
59
62
45
59
64
41
71
80
49
82
97
58
96
118
71
105
133
75
111
146
82
117
157
86
121
163
89
121
163
91
120
159
96
110
145
87
96
131
73
90
125
67
91
126
68
94
129
71
94
129
71
93
128
70
96
131
73
97
132
74
98
133
75
98
133
75
97
132
74
96
131
73
95
130
72
94
129
73
87
124
73
76
112
66
66
102
56
104
97
81
104
97
81
104
97
81
105
98
82
105
98
82
106
99
83
106
99
83
106
99
83
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
107
97
85
110
100
90
112
102
92
108
99
90
107
100
92
114
109
103
132
129
124
145
144
140
167
163
160
181
177
174
196
193
188
201
198
193
201
196
190
197
192
186
192
185
177
185
178
168
164
156
143
136
130
114
102
99
84
80
81
63
70
73
56
62
69
51
59
68
49
58
70
50
56
69
51
56
69
51
56
67
50
58
66
51
60
66
52
64
67
56
67
69
58
71
69
57
64
52
40
83
56
47
104
61
54
129
68
65
163
79
79
192
87
91
204
81
86
202
67
73
211
71
74
214
70
70
216
70
70
218
70
68
219
70
66
219
70
66
220
68
63
220
68
65
221
69
68
221
68
70
219
69
70
219
69
70
218
68
67
218
68
67
218
68
67
217
69
67
217
69
65
215
70
65
214
69
64
212
69
61
211
68
60
210
69
60
211
70
61
212
69
61
212
63
56
213
64
57
208
67
58
198
67
57
182
65
55
165
61
48
150
61
47
140
61
46
134
62
47
130
63
47
129
62
46
128
61
45
129
60
45
131
59
45
132
58
45
133
59
46
130
58
44
130
58
44
130
58
44
130
58
44
129
57
43
127
58
43
127
57
45
126
56
44
126
58
47
126
58
47
126
58
47
126
58
47
125
58
49
125
58
49
125
58
49
125
58
49
122
58
49
122
58
49
121
57
48
121
57
48
120
56
47
118
55
46
118
55
46
116
56
46
119
56
47
117
57
47
117
57
47
115
57
46
116
56
46
115
57
46
114
56
45
114
56
45
113
54
46
113
54
46
112
53
45
111
52
44
110
51
43
109
52
43
108
51
42
106
52
42
104
50
40
108
54
44
105
51
41
102
50
39
104
52
41
101
49
38
105
53
42
123
66
57
142
77
71
146
77
72
145
80
74
143
84
76
141
86
79
135
88
78
129
89
79
124
92
81
115
92
78
107
91
76
96
87
70
86
83
66
80
83
66
74
81
63
64
73
54
52
64
44
51
60
43
50
59
42
48
57
40
48
57
40
50
59
42
53
62
45
57
66
49
60
69
52
63
72
55
63
72
53
62
71
52
58
70
50
58
67
50
53
64
47
51
62
45
49
60
44
51
58
51
51
57
53
52
57
53
52
57
50
56
57
49
57
59
46
59
62
45
59
64
42
67
76
47
79
94
55
93
115
68
105
132
77
112
147
83
119
159
89
121
163
89
119
161
89
113
152
89
102
137
79
90
125
67
88
123
65
92
127
69
93
128
70
93
128
70
94
129
71
96
131
73
96
131
73
96
131
73
96
131
73
96
131
73
95
130
72
94
129
71
93
128
74
85
121
73
74
110
64
65
101
57
104
97
81
104
97
81
104
97
81
105
98
82
105
98
82
106
99
83
106
99
83
106
99
83
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
98
86
106
96
86
106
97
88
107
100
90
107
103
94
117
112
106
136
133
128
159
158
154
176
176
174
195
194
192
211
210
208
226
225
223
231
230
228
231
227
224
227
222
218
222
214
211
217
206
200
206
188
178
184
165
151
151
135
122
121
109
93
95
89
73
75
76
58
64
69
49
59
68
47
62
74
54
61
72
55
61
70
53
59
67
52
59
62
53
58
59
51
57
56
51
58
55
48
63
60
51
69
59
50
81
53
49
99
55
52
137
71
72
177
92
95
199
96
100
202
87
90
205
81
83
209
79
79
213
78
75
216
77
74
219
74
71
221
72
68
224
69
65
223
68
66
222
68
68
221
68
70
221
68
70
220
67
69
220
67
69
219
66
68
219
67
66
217
67
66
218
68
67
216
68
64
213
68
63
212
69
63
211
68
60
210
69
60
211
70
61
212
69
61
217
64
58
217
64
58
209
66
58
200
67
58
188
67
58
172
64
52
153
59
47
140
57
43
137
61
47
131
62
46
129
62
45
128
61
44
129
60
44
131
59
44
131
59
44
132
60
45
130
58
44
130
58
44
130
58
44
129
57
43
127
57
45
127
57
45
126
56
46
125
57
46
126
58
47
126
58
47
125
58
49
125
58
49
125
58
49
125
58
49
125
58
49
125
58
49
122
58
49
121
58
49
120
57
48
120
57
48
119
56
47
116
56
46
115
55
45
114
56
45
116
56
46
115
57
46
114
56
45
113
56
45
114
56
45
112
55
44
112
55
44
112
55
44
111
54
45
111
54
45
110
53
44
110
53
44
109
52
43
108
51
42
107
50
41
105
51
41
103
49
39
107
55
44
104
52
41
101
49
38
104
52
41
98
48
37
102
52
41
122
65
56
144
77
71
146
77
72
145
80
74
142
85
76
136
88
78
129
89
79
120
90
79
114
92
78
107
94
78
94
87
71
82
80
65
75
78
61
67
74
58
57
66
49
50
58
43
48
56
41
49
55
43
49
55
45
48
54
44
47
53
43
47
53
41
48
56
43
50
58
43
52
60
45
57
68
52
59
70
53
61
72
55
60
73
55
59
70
53
54
67
49
51
64
44
49
62
45
46
56
48
48
54
50
49
54
50
50
55
49
55
56
50
57
59
48
58
60
46
58
63
41
63
72
43
75
89
53
91
113
67
104
131
76
113
147
86
119
158
91
119
161
89
115
156
86
103
141
80
94
129
73
85
120
64
87
122
66
91
126
70
91
126
70
91
126
70
93
128
72
95
130
74
95
130
74
95
130
74
95
130
74
94
129
73
94
129
73
93
128
72
93
128
74
85
118
73
74
107
64
64
96
55
104
97
81
104
97
81
104
97
81
105
98
82
105
98
82
106
99
83
106
99
83
106
99
83
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
105
98
82
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
107
98
83
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
99
84
108
98
86
112
103
94
110
103
95
108
103
97
116
113
106
135
132
127
161
160
156
188
188
186
206
208
207
207
207
207
224
224
224
242
242
242
250
248
249
251
247
246
247
243
240
241
236
233
240
226
223
232
208
198
221
193
181
193
171
157
162
145
129
125
117
98
94
92
71
75
77
55
67
72
50
65
73
50
65
73
52
64
71
53
66
68
55
65
65
55
66
61
57
67
57
55
65
57
54
59
58
53
66
61
55
71
53
49
76
45
43
102
54
54
145
81
82
181
102
105
199
109
109
192
92
92
199
90
87
206
87
83
211
84
78
216
78
75
222
74
72
225
70
68
226
68
67
221
68
70
219
69
70
219
69
70
220
67
69
220
67
69
219
66
68
219
67
66
219
67
66
220
68
67
217
67
66
215
67
63
212
67
62
212
67
62
211
68
60
210
69
60
213
69
61
221
66
61
219
65
57
210
66
58
202
68
59
191
69
58
176
65
54
156
58
45
139
53
38
138
60
47
133
61
46
130
61
45
128
61
44
129
60
44
129
60
44
131
60
42
132
60
45
130
58
44
130
58
44
130
58
44
127
58
43
127
57
45
126
56
44
125
57
46
125
57
46
126
58
47
126
58
47
125
58
49
125
58
49
125
58
49
125
58
49
125
58
49
123
59
49
121
58
49
119
59
49
120
57
48
117
57
47
116
56
46
115
57
46
114
56
45
113
55
44
114
56
45
113
56
45
112
55
44
110
56
44
112
55
44
109
55
43
111
54
43
111
54
43
111
54
45
111
54
45
110
53
44
109
52
43
108
51
42
106
52
42
105
51
41
104
52
41
102
50
39
107
55
44
104
52
41
99
49
38
101
51
40
98
48
37
101
51
40
120
66
56
142
77
71
146
79
73
142
82
74
138
85
77
131
88
79
122
90
79
113
91
77
104
92
76
90
84
68
73
74
58
63
67
52
62
68
54
57
65
50
48
56
43
47
55
42
54
60
50
52
55
48
53
56
49
53
55
50
52
55
48
50
53
46
48
54
44
48
56
45
49
57
44
54
65
51
58
69
53
61
74
57
62
75
57
60
73
55
55
70
51
50
65
44
49
62
45
44
54
46
46
52
48
48
53
49
49
54
48
54
55
49
56
58
47
58
60
46
58
63
43
60
69
42
73
87
52
89
111
65
103
130
77
112
146
86
118
157
92
117
158
88
112
153
85
96
134
73
88
123
67
83
118
62
87
122
66
91
126
70
89
124
68
89
124
68
92
127
71
95
130
74
95
130
74
94
129
73
94
129
73
94
129
73
93
128
72
93
128
72
93
127
76
83
116
73
72
104
65
63
93
55
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
106
99
83
107
98
83
107
98
83
107
98
83
108
99
84
108
99
84
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
108
100
87
105
98
90
103
100
93
118
115
110
147
146
142
175
175
173
196
198
197
218
220
219
234
238
239
243
244
246
246
247
249
252
252
254
255
255
255
255
255
253
255
251
250
252
247
244
253
242
236
247
225
212
242
216
199
224
202
181
198
182
159
170
160
135
136
132
105
98
98
70
69
72
45
68
72
47
67
71
48
70
69
49
71
68
53
70
60
51
67
54
48
70
52
50
73
57
57
65
60
57
66
61
58
70
56
55
75
54
53
101
69
70
147
105
106
198
147
146
233
172
169
211
140
136
199
119
112
191
96
90
195
86
81
210
82
79
219
77
76
225
71
73
225
69
72
220
70
71
220
74
75
217
68
70
210
60
62
215
62
64
225
72
74
225
71
73
216
62
64
218
64
64
218
64
64
217
65
64
215
66
62
215
66
62
212
64
60
209
64
59
211
64
57
222
67
62
219
65
57
211
64
57
202
65
57
190
66
56
176
64
53
159
58
46
144
56
42
140
61
48
135
62
47
131
62
46
129
62
45
128
61
44
127
60
43
129
61
42
129
60
44
127
58
43
127
57
45
127
57
45
126
56
44
126
56
46
124
56
45
124
56
45
124
56
45
124
57
48
124
57
48
124
57
48
124
57
48
122
58
49
122
58
49
122
58
49
122
58
49
121
61
53
119
60
52
119
59
51
117
58
50
116
57
49
113
56
47
112
55
46
112
55
46
111
54
45
109
55
45
109
55
45
108
56
45
109
55
45
108
56
45
109
55
45
109
55
45
105
50
43
109
54
47
112
57
50
111
56
49
107
52
45
103
50
42
103
50
42
104
51
43
101
48
40
105
55
46
100
50
41
93
45
35
96
48
38
95
47
37
98
50
40
115
62
54
140
79
74
143
80
75
136
81
74
129
82
74
125
89
77
118
95
81
102
89
73
84
78
62
68
69
53
62
69
53
58
67
50
53
64
48
52
60
47
51
59
48
52
58
48
52
58
48
54
56
51
55
56
51
54
54
52
52
54
49
51
53
48
49
54
47
47
54
46
46
54
43
46
57
43
50
61
45
54
67
50
58
71
54
58
73
54
56
71
52
52
67
46
49
63
46
42
52
43
44
50
46
45
50
46
46
51
45
52
53
47
55
57
46
58
60
47
58
63
43
54
62
38
68
82
49
87
108
67
104
130
82
115
148
93
119
158
95
114
155
87
107
148
82
91
126
68
88
121
68
84
117
64
84
117
64
87
120
67
90
123
70
91
124
71
90
123
70
94
127
74
98
131
78
98
131
78
94
127
74
92
125
72
95
128
75
96
129
76
94
126
77
83
113
75
70
100
66
61
88
55
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
107
98
83
107
98
83
107
98
83
108
99
84
108
99
84
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
108
100
89
103
100
93
111
110
106
134
133
131
164
164
162
193
195
194
214
218
219
234
238
239
247
251
252
247
251
254
248
252
255
252
253
255
254
254
254
255
255
253
255
254
250
255
252
249
255
250
243
255
240
224
251
235
212
237
223
197
215
205
178
192
187
157
164
163
132
130
131
99
103
106
75
76
81
51
71
74
47
69
67
46
71
66
47
72
62
50
73
58
51
76
57
53
75
60
57
66
56
55
71
62
63
81
69
69
96
80
80
124
104
103
166
140
139
207
178
174
232
197
191
255
213
207
234
177
168
201
131
123
188
99
93
199
87
85
213
83
83
219
74
77
216
67
69
213
71
70
210
70
69
215
73
72
219
75
75
218
70
70
212
62
63
214
61
63
221
67
69
217
63
63
217
63
63
218
64
64
218
64
64
217
65
62
214
65
61
212
64
60
212
63
57
220
65
60
219
64
59
211
64
57
203
66
58
191
67
59
177
65
54
160
59
49
146
56
45
140
61
48
135
62
47
131
62
46
129
62
45
128
61
44
127
60
43
129
60
44
129
60
44
127
58
43
127
57
45
127
57
45
126
56
44
125
57
46
124
56
45
124
56
45
124
56
45
124
57
48
124
57
48
124
57
48
124
57
48
122
58
49
123
59
50
123
59
50
122
59
50
119
60
52
117
60
51
117
58
50
115
58
49
114
57
48
110
56
46
111
54
45
109
55
45
109
55
45
108
56
45
108
56
45
108
56
45
108
56
45
106
56
45
108
56
45
108
56
45
107
54
46
110
55
48
111
56
49
110
57
49
108
55
47
105
52
44
103
50
42
101
51
42
99
49
40
100
52
42
95
47
37
94
46
36
98
50
40
94
46
36
96
48
38
112
62
53
129
74
69
135
82
76
135
88
82
126
88
79
114
86
75
100
82
68
81
74
58
62
63
47
58
65
49
54
65
48
53
64
48
51
62
48
52
60
49
52
60
49
53
59
49
55
58
51
55
57
52
55
55
53
55
55
53
53
55
50
52
54
49
49
54
47
47
54
46
47
55
44
46
57
43
49
60
46
52
65
48
56
69
52
57
72
53
56
71
52
52
69
50
52
66
49
46
56
47
46
53
46
46
51
45
45
50
43
50
51
43
52
54
43
56
58
45
57
61
44
55
63
40
68
81
51
87
108
69
104
130
85
116
149
96
119
157
98
112
152
89
103
143
80
89
124
68
87
120
67
84
117
64
84
117
64
86
119
66
89
122
69
89
122
69
89
122
69
91
124
71
95
128
75
95
128
75
92
125
72
91
124
71
94
127
74
94
127
74
94
123
77
82
109
74
70
95
66
59
84
55
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
107
98
83
107
98
83
107
98
83
108
99
84
108
99
84
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
110
101
86
110
101
86
108
102
90
101
100
95
121
123
120
151
153
152
178
182
183
205
209
210
228
233
236
244
249
252
250
255
255
251
255
255
252
255
255
253
254
255
254
254
254
255
254
252
255
255
251
255
255
250
255
255
244
254
252
231
250
249
221
239
240
209
225
229
196
210
216
180
188
196
159
160
168
131
135
145
108
96
103
69
81
88
55
68
71
44
68
67
46
74
68
52
76
68
55
75
65
56
73
62
58
68
58
57
84
74
75
107
95
95
130
118
118
157
146
144
192
178
175
219
206
200
234
220
211
248
228
217
255
240
229
255
224
214
234
167
159
190
101
97
177
67
68
198
72
75
219
86
89
216
88
85
207
79
76
202
70
66
204
69
66
210
70
69
214
70
70
216
68
68
217
67
68
214
61
63
216
62
64
217
63
63
217
63
63
217
65
64
214
64
63
213
65
61
213
64
58
217
64
58
217
64
58
210
66
58
203
69
60
192
68
60
178
66
55
161
60
50
147
57
46
142
60
48
137
61
48
131
62
47
129
62
46
127
61
45
126
60
44
128
61
45
128
61
45
127
57
45
127
57
45
126
58
47
125
57
46
125
57
46
124
56
45
123
56
47
123
56
47
125
58
49
125
58
49
123
59
50
123
59
50
123
59
50
123
59
50
122
59
52
120
60
52
116
59
50
113
59
49
114
57
48
111
57
47
110
56
46
108
56
45
108
54
44
107
55
44
108
56
45
106
56
45
106
56
45
106
56
45
106
56
45
105
57
45
106
56
45
106
56
45
110
57
49
109
56
48
109
56
48
109
56
48
109
56
48
105
55
46
102
52
43
99
49
40
100
50
41
97
49
39
93
45
35
95
48
38
99
52
42
92
45
35
95
48
38
115
68
60
125
80
74
129
86
80
126
89
81
112
84
73
94
74
63
79
67
55
63
60
45
52
54
40
50
59
42
48
59
43
49
60
46
50
61
47
52
60
49
52
60
49
53
59
49
55
58
51
55
57
52
55
57
54
54
56
51
53
55
50
53
55
50
50
55
48
48
55
47
48
56
45
46
57
43
48
59
45
50
63
46
54
67
50
55
69
52
56
71
52
56
71
52
57
70
53
52
62
51
51
58
50
47
52
45
44
50
40
47
48
40
49
51
40
53
55
42
54
58
41
54
62
39
68
81
53
89
109
74
107
132
90
119
151
102
119
156
102
108
147
90
95
135
75
87
120
67
87
117
67
84
114
64
84
114
64
86
116
66
88
118
68
89
119
69
89
119
69
90
120
70
93
123
73
95
125
75
93
123
73
93
123
73
95
125
75
94
124
74
90
119
75
78
101
72
65
87
64
54
76
53
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
105
99
83
106
99
83
107
98
83
107
98
83
108
99
84
108
99
84
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
110
101
86
110
101
86
110
101
86
110
101
86
109
103
91
106
107
102
131
135
136
163
167
168
185
190
193
208
213
216
229
237
239
243
251
253
245
253
255
251
255
255
251
255
255
252
255
255
252
254
251
253
254
249
254
253
248
255
254
247
255
255
243
248
253
230
244
253
222
238
247
216
230
242
206
221
233
195
206
218
180
182
193
153
161
171
134
128
135
101
102
109
76
77
81
54
70
72
48
72
74
53
75
73
58
72
68
57
68
64
55
80
70
68
106
96
95
139
129
128
164
154
153
183
175
172
203
198
194
223
218
212
234
230
221
247
238
229
255
249
239
255
246
235
255
215
207
245
178
172
227
141
140
200
100
100
176
68
66
194
83
76
200
85
78
199
82
73
195
72
65
199
72
66
211
77
74
213
74
71
207
63
62
210
62
62
212
62
63
213
63
62
216
64
63
214
64
63
214
64
63
213
65
61
213
65
61
214
62
57
213
64
58
210
67
61
204
69
63
192
69
62
178
65
57
162
61
53
149
59
50
142
60
49
137
61
48
131
62
47
129
62
46
127
61
47
126
60
44
128
61
45
128
61
45
127
57
45
127
57
45
126
58
47
125
57
46
125
57
46
124
56
45
123
56
47
123
56
47
126
59
50
126
59
50
124
60
51
123
59
50
123
59
50
123
59
50
122
59
52
120
60
52
114
57
48
111
57
47
111
57
47
109
57
46
108
56
45
107
55
44
106
54
43
104
54
43
105
55
44
105
55
44
105
55
44
104
56
44
104
56
44
104
56
44
104
56
44
104
56
44
109
59
50
108
55
47
106
53
45
105
55
46
106
56
47
106
56
47
102
52
43
97
49
39
101
53
43
97
50
40
93
46
36
97
50
40
98
51
41
89
42
32
97
50
40
121
78
69
129
92
86
118
87
82
105
78
71
87
67
58
73
60
51
64
58
46
58
59
45
56
60
46
48
56
41
47
58
42
51
59
46
52
60
47
53
61
50
53
59
49
54
57
48
54
57
48
55
57
52
54
56
51
54
56
51
52
57
50
51
56
49
51
57
47
49
57
46
49
57
46
46
57
43
47
58
44
48
61
44
50
63
46
53
66
49
55
69
52
59
72
54
60
73
56
57
67
56
55
63
52
50
56
46
45
51
39
46
48
37
47
49
36
50
52
38
52
56
39
52
60
39
69
82
56
93
112
80
114
139
100
123
154
110
119
156
105
102
141
88
85
124
69
83
115
65
84
114
64
83
113
63
83
113
63
84
114
64
85
115
65
86
116
66
86
116
66
87
117
67
91
121
71
93
123
73
92
122
72
94
124
74
95
125
75
92
122
72
87
114
73
73
93
68
61
78
60
49
66
48
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
105
99
83
106
99
83
107
98
83
107
98
83
108
99
84
108
99
84
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
110
101
86
110
101
86
111
102
87
111
102
87
110
103
93
113
113
111
135
140
143
164
169
172
183
191
194
205
213
216
227
237
239
242
252
254
244
254
255
248
255
255
250
255
255
251
255
252
253
255
250
252
253
245
250
252
241
249
249
237
246
248
234
246
255
232
243
255
228
240
253
225
238
249
217
233
244
210
222
232
197
202
210
173
182
190
153
157
163
129
125
130
98
89
93
66
71
76
53
70
75
55
70
74
57
67
71
57
65
67
56
98
95
88
133
125
122
170
162
159
192
184
181
203
195
193
213
208
205
230
225
221
241
238
233
249
246
239
250
243
233
251
237
226
255
236
227
255
242
234
255
224
219
233
165
162
182
105
99
164
76
64
178
82
68
192
90
78
199
92
82
197
83
73
190
69
61
193
64
59
202
67
64
203
64
61
206
64
62
209
63
63
210
64
64
211
66
63
211
66
63
211
66
63
211
66
61
211
63
59
210
65
60
208
69
64
201
69
64
189
67
62
175
64
57
160
62
53
149
61
51
142
60
49
137
60
50
131
61
49
129
61
48
127
61
49
126
60
46
127
61
47
127
61
47
126
58
47
126
58
47
126
58
47
125
57
46
124
57
48
123
56
47
123
56
47
123
56
47
124
60
51
124
60
51
124
60
51
123
59
50
122
59
52
121
58
51
121
58
51
119
59
51
110
55
48
109
56
48
108
55
47
108
55
47
107
54
46
104
54
45
104
54
45
104
54
45
105
55
46
104
56
46
104
56
46
103
56
46
103
56
46
103
56
46
103
56
46
104
56
46
107
57
50
104
54
47
102
52
45
103
53
46
105
55
48
104
55
48
101
52
45
98
49
42
99
50
43
98
51
43
95
48
40
93
49
40
93
49
40
86
42
33
98
54
45
123
85
76
116
89
82
97
78
72
78
63
56
65
55
46
61
54
44
59
57
45
57
59
46
57
60
49
51
57
45
51
59
46
53
59
47
54
60
48
54
60
48
55
58
47
53
56
45
52
55
46
54
57
48
52
57
50
51
56
49
51
57
47
51
57
47
50
58
47
49
57
46
49
57
44
46
57
43
46
57
43
47
58
44
48
59
45
51
62
46
54
67
50
58
69
53
60
71
55
61
69
56
58
66
53
53
59
47
48
54
40
48
50
37
47
49
36
49
51
37
49
53
36
50
58
37
71
83
59
98
117
87
119
143
107
124
155
114
113
149
103
92
130
81
74
112
61
79
111
62
81
110
62
81
110
62
81
110
62
81
110
62
81
110
62
83
112
64
83
112
64
87
116
68
91
120
72
93
122
74
93
122
74
95
124
76
95
124
76
90
119
71
82
108
69
65
82
63
54
67
57
43
56
46
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
105
99
83
106
99
83
107
98
83
107
98
83
108
99
84
108
99
84
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
110
101
86
110
101
86
111
102
87
112
103
88
112
103
88
110
103
93
108
110
109
122
130
133
147
154
160
171
181
183
196
206
208
220
230
232
237
247
249
244
254
255
245
254
253
246
255
252
250
255
250
251
255
249
251
254
243
248
250
237
245
246
232
241
244
227
244
253
234
244
253
232
244
252
229
243
249
223
242
246
219
233
236
205
216
215
185
197
196
165
177
176
146
143
142
114
101
103
79
77
80
59
71
75
58
70
77
61
71
79
66
74
82
71
122
123
115
158
155
148
195
190
184
215
207
204
222
212
211
230
220
219
243
235
233
252
247
244
243
240
235
254
251
244
255
255
246
255
251
241
255
243
234
255
237
229
255
226
218
255
210
199
212
143
127
181
100
81
162
75
58
178
85
70
190
88
74
182
71
60
182
63
55
199
74
68
199
67
62
202
67
63
205
66
63
208
66
64
208
66
62
208
66
62
208
66
62
208
65
61
210
65
60
210
67
63
206
68
65
197
68
63
184
65
61
170
61
56
158
59
53
149
60
52
142
59
51
137
60
50
131
61
51
129
61
50
127
61
49
126
60
48
127
61
49
127
61
47
126
58
47
126
58
47
126
58
47
125
57
46
124
57
48
123
56
47
123
56
47
123
56
47
124
60
51
123
59
50
123
59
50
122
58
49
121
58
51
120
57
50
119
56
49
117
57
49
110
55
48
108
55
47
108
55
47
106
56
47
105
55
46
105
55
46
104
54
45
103
55
45
103
55
45
103
55
45
102
55
45
102
55
45
102
55
45
102
55
45
102
55
45
102
55
45
104
55
48
104
54
47
103
53
46
103
53
46
103
54
47
104
55
48
103
54
47
102
53
46
97
50
42
101
54
46
94
50
41
92
48
39
96
52
43
97
53
44
105
61
52
117
83
74
88
71
64
69
62
54
60
53
47
57
53
44
59
56
47
56
58
47
53
55
44
51
54
43
54
57
46
53
59
47
55
58
47
54
57
46
54
57
46
53
56
45
53
55
44
51
54
43
50
56
44
49
57
44
50
56
46
49
57
44
49
57
46
49
57
44
49
57
44
49
57
44
46
57
43
46
57
43
46
57
43
47
58
44
48
59
43
51
62
46
54
65
49
55
66
50
60
68
53
58
67
50
56
63
47
53
59
45
52
54
41
50
52
39
49
51
37
48
52
37
48
55
37
72
84
62
101
120
92
117
141
109
116
146
108
102
137
95
82
119
75
66
104
57
75
107
60
78
107
59
80
109
61
80
109
61
78
107
59
78
107
59
79
108
60
81
110
62
86
115
67
90
119
71
93
122
74
94
123
75
95
124
76
95
124
76
86
115
67
78
102
66
59
73
56
48
59
51
38
47
42
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
105
99
83
106
99
83
107
98
83
107
98
83
108
99
84
108
99
84
109
100
85
109
100
85
109
100
85
109
100
85
109
100
85
110
101
86
110
101
86
111
102
87
112
103
88
113
104
89
111
104
94
104
105
107
109
116
122
133
140
148
163
172
177
191
200
205
211
222
226
229
240
242
240
252
252
242
252
251
245
254
249
248
255
247
250
255
246
252
255
242
250
253
236
248
249
231
246
247
231
246
246
234
247
247
235
250
247
232
252
246
230
253
245
226
247
236
214
231
216
195
213
198
175
192
177
154
158
147
125
117
110
91
90
87
70
80
81
67
83
86
75
91
98
90
99
109
100
146
154
141
176
179
168
210
207
198
230
223
217
242
228
227
249
235
235
255
241
244
255
248
249
255
253
251
255
251
248
249
248
243
247
247
237
251
249
237
255
252
240
255
253
241
255
249
234
255
224
203
234
168
144
183
111
89
168
87
68
174
85
69
183
82
70
185
77
65
191
74
65
195
71
63
198
69
63
202
69
64
203
68
64
205
68
62
204
67
61
203
66
60
204
65
60
209
67
63
208
69
64
204
69
65
194
67
61
179
61
57
166
59
53
156
59
53
149
60
54
140
60
51
135
61
52
133
60
53
129
61
52
127
60
51
126
60
48
127
61
49
127
61
49
126
58
47
126
58
47
126
58
47
125
57
46
124
57
48
123
56
47
123
56
47
123
56
47
123
59
50
123
59
50
121
58
51
120
57
50
119
56
49
118
55
48
118
55
48
115
55
47
110
55
48
106
56
47
106
56
47
106
56
47
106
56
47
104
56
46
104
56
46
104
56
46
103
55
45
102
55
45
102
55
45
102
55
45
100
56
45
100
56
45
100
56
45
102
55
45
103
54
47
104
55
48
105
56
49
105
56
49
104
55
48
105
56
49
106
59
51
108
61
53
103
56
48
107
60
52
98
54
45
97
53
44
111
67
58
120
76
67
116
73
64
110
77
68
67
57
48
53
54
46
52
53
47
56
57
49
56
59
50
52
55
46
49
52
43
50
53
44
53
56
45
53
56
45
53
55
44
52
54
43
52
54
43
52
54
43
52
54
41
51
55
41
47
55
40
46
57
41
48
56
43
46
57
41
46
57
43
46
57
41
48
56
41
48
56
41
49
57
42
48
56
41
48
56
43
47
55
42
48
56
43
49
57
44
51
59
46
52
60
45
57
66
49
58
67
48
59
66
48
58
65
49
58
60
46
54
56
42
50
52
38
47
51
36
50
57
39
73
85
63
99
117
91
109
132
103
102
132
98
87
121
84
73
110
69
63
100
56
72
103
59
76
105
59
78
107
61
78
107
61
76
105
59
76
105
59
77
106
60
79
108
62
85
114
68
89
118
72
91
120
74
93
122
76
94
123
77
92
121
75
81
110
64
71
95
59
54
66
52
43
52
47
34
40
38
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
103
100
83
105
99
83
106
99
83
106
99
83
106
98
85
108
98
86
108
98
86
109
99
87
109
100
85
109
100
85
109
100
85
108
101
85
109
102
86
110
103
87
111
103
90
110
104
92
111
104
94
109
106
101
107
111
114
109
116
122
131
138
144
165
174
179
195
204
209
213
223
225
230
238
241
244
252
254
244
253
250
247
253
249
249
254
247
250
255
244
252
255
242
252
255
241
252
255
238
253
254
240
251
248
239
254
250
241
255
249
239
255
250
237
255
249
233
255
242
223
239
224
203
221
206
183
193
178
155
162
151
129
124
117
98
98
95
78
88
90
76
93
99
87
109
119
110
124
134
125
165
173
162
187
193
181
215
215
207
235
230
226
250
238
238
255
244
244
255
245
247
254
244
245
253
247
247
250
249
247
250
252
247
251
255
249
250
253
244
244
247
238
247
249
238
255
252
239
255
242
225
255
242
224
250
201
184
188
129
113
157
85
71
171
85
72
183
85
74
177
69
59
192
74
64
198
71
64
201
70
62
202
69
62
202
67
61
201
66
60
199
66
59
200
65
59
207
70
62
205
71
62
199
71
62
188
67
59
173
62
53
159
58
50
149
59
51
145
62
54
138
61
53
132
62
52
130
62
53
129
61
52
127
60
51
126
60
48
128
60
49
128
60
49
126
58
47
126
58
47
126
58
49
125
57
48
124
57
48
123
56
47
123
56
47
123
56
47
122
58
49
122
58
49
120
57
50
119
56
49
118
55
48
117
54
47
117
54
47
113
54
46
109
56
48
107
57
48
106
56
47
106
56
47
105
57
47
105
57
47
105
57
47
104
56
46
103
55
45
102
55
45
103
55
45
102
55
45
102
55
45
102
55
45
102
55
45
102
55
45
102
53
46
106
56
49
109
59
52
107
58
51
106
57
50
106
58
48
109
62
52
111
67
56
108
64
53
111
68
59
100
60
50
99
61
50
122
86
74
134
100
90
121
87
77
98
76
65
62
58
49
51
56
49
52
57
50
55
60
53
52
58
48
47
53
43
50
53
44
57
60
51
51
54
43
50
53
42
50
52
41
48
51
40
49
51
40
50
53
42
52
54
43
52
55
44
46
54
41
44
55
41
47
55
42
45
56
42
47
55
42
47
55
42
47
55
42
47
55
42
49
57
44
48
56
43
48
56
45
47
55
44
47
55
44
47
55
44
48
56
45
48
56
43
55
63
48
57
66
49
60
69
52
61
68
52
59
65
51
56
60
46
50
54
39
45
52
36
50
59
42
74
86
66
99
114
91
104
124
97
93
118
88
79
109
75
70
102
65
65
97
58
72
100
59
75
104
60
78
107
63
78
107
63
75
104
60
74
103
57
76
105
61
78
107
61
84
113
69
87
116
72
90
119
75
91
120
76
93
122
78
90
118
77
78
106
65
67
91
59
50
62
52
41
47
47
31
37
37
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
106
100
86
106
100
86
106
100
88
108
100
89
108
99
90
109
101
90
109
101
88
109
102
86
103
96
78
106
101
82
110
107
88
111
108
93
104
101
92
102
101
97
115
113
114
129
130
134
137
142
146
149
157
160
170
178
181
194
199
202
215
220
223
233
237
238
245
246
248
248
250
249
252
254
253
254
254
252
255
255
250
253
254
246
250
253
242
249
252
241
250
253
242
251
254
243
249
250
242
250
250
242
249
249
237
252
250
235
254
252
231
249
247
222
233
229
202
213
212
182
186
185
155
152
152
124
113
116
89
95
100
77
98
107
88
114
125
108
132
144
130
145
155
144
180
185
178
202
204
199
226
227
222
240
239
235
247
243
242
251
247
246
253
249
248
252
248
247
255
254
252
254
254
252
254
254
252
253
255
250
254
255
250
254
255
250
254
255
250
254
255
250
251
253
248
255
253
248
255
251
246
255
229
223
215
166
159
163
96
88
155
68
61
178
75
66
193
79
69
195
71
61
198
67
57
201
68
59
205
72
63
206
73
64
203
72
64
202
71
61
198
71
56
195
72
56
190
73
56
181
71
56
166
69
53
154
66
52
144
66
53
139
67
53
133
67
53
129
67
54
127
65
52
127
63
51
128
62
50
128
60
49
129
59
49
129
59
49
127
57
49
126
58
49
124
57
49
123
56
48
122
55
47
122
55
47
119
55
46
118
54
45
119
55
46
119
55
46
119
56
47
119
56
47
118
55
46
117
54
45
113
53
43
110
51
43
108
55
47
105
56
49
105
56
49
104
55
48
104
55
48
103
54
47
103
54
47
103
54
47
102
53
46
103
54
47
105
55
48
105
56
49
104
55
48
103
54
47
101
52
45
101
51
44
104
51
45
106
53
47
109
56
48
107
57
48
106
58
48
105
59
46
105
61
48
107
65
51
103
66
50
106
73
58
100
72
58
118
95
79
126
107
92
138
120
106
140
127
111
82
74
61
55
57
46
52
58
48
52
58
48
51
57
47
51
57
47
51
57
47
50
56
46
50
56
46
49
55
45
49
55
45
51
54
45
48
54
44
50
53
44
47
53
43
49
52
43
47
53
43
47
52
45
46
53
45
47
52
45
46
53
45
47
52
45
47
52
45
47
52
45
47
52
45
48
53
46
47
52
45
47
52
45
46
51
44
46
51
44
47
52
45
47
52
45
47
54
46
49
59
50
51
61
50
54
64
53
56
66
55
57
68
54
55
66
52
53
64
48
51
62
45
52
63
46
68
81
61
87
101
78
91
107
81
80
98
72
69
88
60
64
84
56
66
87
54
73
99
62
75
101
62
75
102
61
75
102
61
74
101
60
74
101
58
76
103
62
79
106
63
85
112
71
89
116
75
92
118
79
93
119
80
93
119
82
87
113
78
72
98
63
59
79
54
37
48
42
38
43
46
36
41
44
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
106
99
89
107
100
90
108
99
92
109
100
91
109
101
90
109
102
86
105
100
81
106
104
83
109
106
87
106
107
91
103
103
95
108
108
108
126
127
132
144
147
154
167
174
180
183
193
195
208
213
216
225
230
233
241
242
244
250
251
253
255
254
255
255
254
255
254
253
251
255
254
250
255
255
250
254
255
249
251
252
244
248
251
242
246
252
242
247
253
243
247
250
241
248
250
239
248
250
237
247
250
233
249
251
229
242
245
218
223
226
195
204
208
175
180
184
151
145
150
118
107
113
85
89
97
73
95
107
85
116
129
111
140
152
138
157
167
156
191
193
188
211
211
209
231
231
229
243
243
241
247
247
245
251
251
249
252
252
250
251
251
249
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
251
255
254
241
255
255
237
251
252
247
249
248
255
248
247
255
233
230
239
190
185
196
128
119
164
80
70
176
78
65
186
76
63
193
75
63
196
74
61
194
68
56
188
61
52
190
63
54
196
70
58
196
70
55
195
74
55
191
76
58
180
73
55
166
70
54
153
67
50
144
68
54
138
71
55
130
68
55
127
67
56
126
66
55
124
64
53
126
62
52
127
60
51
128
60
49
128
60
51
125
58
50
125
58
50
122
58
49
122
58
49
121
57
48
120
56
47
118
55
46
118
55
46
117
54
45
118
55
46
116
56
46
116
56
46
116
56
46
114
54
44
111
53
42
109
52
43
108
55
47
105
56
49
106
56
49
104
55
48
105
55
48
104
55
48
103
54
47
103
54
47
101
51
44
101
51
44
103
53
46
104
54
47
106
56
49
106
56
49
106
56
49
107
57
50
107
54
48
108
55
49
107
57
48
107
59
49
107
59
47
106
62
49
106
64
48
105
68
50
103
72
54
107
80
61
103
82
65
117
102
83
129
117
101
140
133
115
134
131
114
75
76
60
54
57
46
51
57
47
51
57
47
51
57
47
50
56
46
50
56
46
50
56
46
49
55
45
49
55
45
49
55
45
49
55
45
48
54
44
48
54
44
47
53
43
47
53
43
47
53
43
47
52
46
47
52
46
47
52
46
47
52
46
47
52
46
47
52
46
47
52
46
47
52
46
48
53
47
47
52
46
46
51
45
46
51
45
46
51
45
46
51
45
47
52
46
47
54
47
46
56
48
47
58
50
49
61
51
52
64
54
54
66
54
54
66
54
54
66
52
53
66
49
54
67
49
67
80
62
80
93
73
82
96
73
73
87
64
65
79
54
66
80
55
69
85
58
74
98
64
74
100
63
75
101
64
75
101
62
73
99
60
72
99
58
73
100
59
74
101
60
83
110
69
88
114
75
92
118
81
93
119
84
91
116
84
84
109
79
67
92
62
54
73
53
38
49
45
39
44
48
37
42
46
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
106
99
89
107
100
90
108
99
92
109
100
91
109
101
90
109
102
86
109
104
85
107
105
84
109
106
87
106
107
91
107
107
99
119
119
119
145
146
151
167
172
178
190
197
203
209
219
221
235
240
243
246
251
254
253
254
255
254
255
255
255
254
255
255
253
254
254
253
251
255
254
250
255
255
250
253
254
248
250
251
243
245
248
239
241
247
235
241
247
235
244
247
236
245
249
235
246
248
234
245
248
229
245
247
225
238
238
212
219
219
191
201
202
171
176
177
146
147
150
121
117
121
96
107
112
90
116
123
105
136
144
129
159
167
156
174
181
173
205
207
202
222
222
220
239
239
237
248
248
246
251
251
249
254
254
252
254
254
252
253
253
251
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
253
255
254
245
250
253
251
255
255
254
254
254
255
248
246
255
250
244
255
245
235
248
204
193
215
157
143
147
75
60
164
81
65
168
75
58
167
64
49
183
71
59
207
89
77
206
84
73
189
63
51
196
68
55
198
72
58
195
75
61
185
73
59
170
66
55
157
63
51
149
65
55
144
70
59
134
66
57
129
66
57
126
66
56
123
65
54
122
64
53
122
62
52
122
62
52
122
62
52
120
60
52
120
60
52
119
59
51
119
59
51
117
58
50
116
57
49
116
57
49
116
57
49
114
57
48
114
57
48
114
57
48
114
57
48
111
57
47
110
56
46
108
54
44
107
53
43
108
55
47
106
56
49
108
55
49
106
56
49
107
54
48
105
55
48
105
55
48
104
54
47
103
53
46
103
53
46
102
53
46
103
54
47
104
55
48
105
56
49
107
58
51
107
58
51
108
59
52
106
59
51
105
58
48
104
60
49
107
63
50
106
67
52
105
68
50
102
69
50
102
73
55
108
86
65
106
89
71
117
106
86
134
127
109
145
142
123
128
127
109
69
70
54
53
56
45
50
56
46
50
56
46
50
56
46
49
55
45
49
55
45
49
55
45
48
54
44
49
55
45
49
55
45
49
55
45
48
54
44
48
54
44
47
53
43
47
53
43
47
53
43
47
52
46
47
52
46
47
52
46
47
52
46
47
52
46
47
52
46
47
52
46
47
52
46
47
52
46
47
52
46
46
51
45
45
50
44
45
50
44
46
51
45
47
52
46
46
53
46
43
53
45
43
54
46
45
57
47
47
59
49
50
62
50
52
64
52
54
66
52
55
68
51
56
69
51
63
76
58
69
82
62
67
81
58
59
73
50
56
70
45
62
76
51
68
84
57
74
95
64
74
98
64
76
100
66
75
99
63
73
97
61
71
96
57
70
95
56
71
96
57
82
107
68
88
112
76
93
117
83
94
118
86
90
113
84
80
103
75
61
84
56
46
65
46
38
49
45
39
44
48
37
42
46
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
106
99
89
107
100
90
108
99
92
109
100
91
109
101
90
109
102
86
110
105
86
108
106
85
108
107
87
108
109
93
112
113
105
128
130
129
158
161
166
185
190
196
195
202
208
218
228
230
244
249
252
250
255
255
251
252
254
251
252
254
253
251
252
251
250
248
255
254
252
255
255
251
255
254
249
251
252
244
245
246
238
238
241
230
233
239
227
231
237
223
237
241
226
240
244
229
241
244
227
241
244
223
242
241
220
234
234
208
216
216
188
199
199
171
178
177
149
161
161
137
145
147
125
144
147
128
156
158
145
170
173
162
182
187
180
192
197
191
220
220
218
233
233
231
246
246
244
251
251
249
253
253
251
255
255
253
255
255
253
253
253
251
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
254
251
251
253
255
254
255
255
254
255
245
240
237
246
237
230
255
249
240
255
250
236
255
238
222
232
191
173
183
127
110
148
79
63
163
80
64
185
89
75
184
78
64
180
68
56
190
69
58
193
65
54
198
70
59
197
73
63
188
71
61
173
64
57
161
60
52
152
62
54
149
66
60
139
64
59
133
66
60
128
65
58
124
65
59
121
64
57
119
64
57
117
64
56
118
63
56
118
61
52
118
61
52
118
61
52
117
60
51
115
61
51
114
60
50
114
60
50
113
59
49
111
59
48
111
59
48
111
59
48
110
58
47
107
57
46
106
56
45
106
56
45
105
55
46
109
56
50
110
55
50
109
54
49
108
55
49
108
55
49
107
54
48
105
55
48
105
55
48
108
58
51
106
57
50
104
55
48
102
55
47
102
55
47
102
55
47
104
57
49
102
58
49
106
63
54
102
62
52
101
61
49
102
62
50
104
67
51
105
69
53
102
71
51
99
71
50
99
74
54
104
86
64
105
92
73
115
107
86
133
130
111
143
142
122
112
113
95
57
61
44
52
55
44
50
56
46
49
55
45
49
55
45
49
55
45
48
54
44
48
54
44
48
54
44
49
55
45
49
55
45
49
55
45
48
54
44
48
54
44
47
53
43
47
53
43
47
53
43
46
51
45
46
51
45
46
51
45
46
51
45
46
51
45
46
51
45
46
51
45
46
51
45
46
51
45
46
51
45
45
50
44
45
50
44
45
50
44
45
50
44
46
51
45
45
52
45
42
52
44
41
52
44
42
54
44
43
55
45
46
58
46
49
61
49
52
64
50
53
65
51
54
67
50
58
71
53
60
73
55
56
69
49
50
63
43
50
64
41
56
70
47
62
78
52
68
88
60
69
93
61
75
96
65
74
98
64
75
96
63
71
95
59
72
94
58
70
94
58
83
105
69
88
112
78
96
117
86
94
117
88
90
110
83
76
99
73
57
77
52
41
58
40
38
47
44
37
42
46
35
40
44
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
106
99
89
107
100
90
108
99
92
109
100
91
109
101
90
109
102
86
108
103
84
107
105
84
110
109
89
111
112
96
116
117
109
133
135
134
164
167
172
191
196
202
205
212
218
229
239
241
251
255
255
251
255
255
250
251
253
252
253
255
255
253
254
253
252
250
255
255
253
255
255
251
254
253
248
247
248
240
239
240
232
231
234
223
224
230
218
222
229
213
231
235
218
234
239
219
237
240
219
236
240
217
238
238
214
232
232
208
218
216
191
204
202
177
194
189
167
183
181
160
177
174
157
182
180
167
193
190
181
201
200
195
210
211
206
217
217
215
232
232
230
242
242
240
250
250
248
252
252
250
253
253
251
255
255
253
255
255
253
252
252
250
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
255
254
252
255
251
254
255
247
250
250
246
245
248
247
243
252
255
248
254
255
248
254
255
244
253
252
234
255
254
236
245
220
200
196
155
137
157
98
80
151
75
59
165
76
60
180
80
64
186
74
60
189
65
55
196
68
59
195
71
63
188
69
61
176
63
59
166
58
55
157
59
56
152
63
59
143
62
59
137
63
60
130
65
61
124
65
61
119
66
60
115
66
59
114
67
59
115
66
59
113
63
54
115
62
54
115
62
54
114
61
53
114
61
53
114
61
53
112
62
53
111
61
52
110
62
52
109
61
51
108
60
50
107
59
49
106
59
49
105
58
48
105
58
48
106
58
48
110
55
50
112
54
50
112
54
50
109
54
49
109
54
49
108
55
49
105
55
48
105
55
48
108
59
52
107
58
51
104
57
49
101
57
48
101
57
48
101
57
48
101
58
49
100
60
50
102
66
54
99
65
53
99
67
52
100
68
53
100
71
53
100
73
54
98
73
51
95
73
50
94
76
54
101
86
63
106
98
77
114
109
87
132
131
111
132
135
114
91
96
76
51
58
40
50
56
44
50
56
46
49
55
45
49
55
45
49
55
45
48
54
44
48
54
44
48
54
44
49
55
45
49
55
45
49
55
45
48
54
44
48
54
44
47
53
43
47
53
43
47
53
43
46
51
45
46
51
45
46
51
45
46
51
45
46
51
45
46
51
45
46
51
45
46
51
45
45
50
44
45
50
44
44
49
43
44
49
43
44
49
43
44
49
43
45
50
44
44
51
44
44
54
46
43
53
45
42
52
43
42
52
43
43
53
44
46
56
45
48
59
45
50
61
47
50
61
45
53
64
47
54
65
48
53
65
45
49
61
41
49
61
41
53
65
45
56
70
47
61
79
53
64
84
56
70
89
61
72
92
64
74
93
63
73
94
61
75
94
62
75
96
63
84
103
71
91
112
81
99
118
90
97
117
90
89
107
83
74
93
71
54
71
52
38
52
37
36
45
42
36
41
44
34
39
42
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
106
99
89
107
100
90
108
99
92
109
100
91
109
101
90
108
102
86
106
103
84
107
106
85
108
110
89
111
114
97
115
118
109
129
133
132
159
164
168
184
191
199
213
222
227
236
246
248
251
255
255
251
255
255
250
251
255
253
254
255
255
254
255
253
252
250
255
255
251
255
255
250
252
252
244
243
245
234
235
237
226
227
231
217
221
227
213
220
227
209
228
233
211
231
236
213
233
237
214
234
236
212
235
235
211
232
230
207
223
218
196
211
206
184
211
206
186
204
198
182
203
195
184
208
201
193
215
210
206
224
219
216
234
230
229
240
239
237
243
242
240
249
249
247
253
253
251
253
253
251
253
253
251
255
255
253
254
254
252
252
252
250
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
255
253
252
255
252
253
255
246
247
253
249
248
254
255
253
249
255
253
246
255
250
237
255
243
240
254
239
235
242
226
255
255
237
255
236
217
204
166
147
163
106
87
161
90
70
170
87
69
173
76
59
182
70
58
188
70
60
189
70
62
185
70
63
178
66
62
169
61
58
161
59
57
153
59
57
149
61
60
142
62
61
133
63
61
125
64
59
120
65
60
115
66
59
112
68
59
112
68
59
112
63
56
113
63
54
113
63
54
113
63
54
113
63
54
112
62
53
111
63
53
111
63
53
111
64
54
110
63
53
109
62
52
107
60
50
104
60
49
104
60
49
104
60
49
108
60
50
111
56
51
113
54
50
112
54
50
112
54
50
109
54
49
108
55
49
106
56
49
105
56
49
104
55
48
103
56
48
101
57
48
101
58
49
100
60
50
101
63
52
102
64
53
100
66
54
94
66
52
97
71
56
99
76
58
100
77
59
99
77
56
96
75
54
93
75
51
93
77
52
93
81
57
98
88
63
109
104
82
115
115
91
128
131
110
118
123
101
70
79
58
52
61
42
51
57
45
50
56
46
50
56
46
50
56
46
49
55
45
49
55
45
49
55
45
48
54
44
49
55
45
49
55
45
49
55
45
48
54
44
48
54
44
47
53
43
47
53
43
47
53
43
45
50
44
45
50
44
45
50
44
45
50
44
45
50
44
45
50
44
45
50
44
45
50
44
45
50
44
44
49
43
43
48
42
43
48
42
43
48
42
43
48
42
44
49
43
44
51
44
46
53
46
44
54
46
42
52
43
42
52
43
42
52
43
43
53
44
45
55
44
46
57
43
45
56
42
48
59
43
52
63
47
53
64
47
52
63
46
51
63
43
51
62
45
51
64
44
55
70
47
57
75
51
64
79
56
66
84
58
71
87
60
72
91
63
77
93
66
77
96
68
85
101
74
92
111
83
100
116
90
96
114
90
88
103
82
71
88
69
52
66
49
36
50
37
36
45
42
36
41
44
34
39
42
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
106
100
86
106
100
86
106
99
89
107
100
90
108
99
92
109
100
91
108
102
90
108
102
86
108
105
86
108
107
86
107
109
88
107
110
93
108
114
104
121
127
125
149
157
160
174
183
190
209
218
223
233
242
247
251
255
255
248
253
255
248
249
253
254
255
255
255
254
255
249
248
246
255
255
251
254
253
248
249
249
241
241
243
232
233
235
222
227
231
216
223
230
214
224
232
211
228
233
210
231
237
211
232
236
211
231
233
209
232
232
208
231
229
206
224
219
199
215
208
190
218
211
195
217
209
198
222
212
203
230
221
216
238
228
227
243
233
234
250
241
244
255
249
251
250
249
247
254
254
252
255
255
253
253
253
251
253
253
251
255
255
253
255
255
253
252
252
250
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
255
253
252
255
250
249
255
251
250
255
254
253
252
255
253
245
255
252
234
251
245
234
255
248
242
255
253
245
255
250
239
245
231
255
250
234
255
250
232
242
204
185
171
119
98
143
81
58
170
93
73
175
80
62
179
76
61
179
75
62
179
75
64
177
73
64
172
69
62
162
63
58
154
59
55
150
61
57
143
62
58
136
63
57
129
64
58
122
65
58
117
67
58
113
69
58
113
69
60
112
63
56
113
63
56
113
63
56
113
63
56
113
63
56
113
63
56
112
63
56
112
63
56
113
66
58
112
65
57
110
63
55
108
61
53
105
61
52
106
62
53
106
62
53
110
61
54
111
56
51
114
55
51
112
54
50
112
54
50
110
55
50
108
55
49
106
56
49
105
56
49
100
56
47
100
57
48
99
59
49
100
62
51
100
64
52
99
65
53
99
65
53
95
67
53
90
69
52
95
78
60
103
86
66
104
87
67
99
83
60
94
79
56
91
79
53
92
82
55
93
86
60
93
89
62
110
108
85
116
118
94
124
129
107
99
109
85
50
62
40
54
66
46
51
59
46
51
57
47
51
57
47
51
57
47
50
56
46
50
56
46
50
56
46
49
55
45
49
55
45
49
55
45
49
55
45
48
54
44
48
54
44
47
53
43
47
53
43
47
53
43
45
50
44
45
50
44
45
50
44
45
50
44
45
50
44
45
50
44
45
50
44
45
50
44
44
49
43
44
49
43
43
48
42
42
47
41
42
47
41
43
48
42
44
49
43
43
50
43
45
52
45
43
53
45
42
52
44
42
52
44
43
53
44
43
53
44
44
54
43
45
55
44
44
54
43
47
58
44
50
61
47
54
65
49
55
66
50
55
66
49
53
64
48
52
63
46
50
63
45
52
67
46
58
71
51
61
76
53
65
79
56
68
84
58
74
88
63
76
92
66
84
98
73
92
107
84
99
113
90
94
109
88
85
98
80
68
82
65
50
62
48
35
47
37
38
47
44
38
44
44
36
42
42
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
106
100
86
106
100
86
106
99
89
107
100
90
108
99
92
109
100
91
108
102
90
108
102
86
111
108
89
109
108
87
106
108
87
103
107
90
102
108
98
114
123
120
142
152
154
168
177
184
201
210
215
226
235
240
247
252
255
246
251
254
249
250
254
254
255
255
255
254
255
249
248
244
255
255
250
253
253
245
248
248
238
240
242
229
233
235
221
229
233
216
227
234
216
229
237
214
230
236
210
233
237
210
234
237
210
232
232
206
232
230
207
232
227
207
225
218
200
216
208
195
216
206
196
221
210
204
234
223
219
248
237
235
255
244
246
255
245
249
255
245
251
255
249
252
254
252
253
255
255
253
255
255
253
254
254
252
254
254
252
255
255
253
255
255
253
253
253
251
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
255
254
250
255
249
247
255
249
247
251
250
248
247
251
250
245
255
254
243
255
255
240
255
255
237
253
250
240
252
248
251
255
250
255
253
241
255
242
226
255
239
219
255
225
203
216
174
150
170
114
91
164
89
68
169
84
64
168
81
62
169
82
65
173
81
66
169
77
64
161
68
60
151
61
53
149
62
55
143
63
54
137
62
56
131
64
55
125
65
55
119
67
54
116
68
56
115
69
56
113
63
56
113
63
56
115
62
56
113
63
56
113
63
56
113
63
56
113
63
56
112
63
56
115
66
59
113
66
58
111
64
56
109
62
54
108
61
53
106
62
53
109
62
54
111
62
55
111
56
51
113
55
51
111
56
51
109
56
50
109
56
50
105
56
49
104
57
49
102
58
49
102
59
50
100
62
51
100
64
52
98
66
53
96
67
53
93
65
51
92
64
50
87
66
49
84
71
52
91
83
62
102
94
71
105
94
72
98
88
63
90
83
57
90
83
55
92
87
58
90
86
59
88
87
59
107
109
85
115
119
94
117
125
102
85
97
73
38
50
28
56
68
48
51
59
46
52
58
48
52
58
48
51
57
47
51
57
47
51
57
47
50
56
46
50
56
46
49
55
45
49
55
45
49
55
45
48
54
44
48
54
44
47
53
43
47
53
43
47
53
43
45
50
44
45
50
44
45
50
44
45
50
44
45
50
44
45
50
44
45
50
44
45
50
44
44
49
43
43
48
42
43
48
42
42
47
41
42
47
41
43
48
42
43
48
42
44
49
43
43
50
43
41
51
43
42
52
44
43
53
45
43
53
44
44
54
45
45
55
46
45
55
44
44
54
43
46
56
45
50
60
49
54
65
51
57
68
54
57
68
52
55
66
52
54
65
49
48
59
43
50
63
45
53
66
48
57
70
50
60
73
53
64
78
55
70
84
61
74
88
63
82
96
73
91
104
84
97
110
90
92
105
87
81
94
77
66
78
64
47
59
47
34
44
35
42
48
44
40
46
44
38
44
44
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
107
101
87
107
101
87
107
100
90
108
101
91
109
100
93
110
101
92
109
103
91
109
103
87
113
110
91
107
109
87
107
110
89
107
114
96
106
114
103
112
121
118
135
145
147
159
170
176
187
196
201
208
217
222
234
239
243
246
251
254
252
253
255
254
255
255
255
254
255
254
253
249
255
255
250
254
254
246
250
250
240
246
248
235
244
246
232
240
244
227
234
241
223
231
239
216
240
246
218
237
242
212
235
238
209
236
236
210
232
230
209
225
220
201
221
213
200
222
212
202
224
211
205
231
217
216
241
227
227
249
234
237
253
241
243
255
246
250
255
248
255
255
252
255
255
253
254
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
255
250
255
254
250
255
255
251
255
255
253
254
255
255
252
255
255
250
255
255
249
254
255
249
253
255
254
255
255
254
253
251
254
249
243
255
248
238
255
249
233
255
248
226
255
241
217
255
232
207
182
132
105
162
100
75
147
83
58
155
87
64
165
93
71
160
83
65
154
75
58
155
76
61
142
63
50
141
63
50
135
63
51
128
60
47
121
59
46
116
60
45
115
63
49
115
66
52
114
61
53
114
61
55
116
61
56
115
62
56
117
64
58
117
64
58
116
63
57
113
63
56
115
65
58
113
64
57
113
64
57
113
64
57
112
63
56
110
63
55
111
62
55
111
62
55
107
57
50
109
56
50
108
58
51
108
59
52
108
59
52
104
60
51
103
60
51
99
61
50
99
63
51
94
62
49
100
71
57
99
71
57
84
61
45
78
57
40
77
56
39
64
49
30
87
81
59
80
78
55
75
73
48
79
75
50
93
89
62
105
101
74
101
97
68
86
85
55
88
88
60
93
96
67
111
115
90
122
128
102
100
108
85
61
73
49
45
57
35
50
62
42
52
60
47
53
59
49
52
58
48
52
58
48
52
58
48
51
57
47
51
57
47
51
57
47
51
57
47
51
57
47
51
57
47
50
56
46
49
55
45
49
55
45
49
55
45
48
54
44
49
54
48
49
54
48
48
53
47
48
53
47
47
52
46
47
52
46
47
52
46
47
52
46
44
49
43
44
49
43
44
49
43
44
49
43
44
49
43
44
49
43
43
48
42
43
48
42
46
53
46
45
52
45
45
52
45
44
51
44
45
52
45
45
52
44
46
53
45
46
53
45
43
50
42
45
52
44
48
55
47
53
61
50
57
65
54
61
69
56
63
71
60
63
74
60
55
66
50
53
64
47
51
62
45
49
61
41
50
62
42
57
69
47
66
78
56
73
85
63
79
91
71
86
98
78
93
104
87
89
100
84
76
87
73
60
70
59
47
57
46
41
51
42
42
48
44
41
47
43
40
46
44
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
107
101
87
107
101
87
107
100
90
108
101
91
109
100
93
110
101
92
109
103
91
107
104
87
111
110
90
106
108
86
107
110
89
107
114
96
106
116
105
113
124
120
137
148
150
163
174
180
190
199
204
210
219
224
235
240
244
247
252
255
253
254
255
254
255
255
255
254
252
254
253
249
255
255
250
254
254
244
251
251
239
247
249
235
244
247
230
240
245
225
234
242
221
232
237
214
234
239
209
230
235
203
228
231
202
230
230
204
227
225
204
223
217
201
222
212
202
223
212
206
230
216
215
236
222
222
245
230
233
250
238
242
255
243
247
255
246
250
255
249
253
255
251
254
255
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
253
255
252
250
255
249
250
255
249
253
255
252
253
254
255
253
252
255
253
250
255
255
250
255
255
248
255
255
252
255
255
250
253
255
248
246
255
249
242
255
252
239
255
252
234
254
248
226
255
241
215
255
234
206
217
178
149
163
117
91
142
92
67
146
92
66
152
92
68
152
87
67
150
83
64
138
70
51
137
68
52
135
68
51
131
68
51
126
67
51
122
66
49
118
65
49
115
63
49
117
64
56
115
62
56
117
62
57
117
64
58
116
63
57
111
61
54
112
62
55
115
65
58
113
63
56
112
63
56
112
63
56
110
63
55
111
62
55
109
62
54
110
61
54
109
62
54
107
60
52
108
61
53
109
62
54
107
63
54
106
63
54
103
65
54
101
65
53
98
66
53
100
71
57
91
65
50
94
71
55
94
73
56
78
61
43
70
55
36
71
56
37
62
54
33
76
76
52
71
75
50
68
71
44
68
68
42
77
77
49
92
91
63
98
97
67
96
97
66
89
89
61
100
103
74
115
119
94
115
121
95
88
96
73
58
68
44
47
59
37
55
67
47
54
62
49
54
60
50
54
60
50
54
60
50
53
59
49
53
59
49
53
59
49
53
59
49
53
59
49
53
59
49
53
59
49
52
58
48
52
58
48
51
57
47
51
57
47
51
57
47
50
55
49
50
55
49
50
55
49
49
54
48
49
54
48
49
54
48
48
53
47
48
53
47
46
51
45
46
51
45
45
50
44
45
50
44
44
49
43
44
49
43
43
48
42
43
48
42
42
49
42
42
49
42
41
48
41
41
48
41
41
48
41
42
49
42
43
50
42
44
51
43
46
53
45
45
52
44
46
53
45
48
55
47
52
59
51
57
65
54
63
70
62
66
74
63
59
70
56
58
69
53
55
66
49
52
63
46
51
63
43
54
66
46
60
72
50
64
76
54
76
88
68
83
95
75
89
100
84
87
98
82
76
86
75
61
71
60
49
59
50
43
53
44
44
51
44
43
49
45
42
48
44
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
104
101
86
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
107
101
87
107
101
87
108
99
90
109
100
91
109
100
93
110
101
92
109
103
91
107
104
87
109
108
88
105
109
86
106
111
89
107
116
97
105
117
105
113
125
121
138
152
153
165
178
184
191
202
208
212
221
226
237
242
246
247
252
255
252
253
255
253
255
254
255
254
252
254
253
248
255
255
247
253
253
243
250
251
237
247
250
233
244
247
228
239
244
222
231
239
216
229
235
209
226
231
201
222
225
194
221
221
193
222
222
198
222
219
200
221
215
203
224
213
207
228
217
215
237
225
225
242
230
234
249
237
241
254
243
247
255
246
250
255
248
251
255
249
252
255
252
252
255
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
253
255
252
246
255
250
246
255
250
250
254
253
253
253
253
255
250
254
255
248
255
255
246
255
255
244
254
255
249
255
255
247
252
255
247
247
255
250
245
255
255
246
255
255
242
250
253
236
250
248
227
254
240
214
255
246
219
255
235
208
223
191
166
168
131
105
132
90
66
131
83
60
145
93
71
148
93
72
139
82
62
127
70
50
118
63
42
117
62
42
118
66
45
121
70
51
120
71
54
113
63
52
110
60
51
113
60
52
116
66
57
115
65
56
111
63
53
113
65
55
120
72
62
110
62
52
109
62
52
109
62
52
107
63
52
108
61
51
106
62
51
108
61
51
106
62
51
103
63
53
103
65
54
103
65
54
103
67
55
101
67
55
99
67
54
96
67
53
94
68
53
98
75
59
85
66
49
90
73
55
96
83
64
82
71
51
67
59
38
69
61
40
70
65
43
65
69
44
66
72
46
65
69
42
60
64
37
63
66
37
78
78
50
95
96
65
105
106
75
97
97
69
110
113
84
118
122
97
103
109
83
74
79
57
52
60
37
52
61
40
61
70
51
56
62
50
56
62
52
56
62
52
56
62
52
55
61
51
55
61
51
55
61
51
54
60
50
56
62
52
56
62
52
55
61
51
55
61
51
55
61
51
54
60
50
54
60
50
54
60
50
53
58
52
52
57
51
52
57
51
52
57
51
51
56
50
51
56
50
51
56
50
50
55
49
48
53
47
48
53
47
47
52
46
46
51
45
45
50
44
44
49
43
43
48
42
43
48
42
41
46
40
41
46
40
41
46
40
41
46
40
41
46
40
42
47
41
43
48
42
44
49
43
49
54
48
47
52
46
44
49
43
44
49
43
47
52
46
54
59
52
61
66
60
65
72
64
64
75
61
62
75
58
60
73
55
56
69
51
52
65
45
50
63
43
51
65
42
53
67
44
68
81
61
75
88
68
81
94
77
81
94
77
72
84
72
60
72
60
50
62
52
46
56
47
46
53
46
46
53
46
45
52
45
103
100
85
103
100
85
104
101
86
104
101
86
104
101
86
104
101
86
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
107
101
87
107
101
87
108
99
90
109
100
91
109
100
93
109
102
92
109
103
91
106
105
87
107
109
88
105
109
86
106
111
89
106
115
96
104
116
104
109
124
119
136
152
152
166
179
185
191
202
208
211
220
227
235
240
246
246
251
255
252
253
255
253
255
254
255
255
253
255
254
249
254
254
246
253
253
241
250
251
237
247
250
231
244
247
226
238
243
220
229
237
213
226
232
206
219
224
192
215
218
187
214
214
188
217
215
194
221
215
199
222
215
205
229
219
217
236
224
224
247
235
239
250
239
245
255
244
250
255
248
253
255
249
252
255
250
250
255
250
250
255
252
251
255
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
253
255
252
247
255
253
247
255
253
250
254
253
253
253
253
255
251
253
255
248
254
255
246
253
255
244
250
255
248
251
255
247
250
255
248
248
255
252
249
255
255
248
255
255
248
250
255
244
250
252
238
255
255
237
255
248
228
253
238
219
255
239
219
255
239
217
238
210
188
176
144
121
118
82
60
117
76
54
117
75
53
119
74
53
123
78
57
125
80
59
120
75
54
107
64
45
98
55
38
123
77
64
115
68
58
113
66
56
115
68
58
111
67
56
103
59
48
100
56
45
103
61
49
105
63
51
105
63
51
105
63
51
103
63
51
102
62
50
102
62
50
102
62
50
101
63
50
98
64
52
98
66
53
96
67
53
95
67
53
95
67
53
91
68
52
90
69
52
87
68
51
88
73
54
79
66
47
92
84
63
111
105
83
99
94
72
77
75
52
73
71
48
73
75
51
62
68
42
63
72
45
64
70
42
60
66
38
60
65
35
70
73
44
88
89
58
100
101
70
109
109
81
120
120
92
116
118
94
91
95
70
64
67
46
54
59
37
56
64
43
60
67
49
57
63
51
57
63
53
57
63
53
56
62
52
56
62
52
55
61
51
55
61
51
55
61
51
57
63
53
56
62
52
56
62
52
56
62
52
56
62
52
56
62
52
56
62
52
56
62
52
55
60
54
55
60
54
55
60
54
54
59
53
54
59
53
54
59
53
53
58
52
53
58
52
51
56
50
51
56
50
49
54
48
48
53
47
47
52
46
45
50
44
44
49
43
44
49
43
43
48
42
43
48
42
42
47
41
42
47
41
43
48
42
44
49
43
45
50
44
45
50
44
48
53
47
46
51
45
43
48
42
43
48
42
45
50
46
50
55
49
56
61
57
59
66
58
64
75
61
64
77
59
64
77
59
61
74
54
56
69
49
51
64
44
49
63
40
48
62
39
60
73
53
66
79
59
72
85
67
73
86
69
68
80
66
59
71
59
50
62
52
46
58
48
45
55
47
47
54
47
46
53
46
103
100
85
103
100
85
103
100
85
104
101
86
104
101
86
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
107
101
87
108
100
87
108
99
90
109
100
91
109
100
93
109
102
92
109
103
91
106
105
87
107
109
88
105
110
87
105
113
90
103
115
95
100
114
101
106
121
116
133
149
149
163
178
183
189
200
206
210
219
226
234
239
245
244
249
253
251
252
255
253
255
254
255
255
253
255
255
250
255
255
248
255
255
243
254
255
241
251
254
235
247
250
229
240
245
222
230
238
214
226
232
204
217
220
191
213
213
185
210
208
185
212
209
190
218
212
200
224
217
209
234
224
223
244
233
237
253
242
248
255
246
251
255
250
255
255
252
255
255
253
254
254
253
251
254
253
249
254
253
249
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
253
255
254
250
255
255
250
255
255
252
255
255
255
255
255
255
253
254
255
251
251
255
249
250
255
249
248
255
248
246
255
249
247
255
250
247
255
254
249
255
255
251
255
255
251
253
255
250
252
253
248
244
241
234
255
251
243
255
253
244
255
251
241
255
248
235
255
244
228
246
227
210
228
207
188
208
183
163
183
156
135
146
118
97
118
87
67
104
73
53
104
73
53
109
76
57
112
79
60
108
71
55
105
66
51
103
66
50
106
69
53
110
73
57
110
74
58
111
75
59
111
75
59
101
65
49
99
66
49
99
66
49
99
66
49
99
66
49
99
66
49
98
67
49
96
67
51
93
70
54
92
71
54
92
71
54
90
71
54
90
71
54
87
72
53
84
71
52
82
71
51
80
72
51
78
72
50
102
100
77
126
126
102
114
114
90
86
88
64
74
76
52
71
77
51
60
69
42
60
70
43
63
72
43
65
71
43
65
70
40
68
71
42
77
78
47
84
85
54
120
119
91
123
122
94
110
110
86
85
85
61
67
66
46
63
65
44
62
65
46
58
62
45
59
62
51
57
63
53
57
63
53
56
62
52
56
62
52
55
61
51
55
61
51
55
61
51
56
62
52
56
62
52
56
62
52
56
62
52
56
62
52
56
62
52
56
62
52
56
62
52
57
62
56
57
62
56
57
62
56
56
61
55
56
61
55
56
61
55
55
60
54
55
60
54
54
59
53
53
58
52
52
57
51
51
56
50
50
55
49
48
53
47
47
52
46
47
52
46
45
50
46
45
50
46
44
49
45
43
48
44
43
48
44
44
49
45
45
50
46
45
50
46
44
49
45
44
49
45
44
49
45
44
49
45
46
50
49
48
53
49
51
55
54
52
59
52
59
71
57
61
76
57
64
79
60
64
79
58
60
75
54
55
70
47
52
67
44
50
65
42
54
69
46
58
73
52
64
79
60
66
80
63
63
77
62
56
70
55
49
63
50
45
57
45
45
55
46
46
53
45
45
52
44
102
99
84
102
99
84
103
100
85
104
101
86
104
101
86
105
102
87
106
103
88
106
103
88
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
108
100
87
108
100
87
108
99
90
109
100
91
109
100
93
109
102
92
109
103
91
106
105
87
107
109
88
105
110
87
105
113
90
103
115
95
98
112
99
103
120
114
130
148
148
163
178
183
192
203
209
212
221
228
235
240
246
245
250
254
251
252
255
253
255
254
255
255
253
255
255
250
255
255
248
255
255
244
255
255
243
255
255
239
250
253
232
242
247
224
231
240
213
226
232
204
215
218
189
209
209
181
205
203
180
206
203
186
214
207
197
222
215
209
236
226
227
247
238
243
255
245
253
255
249
255
255
252
255
255
254
255
255
254
252
253
254
249
253
254
248
253
255
249
254
255
250
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
254
253
254
255
254
255
255
254
255
255
254
255
255
254
255
253
254
255
250
253
254
248
253
253
245
251
251
241
252
252
244
254
253
248
255
254
250
255
255
253
254
254
254
254
252
255
254
252
255
251
246
252
255
250
254
254
248
250
249
244
241
253
246
240
255
254
244
255
254
241
255
254
237
255
248
230
252
239
220
242
227
208
226
209
189
205
187
167
179
161
141
155
134
115
140
117
99
122
93
75
125
94
76
116
85
67
97
68
50
87
58
40
89
60
42
91
62
44
88
61
42
96
69
50
96
69
50
96
69
50
95
70
50
96
71
51
96
71
51
96
71
51
93
72
51
92
75
55
92
77
58
92
77
58
89
76
57
88
77
57
85
77
56
82
76
54
81
76
54
78
76
53
81
81
57
105
107
83
124
128
103
109
113
88
83
89
63
70
76
50
62
71
44
60
70
43
60
70
43
63
72
43
68
74
46
69
74
44
68
71
42
72
73
42
76
77
46
121
120
92
116
115
87
102
100
77
84
82
59
76
73
54
75
74
54
70
69
51
60
61
45
60
63
52
57
63
53
57
63
53
57
63
53
56
62
52
56
62
52
56
62
52
56
62
52
55
61
51
55
61
51
56
62
52
56
62
52
56
62
52
56
62
52
57
63
53
57
63
53
58
63
57
58
63
57
58
63
57
58
63
57
57
62
56
57
62
56
57
62
56
56
61
55
56
61
55
56
61
55
55
60
54
54
59
53
53
58
52
52
57
51
51
56
50
51
56
50
47
52
48
46
51
47
45
50
46
44
49
45
43
48
44
43
48
44
43
48
44
43
48
44
43
48
44
44
49
45
45
49
48
46
50
49
47
51
50
47
51
50
47
51
50
45
52
45
51
63
49
55
70
51
61
76
55
64
79
58
63
78
55
60
75
52
57
72
49
56
71
48
53
68
45
56
71
48
60
75
54
62
77
58
60
74
57
55
69
54
49
63
50
45
57
45
44
54
45
46
53
45
45
52
44
102
99
84
102
99
84
103
100
85
104
101
86
104
101
86
105
102
87
106
103
88
106
103
88
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
108
100
87
108
100
87
108
99
90
109
100
91
109
100
93
109
102
92
109
103
91
106
105
87
106
108
87
104
109
86
104
114
90
102
115
95
99
113
100
104
121
115
133
151
151
165
183
187
199
210
216
218
227
234
240
245
251
248
253
255
252
253
255
253
255
254
255
255
253
255
254
249
255
255
245
255
255
242
255
255
240
252
255
236
248
251
230
239
244
221
226
235
208
221
227
199
213
213
185
206
204
179
201
196
176
202
196
180
208
201
191
218
210
207
233
224
227
246
237
242
255
246
254
255
249
255
255
252
255
255
254
255
254
254
252
252
254
249
251
255
247
252
255
248
253
255
250
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
254
254
251
255
255
252
255
254
253
255
253
255
254
250
255
250
247
255
248
246
255
243
245
255
240
243
254
238
245
255
242
250
255
246
253
255
250
255
253
254
255
251
255
255
249
255
255
248
255
252
241
255
254
245
255
255
250
255
255
253
255
255
252
255
252
251
249
248
248
240
245
245
233
255
255
241
255
255
239
255
255
237
255
255
237
255
252
234
255
246
229
251
239
223
248
235
218
249
228
209
255
230
208
227
202
180
165
143
120
116
94
71
101
79
56
98
76
53
92
72
48
94
74
50
92
74
50
92
74
50
93
75
51
93
75
51
94
76
52
94
76
52
92
77
54
89
78
56
87
79
58
87
79
58
86
80
58
85
79
57
83
78
56
80
78
55
78
78
54
77
79
55
80
84
59
96
102
76
102
111
84
85
94
67
69
79
52
65
75
48
60
70
43
63
73
46
63
73
46
67
76
47
72
78
50
72
77
47
70
73
44
79
80
49
89
88
58
113
109
82
106
102
75
97
92
70
91
86
64
89
84
65
84
81
62
77
74
57
66
67
51
61
64
53
59
65
55
59
65
55
59
65
55
58
64
54
58
64
54
58
64
54
57
63
53
56
62
52
56
62
52
56
62
52
57
63
53
57
63
53
58
64
54
58
64
54
58
64
54
59
64
58
59
64
58
59
64
58
58
63
57
58
63
57
57
62
56
57
62
56
57
62
56
58
63
57
58
63
57
57
62
56
57
62
56
56
61
55
56
61
55
55
60
54
55
60
54
52
57
53
50
55
51
49
54
50
47
52
48
45
50
46
44
49
45
43
47
46
43
47
46
44
48
47
44
48
47
45
49
48
46
50
49
46
50
51
46
50
51
45
49
50
43
49
45
43
57
42
47
64
45
54
72
50
59
77
55
61
79
55
60
78
52
58
76
50
58
76
50
53
71
47
55
73
49
58
76
54
60
77
58
58
75
57
54
71
55
48
64
51
45
59
46
45
55
46
46
53
45
45
52
44
99
100
82
99
100
82
100
101
83
101
102
84
101
102
86
104
102
87
105
103
88
105
103
88
104
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
105
102
87
107
101
87
107
101
87
107
101
87
108
100
89
108
101
91
109
100
93
109
102
92
107
103
91
106
105
87
105
107
86
104
109
87
104
114
90
103
116
96
100
113
103
106
123
117
137
155
157
170
187
194
205
216
222
223
232
239
242
249
255
250
255
255
251
255
255
253
255
254
254
255
250
254
254
246
250
250
238
250
251
237
250
251
233
248
250
229
244
246
225
234
238
215
223
227
202
217
219
195
211
209
184
204
199
177
197
190
172
198
190
177
204
195
188
214
206
204
230
221
224
244
234
242
253
246
254
254
249
255
255
253
255
254
254
255
253
253
251
252
254
249
251
255
249
252
255
250
253
255
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
254
254
252
255
254
252
255
254
252
255
254
252
255
253
254
255
250
255
255
251
255
254
252
255
252
254
253
249
255
251
246
255
247
244
255
244
243
255
241
242
254
240
245
255
242
250
255
246
254
255
250
255
252
251
255
249
252
255
247
255
255
246
255
255
248
255
255
247
255
255
249
255
255
252
255
253
252
255
251
253
252
250
255
251
252
255
250
244
250
240
244
250
238
247
249
236
247
248
234
248
246
231
251
248
231
255
250
234
255
253
234
253
241
219
255
251
228
236
220
197
158
142
119
96
80
55
82
66
41
88
72
47
88
72
47
91
77
51
91
77
51
90
78
52
90
78
52
91
79
53
91
79
53
92
80
54
91
81
56
85
79
55
84
79
57
84
79
57
81
79
56
80
78
55
78
78
54
75
77
53
73
77
52
74
78
53
76
82
56
83
92
65
80
89
60
64
73
44
59
70
40
65
76
46
63
74
42
67
78
46
68
79
47
72
81
50
76
83
50
73
78
48
74
77
46
89
90
59
107
106
78
102
101
73
98
94
69
94
89
67
95
90
70
96
91
71
90
85
66
83
78
59
76
73
56
65
66
52
64
66
53
64
66
53
62
65
54
62
65
54
62
65
54
61
64
55
61
64
55
57
63
53
57
63
53
57
63
53
58
64
54
58
64
54
58
66
55
58
66
55
58
66
55
59
64
57
59
64
57
59
64
58
58
63
56
58
63
57
57
62
55
57
62
56
57
62
55
59
64
58
59
64
57
59
64
58
59
64
57
58
63
57
58
63
56
58
63
57
58
63
57
56
63
56
55
61
57
52
59
52
51
56
50
49
54
48
47
52
46
46
51
47
46
51
47
46
50
49
46
50
49
44
50
46
44
50
46
43
50
43
43
50
42
44
52
41
41
54
37
36
54
30
41
61
33
50
70
43
56
76
49
59
79
54
59
77
53
59
77
53
59
77
53
56
74
50
57
75
51
59
77
53
60
78
54
60
78
56
55
72
53
49
66
48
46
60
45
46
56
45
47
54
46
46
53
45
92
102
78
93
103
79
95
103
80
96
104
81
98
106
85
101
106
86
103
106
87
104
107
88
102
103
85
104
103
85
104
101
86
104
101
86
106
100
86
107
101
87
107
101
89
108
102
88
107
101
87
106
103
88
109
103
91
108
104
93
109
102
94
106
102
91
104
100
88
102
100
85
102
103
85
103
108
86
102
111
90
98
111
93
99
112
103
114
130
127
150
168
172
185
199
208
209
219
228
224
233
240
240
247
253
247
252
255
247
253
251
250
255
251
252
255
248
252
254
243
248
249
235
250
251
235
249
248
230
243
240
221
239
236
217
236
233
214
224
222
201
214
209
189
216
208
189
203
191
175
188
179
164
188
178
169
195
184
180
205
195
196
219
210
215
233
223
231
250
243
251
253
248
255
254
251
255
255
254
255
254
254
255
252
254
253
251
253
250
252
254
251
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
255
254
252
255
254
252
255
255
253
255
255
253
255
255
251
255
254
250
255
253
250
255
252
249
255
252
249
255
252
249
255
254
255
255
253
254
253
253
253
253
253
253
254
255
255
254
255
253
251
255
252
249
254
248
253
255
250
251
252
246
253
252
247
255
255
250
255
254
248
255
249
244
255
247
243
255
250
249
255
251
255
255
252
255
254
253
255
254
255
255
254
255
255
251
255
252
251
255
252
250
254
253
252
255
255
252
255
255
255
255
253
255
254
249
255
254
242
255
254
237
255
254
234
255
254
232
251
245
223
255
252
230
233
225
202
145
137
114
86
79
53
88
78
51
85
76
47
83
74
45
87
78
49
87
78
49
85
78
49
85
78
49
84
79
50
84
79
50
84
78
52
83
79
52
81
77
50
82
78
53
80
78
53
80
80
54
77
79
55
75
79
54
72
77
54
71
76
53
74
80
54
70
79
52
65
74
45
61
70
39
62
72
38
68
75
42
70
77
43
68
78
41
77
88
48
75
87
47
70
81
41
67
77
40
79
86
52
94
99
67
96
99
68
90
90
62
89
89
63
91
89
66
95
90
68
95
90
70
95
88
69
92
85
66
88
81
62
85
78
59
75
70
51
71
68
49
68
65
48
65
63
48
64
62
49
62
62
50
63
63
53
63
65
54
60
63
54
58
64
54
58
64
54
57
65
54
57
65
52
55
66
52
55
66
52
55
66
52
58
66
55
57
65
54
57
64
56
56
64
53
56
63
55
57
65
54
57
64
56
58
66
55
58
65
57
58
66
55
57
64
56
58
66
55
59
66
58
60
68
57
61
68
60
62
69
61
56
66
58
55
65
57
55
65
56
55
63
52
53
61
48
51
59
46
50
56
46
49
54
48
46
52
48
47
53
49
45
55
47
45
57
45
46
59
41
48
64
38
54
71
39
55
77
39
59
86
43
59
88
44
60
86
49
59
84
52
59
82
56
60
79
57
61
78
59
61
79
57
58
76
54
63
81
55
68
86
60
68
87
59
67
87
60
64
84
59
55
74
52
46
63
44
45
55
44
44
51
43
41
48
40
89
103
77
90
104
78
92
104
80
93
105
81
97
107
83
98
108
84
103
108
88
103
108
88
103
104
86
103
104
86
105
102
87
105
102
87
107
101
87
108
102
88
109
101
90
109
103
89
108
102
88
106
103
88
107
103
91
108
104
93
107
103
94
106
102
91
104
102
90
101
102
86
104
105
87
103
108
86
103
112
93
102
115
98
106
119
110
121
137
136
156
171
178
186
200
209
208
218
227
224
233
238
240
247
253
246
254
255
249
255
253
249
255
249
251
254
245
249
253
239
249
250
236
248
249
231
246
243
226
238
235
216
235
230
211
230
225
206
221
214
196
210
201
184
203
191
175
193
179
166
184
172
160
187
174
166
194
183
181
205
195
196
219
208
214
228
221
228
251
243
254
253
247
255
254
251
255
255
254
255
254
253
255
252
253
255
252
254
253
252
254
253
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
255
254
252
255
254
252
255
255
253
255
255
253
255
253
250
255
253
250
255
253
250
255
252
249
255
252
248
255
253
248
255
254
253
255
254
252
253
253
253
253
253
253
255
255
255
255
255
255
254
254
254
253
253
253
255
255
253
255
255
251
255
252
249
254
247
241
245
236
229
238
228
219
244
231
222
250
241
234
255
252
253
254
254
255
253
255
254
254
255
253
252
255
251
251
255
250
251
255
254
250
254
255
252
255
255
252
255
255
255
254
255
254
254
252
255
253
244
255
254
236
255
253
231
255
254
229
248
246
223
254
252
231
229
224
202
141
137
112
83
77
51
84
79
50
82
75
46
81
74
45
85
79
47
85
79
47
84
79
49
84
79
49
83
79
50
83
79
50
81
80
52
81
80
52
81
77
50
79
78
50
80
78
53
78
81
54
77
79
55
74
80
54
72
77
54
69
77
53
71
80
53
69
78
49
65
74
45
63
73
39
66
73
40
70
77
43
74
80
44
74
82
43
75
86
43
75
88
42
82
93
51
89
100
60
93
101
64
87
94
60
83
88
58
83
86
57
85
85
61
86
86
62
90
85
65
91
86
66
92
85
66
92
84
65
90
82
61
89
81
60
82
76
54
80
74
52
75
70
51
72
67
48
67
64
49
65
63
50
66
64
52
63
65
54
61
62
54
58
64
54
58
64
54
57
65
54
55
66
52
55
66
52
54
67
50
54
67
50
58
66
55
57
65
54
57
65
54
56
64
53
56
64
53
57
65
54
57
65
54
58
66
55
58
66
55
58
66
55
58
66
55
58
66
55
59
67
56
60
68
57
61
69
58
60
70
59
61
71
63
60
72
62
60
70
59
60
71
55
60
68
53
58
66
51
57
63
51
56
61
54
55
62
55
52
62
54
51
61
52
48
62
45
51
67
41
57
77
42
66
89
47
71
97
49
79
112
59
76
110
59
76
105
61
70
97
62
66
89
63
60
82
61
59
76
58
58
75
56
61
79
57
68
86
60
74
93
65
76
95
65
75
95
67
72
92
64
60
83
57
51
70
48
43
53
42
43
50
42
40
47
39
89
103
77
90
104
78
92
104
80
94
106
82
97
107
83
99
109
85
103
108
88
104
109
89
105
106
88
105
106
88
107
104
89
107
104
89
108
102
88
109
103
89
110
102
91
110
104
90
108
102
88
107
104
89
107
103
91
107
103
92
107
103
94
106
102
91
104
102
90
101
102
86
104
105
87
101
106
84
100
109
90
103
116
99
111
124
115
127
143
142
157
172
179
182
196
205
204
214
223
221
230
235
239
246
252
246
254
255
249
255
253
248
255
248
248
251
242
245
249
235
244
245
231
241
242
224
236
233
216
226
223
204
222
217
198
216
211
192
208
201
183
199
190
173
192
180
164
188
174
161
187
175
163
194
181
173
204
193
191
215
205
206
227
216
222
234
227
234
251
243
254
253
247
255
254
251
255
254
253
255
253
252
255
252
253
255
252
254
253
253
255
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
255
254
252
255
254
252
255
255
253
255
254
252
255
253
250
255
253
250
255
252
249
255
252
249
255
252
248
255
253
248
255
254
253
254
253
251
252
252
252
253
253
253
254
254
254
255
255
255
254
254
254
253
253
253
255
255
253
255
255
251
255
253
250
249
242
236
232
223
216
223
213
204
234
221
212
246
237
230
255
252
253
254
254
255
253
255
254
254
255
253
252
255
251
251
255
250
251
255
254
250
254
255
252
255
255
252
255
255
255
254
255
254
254
252
255
253
244
255
254
236
255
253
231
255
254
229
249
247
224
254
252
231
228
223
201
138
134
109
80
74
48
82
77
48
82
75
46
82
75
46
84
78
46
85
79
47
84
79
49
85
80
50
84
80
51
83
79
50
81
80
52
80
79
51
81
77
50
79
78
50
80
78
53
77
80
53
77
79
55
74
80
54
73
78
55
70
78
54
72
81
54
69
78
49
67
76
47
66
76
42
70
77
44
73
80
46
79
85
49
81
89
50
80
91
48
85
98
52
103
116
73
119
130
90
112
120
83
88
95
61
76
81
51
78
81
52
80
80
56
81
81
57
84
79
59
85
80
60
87
80
61
88
81
62
91
83
62
92
84
63
89
83
61
86
80
58
81
76
57
77
72
53
71
68
53
68
66
53
67
65
53
63
65
54
61
62
54
58
64
54
58
64
54
57
65
54
55
66
52
55
66
52
54
67
50
54
67
50
58
66
55
58
66
55
57
65
54
57
65
54
57
65
54
57
65
54
58
66
55
58
66
55
58
66
55
58
66
55
58
66
55
58
66
55
59
67
56
60
68
57
62
70
59
60
70
59
64
74
66
63
75
65
64
74
63
64
75
59
65
73
58
63
71
56
63
69
57
62
67
60
62
69
62
57
67
59
54
64
55
52
66
49
57
73
47
66
86
51
80
103
61
88
114
66
94
127
74
91
125
74
87
116
72
77
104
69
67
90
64
57
79
58
54
71
53
52
69
50
60
78
56
69
87
61
77
96
68
82
101
71
82
102
74
78
98
70
65
88
62
56
75
53
47
57
46
47
54
46
44
51
43
89
103
77
90
104
78
92
104
80
94
106
82
98
108
84
100
110
86
104
109
89
105
110
90
107
108
90
107
108
90
109
106
91
108
105
90
110
104
90
110
104
90
111
103
92
111
105
91
109
103
89
107
104
89
107
103
91
107
103
92
106
102
93
106
102
91
104
102
90
102
103
87
102
103
85
97
102
80
95
104
85
100
113
96
112
125
116
127
143
142
151
166
173
171
185
194
199
209
218
217
226
231
237
244
250
245
253
255
248
254
252
245
252
245
243
246
237
238
242
228
234
235
221
228
229
211
221
218
201
212
209
190
208
203
184
202
197
178
195
188
170
189
180
163
196
184
168
198
184
171
203
191
179
213
200
192
224
213
211
234
224
225
244
233
239
248
241
248
252
244
255
253
247
255
254
251
255
254
253
255
253
252
255
251
252
254
252
254
253
253
255
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
255
254
252
255
254
252
255
254
252
255
254
252
255
253
250
255
252
249
255
252
249
255
252
249
255
251
247
255
252
247
255
253
252
253
252
250
251
251
251
252
252
252
254
254
254
255
255
255
254
254
254
253
253
253
254
253
251
255
255
251
255
253
250
250
243
237
234
225
218
227
217
208
239
226
217
251
242
235
255
252
253
254
254
255
253
255
254
254
255
253
252
255
251
251
255
250
251
255
254
250
254
255
252
255
255
252
255
255
255
254
255
254
254
252
255
253
244
255
254
236
255
252
230
255
253
228
252
250
227
255
254
233
228
223
201
137
133
108
80
74
48
82
77
48
84
77
48
84
77
48
84
78
46
85
79
47
85
80
50
85
80
50
84
80
51
83
79
50
80
79
51
79
78
50
81
77
50
79
78
50
79
77
52
76
79
52
76
78
54
74
80
54
74
79
56
71
79
55
74
83
56
71
80
51
70
79
50
72
82
48
76
83
50
78
85
51
85
91
55
91
99
60
103
114
71
115
128
82
135
148
105
144
156
116
133
143
106
110
117
83
90
95
65
82
87
57
79
79
55
78
78
54
80
75
55
79
74
54
82
75
56
85
78
59
89
81
60
91
83
62
90
84
62
88
82
60
84
79
60
80
75
56
74
71
56
71
69
56
69
67
55
65
67
56
61
62
54
58
64
54
58
64
54
57
65
54
55
66
52
55
66
52
54
67
50
54
67
50
59
67
56
58
66
55
57
65
54
57
65
54
57
65
54
57
65
54
58
66
55
59
67
56
59
67
56
58
66
55
58
66
55
59
67
56
59
67
56
61
69
58
62
70
59
61
71
60
61
71
63
60
72
62
62
72
61
62
73
57
64
72
57
63
71
56
63
69
57
63
68
61
61
68
61
57
67
59
54
64
55
52
66
49
57
73
47
68
88
53
82
105
63
91
117
69
89
122
69
87
121
70
84
113
69
73
100
65
62
85
59
51
73
52
48
65
47
46
63
44
54
72
50
64
82
56
75
94
66
82
101
71
83
103
75
79
99
71
65
88
62
54
73
51
49
59
48
48
55
47
45
52
44
89
103
77
90
104
78
92
104
80
94
106
82
98
108
84
100
110
86
105
110
90
106
111
91
109
110
92
108
109
91
110
107
92
109
106
91
110
104
90
110
104
90
112
104
93
111
105
91
109
103
89
107
104
89
106
102
90
106
102
91
105
101
92
105
101
90
104
102
90
102
103
87
103
104
86
97
102
80
94
103
84
100
113
96
112
125
116
125
141
140
145
160
167
161
175
184
194
204
213
214
223
228
234
241
247
243
251
253
245
251
249
242
249
242
239
242
233
233
237
223
227
228
214
218
219
201
211
208
191
204
201
182
201
196
177
195
190
171
190
183
165
187
178
161
202
190
174
209
195
182
218
206
194
229
216
208
238
227
225
248
238
239
255
245
251
255
249
255
252
244
255
253
247
255
253
250
255
253
252
255
252
251
255
251
252
254
252
254
253
254
255
255
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
254
255
254
252
255
254
252
255
254
252
255
254
252
255
252
249
255
252
249
255
252
249
255
251
248
255
251
247
255
252
247
255
252
251
252
251
249
251
251
251
252
252
252
254
254
254
255
255
255
254
254
254
253
253
253
254
253
251
254
253
249
255
252
249
254
247
241
245
236
229
239
229
220
247
234
225
254
245
238
255
252
253
254
254
255
253
255
254
254
255
253
252
255
251
251
255
250
251
255
254
250
254
255
251
255
255
251
255
255
254
253
255
254
254
252
255
254
245
255
254
236
255
252
230
255
252
227
254
252
229
255
255
235
229
224
202
138
134
109
81
75
49
84
79
50
86
79
50
87
80
51
84
78
46
85
79
47
86
81
51
86
81
51
85
81
52
83
79
50
79
78
50
78
77
49
81
77
50
79
78
50
79
77
52
76
79
52
76
78
54
74
80
54
74
79
56
73
81
57
76
85
58
73
82
53
74
83
54
78
88
54
81
88
55
82
89
55
91
97
61
99
107
68
129
142
98
146
162
115
161
174
131
157
169
129
148
158
121
138
145
111
115
120
90
90
95
65
80
80
56
78
78
54
78
73
53
76
71
51
78
71
52
81
74
55
86
78
57
88
80
59
88
82
60
86
80
58
83
78
59
80
75
56
76
73
58
73
71
58
71
69
57
67
69
58
61
62
54
58
64
54
58
64
54
57
65
54
55
66
52
55
66
52
54
67
50
54
67
50
59
67
56
59
67
56
58
66
55
57
65
54
57
65
54
58
66
55
59
67
56
59
67
56
59
67
56
59
67
56
59
67
56
59
67
56
60
68
57
61
69
58
62
70
59
61
71
60
59
69
61
58
70
60
60
70
59
60
71
55
62
70
55
61
69
54
62
68
56
61
66
59
58
65
58
55
65
57
53
63
54
52
66
49
57
73
47
66
86
51
78
101
59
85
111
63
83
116
63
81
115
64
79
108
64
69
96
61
57
80
54
46
68
47
44
61
43
44
61
42
49
67
45
60
78
52
73
92
64
80
99
69
82
102
74
78
98
70
63
86
60
52
71
49
43
53
42
43
50
42
40
47
39
```

### `HDF5Examples/C/HL/tfiles/image8.txt`

```
components
1
height
400
width
300
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
12
8
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
61
52
39
25
10
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
93
86
77
65
48
16
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
108
106
104
99
88
48
23
8
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
109
107
106
106
105
98
60
26
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
107
105
104
104
105
105
100
73
37
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
108
105
104
104
105
105
104
93
69
29
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
109
105
103
103
104
106
106
103
89
59
12
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
111
105
103
103
104
106
107
106
100
86
29
10
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
113
105
102
101
102
105
107
108
106
100
55
20
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
115
107
102
101
100
101
104
107
108
107
88
42
15
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
118
109
103
100
99
100
102
105
108
108
99
63
30
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
119
111
104
100
97
100
101
103
106
109
105
82
48
17
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
3
3
4
5
6
7
7
8
9
9
9
9
9
8
7
7
6
4
3
2
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
119
113
106
100
97
100
101
102
104
108
108
95
65
30
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
3
4
6
7
9
11
13
16
18
20
22
24
26
27
27
28
28
27
25
24
22
20
16
13
11
8
6
3
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
119
115
109
102
97
100
103
104
104
106
108
103
79
46
15
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
4
6
9
12
17
21
25
30
34
40
44
48
52
55
58
60
61
62
62
60
58
56
52
48
41
35
29
23
18
11
7
4
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
115
114
111
106
99
97
104
106
106
106
107
106
91
65
30
7
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
3
6
9
13
20
26
33
41
49
59
66
74
80
87
94
98
102
106
109
112
113
114
115
115
115
114
112
109
105
98
91
82
72
61
46
36
26
18
12
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
109
111
110
107
103
97
104
107
108
107
107
107
97
77
44
16
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
4
6
11
17
24
31
40
52
62
71
79
87
95
100
104
107
110
113
115
117
119
120
121
122
123
124
124
125
125
124
124
123
120
118
113
108
101
87
75
62
49
36
21
13
7
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
102
104
106
107
106
98
101
106
109
108
108
107
101
85
56
27
14
6
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
6
10
15
22
33
43
53
64
74
85
93
99
104
109
113
115
117
119
120
121
122
123
124
124
125
126
126
127
128
128
129
129
129
129
129
129
127
125
122
115
106
96
84
70
50
37
25
16
9
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
95
97
101
104
106
100
99
104
109
110
108
108
104
92
67
38
23
12
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
3
6
12
18
27
36
48
63
75
85
95
103
110
113
116
117
118
119
120
120
121
121
122
122
122
123
123
124
124
125
126
126
127
128
129
129
130
130
130
131
131
130
129
127
122
114
104
85
69
53
38
25
11
6
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
90
92
95
99
105
105
98
102
108
111
110
109
106
98
79
48
33
19
9
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
5
10
17
29
41
55
69
83
99
107
112
115
116
117
118
118
119
119
120
120
120
120
120
119
119
119
119
119
119
119
120
121
122
123
124
126
127
128
129
130
131
131
131
131
131
131
129
128
120
108
91
73
53
31
18
10
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
89
90
93
96
99
105
100
98
101
110
111
110
108
101
85
68
54
37
21
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
9
16
26
38
52
74
88
100
109
114
115
116
117
117
118
118
117
117
115
113
111
110
109
108
108
108
109
109
110
110
111
112
113
114
115
116
117
118
119
121
123
124
126
128
129
131
131
131
131
132
131
130
127
120
108
83
62
43
26
14
4
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
88
89
91
95
98
104
103
99
98
106
110
110
108
102
90
71
62
50
36
18
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
4
8
15
29
42
58
73
88
102
108
111
114
116
116
116
116
115
113
110
108
107
106
105
105
105
106
106
107
107
108
109
109
110
111
112
113
114
115
116
117
118
119
120
121
123
124
125
126
128
129
130
131
132
132
132
131
129
126
115
101
83
63
44
21
11
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
87
88
90
93
99
103
104
101
97
99
107
109
108
103
92
74
65
57
47
31
9
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
8
15
25
38
59
74
88
99
107
114
115
115
115
115
113
111
109
107
105
103
102
102
102
102
103
104
105
106
108
109
111
112
113
114
116
117
118
118
119
119
120
120
120
121
122
122
123
124
125
126
128
129
130
131
132
132
133
133
133
130
124
113
99
81
53
34
20
10
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
87
87
88
92
100
103
104
103
99
93
101
104
105
101
93
75
66
60
54
44
18
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
4
11
22
35
51
68
90
102
110
114
115
115
115
113
111
110
106
103
100
99
98
99
100
101
102
104
106
108
110
113
115
118
120
123
124
126
128
129
130
130
130
130
129
129
128
127
126
126
126
126
126
126
127
128
129
130
131
132
133
133
134
134
133
131
125
115
90
68
46
27
13
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
87
87
88
92
102
105
105
104
102
95
90
97
99
97
93
76
66
61
57
54
32
13
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
6
12
28
46
66
86
104
113
114
115
115
115
114
112
107
102
98
95
95
96
96
98
99
101
103
106
109
114
119
124
129
133
138
142
145
148
150
152
153
153
153
152
150
149
147
144
142
138
136
133
131
129
128
128
128
129
129
130
131
132
133
134
134
135
135
134
133
125
108
84
58
33
11
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
90
90
91
95
104
108
107
106
104
102
91
86
86
89
89
83
71
63
60
58
48
25
10
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
5
13
26
44
74
94
107
113
114
115
115
114
112
108
100
96
94
93
93
95
96
97
100
103
110
117
125
133
141
151
158
164
169
174
178
181
183
185
186
187
187
187
187
186
185
183
182
179
176
172
168
164
159
154
148
143
139
136
133
132
132
132
133
133
134
135
135
135
135
134
132
125
110
87
49
25
11
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
93
93
96
102
107
108
107
106
105
104
96
88
83
81
82
80
75
69
64
61
55
35
16
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
9
20
38
59
81
101
108
112
114
114
113
110
105
100
96
93
92
92
93
94
98
102
109
117
127
139
149
157
164
171
178
182
185
187
189
191
192
193
193
194
194
194
193
193
193
192
191
189
188
186
184
181
179
176
173
169
164
160
155
150
143
140
137
135
134
134
135
135
135
135
135
134
132
127
117
89
61
36
17
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
96
97
101
106
109
109
107
106
105
105
101
95
87
80
77
76
75
71
67
63
59
43
23
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
4
9
27
47
69
88
103
112
113
113
113
111
105
100
96
92
90
91
92
94
98
103
114
123
134
145
155
167
174
179
184
188
192
193
195
195
196
196
196
196
196
195
195
194
194
193
193
192
192
191
190
189
188
186
185
183
182
179
177
174
170
166
160
155
150
145
141
138
136
136
135
135
135
135
135
134
131
117
96
71
44
22
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
99
100
104
109
110
108
106
105
105
105
104
101
95
86
76
72
72
71
69
66
62
50
30
11
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
5
13
26
55
78
96
108
112
113
112
111
108
103
95
91
89
89
89
93
97
103
112
122
139
152
163
172
180
187
190
193
195
196
196
196
196
196
196
195
194
193
192
191
189
188
187
186
186
185
185
185
185
185
185
184
184
183
183
182
181
181
179
178
174
170
165
159
153
146
142
139
137
136
136
135
135
135
135
132
123
105
80
51
19
8
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
102
103
107
110
110
104
103
103
104
105
105
105
101
94
81
70
69
69
68
67
64
56
36
15
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
5
14
30
55
93
104
110
113
114
112
110
104
97
90
87
87
87
89
91
99
110
124
140
155
171
180
186
190
193
195
196
197
197
197
197
196
195
193
192
189
186
184
181
179
175
173
172
170
169
169
169
170
171
173
175
177
178
179
180
181
181
181
180
179
178
178
176
173
170
162
155
149
143
139
137
136
136
135
135
135
134
129
116
92
47
23
9
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
103
105
108
110
110
103
101
102
103
105
105
105
103
98
87
70
68
67
67
66
65
60
42
20
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
7
23
47
75
100
109
112
113
113
111
104
95
89
86
84
86
88
92
100
111
135
151
165
177
185
191
194
195
196
197
197
196
195
194
192
189
185
181
177
172
165
159
154
149
144
138
134
130
127
125
123
123
124
125
127
132
136
141
146
151
158
164
169
173
176
177
177
177
176
175
174
172
168
163
156
146
141
138
136
136
135
135
135
132
128
104
68
37
14
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
104
105
107
109
110
103
101
101
102
104
104
104
101
95
86
70
67
66
66
66
65
61
44
23
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
7
27
54
79
97
107
112
112
111
108
101
92
87
85
85
86
90
99
112
128
144
164
175
183
188
193
195
196
196
196
195
194
192
189
185
180
173
167
160
153
145
135
128
122
115
110
103
99
95
92
89
87
87
87
87
88
91
94
98
103
108
117
125
133
141
149
159
165
170
173
174
174
173
172
170
167
161
154
148
142
138
136
136
135
134
133
124
104
76
45
18
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
103
105
107
109
110
105
102
101
102
104
104
102
97
91
84
71
66
65
65
65
65
62
46
24
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
10
24
58
82
98
107
111
112
110
105
98
91
84
83
84
87
92
106
122
138
155
170
182
188
192
195
196
196
196
195
193
191
186
182
176
169
162
151
142
133
124
115
104
97
91
85
80
76
73
71
69
67
66
66
66
66
66
68
69
72
74
78
84
89
96
103
112
125
135
145
153
161
168
171
172
172
171
168
164
159
154
147
141
138
136
135
135
133
124
107
81
48
15
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
103
104
106
108
109
106
103
102
103
104
103
99
92
85
82
74
67
64
64
65
65
62
46
24
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
10
26
51
88
101
108
111
112
109
103
96
88
84
81
84
88
95
107
130
150
165
177
187
192
195
196
196
196
195
193
190
187
182
174
166
157
148
137
123
112
103
93
85
76
71
67
63
61
59
58
57
57
57
57
57
57
57
57
57
58
58
59
60
62
64
67
72
78
88
98
110
121
134
150
158
164
168
170
171
170
168
165
159
150
144
140
137
136
135
133
127
112
85
39
18
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
102
103
105
106
107
107
105
104
104
105
104
93
85
80
80
78
69
65
64
65
65
62
45
23
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
8
25
52
86
106
109
110
111
111
103
92
85
81
80
82
87
98
114
135
161
175
185
191
195
197
197
197
196
194
191
187
182
176
169
156
144
132
120
107
91
81
73
66
61
57
56
55
55
54
54
54
54
54
55
55
55
55
55
55
56
56
56
56
57
57
57
58
59
60
62
66
72
80
92
112
129
143
156
165
170
171
171
170
169
163
155
147
141
137
136
135
134
130
122
79
42
17
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
100
101
102
103
104
107
106
106
107
108
97
85
80
80
81
80
73
67
64
65
65
61
42
19
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
9
33
64
92
106
109
110
110
107
101
87
82
79
79
82
93
110
131
153
171
186
191
195
197
197
197
195
193
190
187
180
172
162
151
138
119
104
91
80
70
61
58
55
54
53
53
53
53
53
53
54
54
54
54
54
54
54
55
55
55
55
55
55
56
56
56
56
56
57
57
57
58
59
60
62
68
76
89
105
123
145
159
166
170
170
169
167
164
157
148
140
137
136
135
133
124
94
58
26
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
99
99
99
100
102
103
105
106
107
109
91
82
80
81
82
80
72
67
65
65
65
57
36
14
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
6
31
64
88
103
108
109
109
105
98
89
81
79
79
84
93
116
138
157
174
185
194
196
197
197
196
193
191
187
182
176
164
151
138
123
108
90
78
69
63
58
55
54
53
53
53
53
53
53
53
53
53
53
54
54
54
54
54
54
54
54
55
55
55
55
55
55
56
56
56
56
57
57
58
58
59
61
64
69
76
86
106
124
139
153
164
168
168
168
165
160
150
144
139
136
135
131
119
94
62
26
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
98
98
97
97
99
100
103
105
107
109
88
81
80
81
81
78
70
66
65
65
64
52
30
10
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
7
21
61
87
101
107
109
109
105
98
89
81
77
79
85
96
113
142
162
177
187
194
197
197
196
195
193
189
184
178
169
159
141
125
110
96
82
69
62
58
55
53
52
52
52
52
52
53
53
53
53
53
53
53
53
53
53
53
53
53
53
53
54
54
54
54
54
54
55
55
55
56
56
56
57
57
57
58
59
60
62
66
78
90
105
122
139
156
164
168
168
166
160
154
147
142
137
135
131
118
95
61
19
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
98
97
96
95
96
98
100
103
106
109
90
82
81
81
78
74
68
66
65
66
63
44
22
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
6
19
45
88
101
107
108
108
106
98
88
81
77
78
84
97
115
138
167
181
190
195
198
198
196
194
191
188
182
174
164
151
137
114
98
84
73
64
57
54
53
52
52
52
52
52
52
52
52
52
52
52
53
53
53
53
53
53
52
52
52
52
52
52
52
53
53
53
53
53
54
54
54
55
55
56
56
56
57
57
58
58
59
62
66
75
89
106
132
148
159
165
167
166
163
157
150
143
137
135
131
120
98
47
21
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
98
97
96
95
95
96
96
99
104
109
98
83
80
80
73
70
67
66
66
66
60
33
14
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
15
39
75
103
106
108
108
107
99
87
80
76
75
82
96
117
141
165
185
192
196
198
198
196
193
190
187
182
172
160
144
127
109
87
74
65
58
54
52
52
52
52
52
52
52
52
52
52
52
52
52
52
52
52
52
52
52
52
52
52
51
51
51
51
51
51
51
51
52
52
52
52
53
53
54
54
55
55
56
57
57
57
58
58
59
61
65
71
94
116
136
153
164
167
168
166
162
153
141
138
135
132
126
88
46
18
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
97
97
96
95
95
95
95
96
100
107
107
79
76
80
73
70
68
67
66
64
40
17
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
14
41
73
100
107
108
107
105
99
85
78
75
76
80
101
126
150
172
188
196
198
199
198
195
191
188
183
176
167
147
129
110
92
77
63
58
54
52
52
51
51
51
51
51
51
52
52
52
52
52
52
52
52
52
51
51
51
51
51
51
51
51
51
51
51
51
51
51
51
51
51
51
51
51
51
51
52
52
53
53
54
55
56
56
57
58
58
59
60
64
72
87
108
132
158
165
167
166
164
157
148
141
137
134
125
96
58
23
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
97
97
96
95
95
95
95
96
98
102
108
78
75
80
75
71
69
67
63
54
23
9
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
8
32
64
91
104
107
107
105
99
90
77
75
76
82
94
125
150
171
186
194
198
199
197
195
191
186
181
173
162
147
122
104
87
74
63
56
54
52
51
51
51
51
51
51
51
51
52
52
52
52
52
52
52
51
51
51
51
51
51
51
51
51
51
51
50
50
50
50
50
50
50
50
50
50
51
51
51
51
51
51
52
52
53
54
54
56
56
57
58
59
60
63
70
80
96
125
144
157
164
165
164
159
151
143
138
132
120
93
58
21
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
97
97
96
96
96
96
95
95
96
99
108
80
75
78
76
72
69
64
55
38
11
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
21
55
83
101
106
107
105
99
91
81
74
75
81
94
114
149
171
185
194
198
199
197
194
191
187
180
171
158
143
124
99
83
71
62
56
53
52
51
51
51
51
51
51
51
51
51
52
52
52
52
52
51
51
51
51
50
50
50
50
50
50
50
50
50
50
50
50
50
49
49
49
49
49
49
50
50
50
50
50
50
51
51
51
52
52
54
54
55
56
57
58
60
62
66
73
92
113
132
149
160
166
164
159
153
145
137
132
117
92
54
14
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
96
97
96
96
96
97
96
95
95
96
107
87
77
74
77
72
67
58
43
22
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
8
41
76
96
105
107
105
100
92
83
74
73
78
91
111
138
171
186
194
198
199
198
194
190
186
181
171
157
140
121
101
78
67
59
55
52
51
51
51
50
50
50
51
51
51
51
52
52
52
52
52
51
51
51
50
50
50
50
50
50
50
50
50
49
49
49
49
49
48
48
48
48
48
48
48
48
48
49
49
49
50
50
50
50
51
51
52
52
53
54
56
57
58
59
60
62
69
83
102
123
145
161
164
164
161
154
144
138
132
119
92
38
15
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
96
96
96
96
96
97
98
97
95
95
103
101
83
69
75
69
61
47
29
11
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
7
17
65
91
103
106
106
102
95
84
76
72
74
86
107
133
161
188
195
198
199
198
195
190
186
181
174
158
139
118
98
79
63
57
54
52
51
51
50
50
50
50
50
51
51
51
52
52
52
52
52
51
51
50
50
50
50
50
49
49
49
49
49
49
48
48
48
47
47
47
47
47
46
46
46
46
46
46
47
47
47
48
49
49
50
50
50
51
51
51
52
53
55
56
57
58
59
61
65
75
91
114
147
159
164
164
163
155
145
139
133
125
79
36
12
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
96
96
96
96
96
97
99
99
98
95
96
103
90
72
66
65
53
36
19
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
4
16
42
92
101
105
106
103
94
83
75
71
72
85
108
135
162
185
198
199
198
197
195
189
184
178
170
157
132
110
90
74
62
54
53
52
51
51
50
50
50
50
50
51
51
52
52
52
52
52
51
51
50
49
49
49
49
49
49
49
49
49
48
48
48
48
47
47
47
47
47
46
46
46
46
46
45
45
45
45
45
45
45
46
46
47
48
49
50
50
51
51
51
52
53
54
56
57
59
60
62
66
74
101
126
146
160
165
164
159
150
142
136
124
88
48
15
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
96
96
96
95
95
97
99
101
101
97
96
102
98
86
65
57
49
34
18
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
10
30
64
99
104
105
103
99
87
77
72
72
76
101
129
156
179
194
199
199
197
195
191
184
178
169
157
139
111
91
75
63
56
53
52
51
51
50
50
50
50
50
51
52
52
52
52
52
52
51
50
50
49
49
49
49
49
49
49
49
49
48
48
48
48
47
47
47
47
47
47
47
47
46
46
46
46
46
45
45
45
45
45
45
45
45
46
46
48
49
50
50
51
51
52
52
53
54
57
58
60
62
65
77
95
117
138
156
164
163
159
152
143
133
117
85
47
13
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
95
95
95
95
95
97
99
101
103
101
96
99
101
94
72
50
41
29
16
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
4
19
45
82
103
105
104
100
94
80
73
72
75
86
120
149
173
190
199
199
198
195
191
186
179
170
158
141
120
91
75
64
56
53
52
51
50
50
50
50
50
50
51
51
52
52
52
52
52
51
50
50
49
49
49
49
49
49
49
49
48
48
48
48
48
48
48
48
48
48
48
48
48
48
48
48
47
47
47
47
46
46
45
45
44
44
44
44
45
45
46
47
49
50
51
51
51
52
53
54
56
58
59
61
66
75
91
111
134
156
162
162
159
152
141
132
113
83
42
8
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
95
95
95
95
95
97
99
101
104
104
98
98
100
98
84
48
34
22
13
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
8
30
60
95
105
105
102
96
88
74
71
73
82
100
139
167
186
197
200
198
196
192
187
181
172
160
143
123
100
75
63
56
53
52
51
50
50
50
49
50
50
51
52
52
53
53
52
52
51
50
49
49
49
49
49
49
49
49
49
48
48
48
48
48
48
48
48
48
48
48
49
49
49
49
49
49
49
49
49
48
48
47
47
46
45
45
44
44
44
44
44
45
46
48
49
50
51
51
51
52
53
55
57
59
61
64
72
85
105
137
153
161
162
160
149
141
132
114
80
25
9
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
95
95
94
95
95
97
99
102
104
106
103
98
99
101
98
57
33
17
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
15
43
76
101
106
103
98
91
81
70
70
77
92
117
159
183
195
200
200
196
193
188
183
176
164
147
126
104
82
63
56
53
52
51
50
50
49
49
49
50
51
52
52
53
53
53
52
51
50
49
49
49
49
49
49
49
49
49
48
48
48
48
48
48
49
49
49
49
49
49
49
49
49
49
49
50
50
50
50
51
51
50
50
49
47
46
45
44
44
44
44
44
44
45
47
48
50
51
51
51
52
53
54
56
59
61
63
68
77
106
134
152
162
163
160
150
140
131
119
59
22
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
94
94
94
95
95
99
102
104
105
107
107
102
100
100
103
89
47
20
6
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
5
27
61
92
104
105
100
93
83
73
68
74
88
111
142
182
194
200
200
198
193
188
182
175
167
147
125
102
82
65
55
53
52
51
50
49
49
49
49
50
51
52
53
53
53
53
52
51
50
49
49
49
49
49
49
49
49
49
48
48
48
49
49
49
49
49
49
49
49
49
49
49
49
49
49
49
49
50
50
50
51
51
51
52
52
52
51
50
48
46
44
44
43
43
43
44
45
47
48
50
51
51
52
52
52
55
58
60
62
64
73
93
119
143
160
163
161
153
143
133
115
66
28
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
94
94
94
95
96
101
103
105
107
107
108
105
102
100
102
97
67
34
9
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
9
39
72
97
104
104
96
87
78
70
69
79
99
127
160
191
198
200
198
195
190
183
177
168
157
131
108
87
70
58
53
52
51
50
49
49
49
49
50
51
52
53
53
53
53
52
51
50
49
49
49
49
49
49
49
49
49
49
49
49
49
49
49
49
49
49
49
48
47
47
46
46
46
46
46
47
47
48
48
49
50
51
51
52
52
53
53
52
52
50
48
46
45
44
43
43
44
44
46
47
50
51
51
52
52
53
54
57
59
62
66
75
91
114
140
159
161
159
153
142
127
101
61
23
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
93
94
94
95
97
102
104
106
107
108
109
108
104
101
102
101
84
51
18
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
16
51
81
100
103
102
92
82
74
69
72
87
113
144
176
198
200
199
196
192
186
179
171
160
145
115
92
74
62
55
52
51
50
50
49
49
49
50
50
52
53
53
53
53
53
51
50
50
49
49
49
49
49
49
49
49
49
49
49
49
49
49
49
49
48
47
46
46
45
44
44
44
44
44
45
45
46
47
48
48
50
50
51
52
52
53
53
54
53
53
51
50
48
46
44
43
43
43
44
45
47
49
50
51
52
52
53
54
56
59
62
66
74
89
112
145
156
160
159
152
136
122
92
53
15
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
93
94
94
96
98
103
105
107
107
108
109
109
106
103
101
102
96
68
32
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
25
63
88
102
103
99
88
78
72
69
75
97
127
160
188
202
200
198
194
189
181
174
164
150
131
99
79
65
56
53
51
51
50
49
49
49
49
50
51
53
53
54
53
53
52
51
50
49
49
49
49
49
49
49
49
49
49
49
49
49
49
49
48
48
46
45
44
43
43
43
43
43
43
44
44
45
46
46
47
48
49
50
51
52
52
53
54
54
54
54
54
53
51
49
47
44
44
43
43
44
45
47
49
50
51
52
52
52
54
56
60
62
66
72
85
120
143
156
161
160
146
135
116
86
38
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
92
93
94
96
100
105
106
107
108
108
109
109
108
106
101
102
101
83
52
12
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
5
33
74
95
102
103
95
83
75
70
69
80
109
143
174
197
203
199
195
191
185
177
168
155
138
116
84
68
58
54
52
51
50
49
49
49
49
50
51
52
53
54
54
53
52
51
50
49
49
49
49
49
49
49
49
49
49
49
50
50
50
49
48
47
45
43
42
42
42
42
42
43
43
43
44
44
45
46
47
47
48
49
50
51
52
52
53
54
54
54
55
55
55
54
53
51
47
45
44
43
43
43
44
46
48
50
52
52
52
52
53
56
60
62
65
69
88
119
143
158
162
158
144
132
112
77
15
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
92
93
95
97
102
106
107
107
107
108
108
109
109
108
103
102
102
96
77
35
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
8
49
83
98
101
101
90
79
73
70
71
91
128
162
188
201
202
196
192
187
180
170
158
142
121
96
69
59
54
52
51
50
49
48
48
48
49
51
52
53
54
54
53
53
52
50
49
49
49
49
49
49
49
49
49
49
50
50
50
50
49
47
44
43
42
41
42
42
42
43
43
44
45
46
47
47
48
49
49
49
50
50
51
51
52
52
53
54
54
54
55
55
55
55
55
54
52
50
47
45
43
43
43
44
45
47
50
51
52
52
52
53
55
58
62
64
69
83
109
137
157
161
157
145
131
118
57
20
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
92
94
96
99
103
107
107
107
107
107
107
108
109
108
106
102
102
100
86
54
10
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
4
13
60
88
99
101
100
85
76
71
70
73
102
142
174
195
202
201
194
189
183
175
164
150
130
107
83
62
56
53
52
51
49
48
48
48
49
50
52
53
54
54
54
53
52
51
50
49
49
49
49
49
49
49
49
49
50
50
50
49
48
46
43
42
41
41
41
42
43
44
45
46
47
48
48
49
49
50
50
50
50
50
51
51
52
52
53
53
54
54
55
55
55
55
55
55
55
54
52
50
48
46
44
43
43
44
45
48
50
51
52
52
53
54
55
58
62
66
72
86
108
138
159
159
154
143
127
94
46
16
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
92
94
96
100
104
107
107
107
107
106
106
107
108
109
108
103
103
102
94
73
17
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
6
19
69
91
100
100
98
82
74
70
71
77
115
155
183
199
202
199
191
186
180
171
157
140
118
94
73
57
54
52
51
50
49
48
48
48
49
51
52
53
54
54
54
53
52
51
50
49
49
49
49
49
49
49
49
50
50
50
49
48
46
43
41
41
41
41
42
44
45
46
47
48
49
49
50
50
50
50
50
50
50
50
51
51
51
52
52
53
54
54
55
55
55
55
55
55
55
55
54
52
50
48
45
44
43
43
44
45
48
50
51
52
53
53
54
56
58
63
66
73
87
111
146
157
158
152
140
116
75
37
9
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
92
94
97
101
105
107
107
107
107
106
105
106
107
109
109
104
103
102
99
88
29
9
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
8
25
77
94
100
99
95
78
72
69
72
82
128
167
190
201
202
196
188
182
176
166
150
129
106
83
64
54
53
51
50
49
48
48
48
49
50
51
53
53
54
54
53
52
51
50
50
49
49
49
49
49
49
49
50
50
50
50
48
46
43
41
40
41
42
43
44
46
48
48
49
49
50
50
50
50
50
50
50
50
50
50
50
50
51
51
51
52
53
54
54
55
55
55
55
55
55
55
55
54
53
51
47
45
44
43
43
44
46
48
50
52
53
53
53
54
56
60
63
67
73
85
124
147
157
158
152
130
103
62
24
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
93
95
98
102
106
107
107
107
106
105
104
105
106
108
109
105
103
103
101
97
46
16
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
11
32
84
96
100
98
93
75
70
68
73
89
141
177
196
202
201
193
186
179
172
162
141
118
94
73
59
53
52
51
50
48
48
48
48
49
50
52
53
53
54
54
53
52
51
50
50
50
49
49
49
49
49
50
50
50
50
49
46
43
41
40
40
41
43
45
47
48
49
49
49
50
50
50
50
50
50
51
51
51
50
50
50
50
49
49
49
50
51
53
53
54
55
55
55
55
55
55
55
55
54
53
50
47
45
44
43
43
44
46
48
50
52
53
53
54
54
57
61
64
67
71
93
127
149
160
160
142
126
90
47
10
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
93
95
98
103
107
107
107
106
106
105
103
104
105
106
109
107
104
103
102
101
65
27
8
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
15
42
90
97
100
97
88
72
69
69
78
100
157
186
199
202
201
189
182
175
166
154
128
103
81
64
55
52
51
50
49
48
48
48
49
49
51
53
53
53
53
53
53
51
50
50
50
50
50
49
49
50
50
50
50
50
50
46
42
40
40
40
42
44
46
48
48
49
49
49
50
50
51
52
53
54
55
56
56
55
54
53
51
50
49
49
48
47
47
48
50
51
53
54
54
55
55
55
55
55
55
54
52
50
48
46
44
43
44
44
46
48
51
52
53
53
54
55
58
61
64
67
73
92
122
149
161
159
140
120
88
35
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
94
95
99
103
107
107
107
106
105
104
103
103
104
105
108
107
104
103
102
102
78
34
11
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
5
19
49
93
98
99
95
85
70
68
70
82
109
167
190
200
201
199
186
179
171
161
147
118
93
73
59
54
52
51
50
48
48
48
48
49
50
51
53
53
53
53
53
52
51
50
50
50
50
50
50
50
50
50
50
50
50
48
42
40
40
40
41
44
46
47
48
48
49
49
50
51
53
58
62
67
71
74
77
77
76
74
71
65
60
56
52
49
47
47
47
47
48
50
51
53
54
54
55
55
55
55
54
53
52
50
47
45
44
44
44
45
46
49
51
52
53
54
55
57
59
62
65
69
78
99
126
154
161
153
136
108
68
15
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
94
95
99
103
107
107
107
106
105
103
103
103
103
105
108
108
104
103
103
102
87
42
15
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
7
23
55
94
98
98
94
83
69
68
72
87
118
175
194
201
201
197
183
176
168
156
139
108
85
67
56
53
51
50
49
48
48
48
48
49
50
51
53
53
53
53
53
52
51
50
50
50
50
50
50
50
50
50
50
50
49
45
40
39
39
40
42
46
47
48
48
48
49
51
53
58
64
73
80
86
91
96
100
101
101
100
97
91
84
76
69
61
54
50
48
46
47
48
49
51
52
53
54
54
54
54
54
54
53
51
49
47
45
44
44
44
45
48
50
52
53
53
54
56
58
60
63
67
71
82
103
134
160
159
147
127
99
34
12
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
94
96
99
104
107
107
107
106
104
103
102
102
103
104
108
108
104
103
103
102
93
50
20
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2
3
5
6
7
7
6
5
3
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
8
26
61
95
98
98
93
81
69
68
73
91
126
183
196
201
200
195
181
173
164
151
131
99
77
62
55
53
51
50
49
48
48
48
49
49
50
51
52
53
53
53
53
51
50
50
50
50
50
50
50
50
50
50
50
49
47
42
39
39
40
41
44
47
47
48
48
48
50
54
60
68
77
88
96
102
108
112
117
119
121
121
121
117
111
103
94
83
69
61
54
50
47
47
48
49
50
52
53
53
54
54
54
54
53
52
50
48
46
44
44
44
44
46
49
51
52
53
54
55
57
59
62
66
68
72
84
108
150
159
155
142
122
61
25
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
94
96
99
104
107
107
107
106
104
103
102
102
102
104
108
108
105
103
103
103
97
58
25
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
3
4
5
6
6
6
6
5
5
4
4
4
6
8
14
18
21
21
19
14
9
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
10
29
67
96
98
97
91
79
68
68
75
95
134
189
198
202
199
192
179
170
160
145
123
90
70
59
54
52
51
49
48
48
48
48
49
50
50
51
52
52
53
53
52
51
50
50
50
50
50
50
50
50
50
50
50
48
45
40
39
39
40
43
46
47
47
47
48
49
54
62
71
82
90
98
103
108
112
116
120
123
125
127
129
131
131
129
124
114
96
82
69
58
50
47
47
47
48
50
52
52
53
53
54
54
53
53
51
50
47
45
44
44
44
45
47
50
52
53
54
54
56
58
61
65
68
70
74
82
127
153
161
154
134
93
44
15
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
94
96
99
103
107
107
107
106
104
102
102
102
102
104
108
108
105
103
103
103
100
65
30
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
5
9
14
23
30
36
41
44
45
44
41
37
33
28
27
28
32
41
57
69
76
79
79
71
57
41
25
12
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
12
34
74
97
98
96
90
77
68
68
76
99
143
193
200
202
198
189
176
167
154
137
113
80
64
56
53
52
50
49
48
48
48
48
49
50
51
51
52
52
52
52
52
51
50
50
50
50
50
50
50
50
50
51
49
46
42
39
38
39
42
45
46
47
47
47
49
53
63
73
81
87
92
99
103
107
109
112
115
118
121
125
128
132
133
134
134
134
130
119
104
88
71
56
50
48
47
48
50
51
52
53
53
53
53
53
52
51
49
47
45
44
43
44
46
48
50
53
54
54
55
57
59
65
67
69
70
73
90
129
152
160
156
125
77
37
10
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
94
95
99
103
107
107
107
106
105
103
102
102
103
105
108
108
104
102
102
103
101
69
33
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
3
10
20
32
44
55
65
70
74
77
79
80
79
78
76
73
69
65
64
67
73
85
91
95
97
98
96
90
82
70
53
25
12
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
14
38
78
97
98
96
89
76
67
68
77
102
149
195
200
202
197
187
174
164
150
131
106
74
61
54
52
52
50
48
48
48
48
48
49
50
50
51
51
52
52
52
52
51
50
50
50
50
50
50
50
50
51
50
48
44
40
38
38
40
43
46
46
46
47
48
51
57
68
75
81
87
90
92
91
89
87
86
86
89
93
98
105
116
123
129
133
135
135
132
126
116
100
76
63
54
49
47
48
50
51
52
52
53
53
53
52
52
50
48
46
45
44
44
45
47
49
52
54
54
55
56
58
63
67
69
70
72
79
105
135
158
161
138
104
61
23
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
94
95
98
103
107
107
107
107
105
103
103
102
104
106
109
108
103
102
102
104
102
72
35
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
5
12
23
41
53
64
73
80
86
88
90
92
93
94
94
93
93
92
90
88
88
89
92
98
101
103
105
105
105
104
101
95
85
64
43
23
9
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
4
16
42
81
97
98
96
88
75
67
68
78
104
152
196
201
202
197
185
172
162
146
125
99
69
59
54
52
51
49
48
48
48
48
49
49
50
50
51
51
51
52
51
51
51
50
50
50
50
50
50
50
51
51
50
47
43
39
38
38
41
44
46
46
46
46
49
53
60
70
75
79
81
80
75
70
67
64
62
62
64
66
70
75
86
96
106
117
127
135
137
136
132
124
103
84
68
57
50
48
49
50
51
52
53
53
53
52
52
50
49
47
45
44
44
44
46
48
51
53
54
54
55
57
61
67
70
71
72
74
87
115
145
162
152
125
85
42
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
93
95
98
102
107
107
107
107
106
104
103
103
105
107
109
108
103
101
102
104
103
74
37
7
2
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
4
11
22
36
53
74
83
88
90
91
91
91
91
92
93
94
95
96
96
97
99
99
99
100
101
103
104
105
105
106
106
106
105
105
104
97
79
54
29
9
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
4
19
45
84
98
98
95
88
74
67
68
79
106
155
197
202
202
196
183
170
159
142
120
93
65
57
53
52
51
49
48
48
48
48
49
49
50
50
51
51
51
51
51
51
51
50
50
50
50
50
50
50
51
51
50
46
41
38
37
39
42
45
46
46
46
47
49
55
61
69
73
73
71
65
56
52
49
47
47
47
47
48
49
51
57
65
76
90
105
123
131
136
139
139
128
110
90
71
57
49
49
49
50
51
52
52
53
52
52
51
49
48
46
45
44
44
45
47
50
53
54
54
55
56
60
66
70
73
73
73
76
95
125
157
162
142
107
66
23
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
93
95
97
101
106
107
107
108
107
105
104
105
107
109
109
106
101
101
102
105
103
75
38
8
3
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
13
34
57
77
87
90
91
92
91
90
86
84
84
83
84
87
90
93
95
98
100
101
102
103
103
104
105
105
105
105
105
105
106
106
106
105
103
93
69
28
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
4
21
48
85
98
98
95
88
74
67
67
79
107
156
198
202
201
195
182
168
157
138
115
88
62
56
53
52
51
49
48
48
48
48
49
49
50
50
50
51
51
51
51
51
51
50
50
50
50
50
50
51
51
51
49
44
40
38
37
39
43
45
46
46
46
47
50
55
61
67
67
64
57
48
45
44
44
43
43
44
44
44
44
45
45
47
50
55
66
92
114
129
139
142
142
135
118
95
71
53
50
49
49
50
52
52
52
52
52
51
50
49
47
45
44
44
45
46
49
53
54
54
55
56
59
64
70
74
76
74
74
80
101
144
164
152
128
92
41
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
92
95
97
100
106
107
107
108
108
108
107
108
108
109
109
103
100
100
102
105
102
73
37
9
6
7
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
4
13
31
68
82
88
90
91
88
82
75
70
65
62
61
61
62
64
68
73
79
86
93
100
102
103
104
104
104
103
103
103
102
102
102
102
103
104
105
105
104
100
90
30
10
2
1
1
3
3
2
2
1
1
1
1
1
1
1
1
1
1
1
1
5
23
51
85
98
98
95
88
75
67
67
79
107
156
198
202
201
195
181
165
153
134
109
83
59
55
52
51
50
48
48
48
48
48
49
50
50
50
50
50
51
51
51
51
51
51
50
50
50
50
51
51
51
51
49
43
39
37
37
40
44
45
45
45
45
46
50
54
59
59
57
51
46
44
43
43
43
43
43
43
43
43
44
44
44
44
44
45
46
51
64
85
110
133
143
145
142
131
108
73
59
52
49
50
51
52
52
52
52
52
51
49
48
46
45
44
44
45
47
51
53
54
55
55
58
62
69
75
80
79
76
76
83
103
158
163
151
121
75
20
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
92
94
96
99
106
107
107
108
108
108
109
109
109
109
108
101
100
100
102
105
101
69
36
12
9
9
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
11
29
50
69
82
87
88
86
82
74
69
65
62
60
58
57
58
59
61
65
70
77
85
93
100
101
102
102
102
103
102
102
102
102
101
100
99
99
99
101
102
103
103
100
73
34
12
2
1
2
7
9
8
5
2
1
1
1
1
1
1
1
1
1
1
5
24
52
85
98
98
95
88
75
67
68
78
106
156
198
202
201
194
180
164
151
131
105
79
58
54
52
51
50
48
48
48
48
49
49
50
50
50
50
50
50
51
51
51
51
51
51
51
51
51
51
51
51
51
48
42
38
37
37
41
44
45
45
45
45
46
49
53
55
55
51
47
44
43
43
43
43
43
43
43
43
43
43
44
44
44
44
44
44
46
50
59
74
97
130
141
145
142
133
101
76
61
52
50
50
51
52
52
52
52
51
50
49
47
45
44
44
45
46
51
53
54
55
55
57
61
67
73
81
83
80
77
78
85
140
159
156
136
103
38
14
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
91
94
96
98
105
107
107
107
108
109
109
109
109
108
106
100
99
100
102
104
99
65
35
15
13
12
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
20
39
57
73
84
86
85
82
78
73
66
63
60
58
57
56
56
57
58
60
64
70
77
85
94
100
101
101
102
102
102
102
101
100
98
97
96
95
95
95
96
97
99
101
103
95
66
36
11
3
3
8
15
20
19
12
5
2
1
1
1
1
1
1
1
1
5
24
52
85
98
98
95
88
76
67
68
77
103
154
198
202
201
194
180
164
150
128
103
76
57
54
52
51
50
48
48
48
48
49
49
50
50
50
50
50
50
50
51
51
51
51
51
51
51
51
51
51
51
51
48
41
38
37
37
41
44
45
45
45
45
45
48
50
51
51
47
45
43
43
43
43
43
43
43
43
43
43
43
43
43
43
43
43
43
43
44
47
54
67
101
125
139
146
145
126
99
76
59
52
50
51
51
52
52
52
52
51
49
48
45
45
44
45
46
50
53
54
55
55
56
60
65
72
81
86
84
81
78
77
113
148
159
150
127
61
26
8
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
89
93
95
97
104
106
106
107
108
109
109
109
108
106
103
99
98
100
103
104
96
60
33
18
17
13
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
8
19
47
69
79
82
83
82
79
74
70
67
63
61
59
58
57
56
56
56
58
60
65
71
78
87
95
101
101
102
102
103
102
100
97
95
93
91
90
90
91
92
93
93
95
97
101
102
93
66
34
11
5
7
17
29
36
29
17
8
3
1
1
1
1
1
1
1
5
23
52
85
98
98
95
89
76
67
68
76
100
151
198
202
202
194
180
163
149
126
100
74
56
53
52
51
50
48
48
48
49
49
50
50
50
50
50
50
50
50
50
51
51
51
51
51
51
51
51
51
51
51
48
41
38
37
37
41
43
44
44
44
44
45
47
48
49
48
45
44
43
43
43
43
43
43
43
43
43
43
43
43
43
43
43
43
43
43
43
43
45
48
68
99
124
141
148
144
123
97
72
56
51
51
51
52
52
52
52
51
50
48
46
45
45
45
45
49
52
54
55
55
56
59
65
72
81
88
88
85
81
76
88
132
156
161
145
86
42
15
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
88
93
95
97
102
106
106
106
106
108
108
108
107
104
99
97
97
100
103
104
93
55
32
22
20
11
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
8
24
56
77
81
82
81
78
72
70
68
66
64
63
61
59
58
57
56
56
57
59
62
67
72
79
88
96
103
105
105
104
103
99
94
90
87
85
84
84
85
87
90
92
92
93
93
95
101
102
97
76
32
10
11
15
26
50
53
40
22
7
2
1
1
1
1
1
1
4
22
50
83
98
98
96
89
77
67
67
74
97
146
198
203
202
195
180
163
148
125
99
73
56
53
52
51
50
48
48
48
49
49
50
50
50
50
50
50
50
50
50
50
51
51
51
51
51
51
51
51
51
51
48
41
38
36
37
41
43
44
44
44
44
44
45
47
47
46
44
44
43
43
43
43
43
43
43
43
43
43
43
43
42
42
42
42
42
42
42
42
43
43
47
67
97
127
145
149
142
120
92
65
52
51
51
51
52
52
52
51
50
49
46
45
45
45
45
48
51
54
55
55
56
58
64
71
79
90
91
89
85
79
77
115
148
165
157
113
61
26
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
85
92
94
96
100
106
106
106
105
105
106
106
104
100
96
96
97
100
103
103
84
50
32
25
22
8
3
1
1
1
1
1
1
1
1
1
1
1
1
1
2
18
46
72
80
79
75
73
72
71
69
68
67
66
65
64
63
61
59
57
56
56
59
62
66
70
78
89
99
106
107
106
104
100
95
86
83
81
80
80
80
81
83
86
89
91
92
92
92
93
94
100
102
101
96
53
30
23
26
32
64
66
58
39
12
2
1
1
1
1
1
3
18
44
79
97
98
96
91
79
67
67
72
91
136
195
202
202
196
182
164
147
124
98
71
55
53
52
51
49
49
48
49
49
49
50
50
50
50
50
50
50
50
50
50
51
51
51
51
51
51
51
51
51
51
48
42
38
36
36
40
42
44
44
44
44
44
44
45
45
45
44
44
44
44
44
44
44
44
43
43
43
42
42
42
42
42
41
40
40
39
40
40
41
42
43
46
63
91
128
147
148
141
123
90
59
53
51
51
52
52
52
52
51
49
47
46
45
45
45
47
51
53
55
55
56
57
63
70
77
90
93
94
92
85
76
87
123
157
164
142
92
48
16
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
83
92
94
95
98
106
106
106
104
103
103
103
100
97
96
95
98
101
103
102
76
46
32
26
21
5
2
1
1
1
1
1
1
1
1
1
1
1
1
3
24
52
69
76
76
74
73
72
72
73
71
69
68
67
67
66
64
61
59
57
57
58
62
66
70
80
92
100
105
107
106
103
97
91
85
80
78
76
75
74
75
77
80
83
87
91
92
92
92
93
94
96
99
101
101
89
71
51
36
37
51
65
67
59
40
9
3
1
1
1
1
2
15
39
75
97
98
97
92
81
68
67
70
87
127
191
202
203
197
183
165
148
125
98
71
55
53
52
51
49
49
48
49
49
50
50
50
50
50
50
50
50
50
50
50
51
51
51
51
51
51
51
51
51
51
49
42
38
36
36
39
42
43
44
43
43
43
44
44
45
45
45
44
44
44
44
44
44
44
43
43
42
42
42
41
41
41
40
39
38
37
37
38
39
41
42
44
51
68
101
139
146
146
136
113
72
59
53
52
52
52
52
52
51
50
48
46
45
45
45
47
50
53
55
55
56
57
62
70
77
89
94
97
95
91
79
80
108
142
165
155
115
69
28
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
81
91
94
95
97
105
106
106
105
102
101
100
98
96
95
96
99
101
102
99
67
44
32
26
17
3
1
1
1
1
1
1
1
1
1
1
1
1
6
18
52
70
75
75
73
74
77
77
76
75
73
70
69
69
68
66
64
61
59
58
59
62
67
72
78
90
100
105
107
106
102
96
89
83
78
73
71
69
68
68
70
72
76
80
84
90
91
92
92
93
94
95
96
99
101
101
93
79
64
51
50
61
68
68
60
28
11
3
1
1
1
2
11
32
69
95
97
97
93
83
68
67
69
83
117
186
200
204
198
185
166
149
126
99
72
55
53
51
51
49
49
49
49
49
50
50
50
50
50
50
50
50
50
50
50
51
51
51
51
51
51
51
51
51
51
50
43
39
36
36
38
41
42
43
43
43
43
43
44
44
45
44
44
44
44
44
44
44
43
43
42
42
41
40
40
40
40
39
38
37
36
35
35
36
38
41
42
45
54
75
124
141
147
143
131
89
67
56
52
52
52
52
52
52
51
48
47
45
45
45
46
50
53
55
56
56
57
62
69
77
87
95
98
98
96
84
79
94
123
161
161
135
90
44
12
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
79
90
94
94
95
104
105
106
105
102
99
98
97
96
96
97
100
101
100
93
59
41
31
24
12
1
1
1
1
1
1
1
1
1
1
1
1
6
18
42
74
75
74
72
75
80
84
85
82
76
73
71
70
69
68
66
64
61
60
60
63
67
72
78
86
99
104
106
105
103
95
87
81
75
71
66
63
62
62
63
65
68
72
76
81
88
90
92
92
93
95
95
95
96
98
100
100
98
90
71
60
60
66
71
72
51
25
9
3
1
1
1
8
26
61
94
97
97
94
86
70
67
68
79
107
179
198
204
199
187
167
150
127
100
73
55
53
51
51
49
49
49
49
49
50
50
50
50
50
50
50
50
50
50
50
50
51
51
51
51
51
51
51
51
51
50
45
40
37
36
37
39
41
42
43
43
43
43
43
44
44
44
44
44
44
44
44
43
43
42
41
40
40
40
39
39
39
38
38
37
34
33
33
33
35
40
41
43
46
55
103
132
144
147
143
108
78
61
53
52
52
52
52
52
51
49
47
46
45
45
46
49
53
55
56
56
57
61
68
77
86
94
99
100
100
90
81
84
106
153
164
150
111
64
21
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
77
90
94
94
94
102
105
106
105
102
99
97
96
96
96
98
100
101
97
86
50
37
29
20
7
1
1
1
1
1
1
1
1
1
1
1
2
16
39
71
76
72
70
74
84
92
92
90
86
77
73
71
70
70
68
66
64
63
62
62
66
70
77
85
94
104
105
105
103
96
84
78
72
67
61
58
58
59
60
61
62
64
67
72
77
85
89
91
92
93
95
96
96
95
94
94
95
95
94
93
77
69
67
71
76
73
46
22
6
2
1
1
5
19
52
91
96
97
95
89
72
67
67
75
96
170
196
205
201
190
170
152
129
102
75
55
53
51
51
49
49
49
49
49
50
50
50
50
50
50
50
49
49
50
50
50
51
51
51
51
51
51
51
51
51
51
46
41
37
36
37
38
40
41
42
43
43
43
43
43
43
44
44
44
44
44
43
42
41
40
40
40
39
39
39
39
38
38
37
36
33
32
31
31
32
37
40
42
44
46
78
119
140
147
147
126
91
68
55
52
52
52
52
52
52
49
47
46
45
45
46
49
52
55
56
56
57
61
67
77
86
93
99
102
102
96
84
81
95
138
164
159
130
86
34
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
75
89
93
93
94
99
103
105
106
105
100
98
97
97
97
100
100
99
92
74
43
30
22
14
3
1
1
1
1
1
1
1
1
1
1
2
15
43
68
75
71
70
77
87
98
101
100
95
88
81
74
73
73
72
71
67
65
64
64
66
74
85
95
103
107
106
101
95
87
79
71
65
59
56
55
56
56
56
56
55
56
57
60
64
69
77
84
88
90
92
94
97
98
98
98
97
95
93
92
91
90
83
77
74
75
79
75
54
28
7
1
1
3
12
36
84
94
97
96
92
75
69
67
71
83
155
190
204
203
193
173
156
133
106
78
56
53
51
51
50
49
49
49
49
50
50
50
50
50
50
50
49
49
50
50
50
51
51
52
52
51
51
51
51
52
51
49
44
39
37
37
38
38
39
39
41
42
42
43
43
43
43
43
43
42
41
40
40
40
40
40
40
40
40
39
39
39
38
37
36
33
31
29
29
29
33
38
41
43
44
56
96
127
145
147
140
109
81
60
53
52
52
52
52
52
50
48
47
46
46
46
49
52
55
56
56
57
60
67
76
86
91
98
102
103
102
90
83
87
109
160
161
147
114
58
12
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
73
88
93
93
93
97
101
104
106
106
102
101
100
99
99
100
100
96
85
65
35
25
17
9
1
1
1
1
1
1
1
1
1
1
1
9
37
59
72
73
70
74
85
96
104
106
104
99
93
87
81
79
77
75
72
68
67
69
74
84
93
99
101
102
102
95
88
83
77
70
59
56
53
52
51
51
50
51
51
53
54
55
57
59
63
70
77
82
86
89
92
95
97
98
99
99
99
98
97
95
93
90
84
78
76
78
77
69
51
23
3
1
2
8
24
74
91
96
96
94
79
70
67
69
77
139
182
202
204
196
177
160
137
110
81
57
53
51
51
50
49
49
49
49
50
50
51
50
50
50
50
49
49
49
50
50
51
51
52
52
52
51
51
51
52
52
50
46
41
38
37
38
38
38
38
39
40
40
41
41
42
42
41
41
40
39
39
39
40
40
40
41
41
42
42
42
42
41
40
38
34
31
29
28
28
31
36
40
42
44
51
82
116
141
147
143
120
91
66
54
53
53
53
52
52
51
49
47
46
46
46
49
52
55
56
57
57
60
66
76
85
90
96
101
104
103
95
86
84
94
151
159
154
130
80
21
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
72
87
93
93
92
95
98
103
106
106
104
102
101
101
101
101
99
91
75
53
27
20
12
5
1
1
1
1
1
1
1
1
1
1
3
24
53
68
73
71
70
80
92
102
107
108
105
101
97
91
86
83
81
77
74
72
77
82
89
96
101
101
98
95
92
85
78
70
63
57
51
48
47
45
44
44
44
46
48
50
52
53
54
55
57
63
69
75
81
84
87
90
93
95
98
100
101
101
100
99
97
96
92
88
83
80
79
76
67
45
11
4
2
5
15
62
86
95
96
95
82
72
67
68
73
122
173
198
205
199
180
164
141
115
85
59
54
52
51
50
49
49
49
49
50
51
51
51
50
50
50
50
49
50
50
50
51
51
52
52
52
51
51
51
52
52
51
49
44
39
38
38
38
38
38
38
38
39
39
39
40
39
39
39
39
39
39
40
40
41
43
44
44
45
45
46
46
45
44
42
37
32
29
28
28
29
34
39
42
43
48
71
105
136
147
145
128
101
73
56
53
53
53
53
52
51
49
47
46
46
47
49
52
55
56
57
57
60
66
76
85
89
94
99
104
104
99
90
83
86
138
156
158
142
102
33
12
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
71
87
92
92
92
93
96
101
105
106
105
103
102
102
102
101
96
83
64
40
20
14
8
3
1
1
1
1
1
1
1
1
1
3
8
44
65
73
72
69
73
87
98
106
109
108
106
103
100
95
90
87
84
82
80
81
89
95
100
101
100
96
90
85
79
72
65
57
51
46
44
42
40
38
37
37
39
41
43
46
48
50
51
52
53
56
61
67
73
78
82
85
87
91
94
98
100
101
102
101
101
100
99
97
92
84
82
80
76
66
26
10
3
4
8
48
79
93
96
96
86
74
68
67
70
107
161
192
205
202
185
168
146
120
90
61
55
52
51
50
49
49
49
49
50
51
51
51
51
50
50
50
50
50
50
50
51
51
52
52
52
52
51
51
51
52
52
51
48
42
39
38
38
38
38
38
38
38
38
38
38
38
38
38
38
39
40
41
42
44
45
46
47
47
48
49
49
49
49
47
41
34
30
28
28
29
33
38
41
43
46
63
96
130
146
146
134
108
79
58
53
53
53
53
53
51
49
48
47
47
47
50
53
55
57
57
58
60
66
75
85
88
93
97
103
104
102
93
85
84
122
151
160
150
123
48
18
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
70
86
92
92
91
91
92
98
104
106
105
104
103
103
102
101
88
71
50
28
13
9
5
1
1
1
1
1
1
1
1
1
1
6
19
63
72
73
71
69
76
92
103
108
110
108
107
105
102
99
95
92
91
90
91
96
98
99
99
98
93
89
83
75
65
52
46
43
42
40
37
35
35
34
34
34
35
36
38
40
44
46
48
48
49
50
53
58
65
71
78
80
82
84
87
94
98
101
102
102
101
101
100
99
98
91
86
83
81
78
51
20
7
4
5
32
69
88
96
96
89
77
70
67
68
92
147
185
204
204
189
173
152
126
96
64
56
52
51
50
49
49
49
49
50
51
51
51
51
51
50
50
50
50
50
50
51
51
52
52
52
52
51
51
51
52
52
52
50
47
40
39
39
39
39
38
38
38
38
38
38
38
38
38
39
41
43
44
46
47
47
48
48
48
48
49
51
52
52
51
46
38
32
28
28
29
33
38
41
43
45
58
89
124
145
146
138
114
85
61
54
53
53
53
53
52
50
48
47
47
47
50
53
56
57
57
58
61
66
75
85
88
91
95
101
104
103
96
87
83
107
145
159
156
140
66
27
8
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
69
86
91
91
91
89
88
92
98
105
106
105
105
103
101
89
71
50
30
14
7
5
3
1
1
1
1
1
1
1
1
1
4
16
42
73
75
72
68
68
79
95
104
109
109
109
108
107
106
105
103
102
101
100
100
99
98
97
93
86
72
63
55
48
43
40
38
37
36
35
35
34
34
34
34
34
34
34
35
37
40
42
44
45
45
45
45
47
51
56
66
72
77
80
81
84
90
95
99
101
101
101
101
101
102
101
97
92
87
85
76
46
21
6
6
16
52
79
94
95
93
81
72
67
67
78
124
169
201
207
195
180
160
136
105
69
58
53
51
50
49
49
49
49
49
51
51
51
51
51
51
50
50
50
50
50
50
51
52
52
52
52
52
51
51
51
52
52
52
51
46
42
40
40
40
40
40
39
39
39
39
40
41
42
44
46
46
47
47
47
48
48
48
48
48
49
51
53
53
53
50
41
34
30
29
30
33
38
41
43
45
55
84
118
144
146
140
119
91
64
54
54
53
53
53
52
50
49
48
48
48
51
54
56
57
57
58
61
67
75
84
87
89
92
97
103
103
99
91
84
93
135
155
159
153
92
42
14
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
68
85
91
91
90
88
85
85
90
100
103
103
101
97
91
74
53
33
17
9
6
3
1
1
1
1
1
1
1
1
1
1
9
27
59
75
74
70
67
67
80
95
104
108
109
109
108
108
107
106
104
102
101
100
99
96
91
82
72
62
51
46
43
40
38
37
37
37
38
38
38
38
38
38
37
36
35
35
35
36
38
40
41
43
43
43
43
43
44
46
52
58
64
71
75
80
83
89
94
98
99
100
100
100
100
101
99
96
93
89
84
66
39
15
7
12
38
67
89
95
94
85
75
68
67
72
107
153
193
207
199
185
167
143
114
75
61
54
51
50
49
49
49
49
49
50
51
51
51
51
51
51
50
50
50
50
50
51
52
53
53
52
52
52
51
51
52
52
52
52
50
46
43
42
41
41
42
42
42
42
43
43
44
45
46
47
47
47
47
48
48
48
48
49
49
50
52
54
55
55
51
42
35
31
30
31
35
39
42
43
45
54
83
117
143
146
141
121
93
66
55
54
53
53
53
52
51
49
48
48
49
51
54
57
57
58
58
61
67
76
83
85
87
89
94
102
103
100
94
84
89
126
150
161
157
110
55
21
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
67
84
90
91
90
89
82
79
81
89
94
95
91
84
75
51
33
20
11
7
4
2
1
1
1
1
1
1
1
1
1
3
17
40
70
77
72
68
66
67
80
94
103
108
109
108
108
108
107
105
103
101
99
95
90
79
70
61
53
46
41
40
39
39
40
41
42
43
44
44
45
45
44
43
42
40
38
37
36
35
37
38
40
41
42
42
42
42
42
42
44
48
52
57
63
69
74
80
87
94
98
99
99
99
99
100
100
99
96
93
90
80
56
30
11
11
27
55
82
94
94
88
79
70
66
69
92
137
181
205
203
191
174
152
123
82
65
55
52
50
49
49
49
49
49
50
51
52
52
52
52
52
51
51
50
50
50
51
52
53
53
53
52
52
51
51
52
52
52
52
52
50
47
45
43
42
43
43
43
44
44
45
45
46
46
47
47
47
47
48
48
48
49
49
50
51
54
55
56
56
51
42
35
31
31
32
36
39
42
43
45
55
83
117
143
145
141
121
94
66
55
54
53
53
53
52
51
50
49
49
49
52
55
57
58
58
58
62
68
76
82
84
85
87
91
100
102
101
96
85
87
117
144
161
160
126
69
29
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
67
84
90
90
90
89
81
74
71
75
80
80
74
65
53
28
17
11
8
6
2
1
1
1
1
1
1
1
1
1
1
7
27
53
75
77
70
66
65
66
80
93
101
106
107
107
107
106
104
102
98
94
89
82
72
58
50
44
41
40
40
41
42
43
44
47
48
49
49
50
50
50
50
49
48
45
43
40
38
36
36
37
38
39
41
41
41
41
41
41
41
42
43
45
49
57
62
69
76
85
94
97
98
99
99
99
100
100
99
96
93
89
73
48
18
12
19
44
72
92
93
91
82
72
66
67
81
119
166
202
205
195
181
160
132
90
69
58
52
50
49
49
49
49
49
50
51
52
52
53
53
53
52
51
51
51
51
51
52
53
53
53
53
52
52
51
51
52
52
52
52
52
51
49
47
45
44
44
44
45
45
46
46
46
47
47
47
48
48
48
49
49
50
50
51
54
56
57
57
57
51
41
35
32
32
33
37
40
43
44
46
58
86
119
143
145
140
120
93
66
55
54
54
53
53
52
51
50
49
49
50
53
56
58
58
58
59
62
68
77
81
82
83
84
87
98
101
101
97
87
86
109
138
160
161
139
83
39
10
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
66
83
89
90
90
89
81
71
64
62
63
58
50
39
26
17
14
10
7
4
1
1
1
1
1
1
1
1
1
1
2
12
37
63
78
77
68
65
64
65
78
90
98
103
105
105
103
102
99
95
86
76
66
56
48
43
42
41
41
41
45
46
48
49
49
49
50
50
50
51
51
52
52
52
52
50
48
45
42
39
36
36
37
38
40
40
41
41
41
41
41
41
41
41
42
44
47
53
60
69
84
92
96
98
98
99
99
100
99
99
96
94
87
68
32
15
17
33
59
87
93
92
85
76
67
66
73
103
146
195
205
200
188
169
143
99
75
61
53
51
50
49
49
49
49
50
51
52
53
54
54
54
53
52
51
51
51
51
52
53
53
53
53
53
52
51
51
51
52
52
53
53
52
52
50
48
47
46
46
46
46
46
46
47
47
48
48
48
48
49
50
50
51
52
53
56
58
58
58
57
48
39
34
32
32
34
38
41
43
44
47
61
91
122
143
144
138
118
91
66
55
54
54
53
53
53
51
50
50
50
51
54
57
58
58
58
59
62
69
77
80
80
81
82
84
95
100
101
98
89
86
102
132
158
162
149
97
49
14
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
66
82
88
89
89
90
87
74
60
53
49
41
35
30
24
17
14
9
4
2
1
1
1
1
1
1
1
1
1
1
4
22
50
74
80
75
67
63
63
63
71
79
84
87
88
87
81
76
70
62
53
48
45
44
43
43
44
45
47
49
49
49
49
49
49
49
49
50
50
51
52
52
53
54
54
54
54
52
49
46
41
38
36
36
37
39
39
40
40
40
40
40
40
40
41
41
42
43
45
48
60
75
87
95
98
99
99
99
100
99
98
97
95
85
64
23
20
25
42
74
92
92
89
82
70
66
68
85
119
175
206
204
195
180
158
114
86
67
56
51
50
49
49
49
48
49
51
52
53
54
55
55
55
54
53
52
52
52
52
53
54
54
54
53
53
52
51
51
51
51
52
52
53
53
53
53
52
51
50
49
49
48
48
48
48
49
49
49
50
51
52
52
53
54
56
59
59
59
57
53
43
37
34
34
34
37
41
43
45
45
50
70
101
129
144
144
134
112
85
63
55
54
54
54
54
53
52
51
51
51
52
56
58
58
59
59
59
63
70
77
78
78
78
78
80
91
98
100
98
91
85
95
124
154
162
156
114
65
23
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
65
81
87
88
88
90
88
79
66
54
49
42
37
32
27
19
15
9
3
1
1
1
1
1
1
1
1
1
1
1
9
33
59
78
81
74
66
62
61
61
65
69
72
72
71
68
62
58
54
50
47
45
44
44
45
46
47
48
48
49
49
48
48
48
48
48
48
48
49
50
51
52
53
55
56
57
57
55
53
50
46
42
39
37
36
37
38
38
39
39
40
40
40
40
40
41
41
41
42
43
48
56
68
82
93
98
99
99
100
100
99
98
97
92
82
39
27
25
35
59
88
91
90
85
74
66
67
76
102
154
200
204
200
188
169
127
96
74
59
52
50
49
49
49
48
49
50
52
53
55
56
56
56
56
55
53
53
53
53
53
54
54
54
54
53
52
51
51
51
51
51
52
52
53
53
53
53
53
52
52
51
51
51
51
51
51
51
52
52
53
53
54
55
56
58
59
59
58
55
49
39
36
35
35
36
40
43
44
45
46
54
80
110
135
144
143
129
105
80
61
55
54
54
54
54
53
52
52
52
52
54
57
58
59
59
59
59
64
70
77
77
76
75
76
77
88
96
99
98
92
85
92
120
150
163
159
125
76
31
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
65
80
86
87
87
89
89
84
73
59
52
47
41
36
32
22
16
8
2
1
1
1
1
1
1
1
1
1
1
2
15
43
67
81
82
73
65
61
60
59
60
61
61
60
58
55
51
49
47
45
45
45
45
46
47
48
48
49
49
48
48
48
48
48
47
47
47
48
48
49
51
52
54
55
57
58
59
59
57
55
50
46
43
40
37
37
37
38
38
39
39
39
39
40
40
40
40
40
41
41
43
46
54
65
80
93
97
99
99
100
99
98
98
96
92
59
36
28
32
46
82
88
90
87
79
67
66
70
88
129
191
202
202
195
179
141
108
82
64
54
50
50
49
49
48
49
50
51
53
55
57
57
57
57
57
55
54
54
54
54
54
54
54
54
54
53
52
51
51
51
51
51
52
52
53
53
53
53
53
53
53
53
53
53
53
53
53
53
54
54
55
56
57
58
59
60
58
55
50
44
37
36
36
36
38
42
44
45
46
48
63
92
119
139
144
141
122
97
74
59
55
54
54
54
54
53
52
52
52
53
56
58
59
59
59
59
60
65
71
76
76
74
74
73
74
85
94
98
97
93
85
90
115
145
163
161
135
88
39
8
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
64
79
85
86
86
89
89
87
80
66
57
53
48
43
39
27
17
8
2
1
1
1
1
1
1
1
1
1
1
3
22
53
74
83
83
72
65
61
59
57
56
55
54
52
50
48
46
46
45
45
45
46
47
48
48
49
49
49
49
49
49
48
48
47
47
47
47
48
48
49
52
54
55
57
58
60
61
61
61
59
54
50
47
43
40
37
37
37
38
38
39
39
39
39
39
39
39
39
40
40
41
42
44
50
62
82
92
97
99
99
99
99
98
98
96
78
49
35
32
38
72
85
89
88
83
69
66
67
78
105
176
197
204
200
189
155
122
93
70
57
51
50
49
49
48
48
49
51
53
55
58
58
59
58
58
57
56
55
55
55
55
55
55
55
55
54
52
51
51
51
50
51
51
51
52
53
53
53
53
53
54
54
54
54
54
54
54
55
55
55
56
58
59
59
60
59
56
51
45
40
36
36
37
38
41
44
45
46
48
52
75
105
128
142
144
137
113
88
68
57
55
54
54
54
54
53
53
53
53
54
58
59
60
60
59
59
60
65
71
76
76
73
72
71
72
82
93
97
97
93
85
88
111
141
163
162
143
98
48
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
62
78
83
84
85
87
89
89
85
76
64
60
56
52
45
33
19
9
2
1
1
1
1
1
1
1
1
1
2
5
30
62
79
85
84
71
65
61
59
56
52
51
49
47
46
46
45
45
45
45
47
48
48
49
49
50
51
51
51
51
49
49
48
47
47
47
47
48
49
50
54
57
59
60
61
62
62
63
63
62
60
55
50
46
44
40
38
38
38
38
39
39
38
38
37
37
37
38
38
39
40
41
42
43
45
63
81
92
97
99
99
99
99
98
98
91
63
44
34
35
60
80
88
89
87
73
67
66
71
85
156
190
203
203
197
169
136
105
79
61
52
50
49
49
48
48
49
50
52
54
58
59
60
60
59
58
57
57
56
56
55
55
55
55
55
54
53
52
51
51
50
50
50
50
51
52
52
53
53
53
54
54
54
55
55
55
55
55
56
57
58
59
59
59
59
56
51
45
41
38
37
37
39
41
44
46
46
47
51
59
90
119
136
144
143
130
102
79
62
55
55
54
54
54
54
54
54
54
54
56
59
60
60
60
59
59
61
66
72
75
75
72
70
70
70
80
91
96
97
94
85
87
108
136
163
163
150
108
57
15
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
60
75
81
82
82
85
88
89
88
83
72
66
62
58
52
39
21
9
2
1
1
1
1
1
1
1
1
1
3
9
43
72
84
86
85
71
65
62
59
56
50
48
47
46
46
46
46
46
46
47
49
49
50
51
51
52
52
52
52
51
48
47
47
47
47
48
50
52
55
58
60
61
61
62
62
63
63
64
64
64
63
61
57
51
47
44
42
41
41
40
39
39
38
38
37
37
37
37
38
38
39
40
41
42
42
44
55
72
88
97
98
98
99
98
98
96
82
59
41
38
47
71
84
88
88
80
70
66
67
72
120
169
196
206
204
186
157
125
95
71
55
52
50
49
49
48
48
49
51
53
57
59
60
61
61
60
60
59
58
57
57
57
57
56
56
55
54
53
52
51
50
50
49
49
49
49
50
50
51
52
52
53
53
53
54
55
55
56
57
57
58
58
58
57
53
47
43
40
39
38
39
41
43
45
46
47
48
51
61
78
112
133
142
143
139
112
86
68
57
55
55
55
55
54
54
54
55
55
56
59
61
61
61
60
60
60
62
67
73
75
74
70
69
68
69
77
89
95
96
94
85
86
104
131
162
164
156
119
69
21
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
57
73
79
80
80
82
86
88
88
86
78
73
69
65
59
45
25
10
2
1
1
1
1
1
1
1
1
1
5
14
52
77
86
87
85
71
65
62
60
56
50
48
46
46
45
46
46
47
48
49
50
50
51
51
52
52
51
51
49
48
47
46
47
48
49
53
55
57
58
59
60
61
61
62
62
63
63
63
63
63
63
63
60
57
52
46
44
43
42
41
40
40
39
38
37
37
37
38
38
38
39
40
41
41
42
42
46
55
69
88
97
98
98
98
98
97
90
70
50
40
44
63
78
87
88
83
73
67
66
68
98
149
184
202
206
196
172
142
110
82
60
54
51
49
49
48
48
49
50
52
56
58
60
61
62
62
61
60
60
59
58
58
58
58
57
57
55
54
52
51
50
49
49
49
48
48
49
49
50
50
51
51
52
52
53
54
54
55
56
56
56
55
52
49
46
43
41
40
40
40
42
44
45
46
47
48
51
60
76
97
127
138
142
138
129
96
75
62
56
55
55
55
55
55
55
55
56
57
58
60
62
62
61
60
60
60
62
68
73
74
73
70
68
68
68
76
88
94
95
93
84
86
102
128
161
165
159
126
79
27
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
55
71
77
78
77
78
85
88
88
87
81
77
74
70
65
48
26
11
2
1
1
1
1
1
1
1
1
2
7
18
60
80
88
89
86
72
66
63
61
58
51
48
46
45
45
46
46
47
48
50
50
51
51
51
51
51
50
49
47
46
46
48
49
51
54
56
58
59
59
60
60
60
60
60
60
60
60
60
61
61
62
62
61
59
55
50
47
45
43
42
41
40
39
38
38
38
38
39
40
40
40
40
41
41
42
42
43
46
55
70
90
96
98
98
98
98
94
79
60
43
44
56
72
85
87
85
77
69
65
65
82
127
167
196
206
203
185
159
128
96
68
58
52
50
49
48
48
48
49
50
54
57
59
61
62
63
62
62
61
60
59
59
59
59
59
58
57
55
54
52
50
49
49
48
47
47
47
47
48
48
49
49
50
51
51
52
52
53
52
52
50
49
46
44
43
41
41
41
42
43
44
46
46
47
48
50
59
75
94
115
136
140
138
130
113
81
66
58
55
55
55
55
55
55
56
56
57
59
60
62
62
62
61
60
60
60
63
69
73
74
72
69
68
68
68
75
87
93
94
93
84
85
100
125
160
166
161
133
88
33
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
52
68
74
75
74
75
82
87
88
88
84
81
77
73
68
48
25
10
2
1
1
1
1
1
1
1
1
3
9
23
66
83
90
90
87
73
66
63
62
60
53
48
46
46
45
46
46
47
48
49
50
50
51
50
50
49
47
47
46
47
48
51
53
56
57
58
58
58
58
58
57
56
55
55
54
54
54
55
55
56
58
59
59
59
57
53
49
46
44
43
42
41
40
39
39
39
40
41
42
42
42
42
42
42
43
43
42
43
45
52
77
90
96
98
98
98
97
86
69
48
45
52
67
81
87
86
80
72
66
64
71
104
147
185
204
207
196
175
146
113
78
64
55
51
50
49
48
48
48
49
52
55
58
61
62
63
63
63
62
62
61
60
60
60
60
59
58
57
55
54
51
50
49
48
47
46
46
45
45
46
46
47
47
48
48
48
48
48
47
46
45
43
42
42
42
41
42
43
44
45
46
47
47
48
50
58
75
94
113
130
140
138
131
117
95
68
59
56
55
55
55
56
56
56
57
58
59
61
62
63
63
62
61
60
60
60
64
69
73
73
71
69
68
68
68
75
86
92
94
92
84
84
98
123
159
167
163
138
96
40
6
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
49
64
70
71
71
72
79
85
88
88
85
82
79
74
67
45
22
8
2
1
1
1
1
1
1
1
1
3
10
26
72
86
91
91
88
74
67
64
63
61
55
50
47
46
45
46
46
47
48
49
50
49
49
48
47
46
46
46
47
50
54
56
57
58
58
58
57
55
54
52
49
48
47
47
46
46
46
47
47
48
51
52
54
56
56
55
51
48
46
44
43
42
41
40
40
40
42
43
44
44
44
44
45
45
45
44
43
43
43
44
57
78
91
97
98
98
97
91
77
54
46
49
62
77
86
86
83
75
67
63
66
84
123
167
200
208
203
189
165
133
92
72
60
53
50
49
49
48
48
48
50
53
56
60
62
63
64
64
63
63
62
61
61
61
60
60
59
58
57
55
53
51
49
48
47
45
44
44
43
42
42
42
42
42
42
42
42
41
41
41
41
41
41
41
42
43
44
45
46
47
47
48
48
50
54
75
97
115
129
138
139
132
118
99
76
59
56
55
55
56
56
56
57
58
59
60
61
63
63
63
63
63
61
60
60
61
66
70
73
73
70
68
68
68
68
74
85
91
93
91
83
84
96
121
158
168
165
143
104
47
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
46
58
64
65
65
66
74
81
86
87
85
81
77
69
58
35
16
5
1
1
1
1
1
1
1
1
1
4
14
33
76
87
92
92
90
76
68
64
63
63
59
54
49
46
45
45
45
46
46
46
46
46
46
46
46
47
50
53
55
57
58
57
57
55
52
49
47
45
44
43
43
42
42
42
43
43
43
43
43
43
44
45
46
47
47
48
48
48
46
45
43
43
42
42
42
43
43
44
44
44
44
45
46
48
52
51
49
46
43
43
44
57
75
90
97
98
98
94
83
60
47
48
58
71
85
86
85
79
71
64
63
70
96
134
178
207
208
202
187
161
118
91
71
59
52
49
49
49
48
48
48
50
53
56
59
63
64
64
64
64
64
63
63
63
62
62
61
60
58
56
54
52
50
49
47
45
44
43
42
42
41
40
40
40
40
40
40
40
40
41
42
42
44
45
46
47
47
47
47
48
49
50
54
64
82
110
124
133
137
138
128
112
93
75
60
56
55
56
56
56
57
58
59
60
61
63
64
64
64
64
64
62
61
60
60
62
67
71
73
73
69
68
68
68
68
74
84
90
91
90
83
83
95
119
157
169
166
149
113
56
10
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
43
53
58
60
60
60
66
75
81
83
82
78
71
61
48
25
11
3
1
1
1
1
1
1
1
1
2
6
16
37
79
88
92
92
91
79
69
64
63
63
61
58
54
49
47
46
46
46
46
46
46
46
47
48
51
54
56
57
57
57
54
51
49
47
45
44
43
42
42
42
42
42
42
42
42
42
43
43
43
43
44
44
45
45
46
46
46
45
45
44
43
43
43
43
43
44
44
44
45
45
45
45
46
49
55
58
57
53
48
44
43
48
62
80
94
98
97
94
84
63
47
48
56
68
83
86
85
81
74
66
62
66
81
110
155
195
205
207
198
180
140
110
86
68
57
51
50
49
48
48
48
48
50
52
55
59
62
63
64
65
64
64
64
64
64
64
63
62
61
59
56
54
52
50
48
46
45
44
43
42
41
41
41
41
41
41
42
43
44
45
46
46
47
47
47
48
48
48
48
49
52
60
73
90
109
128
133
136
136
130
109
91
75
63
57
55
56
56
57
58
59
60
61
62
63
64
65
65
64
64
63
62
61
61
61
63
69
72
73
72
69
68
68
68
69
74
84
89
91
89
82
82
94
118
156
169
167
152
118
62
12
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
41
48
52
53
53
51
56
63
70
73
72
67
59
48
35
14
6
2
1
1
1
1
1
1
1
1
2
7
18
39
81
89
93
93
91
81
70
65
63
63
63
61
58
55
52
49
48
48
48
49
50
51
53
54
56
57
56
55
53
50
47
46
44
43
42
42
41
41
41
41
42
42
42
42
42
42
43
43
43
43
43
44
44
45
45
45
45
45
44
44
44
44
44
44
44
44
45
45
45
45
46
47
49
52
58
65
65
63
57
49
44
45
53
67
87
96
96
94
85
64
47
48
54
66
81
86
86
83
77
69
63
63
71
91
129
178
198
207
206
196
163
132
104
81
64
54
51
50
49
48
48
48
48
50
52
56
59
61
63
64
64
64
64
64
64
64
64
64
63
62
59
58
56
54
52
50
48
47
46
45
45
44
44
44
45
45
46
46
47
47
48
48
48
48
48
48
48
49
51
56
67
83
99
114
127
136
136
133
125
112
89
74
64
57
55
56
57
57
58
59
61
62
63
64
65
65
65
65
64
64
63
62
61
61
61
65
70
72
73
72
68
68
68
68
69
73
83
88
89
88
81
82
93
118
156
170
168
155
124
68
14
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
40
43
45
46
46
42
43
47
51
53
53
48
40
30
20
5
3
1
1
1
1
1
1
1
1
1
2
7
19
42
83
90
93
93
91
83
72
65
62
62
63
63
62
60
58
54
53
53
53
53
55
57
58
58
57
55
52
49
46
44
42
42
41
41
41
41
41
41
41
41
42
42
42
42
43
43
43
43
43
43
44
44
44
44
45
45
45
45
45
44
44
44
44
45
45
45
45
46
47
48
49
53
56
60
65
73
73
72
68
59
46
45
47
56
74
92
93
92
83
63
47
47
53
64
80
86
86
84
79
72
64
62
65
77
103
156
184
202
209
206
184
156
126
99
76
59
54
51
50
49
48
48
48
48
49
52
55
57
60
62
63
64
64
64
64
64
64
64
64
63
62
60
59
57
56
54
52
51
50
50
49
49
49
49
49
50
49
49
49
49
49
49
48
49
49
49
52
55
62
72
92
109
122
132
136
137
132
122
107
90
70
62
58
56
56
57
58
59
60
61
63
64
65
66
66
66
65
65
64
64
63
61
61
61
61
67
71
72
72
71
68
68
68
69
69
73
82
87
88
87
81
81
93
117
156
170
169
157
128
74
17
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
38
39
39
39
39
31
29
27
26
24
22
17
12
7
4
1
1
1
1
1
1
1
1
1
1
2
3
8
21
46
84
91
93
93
91
85
75
67
61
61
61
62
62
62
61
60
60
59
59
59
59
59
58
55
50
45
43
42
42
41
41
41
41
41
41
41
41
41
42
42
42
43
43
44
44
45
45
45
45
45
45
45
45
45
45
45
45
45
45
46
46
46
46
46
46
47
47
48
50
52
56
62
68
75
79
82
83
82
80
74
52
47
46
49
57
82
87
86
78
59
45
46
52
63
79
86
86
85
81
75
67
63
63
68
81
129
165
191
206
212
202
179
151
121
93
68
59
54
51
50
49
48
48
48
48
49
50
52
55
58
61
62
63
64
64
64
64
64
64
63
62
61
61
60
59
57
56
56
55
54
53
53
52
52
51
51
50
50
49
49
49
49
49
49
50
52
59
70
84
101
123
131
135
136
137
133
119
102
84
68
59
57
56
57
57
59
60
61
62
63
65
66
67
67
67
66
65
65
64
64
62
61
61
61
62
69
72
73
72
70
68
68
68
69
70
73
81
86
87
86
80
81
93
117
156
171
169
160
132
79
20
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
37
36
34
32
30
25
22
18
15
13
8
7
5
3
3
2
2
1
1
1
1
1
2
2
3
3
5
11
24
47
85
91
93
93
92
86
77
68
61
57
58
59
61
61
61
61
60
59
58
56
49
44
42
42
42
41
41
41
41
40
40
41
41
41
41
42
43
45
46
48
50
52
54
55
56
58
58
58
58
58
58
57
56
55
53
52
51
51
50
50
50
50
51
52
53
56
59
62
66
71
80
86
92
98
105
106
103
99
95
92
73
56
48
47
49
59
66
67
63
52
44
44
51
63
79
87
87
85
82
78
70
65
62
62
65
94
131
165
191
207
212
202
184
158
129
93
74
61
54
51
49
49
48
48
48
48
48
48
49
50
52
54
56
57
59
60
61
61
62
62
61
61
60
59
58
57
56
56
55
54
53
52
52
51
50
50
49
49
49
49
49
50
52
57
64
81
97
112
125
133
135
136
136
132
124
101
83
71
63
59
57
57
58
59
60
61
63
64
65
67
68
68
68
67
67
66
65
65
64
63
62
61
61
62
65
71
72
72
72
70
68
68
69
70
71
73
80
84
86
85
80
80
92
117
156
171
170
162
137
86
23
8
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
37
35
33
30
28
24
21
18
14
12
9
8
7
6
5
4
3
3
2
2
2
2
2
3
3
4
5
11
24
47
84
90
93
93
92
87
78
69
61
56
54
55
56
57
57
55
54
51
49
47
45
44
43
43
42
40
40
40
40
40
42
43
45
47
49
53
55
58
60
62
65
67
69
70
72
73
74
75
75
76
76
75
75
75
74
73
72
71
71
70
71
71
73
74
77
80
84
89
94
99
107
112
117
121
124
125
123
118
111
105
91
69
55
48
48
51
53
52
49
43
41
42
51
65
80
87
87
86
83
79
72
68
64
61
61
74
104
138
170
195
210
208
199
182
158
121
96
77
64
56
52
51
50
49
49
48
48
48
48
49
50
50
51
52
53
55
55
56
56
56
56
56
56
55
54
54
53
52
52
51
50
50
50
49
49
49
49
50
51
52
58
65
74
86
100
116
124
130
133
136
135
130
121
109
93
76
68
63
59
58
58
59
60
61
62
64
66
67
68
68
69
68
68
67
67
66
65
64
64
63
62
62
62
64
67
72
72
72
71
69
69
69
70
71
71
73
79
83
85
84
79
80
92
117
157
172
171
163
140
91
26
9
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
37
35
32
29
27
23
21
19
17
16
14
13
12
11
10
7
6
5
4
3
3
3
3
3
3
4
6
13
26
49
83
90
93
93
92
88
80
71
62
55
52
51
51
52
52
52
51
51
50
49
47
44
41
40
39
39
40
41
43
46
49
53
56
59
61
65
67
70
72
74
77
78
80
82
83
85
86
87
88
89
90
90
91
92
92
92
93
93
93
94
95
97
99
101
104
109
113
117
122
127
133
136
139
141
143
142
141
136
129
121
109
86
66
52
49
48
47
44
40
38
38
41
52
67
81
87
88
86
84
80
74
70
66
63
59
64
82
110
143
175
202
208
207
198
183
150
124
101
82
67
56
53
52
51
50
49
49
49
49
49
49
49
49
50
50
51
51
51
51
52
52
51
51
51
51
51
50
50
50
50
49
49
49
50
51
53
56
60
66
73
86
96
106
116
125
133
135
135
134
130
120
109
96
84
73
64
61
59
59
59
60
61
62
64
65
67
68
69
69
69
69
68
68
67
66
65
65
64
63
63
62
62
64
66
70
72
72
72
71
69
69
70
71
72
72
74
78
82
83
83
79
80
92
118
158
172
171
164
142
95
29
10
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
36
34
31
28
27
24
23
22
21
21
20
20
19
17
16
14
11
9
7
5
4
3
3
4
4
5
8
15
29
52
83
90
93
93
93
90
83
73
63
55
50
49
48
48
49
50
51
51
51
49
46
41
37
36
37
39
42
45
49
54
60
64
67
70
73
75
77
79
81
82
84
86
88
89
91
93
94
96
97
99
100
102
103
105
106
108
110
111
113
115
117
120
123
126
130
135
139
143
146
150
153
156
157
159
159
158
156
153
147
137
126
105
81
60
50
48
45
40
36
35
36
41
55
70
83
88
88
87
85
82
77
72
69
65
60
60
66
85
114
148
186
201
208
208
201
178
155
130
106
85
66
59
55
52
51
50
50
49
49
49
49
49
49
49
49
49
48
48
48
48
49
49
49
49
49
49
49
50
50
51
52
54
56
59
63
70
77
85
94
105
119
127
134
137
138
136
133
128
122
113
95
82
72
66
62
60
60
59
60
61
62
64
65
67
68
70
70
70
70
70
69
68
68
67
66
65
65
64
63
63
63
64
66
68
71
72
72
71
70
69
69
71
73
73
74
74
78
81
82
81
79
80
92
119
160
172
172
165
144
100
31
11
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
36
34
31
28
27
25
25
24
24
24
24
24
24
24
23
22
20
17
13
8
5
4
4
4
5
7
10
18
33
56
83
89
92
93
93
91
86
77
65
56
51
49
48
48
50
52
52
48
41
34
32
33
34
36
38
43
49
54
60
64
69
72
74
76
78
80
82
83
85
86
89
91
92
94
96
98
100
102
103
105
107
109
111
112
114
117
119
121
124
126
131
135
139
143
147
152
155
158
161
164
167
168
169
170
171
171
169
167
163
154
141
126
100
73
52
49
45
40
36
35
36
44
58
73
84
88
88
87
86
83
79
74
71
67
62
60
59
66
84
115
162
188
204
211
212
201
184
162
137
112
84
69
60
55
53
51
51
50
50
49
49
49
48
48
48
48
48
48
48
48
48
49
49
49
50
50
51
52
53
55
60
66
74
84
95
110
121
130
136
140
141
142
141
140
137
131
120
106
92
78
67
64
62
61
60
60
60
61
62
63
66
68
69
70
71
71
71
71
70
70
69
68
68
67
66
65
65
64
64
64
64
66
68
71
72
72
72
71
70
70
70
73
74
75
75
75
77
79
81
80
78
79
93
120
161
173
172
166
146
103
34
12
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
36
33
30
27
27
25
25
25
25
25
25
26
25
25
25
24
24
24
24
23
17
12
9
8
12
20
27
35
44
58
79
87
91
91
91
91
89
84
75
63
54
53
53
54
54
51
43
36
31
31
33
35
36
37
40
46
52
58
64
69
74
77
80
82
85
88
91
93
96
99
102
105
107
110
112
115
117
119
121
123
125
127
129
131
133
136
139
142
145
149
155
159
162
165
168
171
172
174
175
176
177
178
178
179
179
179
178
178
176
173
161
150
130
102
68
52
48
42
37
35
37
47
61
75
85
89
90
89
87
84
80
77
74
71
67
62
60
59
62
72
113
148
178
200
210
212
207
198
184
164
134
112
92
75
64
57
55
53
52
51
51
50
50
50
50
50
51
51
52
53
55
58
61
65
71
80
89
98
107
117
129
135
140
142
144
145
146
146
145
143
139
133
123
111
98
81
73
68
64
63
62
61
61
61
61
62
63
65
67
69
71
72
72
72
72
72
71
71
70
70
69
68
68
67
66
65
65
65
65
65
67
69
71
72
72
72
71
71
70
70
72
76
77
78
77
76
77
78
79
79
78
79
93
121
163
174
173
167
148
107
37
14
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
37
33
30
27
26
25
25
26
26
26
27
27
28
27
27
25
24
24
24
24
23
22
21
20
22
35
45
52
57
63
77
84
89
90
90
90
89
86
81
72
63
60
58
57
54
44
36
32
32
33
35
36
37
38
40
48
55
63
69
75
81
84
88
91
94
98
101
104
107
110
114
116
119
121
124
127
129
131
134
136
139
142
145
148
152
157
161
165
168
171
174
175
176
177
177
178
179
179
179
180
180
180
180
180
180
180
180
180
180
179
173
164
150
126
92
59
51
45
41
38
39
47
61
75
86
91
91
91
89
86
81
77
75
73
70
66
62
60
59
62
80
109
140
170
195
209
210
207
201
193
175
157
139
121
104
87
78
71
67
64
62
62
63
64
66
70
74
79
85
91
99
106
112
118
123
128
131
135
138
141
143
144
144
144
143
139
134
127
120
112
100
92
85
79
74
68
65
63
62
62
61
61
62
63
64
66
68
70
71
72
73
73
73
73
73
72
72
71
71
70
69
68
68
67
67
66
66
67
67
68
70
71
72
72
72
72
71
71
71
71
74
79
80
80
79
77
77
78
78
78
77
79
94
123
165
174
173
168
150
110
39
14
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
38
34
30
27
26
25
25
26
26
27
27
27
28
29
29
27
25
24
24
24
24
25
27
31
35
46
57
66
72
75
81
85
87
88
89
89
89
88
85
79
70
66
62
56
49
36
32
31
32
34
36
37
38
39
41
50
59
67
75
81
87
90
94
98
101
105
109
112
115
118
121
124
126
129
131
135
137
140
143
146
150
154
158
163
167
172
175
178
179
181
181
181
181
181
181
181
181
181
181
181
181
181
181
181
181
181
181
181
181
181
179
174
165
148
118
75
59
49
43
41
41
48
60
74
85
92
93
92
90
87
82
78
76
74
72
69
65
62
59
58
64
80
103
131
162
191
202
207
207
205
196
187
175
163
149
132
122
114
108
104
101
101
103
104
107
112
116
120
124
128
133
136
139
141
143
144
144
144
142
140
135
130
126
121
116
108
102
96
90
84
76
71
68
66
64
63
62
62
62
62
63
64
65
67
69
71
72
73
74
74
74
74
74
74
74
73
72
72
71
71
70
69
69
68
68
68
68
69
69
70
71
72
72
72
72
72
71
71
71
72
77
82
84
84
82
77
77
77
77
77
77
78
95
125
166
175
174
168
151
113
41
15
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
40
36
32
28
26
25
25
25
26
26
26
26
27
29
30
29
26
24
23
23
23
24
28
34
39
47
59
71
81
85
86
86
87
87
88
88
88
87
86
83
76
69
62
53
42
32
31
31
32
35
37
38
39
40
43
54
64
72
79
85
91
95
99
103
107
112
115
118
121
124
128
131
133
136
139
143
146
149
152
155
161
166
171
175
178
182
183
183
183
183
182
182
182
181
181
181
181
182
182
182
182
182
182
182
182
181
181
181
181
181
181
180
176
165
145
98
72
55
47
44
44
49
59
73
85
93
94
94
92
89
84
80
77
75
73
71
69
65
61
58
59
63
75
94
120
158
181
195
203
205
204
201
197
191
184
173
165
158
153
148
145
145
145
147
148
150
151
152
151
151
149
146
144
140
137
133
130
126
121
116
108
102
96
89
83
76
72
69
67
65
64
64
63
63
63
63
63
64
64
65
67
69
70
72
73
75
75
76
76
76
75
75
75
75
74
74
73
72
72
72
71
70
70
70
70
70
70
71
71
72
72
72
72
72
72
72
71
71
72
73
81
86
88
87
86
78
77
76
76
76
76
78
96
127
168
176
174
169
152
115
43
16
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
45
42
36
30
26
24
24
24
24
25
24
24
26
28
29
29
26
24
23
23
24
24
23
23
24
32
48
64
77
85
89
89
89
88
88
87
86
86
85
83
77
69
59
48
36
31
31
31
33
36
39
40
41
42
44
57
67
76
83
89
95
100
105
109
113
118
122
125
128
132
136
139
142
145
148
153
157
160
165
169
177
181
184
186
186
185
184
184
183
182
181
181
181
181
181
182
182
183
183
184
185
185
184
184
183
182
181
181
181
181
182
182
181
178
168
127
91
66
51
46
46
49
58
71
84
93
95
95
94
91
86
82
79
76
73
72
71
68
65
60
59
59
62
68
81
112
141
167
188
199
203
201
199
197
194
190
187
183
180
176
173
170
167
165
162
159
157
154
151
148
142
135
127
119
110
98
90
83
78
73
69
68
66
65
65
64
64
64
64
63
63
63
63
63
64
65
66
68
69
71
73
74
75
76
77
77
77
77
77
77
76
76
75
75
75
74
73
73
73
72
72
72
72
72
72
72
72
72
72
72
72
72
72
72
72
72
72
72
73
75
86
90
92
92
90
80
77
76
76
76
76
78
97
129
170
176
174
169
152
117
45
17
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
26
33
36
37
36
34
32
30
30
30
32
32
32
30
29
23
22
22
22
23
26
28
28
28
26
25
27
33
44
59
77
83
87
89
89
88
87
86
84
82
77
69
59
48
37
32
32
33
36
41
42
42
42
44
49
65
76
85
92
98
106
111
116
120
125
130
134
137
141
144
149
153
156
160
163
169
173
179
184
189
191
190
190
188
187
183
181
180
180
180
180
181
182
184
186
189
191
194
196
198
199
199
199
198
196
192
188
184
182
181
181
181
182
182
181
166
131
95
66
50
47
48
54
65
80
93
96
97
97
94
89
85
81
78
75
72
72
71
69
66
61
60
59
60
61
68
84
105
128
151
175
185
191
192
192
189
186
183
179
176
171
167
163
158
152
141
131
120
108
97
84
78
73
70
67
66
65
65
64
64
64
64
64
64
65
65
65
65
66
66
67
69
70
71
73
74
75
76
77
77
78
79
79
79
79
79
78
78
77
77
76
76
75
75
75
74
74
74
74
74
73
73
73
73
73
72
72
72
72
72
72
72
72
72
72
72
73
74
77
81
92
97
99
98
96
84
78
76
75
75
75
78
99
132
172
177
175
169
153
119
47
18
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
16
18
19
20
21
21
21
21
22
22
22
22
22
22
21
21
21
23
25
29
34
36
36
36
35
30
29
29
33
37
46
53
61
70
79
86
87
86
85
83
78
72
63
52
41
37
38
41
44
45
43
42
43
47
56
73
83
91
98
104
112
117
121
126
130
136
140
144
148
151
156
160
163
168
172
179
184
188
191
192
192
189
186
183
180
179
179
179
180
182
185
188
190
193
196
200
203
205
207
209
210
211
211
210
209
206
202
197
192
186
182
181
182
182
182
177
157
126
92
62
49
49
52
60
74
90
96
99
99
98
92
87
83
79
76
73
72
72
71
69
65
63
61
59
59
61
67
76
89
104
125
138
149
158
164
166
166
163
159
154
145
136
127
117
107
96
89
83
79
75
71
70
68
68
67
67
68
69
69
70
71
72
72
73
74
74
75
75
76
77
77
78
78
79
79
80
80
80
80
81
80
80
80
80
79
79
78
78
77
77
76
76
75
75
75
75
74
74
74
74
74
73
73
73
73
73
73
73
73
73
73
72
72
72
73
73
75
78
81
86
97
103
104
104
101
87
80
76
75
74
75
78
100
134
173
178
175
170
154
120
48
18
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
19
19
19
19
19
20
20
20
20
20
20
20
21
22
23
25
27
30
34
37
41
42
43
42
42
38
35
33
32
31
34
37
42
49
57
65
72
78
82
82
80
76
70
63
53
50
51
52
51
47
43
42
46
53
66
81
89
97
104
110
117
122
127
132
136
143
147
151
155
159
164
167
172
178
183
189
192
193
194
192
187
182
179
178
176
178
180
183
185
188
191
194
197
200
203
207
210
212
214
215
217
217
217
217
216
215
212
208
203
197
188
184
182
182
182
181
173
153
122
86
56
51
50
55
65
85
93
98
100
100
96
90
85
81
77
74
73
72
71
70
68
66
64
61
59
58
60
63
68
75
87
97
106
115
122
127
127
125
121
116
107
99
92
86
80
74
72
70
70
70
71
72
73
74
75
76
77
77
78
79
79
80
80
81
81
81
82
82
82
82
83
83
83
83
83
82
82
82
82
82
81
81
80
80
79
78
78
78
77
77
76
76
76
75
75
75
75
75
74
74
74
74
73
73
73
73
73
73
73
73
73
73
73
73
74
75
78
82
87
92
103
108
110
110
107
91
82
77
75
74
74
78
101
136
175
179
175
170
154
121
49
19
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
29
28
28
27
26
26
25
25
25
25
25
26
27
28
30
33
37
40
42
45
46
47
47
47
47
46
43
40
38
35
34
34
33
34
37
42
51
62
70
73
75
76
74
70
64
61
59
57
52
46
42
45
52
63
76
89
96
103
110
116
123
129
134
139
144
150
154
159
163
167
172
177
183
189
193
196
196
194
191
186
179
175
174
175
177
181
184
188
191
193
196
199
202
205
209
213
215
216
218
219
219
220
220
220
220
220
219
216
213
209
199
190
185
183
182
182
181
171
151
118
72
58
51
52
57
76
88
96
101
102
100
95
89
83
78
75
74
73
72
71
70
69
67
64
61
59
59
59
60
61
64
68
73
78
84
88
88
86
84
80
74
71
70
69
69
70
71
72
73
75
77
79
81
82
83
84
85
85
85
85
85
85
85
85
85
85
85
85
85
85
85
85
84
84
84
83
83
82
82
81
81
80
80
79
79
78
78
77
77
77
76
76
76
76
75
75
75
75
75
75
74
74
74
74
73
73
73
73
73
73
74
74
74
75
76
78
82
87
93
98
109
114
116
116
113
96
84
78
75
74
74
78
103
139
176
179
175
170
154
122
50
19
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
38
38
38
37
37
36
36
35
35
35
35
36
37
39
40
43
45
46
47
48
50
50
50
50
50
49
48
46
44
42
39
37
35
34
34
34
35
37
42
48
59
65
66
65
63
58
52
46
42
40
42
51
64
76
86
96
103
110
117
123
130
136
141
147
152
158
163
167
171
175
182
188
194
198
199
198
195
189
182
173
172
172
174
177
183
189
190
192
193
194
198
201
205
210
213
217
219
220
220
221
221
221
221
221
221
221
222
221
219
216
210
202
193
185
183
183
182
181
172
153
101
72
56
50
51
63
80
92
99
103
103
100
94
87
80
76
74
73
72
71
71
71
70
68
65
61
59
59
59
60
60
61
62
63
64
65
65
65
65
65
65
66
67
69
71
74
77
79
81
83
84
85
85
86
86
86
86
86
86
86
86
86
86
86
86
85
85
85
85
85
85
85
84
84
84
83
82
82
81
80
80
79
79
78
78
78
77
77
77
77
76
76
76
76
76
76
76
76
76
75
75
75
75
75
75
75
75
75
75
75
75
76
77
78
79
83
88
94
100
105
114
120
122
122
120
101
87
79
75
73
74
78
104
141
177
180
176
170
155
122
50
19
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
51
51
50
50
50
49
49
49
49
49
49
49
50
51
52
54
55
56
57
58
60
60
60
60
60
59
57
56
54
52
50
48
46
44
42
39
38
37
37
37
38
39
39
39
39
39
39
40
42
46
61
72
81
89
95
106
113
120
126
131
139
145
150
155
161
167
171
176
180
186
195
200
201
201
199
190
177
170
167
168
172
179
184
188
190
191
191
192
193
196
203
209
213
216
218
219
219
219
218
216
213
213
213
214
216
219
221
222
223
223
219
215
207
198
188
183
183
183
181
177
147
110
79
58
50
51
62
77
91
100
104
104
102
97
90
80
76
74
72
72
71
71
71
70
69
67
64
63
61
61
61
60
60
60
61
61
62
63
64
65
68
71
73
75
76
77
76
75
73
70
67
66
66
67
68
69
71
74
77
79
81
81
82
82
83
83
83
83
83
83
82
82
82
82
81
81
80
80
80
79
79
79
78
78
78
78
78
78
78
79
79
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
79
80
80
80
81
82
83
84
87
92
97
103
109
113
121
127
129
130
128
110
93
82
76
73
74
79
106
144
178
180
176
170
155
123
51
20
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
60
60
60
60
59
59
59
59
58
58
59
59
59
60
61
62
63
64
65
65
66
67
67
67
66
65
64
63
61
60
58
56
55
53
51
49
48
46
45
43
43
43
43
43
44
46
48
52
58
65
76
83
90
97
104
114
121
127
132
138
146
152
157
162
168
174
179
184
190
197
201
202
201
196
186
171
168
167
168
173
183
186
188
189
190
189
189
191
194
200
207
211
214
216
217
215
212
208
203
199
196
194
195
196
198
203
209
215
219
222
222
220
216
208
199
187
184
183
182
181
169
141
109
80
58
50
54
63
76
92
102
103
103
101
97
88
82
77
74
72
71
71
71
70
70
69
68
66
65
64
62
61
61
61
61
62
63
65
67
70
73
74
74
73
70
65
61
57
54
52
50
49
48
47
47
49
50
52
54
56
62
66
71
74
77
79
80
81
81
81
81
81
81
81
81
80
80
80
80
79
79
79
80
80
81
82
82
83
84
85
86
86
86
87
87
87
87
88
88
87
87
87
87
86
86
86
86
86
86
86
87
88
89
91
95
99
105
110
115
118
124
131
134
135
134
117
98
84
77
73
74
79
108
145
179
180
176
170
155
123
51
20
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
70
70
69
69
69
69
68
68
68
68
68
68
69
69
70
71
71
72
72
73
73
73
73
73
73
72
71
70
69
67
65
64
63
61
60
58
57
56
55
53
52
52
52
53
55
58
62
66
72
78
86
92
99
106
113
122
128
134
139
145
153
159
164
170
174
181
187
193
198
203
203
200
191
181
171
163
165
170
176
182
188
189
189
188
186
186
187
191
196
202
209
211
213
213
211
206
200
195
190
186
184
184
185
186
189
194
198
204
210
216
222
223
221
217
209
196
188
185
183
183
179
163
138
107
77
55
52
54
63
77
95
100
102
102
100
95
89
84
79
75
72
72
71
71
70
70
69
68
67
66
64
63
62
62
63
65
67
69
71
73
74
72
69
65
61
57
55
53
53
52
52
53
53
52
51
49
47
45
44
44
47
50
54
57
61
67
70
74
76
78
79
80
80
80
80
80
80
80
80
81
81
82
83
84
86
88
89
90
91
92
93
94
94
94
95
95
95
95
95
95
95
95
94
94
94
93
93
93
93
93
94
95
97
99
102
106
111
115
119
122
127
134
138
140
139
125
103
88
78
74
74
80
109
147
180
181
176
170
154
123
51
20
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
79
79
79
79
79
78
78
78
78
78
78
78
78
78
79
79
79
80
80
80
81
81
80
80
80
79
78
77
76
75
73
72
71
70
69
68
67
66
65
64
64
64
65
66
68
72
75
80
84
89
95
102
109
115
122
129
135
141
146
152
160
166
171
177
181
189
195
201
205
205
200
190
177
165
160
163
170
178
185
189
189
188
186
184
183
183
186
191
198
204
208
210
210
208
203
195
189
184
180
178
178
180
181
183
186
189
192
195
200
207
218
222
223
222
218
206
195
188
185
184
183
178
162
137
105
69
57
51
53
61
83
94
99
101
101
99
95
91
86
81
76
74
73
72
71
71
70
70
69
68
66
66
66
66
67
70
72
74
74
74
71
66
61
57
54
53
54
55
57
59
62
64
66
67
66
62
58
53
48
44
41
40
40
42
45
51
56
61
65
69
72
74
76
77
78
79
80
80
81
82
83
85
87
89
91
94
95
97
98
99
100
101
101
102
102
103
103
103
103
103
102
102
102
102
101
101
101
100
101
101
101
102
104
106
109
113
116
120
123
125
129
137
142
144
144
133
110
92
79
74
74
80
110
149
180
181
176
169
154
123
50
19
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
89
89
88
88
88
88
88
87
87
87
87
87
87
87
87
88
88
88
88
88
88
88
88
87
87
86
85
84
83
82
81
80
79
78
77
76
76
75
75
75
75
75
76
78
80
83
86
90
94
99
107
113
119
124
129
137
142
148
153
159
167
173
178
183
188
198
204
206
206
204
188
168
159
157
159
171
182
188
190
189
187
183
181
180
180
181
185
192
198
204
206
206
205
201
196
187
182
178
176
176
177
179
181
183
186
189
192
195
198
200
209
217
222
224
223
216
205
195
187
184
184
183
177
163
137
93
70
55
49
51
64
82
94
99
99
98
96
94
92
88
83
80
76
74
73
72
71
71
71
70
70
71
72
73
75
76
76
75
74
71
62
56
53
52
52
53
53
55
56
59
63
67
72
76
80
82
80
75
68
59
48
42
39
37
37
38
39
42
45
50
57
62
66
70
73
76
78
80
81
82
84
87
89
92
95
98
99
101
102
103
104
105
106
107
108
108
109
109
109
109
109
109
108
108
108
108
107
107
107
108
108
109
110
112
114
117
119
122
125
128
131
139
145
149
149
141
117
97
81
75
75
81
111
150
181
181
176
170
154
122
50
19
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
101
101
101
101
101
101
100
100
100
100
100
99
99
99
99
99
99
99
99
99
98
98
98
97
97
96
95
94
93
92
91
90
90
89
88
88
87
87
87
87
88
89
90
92
94
98
101
105
109
113
119
124
129
134
140
147
152
157
163
169
177
182
187
193
200
207
208
206
198
182
154
153
155
161
176
189
190
190
188
183
178
178
179
179
179
181
186
192
199
202
203
203
200
195
187
178
174
172
172
174
178
182
187
192
195
198
199
199
200
201
204
209
216
222
224
223
217
207
197
187
184
184
183
179
170
135
104
78
59
49
50
59
74
88
96
95
94
94
92
91
89
87
85
83
81
79
78
78
77
77
77
78
78
78
78
77
75
71
65
57
52
51
51
52
52
52
53
53
53
53
57
63
71
78
85
91
92
91
87
81
71
63
56
49
44
39
38
38
38
39
40
42
45
49
53
60
66
71
75
79
83
86
89
92
95
99
100
102
103
104
106
107
108
109
110
111
112
112
112
113
113
113
113
113
113
112
112
113
113
113
114
115
115
116
118
120
121
124
127
130
133
141
148
153
154
149
128
105
85
76
75
82
113
152
182
182
176
170
154
121
48
19
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
111
111
111
110
110
110
110
110
109
109
109
109
108
108
108
108
107
107
107
107
106
106
105
105
104
103
103
102
101
100
99
98
98
97
96
96
96
96
96
97
98
99
100
102
104
108
111
114
118
122
128
133
138
143
147
154
159
164
170
176
183
188
194
201
207
208
206
193
176
157
150
154
165
178
187
190
189
186
181
178
177
177
177
178
178
180
187
193
198
201
202
201
196
189
180
173
171
171
174
178
188
196
202
207
211
213
213
212
210
207
206
209
213
218
223
225
222
215
206
195
186
185
184
183
179
160
133
105
80
61
49
52
59
71
84
92
92
91
90
88
88
87
86
85
83
82
81
81
80
80
80
80
79
79
78
74
69
63
57
52
51
51
51
52
52
52
52
53
53
53
56
61
68
77
87
96
100
100
98
93
84
76
68
61
54
48
45
42
41
39
40
41
43
46
49
55
61
66
71
76
81
85
89
92
95
99
101
102
104
105
107
108
110
111
112
113
113
114
114
114
114
114
114
114
114
114
114
114
114
115
115
116
117
118
119
121
122
125
129
132
136
142
150
156
158
155
136
112
89
77
76
82
114
153
182
182
176
170
153
119
47
18
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
120
120
120
120
120
120
119
119
119
119
118
118
118
117
117
116
116
116
115
115
114
114
113
112
112
111
110
109
108
108
107
106
105
105
105
104
104
105
105
106
107
108
110
112
114
117
121
124
128
132
137
141
146
150
155
161
166
171
177
182
189
196
202
207
209
206
191
173
157
147
153
166
178
187
191
189
185
180
177
176
176
176
177
178
178
181
188
194
198
200
200
197
191
183
175
171
171
173
179
187
200
208
214
219
222
223
223
221
220
217
212
211
213
216
221
225
224
220
214
204
191
187
185
184
183
175
157
133
106
81
58
52
52
58
69
82
87
87
86
85
84
84
84
84
84
83
82
82
81
81
81
80
78
77
74
67
61
56
52
50
50
50
51
51
51
51
51
52
53
53
55
60
67
77
87
100
105
107
106
103
94
86
78
70
63
56
52
48
46
44
43
43
43
45
48
54
59
64
69
74
80
85
89
92
96
99
102
104
106
107
110
111
113
114
115
116
117
117
117
117
117
117
116
116
116
116
116
116
116
116
117
118
118
119
120
123
124
127
131
135
139
144
152
159
162
159
145
120
94
78
77
83
115
153
182
182
177
170
153
118
46
17
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
130
130
130
129
129
129
129
129
128
128
127
127
127
126
126
125
125
124
123
123
122
121
121
120
119
118
117
117
116
115
114
114
113
113
113
113
113
113
114
114
116
117
119
121
123
127
130
133
137
140
145
149
153
157
162
168
173
178
184
188
196
203
208
210
208
194
169
153
145
149
164
180
188
190
189
184
179
176
175
175
176
177
177
178
179
182
188
194
197
198
197
193
186
178
171
170
172
178
187
199
212
219
224
226
228
228
228
228
227
225
220
216
215
216
219
224
225
224
220
212
198
190
187
185
185
183
174
157
134
107
76
60
53
52
55
69
77
81
82
81
79
79
79
80
80
81
81
81
81
80
79
77
75
71
67
58
53
51
49
49
50
50
49
49
48
49
50
52
53
53
55
59
67
77
88
102
109
113
113
111
102
94
86
78
71
63
58
55
52
50
48
47
47
48
50
55
60
65
69
74
80
85
89
93
97
101
103
106
108
110
113
115
116
118
119
120
121
121
121
121
120
120
120
119
119
118
118
118
118
118
119
120
120
121
123
126
127
130
133
138
143
147
154
161
165
164
152
128
100
80
78
84
115
154
183
182
177
170
152
116
44
17
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
139
139
139
139
139
139
138
138
138
137
137
136
136
135
135
134
133
132
132
131
130
129
128
128
127
126
125
124
123
123
122
122
121
121
121
121
121
121
122
123
125
126
128
130
132
136
139
142
145
149
153
157
161
165
168
174
180
185
190
194
205
210
211
209
204
164
147
142
146
158
185
190
191
189
185
177
175
175
175
175
176
178
179
180
181
183
188
193
196
196
195
190
182
174
169
170
174
184
197
210
222
226
228
229
229
230
230
229
229
229
228
223
220
218
219
223
225
225
224
219
206
196
189
186
185
185
183
175
159
136
101
78
62
53
50
55
62
70
75
75
73
71
70
70
72
75
77
78
78
78
75
72
67
62
56
50
49
48
48
49
49
48
47
46
46
46
49
51
52
53
55
59
67
78
90
105
113
118
119
117
109
102
94
86
78
69
64
60
57
55
54
54
55
56
57
60
63
67
71
75
81
86
90
94
98
103
105
108
111
113
116
118
120
121
123
124
125
125
125
125
125
125
124
124
123
123
122
122
122
122
122
123
124
125
126
129
131
134
137
142
147
150
156
163
167
167
160
136
106
82
80
85
116
155
183
182
177
170
152
114
43
16
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
152
152
152
152
151
151
151
151
150
150
149
149
148
147
147
145
145
144
143
142
141
140
139
138
137
136
135
134
133
133
132
132
131
131
131
131
132
132
133
134
136
138
140
142
144
147
150
153
156
159
163
167
170
173
177
184
188
193
198
206
212
212
208
194
162
138
138
148
165
186
191
190
187
182
176
174
174
175
176
179
182
183
183
183
183
184
188
192
194
195
193
185
176
170
169
171
182
197
211
223
229
229
229
230
230
230
230
230
230
230
229
228
226
223
221
223
225
226
226
224
216
206
196
189
186
186
186
184
179
168
141
116
92
72
57
49
50
53
57
64
68
67
65
62
60
57
58
59
60
60
59
58
55
52
49
47
47
47
47
47
46
44
43
43
43
45
48
51
53
53
55
61
70
81
93
109
117
123
125
124
117
109
101
93
85
76
70
66
63
62
61
62
63
64
66
69
71
73
76
79
84
89
94
98
102
106
109
112
114
117
120
122
123
125
126
127
128
128
129
129
130
130
130
130
129
129
129
129
129
129
130
130
131
132
134
137
139
142
144
149
154
156
160
165
170
170
166
146
116
86
82
88
118
155
183
182
177
169
150
111
40
15
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
161
161
161
161
161
161
160
160
160
159
159
158
157
156
156
154
153
152
151
150
149
148
147
146
144
143
142
141
141
140
139
139
139
139
139
139
139
140
141
142
144
146
148
150
152
156
158
161
164
166
170
173
176
180
184
190
194
199
206
212
213
209
191
165
141
136
148
165
181
190
190
186
181
177
175
174
175
178
182
186
187
187
185
184
183
184
188
191
193
193
191
181
173
169
168
174
190
206
219
226
229
229
229
229
229
229
229
229
229
230
229
229
228
226
224
224
225
226
226
225
220
212
203
194
188
186
186
186
184
180
163
144
122
100
79
60
54
52
53
56
59
60
60
59
58
55
53
52
50
49
48
48
47
46
45
45
46
46
46
45
43
42
41
41
42
46
49
51
53
54
56
63
73
85
97
113
121
127
130
129
121
113
105
96
87
78
73
69
67
66
66
67
68
70
71
73
74
76
79
82
88
93
97
101
105
109
112
114
117
120
122
124
126
127
128
130
130
131
131
131
132
132
132
132
132
133
133
134
134
135
135
137
138
139
140
143
146
149
151
155
160
162
164
168
172
172
170
152
124
91
85
90
119
156
183
182
177
169
149
108
38
14
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
171
171
171
171
170
170
170
170
169
169
168
167
166
166
165
163
162
161
160
158
157
156
154
153
152
151
150
149
148
148
147
147
146
146
146
147
147
148
149
150
152
154
156
158
161
164
166
169
171
173
177
180
183
186
191
195
200
206
212
214
211
193
167
144
133
144
164
180
189
190
186
181
177
175
174
175
179
185
190
193
193
192
188
185
184
184
188
190
192
192
189
178
171
168
168
178
198
213
224
228
229
229
229
229
229
229
229
229
229
229
229
229
229
228
226
225
226
226
226
226
223
218
209
200
192
187
187
187
186
185
178
165
148
128
107
82
68
59
54
53
54
56
56
57
56
54
52
49
47
46
44
43
43
42
42
43
43
43
43
42
41
41
41
41
43
47
50
52
53
54
58
67
77
89
102
117
127
133
135
134
125
116
107
98
89
80
75
72
70
70
70
71
72
74
75
76
77
79
82
86
92
97
101
104
107
112
115
117
120
123
125
127
129
130
132
133
133
134
134
134
135
135
135
135
135
136
137
138
138
139
141
142
143
145
147
150
153
155
158
161
166
167
169
171
174
174
172
158
132
96
87
92
121
157
183
182
177
169
147
105
36
13
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
180
180
180
180
180
180
180
179
179
178
177
177
176
175
174
172
171
169
168
167
165
163
162
161
159
158
157
156
155
155
154
154
154
154
154
154
155
155
157
158
160
162
164
166
168
171
174
176
178
180
184
186
189
193
196
201
207
212
215
214
201
169
145
133
136
161
180
189
190
188
180
177
175
174
175
179
186
193
199
200
200
197
191
185
183
184
187
190
191
191
186
175
169
167
168
184
205
219
227
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
228
227
227
227
227
227
225
221
215
207
198
189
188
187
187
187
186
180
169
155
136
109
91
75
64
57
54
53
54
54
54
54
52
50
48
46
44
42
41
40
40
40
41
41
41
41
40
41
41
42
45
49
52
53
54
55
61
72
83
94
106
123
133
139
141
138
129
118
108
98
89
80
76
74
73
73
73
74
75
76
77
78
80
83
86
91
96
100
104
107
110
115
118
121
123
126
129
131
132
134
135
136
137
137
138
138
138
138
138
138
139
140
140
141
142
144
145
147
149
150
152
156
159
161
164
167
171
173
174
175
176
176
175
163
140
102
91
95
123
158
183
182
177
168
146
101
33
12
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
190
190
190
190
190
189
189
189
188
188
187
186
185
184
183
181
180
178
177
175
173
171
169
168
166
164
163
162
161
161
160
160
160
160
160
160
161
162
163
165
167
170
172
174
176
179
181
183
185
187
189
192
195
199
201
208
214
216
215
213
174
142
131
135
149
184
189
190
188
182
176
174
173
174
177
187
196
203
207
208
207
201
193
185
183
183
187
189
190
189
184
173
167
167
169
191
212
223
228
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
228
228
228
227
227
226
224
219
213
204
192
189
187
187
187
188
187
183
175
163
140
121
102
85
70
59
53
51
51
52
52
52
51
50
48
45
43
42
41
40
40
40
40
40
40
41
41
43
45
48
51
53
54
55
56
66
78
89
100
112
130
141
146
146
143
131
119
108
97
88
80
77
76
75
75
76
76
77
77
78
80
83
87
91
96
101
104
107
111
114
118
121
124
127
129
132
134
135
137
138
139
140
140
141
141
142
142
143
143
144
145
145
146
147
148
150
151
153
155
157
160
163
166
169
172
176
178
178
178
178
178
177
168
146
108
94
99
126
160
184
182
177
167
144
97
30
11
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
202
202
202
202
202
202
202
202
201
201
200
199
198
196
195
193
191
189
188
186
184
182
180
179
177
176
175
174
173
172
172
171
171
171
171
172
173
174
175
177
179
181
183
185
186
189
190
192
193
195
197
200
203
206
210
217
217
216
208
185
133
129
136
154
183
191
190
186
181
175
172
172
175
181
190
202
208
212
215
215
214
205
194
185
182
183
185
187
188
188
181
171
166
166
170
199
217
226
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
228
228
228
227
225
223
219
213
201
193
189
188
188
188
188
188
187
183
172
158
143
126
108
87
75
64
57
53
53
52
52
51
51
50
48
47
46
44
43
43
43
43
43
45
46
48
50
51
53
54
55
57
61
77
89
99
109
122
143
153
156
154
147
131
117
104
93
85
79
77
76
76
77
77
78
78
80
82
87
91
95
98
102
106
109
112
116
119
124
127
129
132
134
137
138
140
141
142
143
143
144
145
146
147
148
149
150
151
153
154
155
156
157
159
160
161
163
164
167
169
171
173
176
181
182
182
182
181
180
179
173
154
117
100
104
130
162
184
181
177
166
140
91
26
9
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
212
212
212
212
212
212
212
212
211
211
210
209
207
206
205
202
200
198
196
194
191
188
186
184
182
180
179
178
177
177
177
176
176
176
177
177
178
179
181
182
185
187
189
190
192
194
196
198
199
201
204
207
209
212
215
218
218
209
189
149
128
134
153
175
189
190
187
181
176
171
171
175
182
190
199
208
212
215
216
217
215
206
195
186
182
182
184
186
187
186
180
169
165
166
171
203
220
227
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
227
226
224
221
217
207
199
193
189
188
188
188
189
189
188
184
176
166
154
139
118
104
90
78
68
59
56
54
53
52
51
50
50
49
48
47
47
47
47
48
48
49
50
52
53
54
54
56
62
71
87
97
106
118
133
156
162
163
158
147
127
112
100
90
83
78
77
77
77
78
79
80
83
85
88
93
96
99
102
106
110
113
117
120
124
128
131
134
136
138
140
142
143
144
145
146
147
148
149
151
152
154
155
156
158
159
160
162
163
164
165
167
168
169
170
172
174
175
176
178
182
184
184
184
183
182
181
175
159
123
105
109
134
164
184
181
176
164
137
86
24
8
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
221
221
221
222
222
222
222
222
221
221
220
218
217
215
214
211
209
207
205
203
200
197
195
192
190
187
186
184
183
183
182
182
182
182
182
183
185
186
188
190
193
195
197
199
200
202
204
205
207
208
211
213
215
216
218
219
214
192
162
132
130
150
171
187
191
188
182
176
171
169
173
181
190
199
206
212
215
216
217
217
216
207
196
186
182
182
184
185
186
185
179
168
164
165
172
206
221
228
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
228
226
224
223
220
213
205
198
192
189
188
188
189
190
190
190
187
182
174
164
147
133
119
105
92
77
70
64
59
56
53
52
51
51
50
50
50
50
50
50
51
51
52
53
54
55
57
62
71
83
97
105
116
130
148
168
171
168
159
144
121
106
95
86
81
78
78
78
79
81
83
86
88
91
94
98
101
104
107
110
115
119
122
126
129
133
135
138
140
142
144
145
146
148
149
151
152
153
155
157
159
160
162
163
165
166
168
169
170
171
172
173
174
175
176
177
178
179
180
181
184
185
186
186
185
183
182
178
162
129
110
114
137
166
184
181
176
163
133
81
21
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
231
231
231
231
232
232
232
232
231
231
230
229
227
225
223
220
218
215
213
210
207
205
203
200
198
195
193
192
190
189
189
188
188
188
189
191
192
194
197
199
202
204
205
207
208
210
211
212
213
214
216
217
218
219
219
217
201
169
138
127
143
170
185
190
189
184
176
171
168
170
179
189
198
205
210
214
216
217
218
218
216
208
197
187
182
181
183
184
184
184
178
168
163
165
172
208
222
228
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
230
230
230
230
229
228
226
225
223
222
217
211
204
197
191
188
188
189
189
190
192
192
190
188
183
171
160
147
134
120
103
91
81
73
66
60
57
55
53
52
52
52
51
52
52
52
53
54
54
55
58
64
73
83
94
105
114
128
145
165
179
178
170
157
138
114
100
89
83
79
78
79
81
83
85
89
92
95
97
100
103
106
109
112
116
121
124
128
131
134
137
140
142
144
145
148
149
151
152
154
156
158
160
162
164
166
168
169
171
172
174
175
175
176
177
178
179
179
180
181
182
182
183
183
183
185
186
187
187
186
185
184
180
166
135
115
119
141
168
183
181
176
161
129
75
18
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
240
241
241
241
242
242
241
241
241
241
241
240
239
237
235
231
227
223
219
215
212
209
207
206
204
202
200
199
198
197
196
196
196
197
197
199
201
203
205
207
209
210
211
212
213
214
214
215
216
217
218
219
219
220
219
213
173
142
127
130
170
186
191
189
186
179
170
167
168
173
189
196
203
208
212
216
217
218
218
218
216
209
198
187
181
181
182
183
183
183
178
167
163
164
171
208
222
228
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
229
230
230
230
230
230
229
227
225
224
223
221
216
209
202
195
190
188
188
188
189
191
193
194
194
193
188
181
171
161
148
131
118
106
95
85
74
68
63
59
56
54
54
53
54
54
55
55
55
56
57
66
77
87
97
104
114
127
144
163
182
187
182
169
151
129
105
93
85
80
79
80
82
85
88
91
95
98
100
102
105
108
111
115
118
122
127
130
134
137
139
142
144
146
148
150
152
154
155
157
160
163
165
168
170
172
174
176
177
178
179
180
181
182
182
183
183
184
184
184
185
185
185
186
186
186
187
187
188
188
187
186
185
181
169
141
121
124
145
170
183
181
176
159
125
70
16
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
252
251
249
246
244
241
240
239
239
240
241
241
242
243
243
242
241
240
237
233
226
221
216
212
210
207
206
205
204
204
203
203
203
204
204
206
208
210
212
214
215
215
216
216
217
217
218
219
220
221
221
221
221
219
214
162
133
125
136
166
192
191
189
185
181
166
164
166
172
185
195
201
207
212
216
218
219
219
219
219
217
209
199
189
182
181
181
182
182
182
178
168
163
163
168
206
222
228
229
229
228
229
229
229
229
229
229
229
229
229
229
229
229
229
230
230
230
230
230
230
229
227
226
224
223
222
220
216
210
202
194
191
189
188
188
189
191
193
195
197
197
195
191
185
177
163
152
140
128
117
104
95
87
81
75
69
66
64
62
61
61
62
65
70
78
90
97
103
110
118
135
153
172
187
196
192
177
157
134
112
91
84
82
81
82
87
91
95
98
100
103
105
107
110
113
117
120
124
128
131
135
138
141
143
146
148
150
152
154
157
160
162
165
168
171
174
176
178
180
182
183
185
186
186
187
187
187
187
187
187
187
187
187
187
187
187
187
187
187
188
188
188
188
189
188
187
186
183
172
147
127
131
150
172
183
180
175
155
118
62
13
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
180
181
182
185
190
194
199
200
200
198
196
198
201
205
211
220
227
233
236
235
233
230
226
221
217
213
210
209
208
207
207
207
207
208
209
213
215
216
216
216
215
215
215
215
215
217
218
219
219
216
203
192
178
161
143
124
124
141
165
186
192
189
186
181
173
162
162
168
177
188
198
205
211
215
218
219
219
219
219
219
217
211
201
191
184
180
181
181
181
181
178
169
163
161
165
200
220
227
228
228
228
228
229
229
229
229
229
229
229
229
229
229
229
230
230
230
230
230
230
230
229
227
226
224
223
222
222
219
214
208
199
194
191
189
188
188
190
191
194
196
198
199
198
195
190
180
170
160
149
139
125
115
107
99
92
85
81
78
76
76
78
81
85
90
96
102
107
114
125
138
158
175
189
198
200
187
165
140
117
99
86
84
83
85
89
94
98
100
103
105
108
110
113
116
119
124
128
131
135
138
142
144
147
149
151
154
157
159
162
164
168
171
174
177
179
182
184
186
187
188
188
189
189
189
189
189
189
189
189
189
188
188
188
188
188
188
188
188
188
188
188
189
189
189
189
188
187
184
174
150
132
136
154
175
183
180
174
151
112
56
10
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
148
162
177
189
197
202
206
207
207
205
204
204
204
204
204
208
214
220
226
232
235
234
232
228
224
219
216
213
212
211
210
211
212
214
216
219
218
217
212
202
197
198
201
203
203
198
187
175
160
145
132
128
125
121
115
121
140
164
184
193
190
186
182
176
164
159
162
170
180
190
202
209
214
217
219
219
219
219
219
219
218
212
204
194
186
181
180
180
181
180
178
170
163
160
162
193
216
226
228
228
228
228
228
229
229
229
229
230
230
230
230
230
230
230
230
230
230
230
230
230
229
227
226
224
223
223
222
221
218
212
203
197
193
190
189
188
189
190
192
194
198
200
201
201
198
192
185
176
167
157
144
134
126
118
111
103
99
97
95
95
96
99
102
105
108
114
122
133
146
160
179
192
201
203
198
173
146
122
103
90
85
86
88
92
97
101
104
106
108
110
114
117
120
124
127
132
136
139
142
145
148
151
153
155
158
161
164
167
170
173
177
180
183
185
187
189
189
190
191
191
192
192
192
192
192
191
191
191
191
191
190
190
190
190
190
189
189
189
189
189
189
189
189
189
189
188
188
185
176
153
137
140
158
177
183
180
173
147
105
49
9
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
152
178
200
216
224
226
226
226
226
226
226
225
222
217
211
207
206
209
214
223
232
234
234
232
230
225
222
219
216
215
215
216
218
220
222
217
205
194
182
165
154
155
156
157
153
140
123
105
86
71
71
80
94
108
119
141
168
185
194
193
186
184
178
169
157
157
163
172
183
193
206
213
216
218
219
219
219
219
219
219
219
214
207
197
189
182
180
180
180
180
178
171
164
160
160
184
211
224
228
228
228
228
228
228
228
229
229
230
230
230
230
230
230
230
230
230
230
230
230
230
228
227
225
224
223
223
223
222
220
215
207
200
195
192
190
189
188
189
190
193
197
200
203
204
204
200
195
188
181
172
160
151
143
136
129
122
118
116
114
113
113
114
115
118
121
131
143
156
170
182
197
204
206
201
188
151
124
105
92
87
88
91
95
100
104
108
110
112
114
117
121
125
128
132
136
141
144
147
149
152
155
157
160
163
166
170
173
177
180
183
186
188
190
192
193
193
194
194
194
194
195
195
195
195
195
194
194
194
194
194
193
193
192
192
192
192
191
191
191
191
191
190
190
190
190
189
188
186
177
156
141
145
162
179
183
180
171
143
98
43
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
186
210
221
226
227
227
226
226
225
225
225
225
225
225
224
217
210
207
207
210
225
231
234
234
234
230
228
225
222
220
221
223
224
224
222
202
160
129
109
98
83
72
62
50
36
23
23
28
39
54
80
99
114
128
143
177
190
195
193
189
184
181
172
161
153
154
162
173
184
196
210
215
217
218
218
219
219
219
219
219
219
217
210
202
192
184
181
180
179
179
178
172
165
160
158
174
205
222
228
228
228
228
227
227
228
228
229
229
230
230
230
230
230
230
230
230
230
230
230
229
228
226
225
224
223
223
223
223
221
218
209
203
198
194
191
189
189
189
190
192
197
200
203
205
206
205
201
196
190
184
174
166
159
152
145
139
134
131
127
125
123
123
126
132
141
159
171
181
190
199
208
210
205
192
165
123
103
92
88
88
93
98
103
107
109
112
114
117
120
124
129
133
137
141
145
149
151
154
156
159
163
166
169
173
176
181
184
187
190
192
194
195
195
196
196
197
198
198
199
199
199
199
199
199
199
199
199
199
198
198
197
197
196
196
196
195
195
194
193
193
192
192
191
191
190
189
189
187
178
159
145
150
165
180
183
179
170
138
91
37
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
197
216
224
226
226
224
223
223
223
224
224
224
224
225
225
224
218
212
207
207
211
220
228
233
234
233
232
229
227
227
230
230
218
184
115
59
41
27
18
11
8
5
4
6
10
28
49
73
97
118
135
141
152
166
185
196
196
193
189
184
182
174
162
152
150
152
160
172
186
201
214
216
217
217
217
218
219
219
219
220
220
219
215
208
199
188
183
180
179
179
178
174
167
161
158
162
190
212
226
228
228
227
227
227
227
227
228
228
229
229
229
229
230
230
230
230
229
229
229
228
226
225
224
224
224
223
223
223
222
218
210
203
198
194
191
189
189
189
190
193
198
201
204
207
209
209
207
204
199
192
183
176
169
163
157
149
145
143
141
141
146
153
162
171
180
188
194
200
206
211
211
204
182
151
117
95
92
92
94
99
107
111
114
116
118
122
125
129
133
138
143
147
151
154
157
160
163
166
169
173
178
181
185
188
191
194
196
197
198
199
200
200
201
202
203
204
204
204
205
205
205
206
206
206
206
206
206
205
205
204
204
203
202
202
201
200
200
199
198
197
196
195
194
193
192
190
189
187
179
161
151
156
170
182
183
179
166
130
80
29
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
205
219
225
226
225
223
223
223
223
224
224
224
224
224
225
225
222
216
210
207
211
219
227
232
234
234
233
232
231
231
226
189
138
87
47
24
17
12
8
6
4
5
13
27
50
83
103
120
133
141
152
162
175
186
194
197
194
189
185
183
181
167
156
149
148
150
158
172
188
205
216
216
216
216
216
217
219
219
219
220
220
220
218
214
206
193
186
182
179
179
177
175
169
163
158
159
177
200
220
227
228
227
227
227
227
227
227
227
227
228
228
228
228
228
228
228
228
227
227
226
225
225
224
224
224
224
224
223
220
216
207
201
196
193
190
189
190
191
194
197
202
206
209
211
212
212
210
207
204
198
190
184
178
173
168
164
163
163
164
167
172
177
183
188
192
199
204
209
211
212
199
171
142
116
101
94
95
98
103
108
114
116
119
122
125
131
135
140
144
148
154
157
161
164
168
173
176
180
184
187
191
193
196
197
199
200
201
202
203
203
204
205
205
205
206
206
206
207
207
207
207
207
208
208
208
208
208
208
207
207
207
207
206
205
205
204
203
202
201
200
199
197
196
195
193
191
190
187
180
162
154
160
172
183
183
178
162
123
72
24
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
207
220
225
226
225
222
222
223
224
225
224
224
224
224
225
225
223
217
211
209
211
219
227
233
234
234
234
233
230
222
171
117
70
34
16
8
6
5
4
3
6
19
39
65
92
120
134
145
153
159
171
181
190
196
197
195
190
186
184
183
176
160
151
147
147
149
157
172
191
209
217
216
215
213
213
216
218
219
220
220
220
221
220
217
212
200
191
185
180
179
177
176
172
166
159
157
166
187
210
225
228
227
227
227
227
226
226
226
226
227
227
227
227
227
227
227
226
226
226
225
225
224
224
224
224
224
223
221
217
211
203
197
194
191
191
192
194
197
200
204
208
210
212
214
214
213
211
209
206
202
197
192
188
184
181
180
179
180
182
184
189
192
195
199
203
207
210
211
206
194
161
134
114
101
97
99
104
108
113
117
121
124
128
132
136
143
148
152
156
160
165
170
174
178
182
186
190
192
195
197
199
200
201
202
203
204
204
204
205
205
206
206
206
207
207
207
207
207
207
208
208
208
208
208
209
209
209
209
209
209
209
209
209
208
208
207
206
205
204
204
202
200
199
197
194
191
190
188
180
164
158
164
175
183
183
177
158
115
64
19
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
206
220
225
225
225
222
221
222
224
225
224
224
224
224
225
224
222
217
212
210
214
222
229
233
235
234
233
227
210
180
95
48
22
11
6
4
4
4
4
5
18
45
75
104
126
143
151
159
166
175
187
194
198
198
196
193
187
185
184
183
170
155
148
147
147
148
156
173
194
213
216
215
212
211
211
215
217
219
219
220
220
221
221
220
217
207
197
189
183
179
177
176
173
169
161
157
159
173
195
218
226
227
227
227
227
226
226
226
226
226
226
226
226
226
226
225
225
225
225
225
224
224
224
224
224
223
221
217
211
205
197
193
191
190
191
195
199
202
206
209
212
213
214
215
214
213
211
209
206
204
201
198
196
194
193
192
192
193
194
196
199
201
203
206
209
211
208
199
183
159
121
106
100
100
102
108
114
118
122
125
130
134
139
145
150
157
161
166
170
174
179
183
187
191
194
198
200
201
202
203
203
203
203
204
204
204
204
204
204
204
204
204
205
205
205
206
206
207
207
207
208
208
208
208
209
209
209
209
210
210
210
210
210
210
210
209
209
208
207
206
205
203
201
199
196
192
191
188
180
165
161
168
177
184
182
176
152
106
55
15
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
205
219
225
225
225
222
221
221
222
224
225
225
225
225
225
222
218
215
213
212
219
228
233
235
235
234
230
207
161
88
28
15
7
4
3
3
3
3
5
10
44
81
110
130
144
157
166
174
183
191
198
199
199
197
194
188
185
184
184
183
162
151
147
147
147
148
156
174
197
214
216
213
210
207
208
213
216
217
219
220
220
221
221
221
221
215
205
195
187
180
177
176
174
171
165
157
156
161
176
203
224
226
227
227
227
226
226
226
226
226
226
225
225
225
225
225
225
225
225
225
224
224
224
224
224
222
216
209
202
196
190
188
188
190
192
197
200
204
207
210
212
213
214
214
213
212
210
208
205
203
201
200
199
198
197
197
198
199
201
203
206
208
210
211
212
208
193
166
136
111
102
102
104
108
114
119
122
125
128
133
140
146
152
158
163
170
176
181
187
191
196
199
200
201
202
202
203
203
204
204
204
203
203
202
202
201
201
201
200
200
199
199
199
200
201
202
203
204
205
206
207
207
208
208
208
209
209
209
210
210
210
210
211
211
211
211
211
210
209
209
207
206
204
201
198
193
192
188
180
166
164
172
180
184
182
175
145
97
47
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
198
216
224
225
226
222
221
221
221
221
221
221
221
220
219
217
217
217
220
225
232
234
234
234
232
216
166
106
52
21
8
4
3
2
2
3
3
5
15
39
91
117
137
152
166
180
187
193
197
199
200
199
197
193
191
186
185
185
184
180
155
149
147
147
147
148
155
176
200
216
216
210
205
202
203
208
211
214
216
218
219
220
221
221
222
220
215
207
197
187
179
177
175
173
171
161
157
156
160
171
205
219
225
227
227
226
226
226
226
226
226
225
225
225
225
225
225
225
224
224
224
223
222
220
216
207
200
194
190
187
186
186
187
189
191
195
197
199
201
202
204
204
204
204
204
203
203
202
202
202
201
201
201
201
202
204
206
207
208
208
209
209
207
201
186
149
125
111
105
104
106
111
116
121
125
132
137
144
150
157
165
171
177
182
188
194
197
199
200
201
201
202
202
202
201
199
198
196
194
192
188
184
181
177
174
171
170
169
168
168
168
169
171
173
176
180
184
187
191
195
200
202
205
206
208
209
209
209
210
210
210
210
211
212
212
212
212
212
211
211
210
208
207
204
200
194
192
188
180
168
167
176
182
184
182
171
134
84
36
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
189
211
221
224
226
224
223
222
221
221
221
221
221
221
221
221
223
226
228
230
233
232
227
215
191
127
80
45
21
9
4
2
2
2
2
2
4
14
36
73
117
139
157
171
182
191
194
197
199
200
199
197
195
193
190
186
186
186
183
176
152
150
149
149
148
149
156
178
203
216
215
208
202
198
198
202
206
209
211
214
216
218
220
220
221
221
219
214
207
196
184
179
176
174
172
166
160
157
157
160
178
201
215
223
226
226
226
225
225
225
225
225
225
225
224
224
224
223
223
222
219
216
212
208
202
196
192
189
187
186
186
186
188
189
191
193
195
196
198
199
200
201
201
202
202
202
202
203
203
203
204
205
206
206
207
207
207
207
207
206
200
187
171
153
136
120
114
111
112
116
121
126
130
134
139
146
152
159
166
174
183
188
191
194
196
199
200
200
199
198
196
192
188
184
179
172
167
162
158
154
151
149
147
145
143
141
140
139
139
139
141
142
144
146
149
154
158
163
169
175
184
189
195
199
203
206
208
209
210
210
210
210
211
211
212
213
213
212
212
212
211
210
208
206
201
195
193
187
179
169
170
179
183
184
181
168
124
73
29
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
175
202
215
221
225
224
224
223
223
222
222
222
223
223
224
225
226
228
229
228
222
205
181
149
111
56
30
14
6
3
2
1
2
2
2
3
8
30
64
104
141
160
174
185
192
196
198
199
200
199
198
196
194
192
190
186
187
187
183
173
152
152
152
151
149
149
157
180
205
216
214
205
198
194
192
195
199
202
205
208
212
215
217
218
220
221
220
218
214
206
193
185
180
176
173
170
164
159
156
155
162
178
196
211
221
225
225
225
225
224
224
223
223
222
221
220
219
218
216
214
210
207
203
199
195
191
189
188
187
186
187
188
189
191
192
195
196
198
199
200
202
202
203
204
205
206
206
206
207
207
208
208
208
208
207
205
202
197
189
180
164
151
138
126
117
113
115
117
120
125
132
138
144
150
157
167
173
179
184
189
194
196
197
198
197
193
190
185
180
175
168
163
158
154
149
144
140
137
134
132
130
129
128
128
127
126
125
125
125
125
125
126
127
129
131
134
138
142
148
154
164
172
180
188
194
201
204
207
209
210
210
210
211
211
212
213
213
213
213
213
212
211
210
207
202
195
193
187
179
171
173
181
184
184
180
163
113
63
23
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
157
188
204
212
220
222
223
223
223
223
223
223
224
224
224
223
221
217
210
199
176
145
108
69
34
11
6
3
1
1
1
1
1
1
1
4
19
54
95
131
161
177
187
194
197
198
199
200
199
199
197
195
194
193
190
188
188
188
183
170
154
155
155
154
150
149
158
183
207
216
213
203
195
190
186
187
191
193
196
200
204
209
212
215
217
220
220
220
219
215
204
194
186
180
175
172
168
163
158
155
153
160
173
190
207
220
222
223
224
223
222
221
220
219
218
216
214
212
210
207
203
200
197
195
193
191
191
190
190
191
192
193
195
197
198
201
202
203
205
206
207
207
208
208
209
209
210
210
209
209
207
206
203
200
196
188
181
171
159
145
129
122
118
116
117
120
124
129
134
139
148
154
162
170
178
187
191
195
196
197
195
193
190
186
181
173
166
159
152
145
138
133
130
127
125
124
123
123
122
122
121
121
121
121
121
121
121
121
121
120
119
119
119
119
120
122
124
127
130
135
145
154
163
173
182
192
198
203
206
209
209
210
210
211
212
213
213
214
214
214
213
212
211
208
202
196
193
186
178
173
175
183
185
183
179
155
102
53
17
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
138
167
185
197
209
215
218
220
220
221
221
221
221
220
218
212
202
183
157
123
72
42
22
10
3
2
1
1
1
1
1
1
1
1
1
8
40
83
125
154
179
190
195
197
197
199
199
199
199
198
197
196
195
194
191
189
190
189
183
168
157
158
159
157
151
150
160
185
209
215
211
201
192
186
180
179
181
182
185
189
194
200
204
208
212
217
218
220
220
219
215
206
196
187
179
174
172
168
163
156
153
152
154
162
179
206
215
221
224
223
223
221
220
219
217
214
212
210
208
206
202
200
198
196
195
195
195
197
199
201
205
207
209
211
212
213
214
214
214
214
213
213
213
212
211
210
210
209
207
205
201
195
186
176
163
146
135
127
122
119
117
117
118
120
124
133
140
148
157
165
176
183
189
193
195
197
197
197
196
193
184
174
163
152
142
133
128
126
124
123
122
122
122
121
121
121
122
122
122
123
123
123
123
123
123
123
122
122
121
121
120
119
118
118
117
117
117
118
120
122
128
136
145
156
166
180
189
196
202
206
209
209
210
211
212
213
213
214
214
214
214
213
212
209
203
197
193
186
178
174
179
185
185
182
177
146
89
43
12
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
114
131
144
154
166
173
177
180
180
178
172
165
154
139
121
92
70
50
34
21
10
6
3
1
1
1
1
1
1
1
1
1
1
1
3
23
78
122
155
176
193
195
196
197
197
199
199
199
199
198
198
198
197
195
192
191
192
191
184
169
163
164
164
160
151
150
164
188
210
215
209
198
189
182
173
169
167
166
167
170
175
181
187
193
199
207
211
215
218
219
219
216
211
203
193
181
176
173
170
167
158
154
151
150
150
158
174
191
206
217
220
221
221
220
219
218
216
215
214
213
212
211
211
212
213
214
214
215
215
216
216
216
215
215
215
214
213
213
212
210
208
205
202
198
193
185
177
169
161
153
141
135
130
127
125
124
124
125
126
128
133
138
144
149
154
159
165
171
178
185
191
194
197
197
196
188
177
164
151
139
129
125
123
122
121
121
121
121
121
122
124
126
128
130
132
136
138
141
143
146
148
148
148
148
146
144
141
139
135
132
127
124
122
120
118
116
115
115
115
115
117
121
127
135
145
161
172
183
192
200
205
207
209
210
211
212
213
214
214
215
214
214
213
210
203
197
193
185
178
176
182
186
185
181
174
130
72
31
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
102
110
115
119
123
125
122
120
115
109
100
94
86
77
67
54
44
34
25
17
9
5
3
1
1
1
1
1
1
1
1
1
1
3
7
48
105
144
170
186
195
195
196
196
197
199
199
199
199
199
200
200
198
196
193
193
194
192
186
172
168
168
167
161
150
150
166
191
211
214
208
197
188
180
170
163
159
156
155
156
158
163
169
175
182
192
199
205
211
215
218
217
215
211
206
195
186
179
174
171
166
161
157
153
150
150
155
160
167
179
194
203
210
214
215
216
216
215
215
214
214
213
213
213
213
213
213
213
212
212
210
208
205
202
198
192
187
183
178
173
167
163
158
154
150
146
143
140
137
134
131
130
128
128
127
128
130
132
136
140
146
153
161
169
177
187
191
195
197
198
194
186
175
164
153
142
136
131
127
124
122
121
121
122
123
126
129
134
139
144
152
157
162
167
171
175
178
179
180
181
182
182
181
180
179
176
173
169
164
159
151
145
138
131
125
120
117
114
113
113
113
115
119
124
133
147
159
171
182
192
201
204
207
208
210
212
213
214
215
215
215
214
213
210
203
198
191
184
178
178
185
186
184
179
170
115
59
23
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
94
97
98
97
95
92
88
84
80
75
69
65
61
56
51
47
42
35
28
21
12
6
3
1
1
1
1
1
1
1
1
1
1
5
13
76
130
162
180
192
195
195
195
196
197
199
199
199
199
200
201
201
199
197
194
195
195
194
189
177
172
172
169
161
150
150
169
193
211
214
208
196
187
178
168
159
154
148
145
144
144
147
151
157
163
174
181
190
198
205
213
215
216
215
213
206
198
190
182
176
171
168
164
159
155
151
149
148
149
154
165
171
178
183
187
193
195
197
198
199
200
199
199
198
196
194
192
190
187
184
180
176
172
169
165
160
157
153
150
146
142
140
137
135
133
132
131
130
130
130
131
132
133
135
137
142
148
154
161
167
176
182
187
192
196
197
193
187
179
171
161
152
144
135
128
123
121
120
120
121
124
128
132
138
144
152
159
165
171
176
182
186
190
193
196
198
199
200
200
201
201
201
200
200
199
197
195
192
188
184
177
169
161
152
142
131
124
118
114
112
111
111
113
117
123
135
147
159
172
184
195
200
204
207
209
211
213
214
215
215
215
215
213
209
203
198
190
183
179
180
187
186
183
176
163
99
47
17
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
89
89
89
87
83
75
71
68
66
65
64
64
63
62
61
58
54
47
39
30
16
8
3
1
1
1
1
1
1
1
1
1
2
8
22
104
151
175
187
195
195
194
194
195
198
200
199
199
200
201
203
202
200
197
196
197
197
196
191
181
176
175
169
160
149
150
172
195
212
214
208
196
186
178
168
157
151
144
138
135
132
133
136
139
144
153
161
169
179
189
201
208
212
214
214
213
209
203
195
186
178
174
170
167
163
157
153
150
147
145
145
146
148
151
156
163
167
170
172
173
174
174
173
171
169
166
163
160
156
152
147
144
141
139
137
135
134
133
133
132
132
132
132
132
133
133
134
135
136
137
141
144
148
153
159
167
174
182
189
196
201
201
199
195
191
181
170
158
145
133
124
120
118
118
118
120
122
125
129
134
142
149
158
166
174
184
191
196
199
202
205
206
207
207
208
208
209
209
209
209
209
209
209
209
209
208
207
206
204
202
197
191
184
175
164
149
137
127
119
114
110
109
110
112
115
125
135
148
161
174
188
196
201
205
207
210
212
214
215
215
215
215
213
209
203
197
188
182
180
182
188
186
182
173
154
82
36
12
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
85
86
86
85
83
74
70
67
65
65
69
73
77
80
82
78
72
64
54
41
22
11
4
1
1
1
1
1
1
1
1
1
3
12
36
129
164
182
190
196
194
192
193
195
198
200
199
199
200
202
204
203
200
198
198
199
199
197
193
185
179
176
168
158
149
150
174
197
212
213
208
196
187
178
168
157
150
142
135
131
126
124
124
125
127
132
138
146
155
165
182
194
204
212
214
215
214
213
210
204
190
181
175
171
168
166
163
160
157
152
147
145
143
142
142
141
142
142
142
142
142
142
141
141
140
139
138
137
136
136
136
135
135
135
134
134
134
134
134
134
135
135
136
137
138
141
144
147
151
155
163
171
179
187
195
203
207
210
212
211
201
185
169
153
140
128
123
119
116
115
115
115
116
117
119
125
133
143
155
167
182
191
197
201
204
206
207
208
208
209
209
209
209
209
210
210
210
210
210
210
210
210
210
210
210
210
210
210
210
210
208
206
201
195
186
171
157
142
129
119
111
108
108
108
110
116
126
137
150
164
181
190
198
202
206
209
212
214
215
215
215
215
213
208
203
195
187
182
182
184
189
185
180
168
142
66
26
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
83
84
85
85
84
77
73
71
71
74
94
102
106
106
105
95
86
74
60
45
25
12
5
1
1
1
1
1
1
1
1
1
4
19
55
150
175
187
192
196
193
191
192
194
198
198
198
199
201
204
205
203
201
201
201
202
202
199
194
187
178
173
164
155
148
150
176
198
212
213
209
200
190
180
172
160
152
144
137
131
126
122
120
119
118
119
120
122
126
131
143
155
170
184
199
210
213
214
214
214
212
206
198
188
179
172
169
166
165
163
160
158
156
153
151
149
147
146
145
144
143
142
142
141
140
140
140
140
140
140
140
140
141
142
143
143
144
145
147
149
152
154
157
161
167
175
182
189
195
201
208
211
213
214
213
199
180
162
146
136
129
124
121
118
116
114
113
113
113
114
117
124
134
146
161
180
190
197
202
205
207
207
208
208
209
209
209
209
209
210
210
210
210
210
210
210
210
210
210
210
210
210
210
210
211
211
211
211
211
211
211
210
210
208
204
194
182
167
150
134
117
110
107
106
106
109
116
126
138
151
170
183
192
199
204
208
211
213
215
216
216
215
212
207
202
192
185
183
183
187
189
184
177
159
121
45
16
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
83
85
86
87
86
82
79
78
81
91
112
116
117
116
111
99
87
72
57
40
23
11
4
1
1
1
1
1
1
1
1
1
5
23
64
157
177
187
192
196
193
191
191
192
195
196
197
200
203
206
206
204
203
203
204
204
202
198
191
182
172
167
160
153
148
150
175
197
211
213
210
203
196
188
179
167
158
150
143
137
130
126
123
121
119
118
118
118
119
121
125
130
138
149
163
183
195
205
211
214
214
212
209
205
200
191
184
177
172
168
164
162
160
159
157
155
153
152
151
150
149
149
149
149
148
148
148
148
148
148
149
149
150
151
153
156
159
163
168
173
180
185
190
195
199
203
205
207
208
206
188
171
159
152
149
144
139
133
128
123
119
115
112
111
111
111
113
119
128
139
157
169
179
187
193
200
203
205
207
208
208
209
209
209
210
210
211
211
211
212
212
212
213
213
213
213
213
213
213
212
212
212
212
211
211
211
211
211
211
211
211
211
210
210
208
203
195
182
167
149
126
115
108
105
104
106
111
119
130
142
163
177
188
196
202
207
211
213
215
216
216
215
211
206
201
190
185
183
185
189
188
182
172
148
101
31
11
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
83
86
89
90
90
87
85
85
91
107
124
126
126
122
114
99
83
66
50
35
18
9
3
1
1
1
1
1
1
1
1
1
5
24
68
158
177
187
192
196
194
192
191
191
193
196
198
202
206
207
207
205
205
206
206
204
199
191
181
170
164
161
156
151
147
149
173
194
209
211
210
206
201
195
189
178
170
162
155
148
139
134
130
126
123
120
118
117
117
117
117
119
122
127
135
150
163
177
190
201
209
212
213
213
211
205
200
195
190
185
178
174
171
169
166
164
162
161
160
159
158
157
157
156
157
157
158
159
161
162
166
169
172
175
178
183
186
190
193
197
201
204
205
202
193
173
167
165
164
163
153
142
135
129
126
123
120
116
113
110
109
107
108
112
118
131
142
154
164
175
188
195
201
204
206
208
208
209
209
210
210
211
211
212
213
213
214
214
214
215
215
215
215
215
216
216
216
215
215
215
215
214
214
213
213
212
212
212
211
211
211
211
211
211
210
208
203
194
181
164
137
121
111
105
103
104
108
114
123
135
155
171
184
194
201
207
210
213
215
216
216
215
210
205
199
188
185
184
186
190
186
180
165
135
82
20
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
83
89
93
95
96
93
92
94
103
122
132
133
131
124
113
93
74
56
41
28
14
6
2
1
1
1
1
1
1
1
1
1
5
23
66
156
175
185
191
196
196
193
192
192
194
197
201
205
208
208
208
208
208
207
206
199
188
175
163
154
154
155
154
150
146
147
168
190
206
209
209
208
205
202
199
191
185
178
171
165
154
148
142
137
132
126
123
120
118
117
116
116
116
117
118
123
132
144
159
174
191
200
206
210
212
213
213
211
208
203
197
192
188
184
182
178
177
175
174
173
172
171
171
172
172
174
176
178
180
183
187
192
196
200
204
207
206
203
199
194
189
186
183
176
162
132
125
122
122
122
124
124
123
121
117
113
109
106
103
103
108
114
122
133
145
165
180
191
199
203
206
207
208
208
209
210
210
211
211
212
213
214
214
215
216
216
217
217
217
217
218
218
218
218
218
218
218
217
217
217
217
217
216
216
215
214
214
213
212
212
211
211
211
211
210
210
208
203
193
177
149
129
115
106
102
102
105
110
118
128
149
166
181
192
200
206
210
213
215
216
217
214
209
203
196
186
185
185
188
191
184
177
156
119
62
12
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
84
93
99
102
102
99
98
105
117
134
139
138
132
121
107
80
60
44
31
18
10
4
1
1
1
1
1
1
1
1
1
1
4
20
58
150
171
182
189
197
198
197
195
195
196
202
206
209
210
210
210
210
209
207
203
185
165
149
138
132
144
150
151
149
145
145
162
184
203
207
208
208
207
206
206
203
201
197
192
186
176
168
160
153
146
138
132
128
124
120
117
116
115
115
115
115
116
118
123
132
152
169
184
197
206
212
214
214
214
214
214
213
211
208
204
200
198
196
195
195
197
198
200
201
202
205
207
209
210
211
212
213
213
212
212
208
198
181
160
138
117
110
108
111
119
129
136
138
138
133
126
118
109
100
96
93
94
95
99
106
127
149
169
186
197
203
205
207
207
208
208
209
209
210
211
212
213
214
215
216
217
217
217
218
218
218
218
218
218
218
218
218
218
218
218
218
218
218
218
218
218
218
218
218
217
217
216
215
214
213
212
211
211
211
211
210
210
207
201
189
162
139
121
108
102
100
103
107
113
122
143
163
179
191
199
206
211
214
215
216
217
213
207
201
193
186
185
187
189
191
182
173
145
101
43
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
85
99
106
109
109
108
110
120
133
143
144
138
128
111
87
55
42
29
19
10
4
2
1
1
1
1
1
1
1
1
1
1
3
14
43
133
160
173
181
192
199
200
201
202
204
209
210
211
211
211
211
209
205
194
176
141
120
108
103
103
128
142
148
148
144
142
150
172
193
201
204
206
207
207
207
207
207
207
207
206
202
197
190
183
175
165
157
150
143
136
130
125
121
119
116
115
114
114
114
114
117
120
127
136
149
167
180
192
201
208
213
216
218
220
221
221
220
219
218
217
215
215
215
215
215
214
213
212
209
205
193
175
156
137
122
106
105
108
116
129
144
148
149
149
145
137
124
111
99
92
90
89
89
90
93
102
118
139
162
183
197
202
205
206
207
208
208
209
209
210
211
213
214
215
216
217
217
217
218
218
218
218
218
218
218
219
219
219
219
219
219
219
219
219
219
219
219
219
219
219
219
219
219
218
218
218
218
217
216
215
214
212
212
211
211
210
210
210
207
200
176
151
130
113
102
99
101
104
108
116
139
160
177
190
199
207
211
214
216
217
216
211
205
197
190
186
186
189
190
189
178
163
124
73
23
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
86
102
110
113
114
115
121
131
141
147
145
134
116
94
72
53
39
25
15
8
3
2
1
1
1
1
1
1
1
1
1
1
2
9
29
114
148
163
169
177
196
202
204
206
209
211
212
212
212
211
209
202
187
166
140
116
105
98
95
95
110
132
142
145
144
140
142
159
179
191
196
201
204
205
205
207
207
208
208
208
207
205
202
198
192
183
176
169
161
154
145
140
135
130
127
122
119
117
115
114
114
115
117
119
123
129
135
142
150
159
169
176
183
188
193
196
199
200
202
202
201
198
192
184
174
160
152
144
137
130
125
124
127
131
136
142
144
145
146
149
149
141
131
120
109
101
97
93
90
89
89
94
102
114
131
158
172
184
193
200
204
206
207
208
208
209
210
212
213
214
215
216
216
217
217
218
218
218
218
218
219
219
219
220
221
222
222
223
223
224
224
224
224
224
224
223
222
221
221
220
220
219
219
219
219
219
218
218
218
217
215
214
213
212
211
210
210
210
208
205
185
160
137
118
104
99
101
102
106
113
137
159
177
191
200
208
212
215
216
217
215
209
202
195
188
186
187
190
190
185
174
150
104
53
14
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
87
105
114
117
118
122
131
141
147
150
143
126
106
87
71
57
44
30
17
9
3
2
1
1
1
1
1
1
1
1
1
1
1
5
16
89
134
152
156
160
180
196
203
207
210
211
212
212
211
209
198
182
160
138
121
106
99
95
92
91
98
117
133
143
144
138
137
147
162
176
182
189
194
197
198
202
204
206
207
208
208
209
208
206
203
196
191
184
176
169
159
153
147
142
139
134
130
126
123
120
117
116
114
114
113
114
116
119
123
128
134
139
143
146
149
151
153
154
154
152
149
146
143
138
132
127
127
128
128
127
129
133
139
146
152
157
157
154
148
139
125
115
107
98
91
87
87
88
91
96
107
122
138
155
171
190
198
202
205
206
207
208
209
210
211
213
214
215
216
216
217
217
218
218
218
218
218
219
220
220
221
222
223
224
225
227
227
228
229
229
230
229
229
229
229
228
227
226
225
224
223
222
221
220
219
219
219
219
219
218
217
216
214
212
211
210
210
210
209
207
192
168
144
122
106
99
100
101
104
110
137
160
179
192
201
209
213
215
216
216
213
206
199
192
187
187
189
191
189
181
169
133
83
36
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
88
109
118
121
122
130
141
148
151
151
138
118
102
90
83
73
60
45
31
18
7
3
2
1
1
1
1
1
1
1
1
1
1
2
7
59
115
140
146
145
157
180
194
202
207
209
209
208
205
199
178
155
135
120
114
108
104
101
98
93
93
104
122
138
143
137
133
135
143
156
163
170
176
180
182
189
193
196
200
203
205
207
209
209
208
205
201
196
190
182
173
166
159
153
149
145
142
138
135
131
126
123
120
118
117
115
114
114
114
114
114
114
115
115
115
114
114
114
113
112
111
111
112
113
115
123
132
140
148
153
159
161
163
163
162
156
149
140
129
114
96
90
87
86
86
88
91
96
105
117
140
160
177
192
200
205
206
207
208
208
209
210
212
213
214
216
217
217
217
217
218
218
218
218
219
219
220
221
222
223
225
226
227
229
230
231
232
232
233
233
233
233
233
233
233
232
231
231
230
228
227
225
224
222
221
220
219
219
219
219
218
217
215
213
212
211
210
210
210
209
197
175
150
127
107
99
99
100
103
109
138
162
180
194
203
210
214
216
216
216
211
203
196
190
187
187
190
191
187
177
160
112
62
22
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
90
112
121
124
126
138
148
153
153
150
130
113
106
105
106
105
89
76
60
39
16
6
2
2
1
1
1
1
1
1
1
1
1
1
3
25
92
127
139
137
132
149
172
192
198
204
204
200
192
178
148
130
122
118
116
117
121
121
114
100
93
95
109
126
139
140
132
128
128
133
140
144
148
153
154
160
167
174
181
189
196
202
206
209
210
211
210
208
203
196
187
178
169
161
155
151
149
147
146
144
141
138
135
132
130
126
123
120
118
116
115
114
114
113
113
114
115
117
121
126
135
141
145
148
151
155
159
161
163
165
166
166
165
163
159
143
124
108
96
89
86
85
85
86
87
95
102
116
133
154
184
195
202
205
206
207
207
208
209
210
213
215
216
217
217
217
217
218
218
218
218
218
219
220
221
222
223
224
225
226
228
229
230
230
231
232
233
233
234
234
234
234
234
234
233
233
233
232
232
231
230
229
228
226
224
221
220
220
219
219
219
218
217
215
212
211
210
210
210
209
201
180
156
131
109
99
99
100
103
109
141
165
183
196
204
212
215
216
216
216
208
200
193
189
187
188
192
190
184
173
146
89
42
11
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
93
116
126
128
131
145
153
156
154
146
124
120
121
124
125
119
110
103
92
73
49
27
12
3
1
1
1
1
1
1
1
1
1
1
1
4
38
90
133
139
119
117
124
137
152
159
162
159
151
136
126
124
123
125
134
147
150
152
150
144
114
100
98
108
132
140
137
130
122
121
119
118
118
118
117
117
119
122
127
134
149
164
180
194
204
211
214
216
216
213
208
202
193
183
172
161
156
152
150
149
148
147
146
145
145
144
144
143
143
143
143
143
143
144
144
145
147
149
151
153
155
155
157
159
163
167
170
173
174
174
170
165
155
142
125
105
97
91
88
86
85
87
89
94
103
118
139
161
182
197
204
206
207
207
208
209
210
212
214
216
217
217
217
217
217
218
218
218
218
219
220
221
222
223
224
224
225
226
226
227
228
229
230
231
232
232
233
233
233
233
233
232
232
232
232
232
232
232
232
231
231
230
229
229
228
225
223
221
220
220
219
219
218
216
214
211
210
210
210
210
204
186
161
135
112
100
99
99
103
112
148
171
187
199
207
213
216
217
216
214
203
195
191
188
188
191
192
185
175
166
115
58
22
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
93
118
129
131
133
148
155
157
153
142
128
129
132
136
137
131
123
115
105
91
72
52
32
15
3
1
1
1
1
1
1
1
1
1
1
2
14
51
97
135
130
117
113
117
126
138
140
138
134
129
126
126
130
137
144
150
152
154
154
154
142
122
106
101
112
137
138
132
124
120
117
114
113
112
112
110
110
110
110
111
114
122
134
149
166
188
200
207
212
214
213
211
207
201
194
179
171
164
158
154
152
150
149
148
148
147
147
146
146
146
146
147
147
148
149
150
152
154
157
159
163
166
169
172
174
176
177
177
175
171
161
149
137
124
112
99
93
89
87
86
88
93
100
110
124
152
173
188
198
204
206
207
208
208
210
212
214
215
216
217
217
217
217
217
218
218
218
219
220
221
222
222
223
223
224
224
225
225
225
226
227
228
229
229
230
230
230
230
229
229
229
229
228
228
229
229
229
230
230
230
230
230
230
229
228
227
225
223
221
220
220
219
219
217
214
212
211
210
210
210
205
188
164
137
113
100
99
99
104
117
155
176
191
202
209
214
216
216
215
210
199
193
189
188
189
192
192
182
168
153
85
38
12
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
93
120
131
133
135
149
156
158
153
141
133
136
140
144
144
141
135
128
118
105
85
70
52
32
12
1
1
1
1
1
1
1
1
1
1
1
3
22
56
105
134
125
115
110
112
120
126
128
129
128
128
132
137
143
148
150
152
154
156
157
155
142
123
106
101
122
134
135
129
121
117
114
112
111
110
108
106
105
103
102
100
101
105
112
122
145
167
184
198
207
213
214
213
211
207
197
188
179
171
165
160
157
156
154
153
152
151
151
151
151
151
152
153
154
155
158
160
163
165
168
171
174
177
179
180
181
179
176
172
165
151
139
127
115
104
94
90
88
88
89
95
103
116
132
152
180
194
201
205
206
207
208
210
211
213
215
216
217
217
217
217
217
217
218
218
219
220
220
221
222
223
223
223
223
223
223
223
223
224
225
226
227
228
228
228
228
228
228
227
227
227
227
227
227
227
227
227
228
228
229
229
229
229
229
228
228
226
225
223
221
220
220
219
218
215
212
211
210
210
210
206
189
165
138
114
101
99
100
107
123
163
182
195
204
211
216
216
216
213
207
195
191
189
189
190
193
188
177
159
132
58
23
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
93
121
132
135
137
149
156
158
153
141
137
142
147
149
150
149
145
139
130
117
95
81
67
50
27
4
2
1
1
1
1
1
1
1
1
1
1
4
20
58
117
128
123
114
109
108
117
123
128
130
131
137
142
145
147
149
151
153
155
157
158
156
142
121
100
103
124
135
136
126
119
116
113
112
110
109
108
106
104
102
97
94
91
89
88
101
125
149
171
189
207
212
213
214
214
209
203
195
187
178
170
166
164
161
160
158
157
156
156
156
156
157
158
160
162
166
168
171
173
176
179
180
182
183
183
182
179
174
167
158
143
132
120
109
99
90
89
89
90
94
105
119
137
158
180
199
204
206
207
207
209
210
212
214
216
217
217
217
217
217
217
217
218
218
219
220
221
221
222
222
222
222
222
221
221
221
221
222
223
224
226
226
227
227
227
227
227
227
227
227
226
226
226
226
226
226
226
226
226
227
228
228
229
229
228
228
227
226
224
222
220
220
219
218
215
212
211
210
210
210
206
190
165
138
115
101
99
101
110
133
171
187
199
207
212
216
216
215
210
202
192
190
189
190
191
192
181
169
146
106
34
12
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
91
121
133
137
138
149
156
157
153
142
141
148
152
154
154
153
152
148
141
129
104
88
75
63
50
11
4
1
1
1
1
1
1
1
1
1
1
1
2
7
65
114
131
128
117
106
107
115
125
132
137
140
142
144
145
146
148
150
154
157
159
158
155
142
111
93
107
127
140
137
123
120
117
114
113
113
112
112
110
108
100
95
90
85
81
77
85
105
130
157
191
205
211
213
215
213
211
207
201
192
182
176
171
169
167
164
163
162
161
161
161
162
163
165
167
169
171
174
177
179
183
185
186
187
186
183
178
172
163
153
138
125
112
101
94
90
90
92
96
102
119
140
162
183
200
205
206
207
208
209
211
214
216
217
217
217
217
217
217
217
217
218
219
219
220
221
221
221
222
221
221
220
219
219
219
220
221
222
223
225
226
226
227
227
227
227
227
227
227
227
227
227
226
226
226
226
226
226
226
226
226
227
227
228
228
228
227
226
225
223
221
220
220
218
215
212
211
210
210
210
206
189
165
138
115
101
100
103
116
144
179
193
203
209
215
216
216
213
207
197
190
189
189
191
193
191
174
157
129
74
16
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
87
118
133
137
139
148
156
157
153
146
146
152
155
155
155
154
154
153
152
150
127
101
82
68
61
35
12
2
1
1
1
1
1
1
1
1
1
1
1
1
4
34
81
121
133
127
115
111
113
118
127
134
138
139
139
139
141
145
151
157
160
160
159
156
149
99
91
100
119
141
139
130
125
122
120
122
125
127
129
129
124
117
106
93
82
74
69
67
73
91
137
167
190
204
212
215
213
213
211
207
198
189
182
176
172
169
167
165
164
163
164
165
166
169
171
176
179
182
184
186
188
188
188
188
187
182
176
169
159
148
131
118
107
98
93
91
93
97
105
118
147
170
189
202
206
207
207
209
210
212
216
216
217
217
217
217
217
217
217
217
218
219
219
220
220
220
220
220
219
218
217
218
218
219
219
220
222
223
225
225
226
226
226
227
227
227
227
227
227
227
227
227
227
227
227
227
226
226
226
226
226
226
226
227
227
227
227
226
225
224
222
221
220
218
215
212
211
210
210
210
206
187
161
135
114
101
102
109
128
162
189
199
207
213
216
216
215
208
200
192
189
189
190
192
192
182
165
136
94
37
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
84
113
130
137
140
147
155
157
154
148
149
154
155
155
154
154
154
154
155
154
146
120
93
71
62
47
21
7
1
1
1
1
1
1
1
1
1
1
1
1
1
10
28
56
94
119
122
123
123
123
124
126
128
128
128
130
136
142
148
155
161
161
160
159
156
132
102
91
97
114
139
137
134
130
127
128
130
132
133
133
132
131
126
117
103
81
71
64
62
63
83
114
146
177
202
213
213
213
212
210
206
202
195
188
181
176
173
171
169
168
168
168
170
172
174
179
181
184
186
188
189
189
190
189
187
182
175
167
157
145
127
114
104
97
93
93
97
105
119
138
168
187
199
205
207
208
209
211
213
215
216
217
217
217
217
217
217
217
217
218
219
219
219
220
220
219
218
217
217
217
217
217
218
219
220
222
224
225
226
226
226
226
227
227
227
227
227
227
227
227
227
227
227
227
227
227
227
227
226
226
226
226
226
226
227
227
226
226
225
224
222
222
220
218
215
211
211
210
210
210
204
184
158
132
112
102
103
118
142
175
195
203
210
215
216
216
211
204
196
191
190
190
191
192
191
169
152
113
64
19
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
82
107
126
137
140
147
155
157
154
149
152
155
155
155
154
154
154
155
157
158
156
138
108
77
60
52
33
14
2
1
1
1
1
1
1
1
1
1
1
1
1
1
5
18
42
70
94
109
118
123
128
130
131
131
132
134
139
144
149
154
160
161
162
161
159
150
123
100
87
92
117
132
137
137
134
133
133
134
135
135
136
136
136
132
124
103
84
69
58
52
54
71
98
133
171
204
210
213
213
212
210
208
203
198
191
183
179
176
174
172
170
171
171
173
176
180
184
186
188
190
190
191
190
190
188
181
174
165
154
142
123
111
102
96
93
95
103
116
134
156
184
198
204
207
208
209
211
213
215
216
217
217
217
217
217
217
217
217
218
218
219
219
219
219
218
217
216
216
216
216
217
218
219
221
223
225
225
226
226
226
226
226
226
226
226
226
226
226
226
227
227
227
227
227
227
227
227
227
227
226
226
226
226
226
226
226
226
226
225
224
223
222
220
217
214
211
211
210
210
209
201
179
154
129
109
104
107
129
159
185
199
207
213
216
216
214
207
199
193
190
190
191
192
190
184
160
133
87
39
9
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
81
102
121
135
141
147
154
157
155
151
154
155
155
154
153
153
154
156
159
161
161
153
125
89
58
51
42
22
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
19
48
70
87
101
117
125
130
135
139
143
147
150
153
155
159
161
162
162
160
158
145
120
94
80
89
113
129
138
140
137
137
136
136
136
136
137
139
139
139
128
106
83
64
51
43
44
58
85
124
180
199
210
214
214
212
210
208
205
200
191
186
181
178
175
173
172
173
174
177
182
185
188
191
191
191
191
191
190
188
181
173
163
152
139
120
109
100
96
94
100
111
128
149
172
196
204
207
208
209
210
213
215
216
217
217
217
217
217
217
217
217
218
218
218
219
218
218
217
216
215
215
215
215
216
218
219
221
223
225
226
226
226
226
226
226
226
226
226
225
225
225
225
226
226
226
226
226
227
227
227
227
227
227
227
226
226
226
226
226
226
226
225
225
223
223
222
219
216
213
211
210
210
210
209
197
173
148
126
108
106
115
144
175
193
203
211
215
216
216
211
201
195
191
190
190
192
191
186
173
151
107
59
20
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
80
96
115
132
142
147
154
157
155
152
155
156
155
153
153
153
154
158
162
165
166
162
143
108
57
43
43
26
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
7
18
39
72
91
109
124
138
151
155
157
157
157
159
160
162
162
162
160
157
144
120
82
72
79
103
128
141
142
140
139
137
136
136
138
139
141
142
142
133
110
82
54
41
37
37
44
65
133
177
202
213
215
214
211
210
208
206
199
192
187
182
178
175
174
175
176
178
183
187
190
191
192
192
192
191
190
187
180
172
162
150
136
117
106
99
96
96
105
122
142
164
186
205
207
208
209
210
212
214
216
216
217
217
217
217
217
217
217
217
218
218
218
218
217
216
215
215
214
214
215
215
217
220
223
225
226
226
226
226
226
226
225
225
224
224
224
224
224
224
224
224
225
225
225
226
226
226
227
227
227
227
227
226
226
226
225
225
225
225
225
224
223
223
222
219
215
212
210
210
210
210
209
192
167
142
120
108
108
131
162
188
198
208
214
216
216
215
205
196
192
190
190
192
192
190
181
161
135
77
33
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
81
91
109
129
143
149
155
157
156
154
155
155
154
153
153
153
155
161
168
174
175
171
161
135
85
37
39
29
14
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
3
7
15
29
48
72
106
127
141
151
155
157
159
160
161
162
161
160
158
150
130
77
67
66
80
113
141
142
143
142
140
138
139
140
142
144
145
143
135
117
85
44
36
32
30
31
52
103
156
197
215
216
214
212
210
209
206
202
196
191
185
180
177
176
177
178
184
187
190
192
192
193
192
192
190
187
180
171
160
147
133
114
104
98
97
100
117
137
158
180
199
207
208
209
210
211
214
216
216
217
217
217
217
217
217
217
217
217
218
217
217
216
215
214
214
213
214
214
216
219
222
225
226
226
226
226
226
225
224
224
223
223
223
223
223
223
223
222
222
222
222
222
223
223
224
225
226
227
227
227
227
227
226
225
225
225
225
225
224
224
223
223
220
217
213
211
210
210
210
208
205
181
159
133
113
109
118
159
184
198
203
214
216
216
214
210
196
192
191
191
191
192
190
179
164
151
89
38
11
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
81
89
107
128
144
150
155
157
157
155
156
155
154
153
153
153
155
162
169
177
179
177
169
148
110
34
27
22
15
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
5
10
18
31
59
85
108
127
140
149
153
156
158
160
161
160
159
156
149
113
79
62
59
71
115
135
143
144
143
142
142
143
144
146
147
146
140
128
107
63
41
30
27
26
33
55
98
149
196
216
216
215
213
210
207
204
200
195
189
183
180
178
178
179
184
188
191
192
193
193
193
192
190
187
179
170
159
145
130
111
102
98
99
106
126
147
169
189
203
208
209
210
211
213
215
216
216
217
217
217
217
217
217
217
217
217
217
217
216
215
214
213
213
213
214
216
219
222
225
226
226
226
226
225
224
223
223
223
223
223
223
223
222
222
221
221
220
220
220
221
221
222
223
224
225
226
227
227
227
227
226
225
225
224
224
224
224
223
223
222
219
215
212
210
210
210
209
206
199
175
145
124
113
112
146
177
193
202
209
215
216
215
211
202
193
191
191
191
192
191
183
170
151
127
52
19
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
81
87
106
129
145
151
156
157
157
155
156
155
154
153
153
154
155
161
169
179
182
180
175
160
129
38
22
17
14
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
5
10
25
46
68
91
112
129
138
145
150
154
157
158
159
158
157
139
105
74
51
45
76
112
133
144
145
145
145
146
147
149
150
149
145
136
122
81
49
31
23
22
25
32
57
97
152
205
213
216
215
212
210
206
203
199
194
187
183
181
181
181
184
189
191
192
193
193
193
192
190
187
179
169
157
144
128
109
101
99
102
111
135
157
178
195
206
209
209
210
212
214
216
216
216
216
216
216
217
217
217
217
217
217
217
216
215
214
213
212
213
213
216
219
222
224
226
226
226
225
224
223
222
222
223
223
223
223
223
222
221
220
220
220
220
220
220
220
220
221
222
223
225
226
227
227
227
226
225
225
224
224
224
224
223
223
222
220
217
213
211
210
210
209
208
203
194
161
133
119
117
126
171
190
200
206
213
216
216
212
205
196
191
191
191
191
192
187
173
158
134
91
25
8
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
81
86
106
130
146
152
156
158
157
155
156
154
154
154
154
154
155
160
167
178
183
182
180
169
144
49
22
13
10
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
6
16
32
52
75
99
114
126
135
142
149
153
155
157
158
154
132
97
60
34
39
77
111
136
145
147
148
149
150
152
153
152
149
142
131
98
61
35
20
17
20
22
31
54
98
177
203
214
217
215
212
209
206
203
199
192
187
184
183
183
184
189
191
193
193
193
193
192
190
187
179
169
156
142
126
108
101
100
106
117
143
165
185
200
208
209
210
211
213
215
216
216
216
216
216
216
216
217
217
217
217
216
216
215
214
212
212
212
213
214
218
221
224
225
226
225
224
223
222
221
221
222
223
223
223
223
222
221
220
220
220
220
220
220
220
220
220
221
221
223
225
226
227
227
227
226
225
224
224
224
223
223
223
222
221
218
214
212
210
210
209
209
206
199
184
143
125
119
126
149
189
198
205
210
215
216
213
206
198
192
191
191
191
191
190
178
162
142
108
52
8
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
82
87
108
132
147
153
157
158
157
156
156
155
155
155
155
155
155
157
162
173
181
182
181
174
159
69
25
8
5
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
7
16
31
58
78
94
108
120
133
142
149
154
156
156
151
126
86
35
18
34
73
116
143
147
149
151
152
154
156
155
153
148
140
114
78
44
19
12
13
18
23
31
48
130
182
208
217
216
215
212
209
205
201
196
191
187
186
186
187
190
191
193
193
193
193
192
190
187
178
168
155
140
124
106
101
102
110
124
150
172
191
204
209
210
211
212
214
215
216
216
216
216
216
216
216
216
216
216
216
216
215
214
213
212
212
212
213
216
220
223
225
226
226
224
223
221
221
220
221
222
223
224
224
223
221
220
220
220
220
220
220
220
220
220
220
221
222
223
225
226
227
227
227
226
225
224
223
223
223
222
222
221
220
216
212
210
210
209
209
208
204
192
167
125
121
125
143
179
199
204
209
214
216
215
208
200
193
191
191
191
191
190
185
164
152
118
70
19
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
83
90
113
136
147
154
157
158
157
156
156
155
156
157
157
155
155
155
157
162
178
182
182
178
165
86
31
8
3
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
4
12
23
38
54
72
95
114
130
143
151
155
154
148
124
71
14
10
22
54
106
142
147
151
153
155
156
155
153
151
147
126
99
64
32
11
7
9
14
21
28
56
122
175
208
217
216
215
213
210
206
201
198
194
191
190
190
191
192
193
193
193
193
192
190
187
178
167
153
138
121
105
102
105
115
131
158
179
196
206
210
211
211
213
214
215
216
216
216
216
216
216
216
216
216
216
216
215
214
213
212
211
212
213
215
218
222
224
225
225
224
222
220
219
219
220
222
223
224
224
224
222
221
220
220
220
220
220
220
220
220
220
220
221
222
224
226
226
227
227
226
225
223
223
222
222
222
221
221
220
216
212
210
209
209
209
208
205
191
167
134
125
127
150
179
198
205
211
214
215
215
208
198
193
192
191
191
191
189
181
167
153
119
69
23
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
84
95
117
139
148
155
158
158
157
156
156
156
157
158
159
157
156
155
155
158
172
180
182
179
171
98
37
10
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
4
8
14
23
36
60
83
106
127
141
151
153
150
134
97
22
9
9
23
52
108
133
145
151
154
155
154
152
150
146
129
103
71
38
13
6
6
9
13
22
40
79
136
187
212
216
216
215
213
210
204
201
198
195
193
193
193
193
194
194
194
193
192
190
187
178
166
153
137
120
105
103
108
119
135
162
183
198
208
210
211
212
213
214
215
216
216
216
216
216
216
216
216
216
216
216
215
214
213
211
211
212
213
215
219
223
224
224
224
223
220
219
218
219
220
223
224
224
224
224
222
221
221
220
220
220
220
220
220
220
220
221
222
224
225
226
226
226
226
225
223
222
222
221
221
221
221
219
216
212
210
209
209
208
208
206
193
169
146
132
129
151
174
194
204
210
213
215
214
211
199
194
192
192
191
191
188
180
169
159
128
76
35
8
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
85
99
122
142
149
157
159
159
157
156
156
157
158
159
159
159
157
155
155
156
166
177
182
180
174
106
42
12
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
4
8
14
30
51
77
103
127
144
148
148
137
109
28
10
3
7
19
61
97
124
141
149
152
152
150
146
142
127
105
75
42
14
6
5
6
8
13
27
52
98
151
199
214
216
215
214
213
208
204
201
199
197
195
195
195
194
194
194
194
193
190
187
177
166
152
136
119
105
104
110
122
138
165
185
200
208
211
211
212
213
214
215
216
216
216
216
216
216
216
216
216
216
215
214
213
212
211
211
212
213
216
219
223
223
223
223
221
218
217
217
218
220
224
224
224
224
224
223
222
222
221
220
220
220
220
220
221
222
223
224
225
226
226
226
225
224
223
221
221
221
221
221
220
218
215
212
210
208
208
208
208
206
194
172
152
138
133
148
174
192
203
208
213
214
213
210
203
194
192
192
192
192
189
181
171
158
141
86
40
13
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
87
104
126
144
150
158
159
159
157
156
156
157
159
160
160
159
159
157
155
155
161
174
181
181
175
111
46
14
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
3
10
24
45
71
100
126
135
137
129
107
31
10
2
2
3
20
54
87
116
136
143
145
144
141
137
122
104
75
44
14
5
5
4
5
6
17
34
65
112
175
210
214
216
215
214
212
207
205
202
200
197
197
196
195
195
194
194
193
191
187
178
166
151
135
117
106
106
112
124
141
167
187
200
209
211
212
212
213
214
215
215
216
216
216
216
216
216
216
216
216
215
214
213
212
211
211
212
213
216
219
222
222
222
221
219
216
216
216
217
220
224
225
225
225
225
224
223
223
222
222
221
221
221
222
222
224
225
225
226
226
226
225
224
222
221
220
220
220
220
219
217
214
211
209
208
208
208
207
204
198
174
152
142
140
144
174
193
203
209
212
214
213
210
204
196
192
192
192
191
190
182
171
159
140
109
41
14
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
87
110
131
147
151
158
160
159
157
156
156
158
159
160
160
160
159
158
156
155
158
171
179
181
177
115
48
14
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
5
14
30
54
94
107
111
105
92
29
10
2
1
1
3
13
37
71
108
126
133
134
132
129
116
98
73
43
13
4
4
4
4
4
7
21
45
81
134
201
211
215
216
215
214
211
207
204
203
199
198
198
197
196
195
194
193
191
187
178
166
151
134
117
107
108
114
126
143
169
187
201
209
211
212
213
213
214
215
215
216
216
216
216
216
216
216
216
216
215
214
213
212
211
211
212
213
216
219
221
221
221
220
218
215
215
215
216
220
224
225
225
225
225
225
225
224
224
224
224
224
224
225
225
225
226
226
226
226
225
223
222
221
220
220
220
219
217
216
212
210
208
208
208
208
207
205
196
178
149
144
142
148
168
197
205
209
212
214
214
210
203
196
192
192
192
191
189
184
171
162
144
111
58
9
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
89
118
138
149
152
159
161
159
157
157
157
158
159
160
160
160
160
159
157
155
157
169
178
181
177
117
48
14
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
4
7
24
36
39
37
30
10
4
2
1
1
1
2
6
16
39
80
97
106
108
106
95
80
55
29
7
2
2
3
3
3
4
7
21
47
88
169
200
213
216
216
215
214
210
207
205
203
201
199
198
197
195
194
193
191
188
178
166
151
134
117
108
109
116
127
143
168
186
200
209
211
212
213
213
214
214
215
215
215
216
216
216
216
216
216
216
215
214
213
211
211
211
211
213
215
218
220
220
219
218
216
214
213
213
215
219
224
225
225
225
225
225
225
225
225
225
225
225
225
225
225
225
226
225
225
224
222
220
220
219
218
217
215
214
212
210
208
208
208
208
207
205
198
183
166
152
147
148
161
180
199
209
211
213
213
213
208
200
195
192
192
192
191
186
180
172
160
136
94
48
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
91
124
143
151
153
160
161
159
157
157
157
158
159
160
160
160
160
159
157
155
157
169
178
181
177
114
46
13
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
3
8
12
13
12
10
4
2
1
1
1
1
1
3
6
14
37
59
74
81
82
74
61
41
20
5
2
2
2
3
3
3
5
13
30
61
133
182
206
214
216
216
215
213
210
206
204
202
201
200
199
196
195
193
191
188
179
167
152
135
118
109
110
117
128
143
166
184
198
208
211
212
213
213
214
214
215
215
215
215
216
216
216
216
216
216
215
214
213
212
211
211
211
212
214
217
218
218
218
217
215
212
212
212
213
216
223
224
225
225
225
225
225
225
225
225
225
225
225
225
225
224
224
222
221
220
218
217
216
215
214
213
212
210
210
209
208
208
207
205
203
190
176
165
157
152
155
168
184
199
208
211
212
213
212
208
199
195
193
192
192
189
185
179
172
163
133
90
50
19
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
94
129
147
153
154
160
161
160
158
157
157
158
159
159
160
160
160
159
157
156
157
169
178
181
176
108
43
12
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
3
2
2
1
1
1
1
1
1
1
1
2
4
13
28
41
51
56
51
41
27
13
4
1
1
1
2
3
3
4
7
18
39
101
158
194
212
215
216
216
215
212
209
206
204
203
201
200
197
195
194
192
188
180
168
153
136
119
110
111
117
127
141
163
181
196
206
211
212
213
213
214
214
215
215
215
215
215
216
216
216
216
216
215
214
213
212
211
211
211
212
213
215
217
217
217
216
215
212
211
211
211
213
220
223
224
225
225
225
225
225
224
224
224
224
223
223
222
221
220
218
217
216
215
214
213
212
212
210
210
209
208
208
207
205
201
195
187
174
165
159
157
159
174
189
201
208
211
212
212
210
206
201
195
193
192
191
189
184
179
171
159
139
89
48
21
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
98
133
150
155
156
161
161
160
158
157
157
158
158
159
160
160
160
159
157
156
157
170
179
181
174
100
38
10
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
3
7
16
24
30
28
23
15
8
3
1
1
1
1
2
2
3
4
9
22
72
128
175
206
214
215
216
216
215
212
208
207
205
203
201
198
196
194
192
189
181
170
155
138
120
111
112
117
127
139
160
177
192
204
210
212
213
213
214
214
214
214
215
215
215
215
216
216
216
216
215
214
213
212
211
211
211
211
212
214
216
216
216
215
214
211
210
209
209
210
216
221
223
224
225
225
224
224
223
223
222
222
221
220
219
217
216
215
214
213
212
211
211
210
210
209
208
208
207
206
202
197
190
181
171
163
161
162
166
175
195
205
210
212
212
212
209
205
200
195
193
192
191
188
185
178
171
157
134
99
42
17
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
102
136
151
156
157
161
161
160
158
157
157
157
158
159
159
159
159
159
157
156
158
172
180
180
171
87
32
8
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
3
5
8
9
7
5
3
2
1
1
1
1
1
2
2
3
4
7
44
98
154
197
212
215
216
216
216
216
211
209
207
205
202
200
196
194
192
190
182
171
157
140
122
113
113
117
125
137
156
172
188
200
208
211
213
213
214
214
214
214
214
215
215
215
215
215
215
215
215
215
214
213
212
211
211
211
212
213
214
215
215
214
214
211
209
208
208
208
210
216
220
223
224
224
224
223
222
222
220
219
218
217
216
214
213
212
212
211
211
210
210
209
209
208
207
206
204
200
190
181
174
168
164
163
164
172
184
200
210
211
212
212
212
209
203
198
195
193
192
190
187
183
179
172
159
130
91
45
9
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
108
137
152
157
158
162
162
161
159
157
157
157
158
158
159
159
159
158
157
156
160
175
181
177
162
65
22
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
3
5
19
58
113
168
206
214
215
216
217
217
215
212
210
208
205
201
198
195
193
191
184
174
160
144
126
114
114
117
124
133
149
164
180
193
203
209
211
213
213
214
214
214
214
214
214
215
215
215
215
215
215
215
214
214
213
212
211
211
211
212
213
213
213
213
213
211
209
208
206
206
205
207
211
215
218
221
222
221
221
220
219
218
217
215
214
213
212
211
211
210
210
209
208
207
206
202
197
192
185
179
173
170
169
168
167
176
189
200
207
211
212
212
211
209
206
199
196
194
193
192
188
184
180
176
170
145
105
63
28
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
111
138
152
158
159
162
162
161
159
158
157
158
158
158
159
159
159
158
157
157
164
178
182
173
146
47
15
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
3
12
38
83
138
194
212
214
215
216
217
216
214
212
210
206
202
198
196
193
192
185
176
163
148
130
116
115
117
123
131
144
157
172
186
197
205
209
212
213
213
214
214
214
214
214
214
215
215
215
215
215
215
215
214
213
212
212
211
211
212
212
212
212
212
212
211
209
208
206
205
204
204
205
207
209
212
214
216
216
217
216
215
215
214
213
211
210
209
208
207
205
203
200
197
194
188
184
181
178
175
172
172
174
179
185
196
203
208
210
211
211
209
207
204
200
196
194
192
189
187
183
180
173
161
140
92
54
27
10
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
115
139
152
159
160
163
163
161
160
158
158
158
158
158
158
159
158
158
157
157
169
180
181
165
124
31
10
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
6
23
57
107
171
208
212
214
215
217
217
216
214
211
208
203
199
196
194
192
186
178
166
152
134
118
116
118
122
128
139
151
164
178
190
200
206
209
212
213
213
214
214
214
214
214
214
214
215
215
215
215
215
214
214
213
212
212
212
212
212
212
212
212
212
211
209
208
207
205
203
203
202
203
203
205
206
207
208
209
209
208
208
207
206
205
204
202
200
199
196
193
191
188
185
181
179
177
176
176
178
181
187
193
200
207
210
211
211
210
208
205
202
199
196
193
190
188
185
182
178
169
152
127
93
45
22
8
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
118
139
152
159
161
163
163
162
160
158
158
158
158
158
158
159
158
158
157
159
175
181
178
154
96
17
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
12
36
77
139
200
209
213
214
216
217
216
215
213
209
204
200
197
195
193
188
180
170
156
139
121
118
118
121
126
135
144
156
168
181
194
200
205
209
212
213
213
213
213
214
214
214
214
214
214
215
215
215
215
214
214
213
213
212
212
212
212
212
212
211
211
210
209
207
206
204
203
202
201
200
200
200
200
200
200
200
200
200
199
198
197
195
194
192
190
187
185
183
182
181
179
179
179
181
183
189
195
201
206
210
212
211
210
209
207
204
200
197
195
193
189
186
184
180
177
165
145
116
81
45
13
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
120
140
152
159
161
164
164
163
161
158
158
158
158
158
158
159
158
157
158
161
181
182
173
138
64
8
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
5
19
49
102
187
204
212
214
214
216
216
215
213
211
205
201
198
196
194
189
182
173
161
145
124
120
119
121
125
132
138
147
158
171
185
193
200
205
209
212
213
213
213
213
213
213
214
214
214
214
214
214
214
214
214
214
213
213
212
212
212
212
211
211
211
210
209
208
207
204
203
202
201
200
198
198
197
196
195
194
194
193
192
191
190
189
188
187
186
184
184
183
182
181
182
184
187
192
198
205
209
211
211
212
211
209
207
205
202
199
196
193
190
187
184
182
180
175
167
138
101
63
31
10
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
123
140
152
159
162
164
164
164
162
160
158
158
158
158
159
159
158
158
161
170
183
181
152
100
29
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
9
25
59
145
188
207
213
214
215
215
215
214
211
207
203
200
197
195
191
186
178
167
154
132
124
121
121
124
129
133
138
146
156
170
180
188
195
201
207
210
211
212
213
213
213
213
213
213
213
214
214
214
214
214
214
214
213
213
213
212
212
212
211
211
210
210
209
208
207
205
204
202
200
199
198
197
196
195
194
193
192
192
191
190
189
188
187
187
186
187
188
191
194
199
203
207
209
210
211
211
211
210
208
206
204
202
199
197
193
190
186
184
182
179
174
164
145
114
65
34
14
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
124
139
151
159
162
165
165
164
163
161
159
159
159
159
159
159
158
159
164
177
181
171
124
64
12
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
5
15
36
103
165
198
211
213
214
215
215
214
212
208
205
201
199
196
192
188
181
173
162
141
129
124
122
124
128
131
134
139
146
158
167
176
184
192
200
204
207
210
211
212
212
212
212
213
213
213
213
213
214
214
214
214
213
213
213
213
213
212
212
212
211
211
210
210
209
208
206
205
204
202
201
200
198
197
196
195
195
194
194
193
194
194
195
195
197
199
201
203
205
207
209
209
210
210
210
209
208
206
205
202
200
197
194
190
186
184
182
180
177
168
150
124
93
60
26
13
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
125
139
150
158
162
165
165
165
164
162
160
159
159
160
160
158
158
162
170
180
179
148
91
35
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
7
18
68
134
180
207
213
214
214
214
214
213
209
206
203
200
197
194
190
185
178
169
150
137
128
124
124
127
129
132
135
139
148
156
164
172
180
190
196
201
205
208
210
211
212
212
212
212
212
212
212
213
213
213
213
213
213
213
213
213
213
213
212
212
212
212
211
210
210
209
208
207
206
205
204
203
203
202
201
201
201
201
201
202
202
203
204
206
207
208
209
210
210
210
209
209
209
207
206
204
203
200
197
194
191
187
184
181
179
176
170
161
135
107
77
48
25
7
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
126
139
149
157
162
165
165
165
164
163
161
161
161
161
160
158
160
167
175
181
170
115
57
14
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
7
39
98
155
197
211
213
214
214
214
213
210
207
204
201
199
195
192
188
182
175
160
146
135
128
126
126
129
131
133
135
140
146
152
160
167
178
185
191
197
201
206
208
210
211
211
211
211
212
212
212
212
212
213
213
213
213
213
213
213
213
213
213
212
212
212
212
211
211
211
211
210
209
209
209
208
208
208
208
208
208
208
209
209
209
210
210
210
210
210
210
209
209
208
207
206
204
202
200
198
195
190
187
184
181
179
176
172
163
149
128
89
59
34
16
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
127
138
148
156
161
164
165
165
165
164
163
163
162
161
159
159
163
172
179
178
149
72
25
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
3
14
61
122
177
208
213
213
214
214
214
211
209
206
203
200
196
194
190
185
180
168
156
144
134
128
128
129
130
132
133
135
138
143
148
154
164
171
179
185
192
199
203
206
208
210
210
211
211
211
211
211
211
211
212
212
212
212
212
213
213
213
213
212
212
212
212
212
212
212
212
212
212
212
211
211
211
211
211
211
211
211
211
210
210
210
210
210
209
209
208
207
206
206
204
203
201
198
194
191
187
182
180
178
177
175
169
157
136
109
76
39
18
7
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
127
138
147
155
160
163
164
165
165
164
164
163
161
159
159
161
170
174
172
160
76
28
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
7
24
70
130
191
211
213
213
214
214
212
210
208
205
202
199
196
193
189
185
178
169
158
148
138
132
131
131
132
133
135
135
137
139
142
148
154
160
167
173
182
188
193
198
202
206
208
209
209
210
210
210
210
210
210
210
211
211
211
211
211
212
212
212
212
212
212
212
212
212
212
211
211
211
211
211
211
211
210
210
210
209
209
209
208
207
207
207
206
206
204
203
200
197
194
189
185
182
179
177
176
174
171
166
156
128
100
70
42
23
5
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
128
138
147
153
158
163
164
164
165
164
164
162
161
160
161
167
173
171
154
111
39
15
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
13
43
89
153
204
211
213
213
213
213
211
209
207
204
201
198
195
192
188
183
176
168
159
150
140
137
135
134
134
135
136
137
138
140
143
146
150
154
160
167
173
179
184
190
196
200
203
205
207
208
209
209
209
209
209
209
209
209
210
210
210
210
210
210
210
210
210
210
210
210
210
210
210
210
209
209
209
209
209
208
208
208
207
207
206
205
204
202
200
197
194
190
187
183
179
177
175
174
172
169
162
150
133
111
77
51
31
18
8
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
128
139
147
152
157
161
163
164
164
164
162
161
160
161
162
170
171
156
123
73
23
10
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
6
23
56
111
185
204
212
213
213
213
212
210
208
206
203
200
197
194
191
186
182
177
170
162
152
146
143
140
138
137
138
138
139
139
141
143
145
148
151
156
161
165
170
176
183
188
192
196
199
202
204
206
207
207
208
208
208
208
208
208
208
209
209
209
209
209
209
209
209
209
209
209
208
208
208
208
207
207
207
206
206
205
204
203
200
198
196
193
190
186
183
180
177
175
173
171
169
166
161
147
131
111
89
65
36
20
11
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
129
140
147
152
155
160
162
163
164
163
161
160
161
162
164
171
162
132
91
49
19
9
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
10
31
69
153
190
207
212
213
213
213
211
210
208
205
202
199
197
194
190
187
183
178
173
164
158
153
149
145
142
141
141
140
141
141
142
143
145
146
149
152
155
159
163
169
174
178
183
187
193
196
199
201
203
204
205
206
206
207
207
207
207
207
207
207
207
207
207
207
207
207
207
206
206
206
205
205
204
203
202
200
199
197
195
191
188
185
182
179
176
173
172
170
169
166
163
157
148
136
111
88
66
45
28
9
5
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
130
141
147
151
154
159
162
163
162
160
160
161
161
163
167
169
144
107
70
44
23
11
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
12
32
108
166
198
211
213
213
213
212
211
209
207
204
201
199
196
192
190
187
185
182
175
170
166
162
158
153
150
147
145
143
142
142
143
144
145
146
149
150
152
154
158
161
165
169
172
178
182
186
190
193
197
199
201
203
204
204
205
205
205
205
205
205
205
204
204
204
204
203
203
202
201
200
199
197
196
193
190
188
185
182
178
175
173
171
169
168
167
166
165
163
157
147
132
114
91
61
41
26
15
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
131
142
148
151
153
159
160
159
159
160
161
159
159
162
170
155
117
90
70
52
25
12
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
10
48
109
163
200
211
212
213
213
212
211
209
207
204
202
199
196
193
191
190
188
185
182
179
177
175
172
169
166
163
159
154
151
148
146
146
146
148
149
150
152
154
155
157
158
160
162
165
167
169
172
175
178
181
184
186
189
191
192
194
194
195
195
195
195
194
194
192
191
190
188
186
184
182
180
178
175
173
171
169
167
166
165
164
164
163
161
158
153
144
131
107
87
68
49
31
13
9
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
131
143
149
151
153
158
158
159
160
161
159
157
161
167
168
131
108
95
83
65
32
16
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
24
68
121
170
203
211
212
212
212
212
210
209
206
204
202
198
196
194
192
191
190
188
186
184
183
181
180
178
176
174
171
168
164
161
157
152
151
150
150
151
153
154
155
156
158
160
161
162
164
165
167
168
169
170
172
173
174
175
177
177
178
179
179
180
180
179
179
178
177
176
174
173
171
170
168
166
165
164
163
163
162
161
159
156
152
141
129
115
100
83
60
45
32
20
10
5
3
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
133
144
149
151
153
157
158
159
160
160
156
156
164
168
158
120
113
110
105
92
52
27
11
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
9
35
79
130
180
207
210
212
212
212
211
210
208
206
204
201
199
196
194
193
192
191
191
190
188
188
187
186
186
185
183
181
178
176
172
167
163
160
157
155
155
155
155
156
157
158
159
160
161
162
163
163
164
164
165
165
166
166
166
167
167
167
168
168
167
167
167
166
166
165
164
163
163
162
161
160
159
159
158
157
153
148
142
134
123
105
90
73
57
43
27
19
12
6
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
134
145
149
151
153
157
159
161
161
157
154
160
168
166
144
121
127
130
130
123
84
49
23
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
12
41
85
143
194
206
211
212
212
212
211
210
208
207
204
202
199
197
195
194
193
193
193
192
192
192
192
191
191
191
190
189
188
186
183
179
175
171
166
162
161
159
159
158
159
159
160
161
161
162
162
163
163
163
163
163
162
162
162
162
161
161
161
160
160
159
159
159
158
158
158
157
157
157
156
155
154
151
148
138
128
116
102
86
65
49
35
24
15
7
5
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
136
146
150
151
153
157
163
163
159
153
154
169
172
160
131
135
145
150
150
148
127
82
43
15
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
13
42
92
166
195
208
211
212
212
212
211
210
209
207
204
202
200
197
195
195
194
194
194
195
195
195
195
195
195
195
194
194
193
193
192
191
189
186
181
177
172
168
165
162
161
161
161
162
162
163
163
163
163
163
163
163
162
162
162
161
161
160
160
159
159
158
158
158
157
156
156
155
154
153
151
147
141
133
114
99
83
66
49
29
18
11
6
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
137
146
150
151
154
162
164
161
156
155
168
177
163
144
135
153
155
155
153
151
146
127
86
41
10
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
10
30
96
151
187
207
211
211
212
212
211
210
209
208
206
204
202
199
197
196
195
195
195
196
196
196
196
196
196
197
197
197
197
197
197
197
196
195
193
192
190
187
184
181
179
176
174
172
171
171
170
170
170
169
169
169
168
167
167
166
166
165
164
163
163
162
161
160
159
157
156
154
150
144
134
120
102
77
60
44
30
18
8
4
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
138
146
150
153
158
167
164
160
158
162
179
179
159
140
144
157
157
156
153
150
147
139
110
69
22
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
4
12
49
97
145
185
206
211
211
212
212
212
210
209
208
207
205
203
201
199
198
197
196
196
196
196
196
196
197
197
197
198
198
198
198
198
198
198
197
197
196
196
194
193
191
190
189
187
185
184
183
183
182
181
181
180
180
179
178
178
177
177
175
174
172
170
169
165
162
159
156
152
144
132
115
96
76
52
37
24
14
7
3
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
139
147
150
154
162
169
163
161
164
174
184
174
154
140
151
160
160
157
153
149
146
143
125
91
38
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
20
53
97
143
183
207
210
211
212
212
211
211
210
209
208
206
205
203
201
200
199
199
198
198
197
197
197
198
198
198
199
199
199
199
199
199
199
199
199
200
199
199
198
198
198
197
196
195
194
193
192
191
191
190
189
188
187
186
185
184
181
179
177
174
171
167
162
157
150
141
123
105
86
67
50
31
20
11
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
141
147
151
156
166
170
163
164
171
185
186
166
149
142
156
163
162
157
152
147
145
143
134
107
54
9
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
5
21
51
92
141
189
202
209
211
211
212
211
211
210
210
209
207
206
205
204
203
202
201
200
200
199
199
199
199
200
200
200
201
201
201
200
200
201
201
201
201
201
202
202
202
202
202
202
201
201
200
199
198
197
196
194
193
191
189
186
182
179
176
172
167
159
151
142
131
116
91
72
54
39
26
15
9
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
143
148
152
158
170
169
164
168
177
188
186
157
145
147
159
164
163
157
151
145
143
142
137
117
70
13
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
5
19
44
82
147
184
203
210
210
211
212
211
211
211
210
210
209
208
207
206
205
204
203
202
202
202
202
202
202
203
203
203
203
202
202
202
202
202
201
201
202
202
203
203
204
204
204
204
204
204
203
201
200
198
195
193
190
187
184
179
175
169
162
154
137
123
107
90
73
52
37
25
15
8
5
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
144
148
155
163
173
166
165
173
184
189
181
150
141
147
160
161
155
149
144
143
143
142
138
122
83
19
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
10
26
68
110
150
183
204
210
211
212
212
212
212
212
211
211
211
211
211
210
210
210
209
209
208
208
208
207
207
207
206
206
205
204
203
203
202
202
202
202
203
203
203
204
204
205
205
203
201
198
195
191
185
180
172
164
154
140
128
115
102
88
71
58
44
32
23
14
10
6
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
144
149
157
166
173
164
165
176
187
190
173
147
141
146
155
151
147
144
143
142
141
140
137
123
88
21
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
9
32
60
93
127
159
189
200
206
209
211
211
212
212
212
212
212
211
211
211
211
211
211
210
210
210
209
209
208
208
207
206
205
205
204
203
202
202
202
202
203
203
202
202
201
198
191
185
178
170
161
148
138
127
116
105
89
78
67
56
45
35
26
19
13
8
5
3
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
146
150
159
167
170
162
167
179
189
189
164
144
139
143
146
144
142
142
141
141
139
139
136
123
90
23
8
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
11
27
48
74
104
140
161
177
189
198
204
206
208
209
209
210
210
210
210
210
210
210
210
210
209
209
209
208
208
207
206
205
204
203
202
201
201
201
201
200
199
196
192
186
178
164
154
143
131
119
103
91
80
70
60
48
39
31
24
19
13
8
6
4
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
147
151
159
166
166
160
170
181
190
188
156
141
138
139
139
140
141
141
140
139
138
137
135
122
90
23
8
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
7
18
33
52
83
106
128
147
163
176
183
188
192
195
197
199
200
201
201
202
202
202
203
203
203
203
203
203
203
202
201
201
200
199
197
196
195
193
190
184
177
168
157
144
126
112
99
86
74
59
49
40
32
25
19
14
9
6
5
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
147
152
160
165
162
159
174
184
189
184
148
139
136
136
137
141
141
140
139
137
137
136
133
120
87
22
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
8
17
34
50
66
81
96
115
128
139
149
157
165
169
172
175
178
180
182
184
186
188
189
190
191
192
192
192
192
191
190
189
187
184
179
174
166
153
140
126
112
98
80
67
55
43
33
24
18
12
8
5
4
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
147
152
157
159
155
159
180
187
183
172
142
137
136
138
141
141
141
140
138
137
136
136
132
115
77
17
6
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
6
11
18
24
31
37
41
45
50
54
60
64
67
71
74
78
81
85
89
93
100
104
108
112
115
119
121
122
122
121
117
112
106
99
91
79
70
60
50
41
30
23
17
11
6
4
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
145
151
154
154
152
168
183
184
176
163
141
139
139
141
142
142
141
140
140
138
137
136
130
110
67
13
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
4
6
9
11
14
15
17
18
20
22
23
25
26
28
30
31
33
35
38
41
44
47
50
53
56
58
60
61
61
59
56
53
49
45
38
33
27
22
17
11
8
6
4
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
142
148
150
150
153
177
183
178
167
154
141
141
142
143
142
142
142
142
142
141
139
137
128
103
55
9
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
3
3
4
4
4
4
5
5
6
6
7
8
8
9
10
11
12
14
15
17
18
20
21
22
23
23
23
22
20
19
17
14
11
9
7
5
3
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
138
144
147
150
158
182
179
169
157
148
143
144
143
143
143
143
145
145
145
143
141
137
123
93
43
6
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
3
3
3
3
4
4
4
5
5
5
5
4
4
3
2
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
138
142
145
153
171
181
171
159
150
145
145
144
143
143
143
146
148
149
148
146
143
138
117
81
31
4
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
139
143
151
162
175
168
157
150
146
145
145
143
144
146
150
156
157
155
153
149
145
136
104
62
18
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
142
148
158
167
170
157
150
147
146
145
143
144
148
153
157
159
158
156
153
150
146
130
92
47
12
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
145
153
161
165
159
149
147
146
145
144
143
149
154
158
160
160
159
157
154
152
146
120
78
35
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
148
157
160
158
148
145
145
145
145
144
146
155
158
159
159
159
158
157
155
153
143
108
64
24
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
148
157
154
147
143
144
145
145
145
144
155
158
157
156
155
157
157
157
156
153
139
93
49
15
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
148
148
143
138
143
144
145
144
145
148
159
153
151
150
151
153
155
156
155
151
118
68
30
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
143
139
137
138
143
144
145
145
147
153
155
151
149
149
150
151
152
152
149
141
92
47
18
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
138
134
135
138
143
145
145
145
148
154
152
150
149
149
149
149
149
147
138
121
65
29
10
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
134
131
134
138
143
145
145
145
148
153
151
150
149
149
148
147
145
138
123
94
40
16
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
130
130
134
139
143
145
144
145
148
151
150
150
149
149
148
146
140
126
101
64
19
7
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
128
130
134
139
144
145
144
146
149
150
151
150
150
149
148
144
129
101
67
31
6
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
128
130
134
139
144
145
144
146
149
150
151
151
150
148
147
141
115
81
46
17
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
128
130
134
139
144
145
144
146
148
150
151
150
149
148
146
135
99
61
29
8
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
128
130
134
139
145
145
144
146
148
150
151
150
148
147
145
126
82
44
16
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
128
130
134
140
145
145
145
146
148
150
150
149
148
146
143
112
64
28
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
128
130
134
139
145
145
145
146
147
149
149
148
146
143
137
87
39
13
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
128
129
133
139
145
145
145
146
147
149
148
146
144
140
129
62
25
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
128
129
133
139
145
146
146
147
148
148
147
145
143
134
112
39
14
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
128
128
132
138
145
146
147
147
148
147
145
144
140
124
88
21
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
127
128
131
137
145
146
147
147
147
145
143
142
135
111
58
7
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
127
127
130
136
143
146
146
145
144
142
142
139
118
78
22
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
127
127
130
134
139
144
143
142
142
142
141
130
93
46
9
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
127
127
130
133
137
140
140
141
141
141
137
108
64
22
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
128
128
129
132
135
138
139
140
140
139
123
76
35
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
128
128
129
132
135
136
139
140
139
136
94
39
11
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
129
130
131
132
134
138
139
138
129
107
29
9
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
130
131
132
133
136
139
138
127
101
52
10
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
132
133
134
136
138
137
126
97
59
18
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
135
135
136
137
137
125
97
57
20
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
137
137
137
137
134
94
44
16
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
136
134
125
104
66
18
6
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
105
88
67
45
22
6
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
55
41
26
13
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
13
7
4
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
```

### `HDF5Examples/C/Perf/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_C_PERFORM C)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (C_sourcefiles.cmake)

foreach (example ${examples})
  get_filename_component (example_name ${example} NAME_WE)
  add_executable (${EXAMPLE_VARNAME}_${example_name} ${PROJECT_SOURCE_DIR}/${example})
  target_compile_options(${EXAMPLE_VARNAME}_${example_name}
      PRIVATE
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  if (H5_HAVE_PARALLEL)
    target_include_directories (${EXAMPLE_VARNAME}_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
  endif ()
  target_link_libraries (${EXAMPLE_VARNAME}_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
endforeach ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_${testname}-clearall
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${testname}.h5
    )
    add_test (
        NAME ${EXAMPLE_VARNAME}_${testname}
        COMMAND "${CMAKE_COMMAND}"
            -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_${testname}>"
            -D "TEST_ARGS:STRING="
            -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
            -D "TEST_EXPECT=0"
            -D "TEST_OUTPUT=${testname}.out"
            -D "TEST_REFERENCE=${testname}.tst"
            -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
            -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
    )
    set_tests_properties (${EXAMPLE_VARNAME}_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_${testname}-clearall)
    if (HDF5_PROVIDES_TOOLS)
      add_test (
          NAME ${EXAMPLE_VARNAME}_H5DUMP-${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
              -D "TEST_ARGS:STRING=${ARGN};${testname}.h5"
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_OUTPUT=${testname}.dmp.out"
              -D "TEST_EXPECT=0"
              -D "TEST_REFERENCE=${testname}.dmp"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_${testname})
    endif ()
  endmacro ()

  foreach (example ${examples})
    get_filename_component (example_name ${example} NAME_WE)
    set (testdest "${PROJECT_BINARY_DIR}/${example_name}.test")
    #message (STATUS " Copying ${example_name}.test")
    add_custom_command (
        TARGET     ${EXAMPLE_VARNAME}_${example_name}
        POST_BUILD
        COMMAND    ${CMAKE_COMMAND}
        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/${example_name}.test ${testdest}
    )
    ADD_H5_TEST (${example_name})
  endforeach ()
endif ()
```

### `HDF5Examples/C/Perf/C_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (examples
    h5slabread.c
    h5slabwrite.c
    h5efc.c
)
```

### `HDF5Examples/C/Perf/h5efc.c`

```c
/************************************************************

  This example shows how to use the external file cache.

  This file is intended for use with HDF5 Library version
  1.8.7 or newer

 ************************************************************/

#include "hdf5.h"

#define FILENAME  "h5efc.h5"
#define EXT_FILE1 "h5efc1.h5"
#define EXT_FILE2 "h5efc2.h5"
#define EXT_FILE3 "h5efc3.h5"

int
main(void)
{
    hid_t  file1, file2, group, fapl; /* Handles */
    herr_t status;

    /*
     * Create file access property list and set it to allow caching of open
     * files visited through external links.
     */
    fapl   = H5Pcreate(H5P_FILE_ACCESS);
    status = H5Pset_elink_file_cache_size(fapl, 8);

    /*
     * Create a new file using the file access property list.
     */
    file1 = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, fapl);

    /*
     * Create files to serve as targets for external links.
     */
    file2  = H5Fcreate(EXT_FILE1, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Fclose(file2);
    file2  = H5Fcreate(EXT_FILE2, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Fclose(file2);
    file2  = H5Fcreate(EXT_FILE3, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    status = H5Fclose(file2);

    /*
     * Create external links to the target files.
     */
    status = H5Lcreate_external(EXT_FILE1, "/", file1, "link_to_1", H5P_DEFAULT, H5P_DEFAULT);
    status = H5Lcreate_external(EXT_FILE2, "/", file1, "link_to_2", H5P_DEFAULT, H5P_DEFAULT);
    status = H5Lcreate_external(EXT_FILE3, "/", file1, "link_to_3", H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Open and close the targets of all three external links (these will be the
     * root groups of the target files).  The target files should be held open
     * by the root file's external file cache after traversal.
     */
    group  = H5Gopen(file1, "/link_to_1", H5P_DEFAULT);
    status = H5Gclose(group);
    group  = H5Gopen(file1, "/link_to_2", H5P_DEFAULT);
    status = H5Gclose(group);
    group  = H5Gopen(file1, "/link_to_3", H5P_DEFAULT);
    status = H5Gclose(group);

    /*
     * Open and close the targets of all three external links again.  The target
     * files should already be held open by the root file's external file cache,
     * so the library will not actually have to issue an "open" system call.
     */
    group  = H5Gopen(file1, "/link_to_1", H5P_DEFAULT);
    status = H5Gclose(group);
    group  = H5Gopen(file1, "/link_to_2", H5P_DEFAULT);
    status = H5Gclose(group);
    group  = H5Gopen(file1, "/link_to_3", H5P_DEFAULT);
    status = H5Gclose(group);

    /*
     * Release the root file's external file cache.  This will close all the
     * external link target files.
     */
    status = H5Frelease_file_cache(file1);

    /*
     * Close and release resources.
     */
    status = H5Pclose(fapl);
    status = H5Fclose(file1);

    return 0;
}
```

### `HDF5Examples/C/Perf/h5slab.h`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/* Common definitions used by h5slab example programs (h5slabread, h5slabwrite).
 *
 * Created by Albert Cheng 2010/7/13.
 */
#include <stdlib.h>
#include <string.h>
#include "hdf5.h"
#if 1
#define NX 65536
#define NY 65536 /* dataset dimensions */
#define CX 256   /* height of hyperslab */
#define CY 4096  /* width of hyperslab */
#else
#define NX 256
#define NY 256 /* dataset dimensions */
#define CX 32  /* height of hyperslab */
#define CY 16  /* width of hyperslab */
#endif
#define RC (NX / CX)
#define CC (NY / CY)
```

### `HDF5Examples/C/Perf/h5slabread.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include <stdlib.h>
#include <string.h>

#include "../Perf/h5slab.h"
#include "hdf5.h"

/* Read the chunks in a row pattern.
 *
 * Created by Albert Cheng and Christian Chilan 2010/7/13.
 */
int
main(int argc, char **argv)
{
    hid_t         file_id, dset_id, filespace, memspace, fapl, dxpl;
    hsize_t       dimsf[2], count[2], offset[2], chunk_dims[2] = {CX, CY};
    char         *data, table[RC];
    unsigned long i, j, cx;

    fapl = H5Pcreate(H5P_FILE_ACCESS);
    dxpl = H5Pcreate(H5P_DATASET_XFER);
    fapl = dxpl = H5P_DEFAULT;
    file_id     = H5Fopen(argv[1], H5F_ACC_RDONLY, fapl);
    dset_id     = H5Dopen(file_id, "dataset1", H5P_DEFAULT);
    filespace   = H5Dget_space(dset_id);
    count[0]    = CX;
    count[1]    = NY;
    memspace    = H5Screate_simple(2, count, NULL);

    data = (char *)malloc(count[0] * count[1] * sizeof(char));
    for (i = 0; i < RC; i++) {
        offset[0] = i * CX;
        offset[1] = 0;
        H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, count, NULL);
        H5Dread(dset_id, H5T_NATIVE_CHAR, memspace, filespace, dxpl, data);
    }
    free(data);
    H5Dclose(dset_id);
    H5Sclose(filespace);
    H5Sclose(memspace);
    H5Pclose(dxpl);
    H5Pclose(fapl);
    H5Fclose(file_id);
    return 0;
}
```

### `HDF5Examples/C/Perf/h5slabwrite.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/* This example shows different data writing patterns to generate data files
 * that will exhibit substantially different data read speed. The files
 * contain 2-D chunk storage datasets.
 * The 2 writing patterns are:
 * 1. Random--chunks are wriitten in the random order.
 * 2. ByRow--chunks are written by row order.
 */

#include <stdlib.h>
#include <string.h>

#include "../Perf/h5slab.h"
#include "hdf5.h"

/* Write the chunks in the row order.  This provides good and bad read
 * performance if the read pattern is by row and by column respectively.
 *
 * Created by Albert Cheng and Christian Chilan 2010/7/13.
 */
int
createfilebyrow(void)
{
    hid_t         file_id, dset_id, filespace, memspace, fapl, dxpl, dcpl;
    hsize_t       dimsf[2], count[2], offset[2], chunk_dims[2] = {CX, CY};
    char         *data, dataval, table[RC];
    unsigned long i, j, l, cx;
    fapl = H5Pcreate(H5P_FILE_ACCESS);
    dcpl = H5Pcreate(H5P_DATASET_CREATE);
    dxpl = H5Pcreate(H5P_DATASET_XFER);
    H5Pset_chunk(dcpl, 2, chunk_dims);
    fapl = dxpl = H5P_DEFAULT;
    file_id     = H5Fcreate("row_alloc.h5", H5F_ACC_TRUNC, H5P_DEFAULT, fapl);
    dimsf[0]    = NX;
    dimsf[1]    = NY;
    filespace   = H5Screate_simple(2, dimsf, NULL);
    dset_id     = H5Dcreate(file_id, "dataset1", H5T_NATIVE_CHAR, filespace, H5P_DEFAULT, dcpl, H5P_DEFAULT);
    count[0]    = CX;
    count[1]    = NY;
    memspace    = H5Screate_simple(2, count, NULL);

    data = (char *)malloc(count[0] * count[1] * sizeof(char));

    /* writing the whole chunked rows each time. */
    for (l = 0; l < RC; l++) {

        offset[0] = l * CX;
        offset[1] = 0;

        /* fill with values according to row number */
        for (i = 0; i < count[0]; i++)
            for (j = 0; j < count[1]; j++)
                data[i * count[1] + j] = l;

        H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, count, NULL);
        H5Dwrite(dset_id, H5T_NATIVE_CHAR, memspace, filespace, dxpl, data);
    }

    free(data);
    H5Dclose(dset_id);
    H5Sclose(filespace);
    H5Sclose(memspace);
    H5Pclose(dxpl);
    H5Pclose(dcpl);
    H5Pclose(fapl);
    H5Fclose(file_id);
    return 0;
}

/* Write the chunks in a random pattern.  This provides a read performance
 * worse than when the chunks are written and read in the same order, whether
 * it is by row or by column.
 *
 * Created by Albert Cheng and Christian Chilan 2010/7/13.
 */
int
createfilerandom(void)
{
    hid_t         file_id, dset_id, filespace, memspace, fapl, dxpl, dcpl;
    hsize_t       dimsf[2], count[2], offset[2], chunk_dims[2] = {CX, CY};
    char         *data, table[RC][CC];
    unsigned long i, j, cx, cy;
    fapl = H5Pcreate(H5P_FILE_ACCESS);
    dcpl = H5Pcreate(H5P_DATASET_CREATE);
    dxpl = H5Pcreate(H5P_DATASET_XFER);
    H5Pset_chunk(dcpl, 2, chunk_dims);
    fapl = dxpl = H5P_DEFAULT;
    file_id     = H5Fcreate("random_alloc.h5", H5F_ACC_TRUNC, H5P_DEFAULT, fapl);
    dimsf[0]    = NX;
    dimsf[1]    = NY;
    filespace   = H5Screate_simple(2, dimsf, NULL);
    dset_id     = H5Dcreate(file_id, "dataset1", H5T_NATIVE_CHAR, filespace, H5P_DEFAULT, dcpl, H5P_DEFAULT);
    count[0]    = CX;
    count[1]    = CY;
    memspace    = H5Screate_simple(2, count, NULL);
    data        = (char *)malloc(count[0] * count[1] * sizeof(char));

    for (i = 0; i < RC; i++)
        for (j = 0; j < CC; j++)
            table[i][j] = 0;

    for (i = 0; i < RC * CC; i++) {
        do {
            cx = rand() % RC;
            cy = rand() % CC;
        } while (table[cx][cy]);

        for (j = 0; j < count[0] * count[1]; j++) {
            data[j] = cx + cy;
        }

        table[cx][cy] = 1;

        offset[0] = cx * CX;
        offset[1] = cy * CY;

        H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, count, NULL);
        H5Dwrite(dset_id, H5T_NATIVE_CHAR, memspace, filespace, dxpl, data);
    }

    free(data);
    H5Dclose(dset_id);
    H5Sclose(filespace);
    H5Sclose(memspace);
    H5Pclose(dxpl);
    H5Pclose(dcpl);
    H5Pclose(fapl);
    H5Fclose(file_id);
    return 0;
}

int
main(int argc, char **argv)
{
    createfilebyrow();
    createfilerandom();
}
```

### `HDF5Examples/C/TUTR/Attributes.txt`

```
Attribute Examples:

H5Acreate2 example: Show how to create an attribute for a dataset and a group
----------------
{
    hid_t file;
    hid_t group;
    hid_t dataset;
    hid_t attr;
    hid_t dataspace;
    int32 attr_data;
    int rank;
    size_t dimsf[2];

    /* Open the file */
    file=H5Fopen("example.h5", H5F_ACC_RDWR, H5P_DEFAULT);

    /* Describe the size of the array and create the data space */
    rank=2;
    dimsf[0] = H5S_UNLIMITED;
    dimsf[1] = H5S_UNLIMITED;
    dataspace = H5Screate_simple(rank, dimsf, NULL);

    /* Create a dataset */
    dataset = H5Dcreate2(file, "Dataset1", H5T_UINT8, dataspace, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    <Write data to first dataset>

    /* Create an attribute for the dataset */
    attr = H5Acreate2(dataset, "Attr1", H5T_INT32, H5S_SCALAR, H5P_DEFAULT, H5P_DEFAULT);

    /* Write attribute information */
    H5Awrite(attr, H5T_INT32, &attr_data);

    /* Close attribute */
    H5Aclose(attr);

    /* Close dataset */
    H5Dclose(dataset);

    /* Create a group */
    group = H5Gcreate2(file, "/Group One", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /* Create an attribute for the dataset */
    attr = H5Acreate2(group, "Attr1", H5T_INT32, H5S_SCALAR, H5P_DEFAULT, H5P_DEFAULT);

    /* Write attribute information */
    H5Awrite(attr, H5T_INT32, &attr_data);

    /* Close attribute */
    H5Aclose(attr);

    /* Close the group */
    H5Gclose(group);

    /* Close file */
    H5Fclose(file);
}


H5Aiterate example: Print all the names of attributes of a dataset, without
                        any buffers.
---------------------

herr_t print_names (hid_t loc_id, const char *name, void *opdata)
{
    puts (name);
    return 0;
}

{
    H5Aiterate (dataset_or_group_id, NULL, print_names, NULL);
}


H5Aread Example: Attach to an attribute of a dataset and read in attr. data
----------------
{
    hid_t file;
    hid_t dataset;
    hid_t attr;
    int32 attr_data;

    /* Open the file */
    file=H5Fopen("example.h5", H5F_ACC_RDWR, H5P_DEFAULT);

    /* Open the dataset */
    dataset=H5Dopen2(file, "Dataset1", H5P_DEFAULT);

    /* Get the OID of the attribute */
    attr=H5Aopen(dataset, "Attr1", H5P_DEFAULT);

    /* Read attribute */
    H5Aread(attr,H5T_INT32,attr_data);

    /* Close attribute dataset */
    H5Aclose(attr);

    /* Close first dataset */
    H5Dclose(dataset);

    /* Close file */
    H5Fclose(file);
}

H5Alink Example: Shows how to share an attribute between two datasets.
----------------
{
    hid_t file;
    hid_t dataset1, dataset2;
    hid_t attr;

    /* Open the file */
    file=H5Fopen("example.h5", H5F_ACC_RDWR, H5P_DEFAULT);

    /* Open the first dataset */
    dataset1=H5Dopen2(file, "Dataset1", H5P_DEFAULT);

    /* Open the first dataset */
    dataset2=H5Dopen2(file, "Dataset2", H5P_DEFAULT);

    /* Get the OID of the attribute */
    attr=H5Aopen(dataset1, "Foo", H5P_DEFAULT);

    /*
     * Create an attribute in the second dataset to the attribute in dataset1,
     * changing the name of the attribute information in dataset2.
     */
    H5Alink(dataset2, attr, "Bar");

    /* Close attribute dataset */
    H5Aclose(attr);

    /* Close datasets */
    H5Dclose(dataset1);
    H5Dclose(dataset2);

    /* Close file */
    H5Fclose(file);
}
```

### `HDF5Examples/C/TUTR/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_C_TUTR C)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (C_sourcefiles.cmake)

foreach (example_name ${examples})
  add_executable (${EXAMPLE_VARNAME}_tutr_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
  target_compile_options (${EXAMPLE_VARNAME}_tutr_${example_name}
      PRIVATE
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  if (H5_HAVE_PARALLEL)
    target_include_directories (${EXAMPLE_VARNAME}_tutr_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
  endif ()
  target_link_libraries (${EXAMPLE_VARNAME}_tutr_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
endforeach ()


if (H5EXAMPLE_BUILD_TESTING)
  file (MAKE_DIRECTORY ${PROJECT_BINARY_DIR}/red ${PROJECT_BINARY_DIR}/blue ${PROJECT_BINARY_DIR}/u2w)

  set (${EXAMPLE_VARNAME}_tutr_CLEANFILES
      Attributes.h5
      btrees_file.h5
      cmprss.h5
      default_file.h5
      dset.h5
      extend.h5
      extlink_prefix_source.h5
      extlink_source.h5
      extlink_target.h5
      group.h5
      groups.h5
      hard_link.h5
      mount1.h5
      mount2.h5
      one_index_file.h5
      only_dspaces_and_attrs_file.h5
      only_huge_mesgs_file.h5
      REF_REG.h5
      refere.h5
      refer_deprec.h5
      refer_extern1.h5
      refer_extern2.h5
      SDS.h5
      SDScompound.h5
      SDSextendible.h5
      Select.h5
      separate_indexes_file.h5
      small_lists_file.h5
      soft_link.h5
      subset.h5
      unix2win.h5
      blue/prefix_target.h5
      red/prefix_target.h5
      u2w/u2w_target.h5
  )

   # Remove any output file left over from previous test run
  add_test (
      NAME ${EXAMPLE_VARNAME}_tutr-clear-objects
      COMMAND ${CMAKE_COMMAND} -E remove ${${EXAMPLE_VARNAME}_tutr_CLEANFILES}
  )
  set_tests_properties (${EXAMPLE_VARNAME}_tutr-clear-objects PROPERTIES
      FIXTURES_SETUP clear_${EXAMPLE_VARNAME}_tutr
      WORKING_DIRECTORY ${PROJECT_BINARY_DIR}
  )
  add_test (
      NAME ${EXAMPLE_VARNAME}_tutr-clean-objects
      COMMAND ${CMAKE_COMMAND} -E remove ${${EXAMPLE_VARNAME}_tutr_CLEANFILES}
  )
  set_tests_properties (${EXAMPLE_VARNAME}_tutr-clean-objects PROPERTIES
      FIXTURES_CLEANUP clear_${EXAMPLE_VARNAME}_tutr
      WORKING_DIRECTORY ${PROJECT_BINARY_DIR}
  )

  macro (ADD_H5_TEST testname)
    if (HDF5_USING_ANALYSIS_TOOL)
      add_test (NAME ${EXAMPLE_VARNAME}_tutr_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_tutr_${testname}>)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_tutr_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_tutr_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_SKIP_COMPARE=TRUE"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
    endif ()
    set_tests_properties (${EXAMPLE_VARNAME}_tutr_${testname} PROPERTIES
        FIXTURES_REQUIRED clear_${EXAMPLE_VARNAME}_tutr
        ENVIRONMENT "${CROSSCOMPILING_PATH}"
    )
    if (last_test)
      set_tests_properties (${EXAMPLE_VARNAME}_tutr_${testname} PROPERTIES DEPENDS ${last_test})
    endif ()
    set (last_test "${EXAMPLE_VARNAME}_tutr_${testname}")
  endmacro ()

  foreach (example_name ${examples})
    ADD_H5_TEST (${example_name})
  endforeach ()
endif ()
```

### `HDF5Examples/C/TUTR/C_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (examples
    h5_crtdat
    h5_rdwt
    h5_crtatt
    h5_crtgrp
    h5_crtgrpar
    h5_crtgrpd
    h5_cmprss
    h5_extend
    h5_subset
    h5_write
    h5_read
    h5_extend_write
    h5_chunk_read
    h5_compound
    h5_group
    h5_interm_group
    h5_select
    h5_attribute
    h5_mount
    h5_ref_extern
    h5_ref_compat
    h5_reference_deprec
    h5_drivers
    h5_ref2reg_deprec
    h5_extlink
    h5_elink_unix2win
    h5_shared_mesg
    h5_debug_trace
)
```

### `HDF5Examples/C/TUTR/h5_attribute.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This program illustrates the usage of the H5A Interface functions.
 *  It creates and writes a dataset, and then creates and writes array,
 *  scalar, and string attributes of the dataset.
 *  Program reopens the file, attaches to the scalar attribute using
 *  attribute name and reads and displays its value. Then index of the
 *  third attribute is used to read and display attribute values.
 *  The H5Aiterate function is used to iterate through the dataset attributes,
 *  and display their names. The function is also reads and displays the values
 *  of the array attribute.
 */

#include <stdlib.h>

#include "hdf5.h"

#define H5FILE_NAME "Attributes.h5"

#define RANK 1 /* Rank and size of the dataset  */
#define SIZE 7

#define ARANK  2 /* Rank and dimension sizes of the first dataset attribute */
#define ADIM1  2
#define ADIM2  3
#define ANAME  "Float attribute"     /* Name of the array attribute */
#define ANAMES "Character attribute" /* Name of the string attribute */

static herr_t attr_info(hid_t loc_id, const char *name, const H5A_info_t *ainfo, void *opdata);
/* Operator function */

int
main(void)
{

    hid_t file, dataset; /* File and dataset identifiers */

    hid_t       fid;                 /* Dataspace identifier */
    hid_t       attr1, attr2, attr3; /* Attribute identifiers */
    hid_t       attr;
    hid_t       aid1, aid2, aid3; /* Attribute dataspace identifiers */
    hid_t       atype, atype_mem; /* Attribute type */
    H5T_class_t type_class;

    hsize_t fdim[] = {SIZE};
    hsize_t adim[] = {ADIM1, ADIM2}; /* Dimensions of the first attribute  */

    float matrix[ADIM1][ADIM2]; /* Attribute data */

    herr_t      ret;            /* Return value */
    H5O_info2_t oinfo;          /* Object info */
    unsigned    i, j;           /* Counters */
    char        string_out[80]; /* Buffer to read string attribute back */
    int         point_out;      /* Buffer to read scalar attribute back */

    /*
     * Data initialization.
     */
    int  vector[] = {1, 2, 3, 4, 5, 6, 7}; /* Dataset data */
    int  point    = 1;                     /* Value of the scalar attribute */
    char string[] = "ABCD";                /* Value of the string attribute */

    for (i = 0; i < ADIM1; i++) { /* Values of the array attribute */
        for (j = 0; j < ADIM2; j++)
            matrix[i][j] = -1.;
    }

    /*
     * Create a file.
     */
    file = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create the dataspace for the dataset in the file.
     */
    fid = H5Screate(H5S_SIMPLE);
    ret = H5Sset_extent_simple(fid, RANK, fdim, NULL);

    /*
     * Create the dataset in the file.
     */
    dataset = H5Dcreate2(file, "Dataset", H5T_NATIVE_INT, fid, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Write data to the dataset.
     */
    ret = H5Dwrite(dataset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, vector);

    /*
     * Create dataspace for the first attribute.
     */
    aid1 = H5Screate(H5S_SIMPLE);
    ret  = H5Sset_extent_simple(aid1, ARANK, adim, NULL);

    /*
     * Create array attribute.
     */
    attr1 = H5Acreate2(dataset, ANAME, H5T_NATIVE_FLOAT, aid1, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Write array attribute.
     */
    ret = H5Awrite(attr1, H5T_NATIVE_FLOAT, matrix);

    /*
     * Create scalar attribute.
     */
    aid2  = H5Screate(H5S_SCALAR);
    attr2 = H5Acreate2(dataset, "Integer attribute", H5T_NATIVE_INT, aid2, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Write scalar attribute.
     */
    ret = H5Awrite(attr2, H5T_NATIVE_INT, &point);

    /*
     * Create string attribute.
     */
    aid3  = H5Screate(H5S_SCALAR);
    atype = H5Tcopy(H5T_C_S1);
    H5Tset_size(atype, 5);
    H5Tset_strpad(atype, H5T_STR_NULLTERM);
    attr3 = H5Acreate2(dataset, ANAMES, atype, aid3, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Write string attribute.
     */
    ret = H5Awrite(attr3, atype, string);

    /*
     * Close attribute and file dataspaces, and datatype.
     */
    ret = H5Sclose(aid1);
    ret = H5Sclose(aid2);
    ret = H5Sclose(aid3);
    ret = H5Sclose(fid);
    ret = H5Tclose(atype);

    /*
     * Close the attributes.
     */
    ret = H5Aclose(attr1);
    ret = H5Aclose(attr2);
    ret = H5Aclose(attr3);

    /*
     * Close the dataset.
     */
    ret = H5Dclose(dataset);

    /*
     * Close the file.
     */
    ret = H5Fclose(file);

    /*
     * Reopen the file.
     */
    file = H5Fopen(H5FILE_NAME, H5F_ACC_RDONLY, H5P_DEFAULT);

    /*
     * Open the dataset.
     */
    dataset = H5Dopen2(file, "Dataset", H5P_DEFAULT);

    /*
     * Attach to the scalar attribute using attribute name, then read and
     * display its value.
     */
    attr = H5Aopen(dataset, "Integer attribute", H5P_DEFAULT);
    ret  = H5Aread(attr, H5T_NATIVE_INT, &point_out);
    printf("The value of the attribute \"Integer attribute\" is %d \n", point_out);
    ret = H5Aclose(attr);

    //! [H5Oget_info3_snip]

    /*
     * Find string attribute by iterating through all attributes
     */
    ret = H5Oget_info3(dataset, &oinfo, H5O_INFO_NUM_ATTRS);
    for (i = 0; i < (unsigned)oinfo.num_attrs; i++) {
        attr       = H5Aopen_by_idx(dataset, ".", H5_INDEX_CRT_ORDER, H5_ITER_INC, (hsize_t)i, H5P_DEFAULT,
                                    H5P_DEFAULT);
        atype      = H5Aget_type(attr);
        type_class = H5Tget_class(atype);
        if (type_class == H5T_STRING) {
            atype_mem = H5Tget_native_type(atype, H5T_DIR_ASCEND);
            ret       = H5Aread(attr, atype_mem, string_out);
            printf("Found string attribute; its index is %d , value =   %s \n", i, string_out);
            ret = H5Tclose(atype_mem);
        }
        ret = H5Aclose(attr);
        ret = H5Tclose(atype);
    }

    //! [H5Oget_info3_snip]
    /*
     * Get attribute info using iteration function.
     */
    ret = H5Aiterate2(dataset, H5_INDEX_NAME, H5_ITER_INC, NULL, attr_info, NULL);

    /*
     * Close the dataset and the file.
     */
    H5Dclose(dataset);
    H5Fclose(file);

    return 0;
}

/*
 * Operator function.
 */
static herr_t
attr_info(hid_t loc_id, const char *name, const H5A_info_t *ainfo, void *opdata)
{
    hid_t   attr, atype, aspace; /* Attribute, datatype and dataspace identifiers */
    int     rank;
    hsize_t sdim[64];
    herr_t  ret;
    int     i;
    size_t  npoints;     /* Number of elements in the array attribute. */
    float  *float_array; /* Pointer to the array attribute. */

    /* avoid warnings */
    (void)opdata;

    /*
     * Open the attribute using its name.
     */
    attr = H5Aopen(loc_id, name, H5P_DEFAULT);

    /*
     * Display attribute name.
     */
    printf("\nName : %s\n", name);

    /*
     * Get attribute datatype, dataspace, rank, and dimensions.
     */
    atype  = H5Aget_type(attr);
    aspace = H5Aget_space(attr);
    rank   = H5Sget_simple_extent_ndims(aspace);
    ret    = H5Sget_simple_extent_dims(aspace, sdim, NULL);

    /*
     *  Display rank and dimension sizes for the array attribute.
     */

    if (rank > 0) {
        printf("Rank : %d \n", rank);
        printf("Dimension sizes : ");
        for (i = 0; i < rank; i++)
            printf("%d ", (int)sdim[i]);
        printf("\n");
    }

    /*
     * Read array attribute and display its type and values.
     */

    if (H5T_FLOAT == H5Tget_class(atype)) {
        printf("Type : FLOAT \n");
        npoints     = H5Sget_simple_extent_npoints(aspace);
        float_array = (float *)malloc(sizeof(float) * (int)npoints);
        ret         = H5Aread(attr, atype, float_array);
        printf("Values : ");
        for (i = 0; i < (int)npoints; i++)
            printf("%f ", float_array[i]);
        printf("\n");
        free(float_array);
    }

    /*
     * Release all identifiers.
     */
    H5Tclose(atype);
    H5Sclose(aspace);
    H5Aclose(attr);

    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_chunk_read.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *   This example shows how to read data from a chunked dataset.
 *   We will read from the file created by h5_extend_write.c
 */

#include "hdf5.h"

#define H5FILE_NAME "SDSextendible.h5"
#define DATASETNAME "ExtendibleArray"
#define RANK        2
#define RANKC       1
#define NX          10
#define NY          5

int
main(void)
{
    hid_t   file; /* handles */
    hid_t   dataset;
    hid_t   filespace;
    hid_t   memspace;
    hid_t   cparms;
    hsize_t dims[2]; /* dataset and chunk dimensions*/
    hsize_t chunk_dims[2];
    hsize_t col_dims[1];
    hsize_t count[2];
    hsize_t offset[2];

    herr_t status, status_n;

    int data_out[NX][NY]; /* buffer for dataset to be read */
    int chunk_out[2][5];  /* buffer for chunk to be read */
    int column[10];       /* buffer for column to be read */
    int rank, rank_chunk;
    int i, j;

    /*
     * Open the file and the dataset.
     */
    file    = H5Fopen(H5FILE_NAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dataset = H5Dopen2(file, DATASETNAME, H5P_DEFAULT);

    /*
     * Get dataset rank and dimension.
     */

    filespace = H5Dget_space(dataset); /* Get filespace handle first. */
    rank      = H5Sget_simple_extent_ndims(filespace);
    status_n  = H5Sget_simple_extent_dims(filespace, dims, NULL);
    printf("dataset rank %d, dimensions %lu x %lu\n", rank, (unsigned long)(dims[0]),
           (unsigned long)(dims[1]));

    /*
     * Define the memory space to read dataset.
     */
    memspace = H5Screate_simple(RANK, dims, NULL);

    /*
     * Read dataset back and display.
     */
    status = H5Dread(dataset, H5T_NATIVE_INT, memspace, filespace, H5P_DEFAULT, data_out);
    printf("\n");
    printf("Dataset: \n");
    for (j = 0; j < dims[0]; j++) {
        for (i = 0; i < dims[1]; i++)
            printf("%d ", data_out[j][i]);
        printf("\n");
    }

    /*
     * Close/release resources.
     */
    H5Sclose(memspace);

    /*
     *	    dataset rank 2, dimensions 10 x 5
     *	    chunk rank 2, dimensions 2 x 5

     *	    Dataset:
     *	    1 1 1 3 3
     *	    1 1 1 3 3
     *	    1 1 1 0 0
     *	    2 0 0 0 0
     *	    2 0 0 0 0
     *	    2 0 0 0 0
     *	    2 0 0 0 0
     *	    2 0 0 0 0
     *	    2 0 0 0 0
     *	    2 0 0 0 0
     */

    /*
     * Read the third column from the dataset.
     * First define memory dataspace, then define hyperslab
     * and read it into column array.
     */
    col_dims[0] = 10;
    memspace    = H5Screate_simple(RANKC, col_dims, NULL);

    /*
     * Define the column (hyperslab) to read.
     */
    offset[0] = 0;
    offset[1] = 2;
    count[0]  = 10;
    count[1]  = 1;
    status    = H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, count, NULL);
    status    = H5Dread(dataset, H5T_NATIVE_INT, memspace, filespace, H5P_DEFAULT, column);
    printf("\n");
    printf("Third column: \n");
    for (i = 0; i < 10; i++) {
        printf("%d \n", column[i]);
    }

    /*
     * Close/release resources.
     */
    H5Sclose(memspace);

    /*
     *	    Third column:
     *	    1
     *	    1
     *	    1
     *	    0
     *	    0
     *	    0
     *	    0
     *	    0
     *	    0
     *	    0
     */

    /*
     * Get creation properties list.
     */
    cparms = H5Dget_create_plist(dataset); /* Get properties handle first. */

    if (H5D_CHUNKED == H5Pget_layout(cparms)) {

        /*
         * Get chunking information: rank and dimensions
         */
        rank_chunk = H5Pget_chunk(cparms, 2, chunk_dims);
        printf("chunk rank %d, dimensions %lu x %lu\n", rank_chunk, (unsigned long)(chunk_dims[0]),
               (unsigned long)(chunk_dims[1]));

        /*
         * Define the memory space to read a chunk.
         */
        memspace = H5Screate_simple(rank_chunk, chunk_dims, NULL);

        /*
         * Define chunk in the file (hyperslab) to read.
         */
        offset[0] = 2;
        offset[1] = 0;
        count[0]  = chunk_dims[0];
        count[1]  = chunk_dims[1];
        status    = H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, count, NULL);

        /*
         * Read chunk back and display.
         */
        status = H5Dread(dataset, H5T_NATIVE_INT, memspace, filespace, H5P_DEFAULT, chunk_out);
        printf("\n");
        printf("Chunk: \n");
        for (j = 0; j < chunk_dims[0]; j++) {
            for (i = 0; i < chunk_dims[1]; i++)
                printf("%d ", chunk_out[j][i]);
            printf("\n");
        }
        /*
         *	 Chunk:
         *	 1 1 1 0 0
         *	 2 0 0 0 0
         */

        /*
         * Close/release resources.
         */
        H5Sclose(memspace);
    }

    /*
     * Close/release resources.
     */
    H5Pclose(cparms);
    H5Dclose(dataset);
    H5Sclose(filespace);
    H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_cmprss.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to create a compressed dataset.
 *  It is used in the HDF5 Tutorial.
 */

#include "hdf5.h"

#define FILENAME "cmprss.h5"
#define RANK     2
#define DIM0     100
#define DIM1     20

int
main(void)
{

    hid_t file_id, dataset_id, dataspace_id; /* identifiers */
    hid_t plist_id;

    size_t       nelmts;
    unsigned     flags, filter_info;
    H5Z_filter_t filter_type;

    herr_t  status;
    hsize_t dims[2];
    hsize_t cdims[2];

    int i, j, numfilt;
    int buf[DIM0][DIM1];
    int rbuf[DIM0][DIM1];

    /* Uncomment these variables to use SZIP compression
    unsigned szip_options_mask;
    unsigned szip_pixels_per_block;
    */

    /* Create a file.  */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create dataset "Compressed Data" in the group using absolute name.  */
    dims[0]      = DIM0;
    dims[1]      = DIM1;
    dataspace_id = H5Screate_simple(RANK, dims, NULL);

    plist_id = H5Pcreate(H5P_DATASET_CREATE);

    /* Dataset must be chunked for compression */
    cdims[0] = 20;
    cdims[1] = 20;
    status   = H5Pset_chunk(plist_id, 2, cdims);

    /* Set ZLIB / DEFLATE Compression using compression level 6.
     * To use SZIP Compression comment out these lines.
     */
    status = H5Pset_deflate(plist_id, 6);

    /* Uncomment these lines to set SZIP Compression
    szip_options_mask = H5_SZIP_NN_OPTION_MASK;
    szip_pixels_per_block = 16;
    status = H5Pset_szip (plist_id, szip_options_mask, szip_pixels_per_block);
    */

    dataset_id = H5Dcreate2(file_id, "Compressed_Data", H5T_STD_I32BE, dataspace_id, H5P_DEFAULT, plist_id,
                            H5P_DEFAULT);

    for (i = 0; i < DIM0; i++)
        for (j = 0; j < DIM1; j++)
            buf[i][j] = i + j;

    status = H5Dwrite(dataset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, buf);

    status = H5Sclose(dataspace_id);
    status = H5Dclose(dataset_id);
    status = H5Pclose(plist_id);
    status = H5Fclose(file_id);

    /* Now reopen the file and dataset in the file. */
    file_id    = H5Fopen(FILENAME, H5F_ACC_RDWR, H5P_DEFAULT);
    dataset_id = H5Dopen2(file_id, "Compressed_Data", H5P_DEFAULT);

    /* Retrieve filter information. */
    plist_id = H5Dget_create_plist(dataset_id);

    numfilt = H5Pget_nfilters(plist_id);
    printf("Number of filters associated with dataset: %i\n", numfilt);

    for (i = 0; i < numfilt; i++) {
        nelmts      = 0;
        filter_type = H5Pget_filter2(plist_id, i, &flags, &nelmts, NULL, 0, NULL, &filter_info);
        printf("Filter Type: ");
        switch (filter_type) {
            case H5Z_FILTER_DEFLATE:
                printf("H5Z_FILTER_DEFLATE\n");
                break;
            case H5Z_FILTER_SZIP:
                printf("H5Z_FILTER_SZIP\n");
                break;
            default:
                printf("Other filter type included.\n");
        }
    }

    status = H5Dread(dataset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rbuf);

    status = H5Dclose(dataset_id);
    status = H5Pclose(plist_id);
    status = H5Fclose(file_id);
}
```

### `HDF5Examples/C/TUTR/h5_compound.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 * This example shows how to create a compound data type,
 * write an array which has the compound data type to the file,
 * and read back fields' subsets.
 */

#include "hdf5.h"

#define H5FILE_NAME "SDScompound.h5"
#define DATASETNAME "ArrayOfStructures"
#define LENGTH      10
#define RANK        1

int
main(void)
{

    /* First structure  and dataset*/
    typedef struct s1_t {
        int    a;
        float  b;
        double c;
    } s1_t;
    s1_t  s1[LENGTH];
    hid_t s1_tid; /* File datatype identifier */

    /* Second structure (subset of s1_t)  and dataset*/
    typedef struct s2_t {
        double c;
        int    a;
    } s2_t;
    s2_t  s2[LENGTH];
    hid_t s2_tid; /* Memory datatype handle */

    /* Third "structure" ( will be used to read float field of s1) */
    hid_t s3_tid; /* Memory datatype handle */
    float s3[LENGTH];

    int     i;
    hid_t   file, dataset, space; /* Handles */
    herr_t  status;
    hsize_t dim[] = {LENGTH}; /* Dataspace dimensions */

    /*
     * Initialize the data
     */
    for (i = 0; i < LENGTH; i++) {
        s1[i].a = i;
        s1[i].b = i * i;
        s1[i].c = 1. / (i + 1);
    }

    /*
     * Create the data space.
     */
    space = H5Screate_simple(RANK, dim, NULL);

    /*
     * Create the file.
     */
    file = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create the memory data type.
     */
    s1_tid = H5Tcreate(H5T_COMPOUND, sizeof(s1_t));
    H5Tinsert(s1_tid, "a_name", HOFFSET(s1_t, a), H5T_NATIVE_INT);
    H5Tinsert(s1_tid, "c_name", HOFFSET(s1_t, c), H5T_NATIVE_DOUBLE);
    H5Tinsert(s1_tid, "b_name", HOFFSET(s1_t, b), H5T_NATIVE_FLOAT);

    /*
     * Create the dataset.
     */
    dataset = H5Dcreate2(file, DATASETNAME, s1_tid, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Wtite data to the dataset;
     */
    status = H5Dwrite(dataset, s1_tid, H5S_ALL, H5S_ALL, H5P_DEFAULT, s1);

    /*
     * Release resources
     */
    H5Tclose(s1_tid);
    H5Sclose(space);
    H5Dclose(dataset);
    H5Fclose(file);

    /*
     * Open the file and the dataset.
     */
    file = H5Fopen(H5FILE_NAME, H5F_ACC_RDONLY, H5P_DEFAULT);

    dataset = H5Dopen2(file, DATASETNAME, H5P_DEFAULT);

    /*
     * Create a data type for s2
     */
    s2_tid = H5Tcreate(H5T_COMPOUND, sizeof(s2_t));

    H5Tinsert(s2_tid, "c_name", HOFFSET(s2_t, c), H5T_NATIVE_DOUBLE);
    H5Tinsert(s2_tid, "a_name", HOFFSET(s2_t, a), H5T_NATIVE_INT);

    /*
     * Read two fields c and a from s1 dataset. Fields in the file
     * are found by their names "c_name" and "a_name".
     */
    status = H5Dread(dataset, s2_tid, H5S_ALL, H5S_ALL, H5P_DEFAULT, s2);

    /*
     * Display the fields
     */
    printf("\n");
    printf("Field c : \n");
    for (i = 0; i < LENGTH; i++)
        printf("%.4f ", s2[i].c);
    printf("\n");

    printf("\n");
    printf("Field a : \n");
    for (i = 0; i < LENGTH; i++)
        printf("%d ", s2[i].a);
    printf("\n");

    /*
     * Create a data type for s3.
     */
    s3_tid = H5Tcreate(H5T_COMPOUND, sizeof(float));

    status = H5Tinsert(s3_tid, "b_name", 0, H5T_NATIVE_FLOAT);

    /*
     * Read field b from s1 dataset. Field in the file is found by its name.
     */
    status = H5Dread(dataset, s3_tid, H5S_ALL, H5S_ALL, H5P_DEFAULT, s3);

    /*
     * Display the field
     */
    printf("\n");
    printf("Field b : \n");
    for (i = 0; i < LENGTH; i++)
        printf("%.4f ", s3[i]);
    printf("\n");

    /*
     * Release resources
     */
    H5Tclose(s2_tid);
    H5Tclose(s3_tid);
    H5Dclose(dataset);
    H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_crtatt.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to create an attribute attached to a
 *  dataset.  It is used in the HDF5 Tutorial.
 */

#include "hdf5.h"

#define FILENAME "dset.h5"

int
main(void)
{

    hid_t   file_id, dataset_id, attribute_id, dataspace_id; /* identifiers */
    hsize_t dims;
    int     attr_data[2];
    herr_t  status;

    /* Initialize the attribute data. */
    attr_data[0] = 100;
    attr_data[1] = 200;

    /* Open an existing file. */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDWR, H5P_DEFAULT);

    /* Open an existing dataset. */
    dataset_id = H5Dopen2(file_id, "/dset", H5P_DEFAULT);

    /* Create the data space for the attribute. */
    dims         = 2;
    dataspace_id = H5Screate_simple(1, &dims, NULL);

    /* Create a dataset attribute. */
    attribute_id = H5Acreate2(dataset_id, "Units", H5T_STD_I32BE, dataspace_id, H5P_DEFAULT, H5P_DEFAULT);

    /* Write the attribute data. */
    status = H5Awrite(attribute_id, H5T_NATIVE_INT, attr_data);

    /* Close the attribute. */
    status = H5Aclose(attribute_id);

    /* Close the dataspace. */
    status = H5Sclose(dataspace_id);

    /* Close to the dataset. */
    status = H5Dclose(dataset_id);

    /* Close the file. */
    status = H5Fclose(file_id);
}
```

### `HDF5Examples/C/TUTR/h5_crtdat.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to create a dataset that is a 4 x 6
 *  array.  It is used in the HDF5 Tutorial.
 */

#include "hdf5.h"

#define FILENAME "dset.h5"

int
main(void)
{

    hid_t   file_id, dataset_id, dataspace_id; /* identifiers */
    hsize_t dims[2];
    herr_t  status;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create the data space for the dataset. */
    dims[0]      = 4;
    dims[1]      = 6;
    dataspace_id = H5Screate_simple(2, dims, NULL);

    /* Create the dataset. */
    dataset_id =
        H5Dcreate2(file_id, "/dset", H5T_STD_I32BE, dataspace_id, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /* End access to the dataset and release resources used by it. */
    status = H5Dclose(dataset_id);

    /* Terminate access to the data space. */
    status = H5Sclose(dataspace_id);

    /* Close the file. */
    status = H5Fclose(file_id);
}
```

### `HDF5Examples/C/TUTR/h5_crtgrp.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to create and close a group.
 *  It is used in the HDF5 Tutorial.
 */

#include "hdf5.h"

#define FILENAME "group.h5"

int
main(void)
{

    hid_t  file_id, group_id; /* identifiers */
    herr_t status;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create a group named "/MyGroup" in the file. */
    group_id = H5Gcreate2(file_id, "/MyGroup", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /* Close the group. */
    status = H5Gclose(group_id);

    /* Terminate access to the file. */
    status = H5Fclose(file_id);
}
```

### `HDF5Examples/C/TUTR/h5_crtgrpar.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates the creation of groups using absolute and
 *  relative names.  It is used in the HDF5 Tutorial.
 */

#include "hdf5.h"

#define FILENAME "groups.h5"

int
main(void)
{

    hid_t  file_id, group1_id, group2_id, group3_id; /* identifiers */
    herr_t status;

    /* Create a new file using default properties. */
    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create group "MyGroup" in the root group using absolute name. */
    group1_id = H5Gcreate2(file_id, "/MyGroup", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /* Create group "Group_A" in group "MyGroup" using absolute name. */
    group2_id = H5Gcreate2(file_id, "/MyGroup/Group_A", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /* Create group "Group_B" in group "MyGroup" using relative name. */
    group3_id = H5Gcreate2(group1_id, "Group_B", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /* Close groups. */
    status = H5Gclose(group1_id);
    status = H5Gclose(group2_id);
    status = H5Gclose(group3_id);

    /* Close the file. */
    status = H5Fclose(file_id);
}
```

### `HDF5Examples/C/TUTR/h5_crtgrpd.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to create a dataset in a group.
 *  It is used in the HDF5 Tutorial.
 */

#include "hdf5.h"

#define FILENAME "groups.h5"

int
main(void)
{

    hid_t   file_id, group_id, dataset_id, dataspace_id; /* identifiers */
    hsize_t dims[2];
    herr_t  status;
    int     i, j, dset1_data[3][3], dset2_data[2][10];

    /* Initialize the first dataset. */
    for (i = 0; i < 3; i++)
        for (j = 0; j < 3; j++)
            dset1_data[i][j] = j + 1;

    /* Initialize the second dataset. */
    for (i = 0; i < 2; i++)
        for (j = 0; j < 10; j++)
            dset2_data[i][j] = j + 1;

    /* Open an existing file. */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDWR, H5P_DEFAULT);

    /* Create the data space for the first dataset. */
    dims[0]      = 3;
    dims[1]      = 3;
    dataspace_id = H5Screate_simple(2, dims, NULL);

    /* Create a dataset in group "MyGroup". */
    dataset_id = H5Dcreate2(file_id, "/MyGroup/dset1", H5T_STD_I32BE, dataspace_id, H5P_DEFAULT, H5P_DEFAULT,
                            H5P_DEFAULT);

    /* Write the first dataset. */
    status = H5Dwrite(dataset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, dset1_data);

    /* Close the data space for the first dataset. */
    status = H5Sclose(dataspace_id);

    /* Close the first dataset. */
    status = H5Dclose(dataset_id);

    /* Open an existing group of the specified file. */
    group_id = H5Gopen2(file_id, "/MyGroup/Group_A", H5P_DEFAULT);

    /* Create the data space for the second dataset. */
    dims[0]      = 2;
    dims[1]      = 10;
    dataspace_id = H5Screate_simple(2, dims, NULL);

    /* Create the second dataset in group "Group_A". */
    dataset_id =
        H5Dcreate2(group_id, "dset2", H5T_STD_I32BE, dataspace_id, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /* Write the second dataset. */
    status = H5Dwrite(dataset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, dset2_data);

    /* Close the data space for the second dataset. */
    status = H5Sclose(dataspace_id);

    /* Close the second dataset */
    status = H5Dclose(dataset_id);

    /* Close the group. */
    status = H5Gclose(group_id);

    /* Close the file. */
    status = H5Fclose(file_id);
}
```

### `HDF5Examples/C/TUTR/h5_debug_trace.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/* This example demonstrates debug trace output.
 *
 * Debug/trace/performance output is not tested as a regular part of our
 * testing so this program gives a quick check that it's all working.
 *
 * Preconditions:
 *
 * You need to set an environment variable named HDF5_DEBUG to have a value
 * of "+all trace ttimes".  In the bash shell, you'd use:
 *
 *      export HDF5_DEBUG="+all trace ttimes"
 *
 * When you are done with this test program, you can set the variable back
 * to "-all" to suppress trace output.
 *
 * Usage:
 *
 * Compile and run the test program, then inspect the output.  You should see
 * trace information for each HDF5 function call that increase over time.
 * Each time stamp is in seconds and designated with an '@' sign.  The
 * elapsed time for the function call is given in seconds in the [dt= ]
 * part.
 *
 * You will also get summary output for the shuffle filter performance and
 * data type conversion performance.  These will include the elapsed time
 * (always) and the system and user times (if available on your system).  On
 * fast machines, these numbers may be 0.0.  Adjust the loop variables in
 * the program as needed to generate reasonable output.
 */

#include <stdio.h>
#include <stdlib.h>

#include "hdf5.h"

#define BUF_SIZE 1048576
#define N_LOOPS  64

#define TESTFILE "h5_debug_trace_out.h5"

int
main(int argc, char **argv)
{
    int  i, j;
    int *data;

    hid_t fid;
    hid_t pid;
    hid_t did;
    hid_t sid;

    hsize_t dims[1]        = {BUF_SIZE};
    hsize_t chunk_sizes[1] = {1024};

    herr_t err;

    /*************************************************************************/

    /* Warn the user about trace deluge to come */

    printf("Testing debug/trace/performance data generation\n");
    printf("\n");
    printf("This test should generate a large amount of trace data\n");
    printf("\n");
    printf("*** BEGIN TRACE OUTPUT ***\n");
    printf("\n");
    fflush(stdout);

    /* This will emit H5Tconvert() performance information */

    for (i = 0; i < N_LOOPS; i++) {

        /* The buffer has to be large enough to hold the conversion output */
        data = (int *)malloc(BUF_SIZE * sizeof(double));

        for (j = 0; j < BUF_SIZE; j++) {
            data[j] = j;
        }

        err = H5Tconvert(H5T_NATIVE_INT, H5T_NATIVE_DOUBLE, BUF_SIZE, data, NULL, H5P_DEFAULT);

        if (err < 0) {
            fprintf(stderr, "ERROR: Conversion failed\n");
            free(data);
            return err;
        }

        free(data);
    }

    /* This will emit H5Z performance information */

    data = (int *)malloc(BUF_SIZE * sizeof(int));

    for (i = 0; i < BUF_SIZE; i++) {
        data[i] = i;
    }

    fid = H5Fcreate(TESTFILE, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    pid = H5Pcreate(H5P_DATASET_CREATE);
    err = H5Pset_chunk(pid, 1, chunk_sizes);
    err = H5Pset_shuffle(pid);

    sid = H5Screate_simple(1, dims, dims);
    did = H5Dcreate2(fid, "somedata", H5T_NATIVE_INT, sid, H5P_DEFAULT, pid, H5P_DEFAULT);
    err = H5Dwrite(did, H5T_NATIVE_INT, sid, sid, H5P_DEFAULT, data);

    H5Sclose(sid);
    H5Dclose(did);
    H5Pclose(pid);
    H5Fclose(fid);

    free(data);

    /* Finished */
    fflush(stdout);
    printf("\n");
    printf("*** END TRACE OUTPUT ***\n");
    printf("\n");

    remove(TESTFILE);

    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_drivers.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 * This shows how to use the hdf5 virtual file drivers.
 * The example codes here do not check return values for the
 * sake of simplicity.  As in all proper programs, return codes
 * should be checked.
 */

#include "hdf5.h"
#include "stdlib.h"

/* global variables */
int cleanup_g = -1; /* whether to clean.  Init to not set. */

/* prototypes */
void cleanup(const char *);
void split_file(void);

/*
 * Cleanup a file unless $HDF5_NOCLEANUP is set.
 */
void
cleanup(const char *filename)
{
    if (cleanup_g == -1)
        cleanup_g = getenv(HDF5_NOCLEANUP) ? 0 : 1;
    if (cleanup_g)
        remove(filename);
}

/*
 * This shows how to use the split file driver.
 */
void
split_file(void)
{
    hid_t fapl, fid;

    /* Example 1: Both metadata and rawdata files are in the same  */
    /*    directory.   Use Station1-m.h5 and Station1-r.h5 as      */
    /*    the metadata and rawdata files.                          */
    fapl = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_fapl_split(fapl, "-m.h5", H5P_DEFAULT, "-r.h5", H5P_DEFAULT);
    fid = H5Fcreate("Station1", H5F_ACC_TRUNC, H5P_DEFAULT, fapl);
    /* using the file ... */
    H5Fclose(fid);
    H5Pclose(fapl);
    /* Remove files created */
    cleanup("Station1-m.h5");
    cleanup("Station1-r.h5");

    /* Example 2: metadata and rawdata files are in different      */
    /*    directories.  Use PointA-m.h5 and /tmp/PointA-r.h5 as    */
    /*    the metadata and rawdata files.                          */
    fapl = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_fapl_split(fapl, "-m.h5", H5P_DEFAULT, "/tmp/%s-r.h5", H5P_DEFAULT);
    fid = H5Fcreate("PointA", H5F_ACC_TRUNC, H5P_DEFAULT, fapl);
    /* using the file ... */
    H5Fclose(fid);
    H5Pclose(fapl);
    /* Remove files created */
    cleanup("PointA-m.h5");
    cleanup("/tmp/PointA-r.h5");

    /* Example 3: Using default extension names for the metadata   */
    /*    and rawdata files.  Use Measure.meta and Measure.raw as  */
    /*    the metadata and rawdata files.                          */
    fapl = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_fapl_split(fapl, NULL, H5P_DEFAULT, NULL, H5P_DEFAULT);
    fid = H5Fcreate("Measure", H5F_ACC_TRUNC, H5P_DEFAULT, fapl);
    /* using the file ... */
    H5Fclose(fid);
    H5Pclose(fapl);
    /* Remove files created */
    cleanup("Measure.meta");
    cleanup("Measure.raw");
}

/* Main Body */
int
main(void)
{

    split_file();

    return (0);
}
```

### `HDF5Examples/C/TUTR/h5_dtransform.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example demonstrates how the data transform features of
 *  HDF5 works.
 *
 * (1)
 *  The test first writes out data, with no data transform set.
 *  Then, the test reads back this data with a data transform applied.
 *
 * (2)
 *  Then, the test writes a new set of data, with a data transform set.
 *  Then, the test reads this new set of data, without a data set.
 *
 * (3)
 *  Lastly, the test reads the previous set of data (that was written out
 *  with a data transform) with a data transform set for the read.
 *
 *  (4)
 *  Get the transform from the property using H5Pget_data_transform.
 */

#include "hdf5.h"

#define ROWS 12
#define COLS 18

/* clang-format off */
const float windchillF[ROWS][COLS] =
    {   {36.0, 31.0, 25.0, 19.0, 13.0, 7.0,   1.0,  -5.0,  -11.0, -16.0, -22.0, -28.0, -34.0, -40.0, -46.0, -52.0, -57.0, -63.0},
        {34.0, 27.0, 21.0, 15.0, 9.0,  3.0,  -4.0,  -10.0, -16.0, -22.0, -28.0, -35.0, -41.0, -47.0, -53.0, -59.0, -66.0, -72.0},
        {32.0, 25.0, 19.0, 13.0, 6.0,  0.0,  -7.0,  -13.0, -19.0, -26.0, -32.0, -39.0, -45.0, -51.0, -58.0, -64.0, -71.0, -77.0},
        {30.0, 24.0, 17.0, 11.0, 4.0, -2.0,  -9.0,  -15.0, -22.0, -29.0, -35.0, -42.0, -48.0, -55.0, -61.0, -68.0, -74.0, -81.0},
        {29.0, 23.0, 16.0, 9.0,  3.0, -4.0,  -11.0, -17.0, -24.0, -31.0, -37.0, -44.0, -51.0, -58.0, -64.0, -71.0, -78.0, -84.0},
        {28.0, 22.0, 15.0, 8.0,  1.0, -5.0,  -12.0, -19.0, -26.0, -33.0, -39.0, -46.0, -53.0, -60.0, -67.0, -73.0, -80.0, -87.0},
        {28.0, 21.0, 14.0, 7.0,  0.0, -7.0,  -14.0, -21.0, -27.0, -34.0, -41.0, -48.0, -55.0, -62.0, -69.0, -76.0, -82.0, -89.0},
        {27.0, 20.0, 13.0, 6.0, -1.0, -8.0,  -15.0, -22.0, -29.0, -36.0, -43.0, -50.0, -57.0, -64.0, -71.0, -78.0, -84.0, -91.0},
        {26.0, 19.0, 12.0, 5.0, -2.0, -9.0,  -16.0, -23.0, -30.0, -37.0, -44.0, -51.0, -58.0, -65.0, -72.0, -79.0, -86.0, -93.0},
        {26.0, 19.0, 12.0, 4.0, -3.0, -10.0, -17.0, -24.0, -31.0, -38.0, -45.0, -52.0, -60.0, -67.0, -74.0, -81.0, -88.0, -95.0},
        {25.0, 18.0, 11.0, 4.0, -3.0, -11.0, -18.0, -25.0, -32.0, -39.0, -46.0, -54.0, -61.0, -68.0, -75.0, -82.0, -89.0, -97.0},
        {25.0, 17.0, 10.0, 3.0, -4.0, -11.0, -19.0, -26.0, -33.0, -40.0, -48.0, -55.0, -62.0, -69.0, -76.0, -84.0, -91.0, -98.0}
    };
/* clang-format on */

#define PRINT(array)                                                                                         \
    {                                                                                                        \
        for (i = 0; i < ROWS; i++) {                                                                         \
            for (j = 0; j < COLS; j++)                                                                       \
                printf("%6.2f ", array[i][j]);                                                               \
            printf("\n");                                                                                    \
        }                                                                                                    \
    }

int
main(void)
{
    hid_t       file, dataset; /* file and dataset handles */
    hid_t       dataspace;     /* handles */
    hsize_t     dimsf[2];      /* dataset dimensions */
    herr_t      status;
    hid_t       dxpl_id_f_to_c, dxpl_id_c_to_f; /* data transform handles */
    const char *f_to_c = "(5/9.0)*(x-32)";
    const char *c_to_f = "(9/5.0)*x + 32";
    char       *transform;
    float       windchillC[ROWS][COLS];
    int         i, j, transform_size;

    /*
     * Create a new file using H5F_ACC_TRUNC access,
     * default file creation properties, and default file
     * access properties.
     */
    file = H5Fcreate("dtransform.h5", H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Describe the size of the array and create the data space for fixed
     * size dataset.
     */
    dimsf[0]  = ROWS;
    dimsf[1]  = COLS;
    dataspace = H5Screate_simple(2, dimsf, NULL);

    /*
     * Create a new dataset within the file using defined dataspace and
     * datatype and default dataset creation properties.
     */
    dataset =
        H5Dcreate2(file, "data_no_trans", H5T_NATIVE_FLOAT, dataspace, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    printf("\nOriginal Data: \n");

    PRINT(windchillF);

    /****************  PART 1 **************/
    /*
     * Write the data to the dataset using default transfer properties (ie, no transform set)
     */
    status = H5Dwrite(dataset, H5T_NATIVE_FLOAT, H5S_ALL, H5S_ALL, H5P_DEFAULT, windchillF);

    /* Create the dataset transfer property list */
    dxpl_id_f_to_c = H5Pcreate(H5P_DATASET_XFER);

    /* Set the data transform */
    H5Pset_data_transform(dxpl_id_f_to_c, f_to_c);

    /* Read out the data with the data transform */
    H5Dread(dataset, H5T_NATIVE_FLOAT, H5S_ALL, H5S_ALL, dxpl_id_f_to_c, windchillC);

    /* Print the data from the read*/
    printf("\nData with no write transform, but a read transform: \n");
    PRINT(windchillC);

    /****************  PART 2 **************/
    /*
     * Write the data to the dataset with the f_to_c transform set
     */
    status = H5Dwrite(dataset, H5T_NATIVE_FLOAT, H5S_ALL, H5S_ALL, dxpl_id_f_to_c, windchillF);

    /* Read out the data with the default transfer list (ie, no transform set) */
    H5Dread(dataset, H5T_NATIVE_FLOAT, H5S_ALL, H5S_ALL, H5P_DEFAULT, windchillC);

    /* Print the data from the read*/
    printf("\nData with write transform, but no read transform: \n");
    PRINT(windchillC);

    /************** PART 3 ***************/

    /* Create the dataset transfer property list */
    dxpl_id_c_to_f = H5Pcreate(H5P_DATASET_XFER);

    /* Set the data transform to be used on the read*/
    H5Pset_data_transform(dxpl_id_c_to_f, c_to_f);

    /*
     * Write the data to the dataset using the f_to_c transform
     */
    status = H5Dwrite(dataset, H5T_NATIVE_FLOAT, H5S_ALL, H5S_ALL, dxpl_id_f_to_c, windchillF);

    /* Read the data with the c_to_f data transform */
    H5Dread(dataset, H5T_NATIVE_FLOAT, H5S_ALL, H5S_ALL, dxpl_id_c_to_f, windchillC);

    /* Print the data from the read*/
    printf("\nData with both read and write data transform: \n");
    PRINT(windchillC);

    /************** PART 4 **************/
    transform_size = H5Pget_data_transform(dxpl_id_f_to_c, NULL, 0);
    transform      = (char *)malloc(transform_size + 1);
    H5Pget_data_transform(dxpl_id_f_to_c, transform, transform_size + 1);

    printf("\nTransform string (from dxpl_id_f_to_c) is: %s\n", transform);

    /*
     * Close/release resources.
     */
    H5Pclose(dxpl_id_c_to_f);
    H5Pclose(dxpl_id_f_to_c);
    H5Sclose(dataspace);
    H5Dclose(dataset);
    H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_elink_unix2win.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/* This program demonstrates how to translate an external link created on
 * a Windows machine into a format that a *nix machine can read.
 * This is done by registering a new traversal function for external links.
 *
 * This example is designed to be run on Unix and will create an external
 * link with a Windows-style path.  Using the traversal function below,
 * the example then successfully follows the external link.
 *
 * The external link will create a  file called "u2w/u2w_target.h5".
 * The example will fail if the directory u2w does not exist.
 */

#include "hdf5.h"
#include <stdlib.h>
#include <string.h>

/* "Windows to Unix" traversal function for external links
 *
 * Translates a filename stored in Unix format to Windows format by replacing
 * forward slashes with backslashes.
 * Makes no attempt to handle Windows drive names (e.g., "C:\"), spaces within
 * file names, quotes, etc.  These are left as an exercise for the user. :)
 * Note that this may not be necessary on your system; many Windows systems can
 * understand Unix paths.
 */
static hid_t
elink_unix2win_trav(const char *link_name, hid_t cur_group, const void *udata, size_t udata_size,
                    hid_t lapl_id, hid_t dxpl_id)
{
    hid_t       fid;
    const char *file_name;
    const char *obj_name;
    char       *new_fname = NULL; /* Buffer allocated to hold Unix file path */
    ssize_t     prefix_len;       /* External link prefix length */
    size_t      fname_len;
    size_t      start_pos; /* Initial position in new_fname buffer */
    size_t      x;         /* Counter variable */
    hid_t       ret_value = -1;

    printf("Converting Unix path to Windows path.\n");

    if (H5Lunpack_elink_val(udata, udata_size, NULL, &file_name, &obj_name) < 0)
        goto error;
    fname_len = strlen(file_name);

    /* See if the external link prefix property is set */
    if ((prefix_len = H5Pget_elink_prefix(lapl_id, NULL, 0)) < 0)
        goto error;

    /* If so, prepend it to the filename.  We assume that the prefix
     * is in the correct format for the current file system.
     */
    if (prefix_len > 0) {
        /* Allocate a buffer to hold the filename plus prefix */
        new_fname = malloc(prefix_len + fname_len + 1);

        /* Copy the prefix into the buffer */
        if (H5Pget_elink_prefix(lapl_id, new_fname, (size_t)(prefix_len + 1)) < 0)
            goto error;

        start_pos = prefix_len;
    }
    else {
        /* Allocate a buffer to hold just the filename */
        new_fname = malloc(fname_len + 1);
        start_pos = 0;
    }

    /* We should now copy file_name into new_fname starting at position pos.
     * We'll convert '/' characters into '\' characters as we go.
     */
    for (x = 0; file_name[x] != '\0'; x++) {
        if (file_name[x] == '/')
            new_fname[x + start_pos] = '\\';
        else
            new_fname[x + start_pos] = file_name[x];
    }
    new_fname[x + start_pos] = '\0';

    /* Now open the file and object within it */
    if ((fid = H5Fopen(new_fname, H5F_ACC_RDWR, H5P_DEFAULT)) < 0)
        goto error;
    ret_value = H5Oopen(fid, obj_name, lapl_id); /* If this fails, our return value will be negative. */
    if (H5Fclose(fid) < 0)
        goto error;

    /* Free file name if it's been allocated */
    if (new_fname)
        free(new_fname);

    return ret_value;

error:
    /* Free file name if it's been allocated */
    if (new_fname)
        free(new_fname);
    return -1;
}

const H5L_class_t elink_unix2win_class[1] = {{
    H5L_LINK_CLASS_T_VERS,    /* H5L_class_t version       */
    H5L_TYPE_EXTERNAL,        /* Link type id number            */
    "unix2win external link", /* Link class name for debugging  */
    NULL,                     /* Creation callback              */
    NULL,                     /* Move callback                  */
    NULL,                     /* Copy callback                  */
    elink_unix2win_trav,      /* The actual traversal function  */
    NULL,                     /* Deletion callback              */
    NULL                      /* Query callback                 */
}};

/* The example function.
 * Creates a file named "unix2win.h5" with an external link pointing to
 * the file "u2w/u2w_target.h5".
 *
 * Registers a new traversal function for external links and then
 * follows the external link to open the target file.
 */
static int
unix2win_example(void)
{
    hid_t fid = (-1); /* File ID */
    hid_t gid = (-1); /* Group ID */

    /* Create the target file. */
#ifdef H5_HAVE_WIN32_API
    if ((fid = H5Fcreate("u2w\\u2w_target.h5", H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT)) < 0)
        goto error;
#else
    if ((fid = H5Fcreate("u2w/u2w_target.h5", H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT)) < 0)
        goto error;
#endif
    if (H5Fclose(fid) < 0)
        goto error;

    /* Create the source file with an external link in Windows format */
    if ((fid = H5Fcreate("unix2win.h5", H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT)) < 0)
        goto error;

    /* Create the external link */
    if (H5Lcreate_external("u2w/../u2w/u2w_target.h5", "/", fid, "ext_link", H5P_DEFAULT, H5P_DEFAULT) < 0)
        goto error;

        /* If we are not on Windows, assume we are on a Unix-y filesystem and
         * follow the external link normally.
         * If we are on Windows, register the unix2win traversal function so
         * that external links can be traversed.
         */

#ifdef H5_HAVE_WIN32_API
    /* Register the elink_unix2win class defined above to replace default
     * external links
     */
    if (H5Lregister(elink_unix2win_class) < 0)
        goto error;
#endif

    /* Now follow the link */
    if ((gid = H5Gopen2(fid, "ext_link", H5P_DEFAULT)) < 0)
        goto error;
    printf("Successfully followed external link.\n");

    /* Close the group and the file */
    if (H5Gclose(gid) < 0)
        goto error;
    if (H5Fclose(fid) < 0)
        goto error;

    return 0;

error:
    printf("Error!\n");
    H5E_BEGIN_TRY
    {
        H5Gclose(gid);
        H5Fclose(fid);
    }
    H5E_END_TRY
    return -1;
}

/* Main function
 *
 * Invokes the example function.
 */
int
main(void)
{
    int ret;

    printf("Testing unix2win external links.\n");
    ret = unix2win_example();

    return ret;
}
```

### `HDF5Examples/C/TUTR/h5_extend.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example how to work with extendible datasets. The dataset
 *  must be chunked in order to be extendible.
 *
 *  It is used in the HDF5 Tutorial.
 */

#include "hdf5.h"

#define FILENAME    "extend.h5"
#define DATASETNAME "ExtendibleArray"
#define RANK        2

int
main(void)
{
    hid_t file; /* handles */
    hid_t dataspace, dataset;
    hid_t filespace, memspace;
    hid_t prop;

    hsize_t dims[2]    = {3, 3}; /* dataset dimensions at creation time */
    hsize_t maxdims[2] = {H5S_UNLIMITED, H5S_UNLIMITED};
    herr_t  status;
    hsize_t chunk_dims[2] = {2, 5};
    int     data[3][3]    = {{1, 1, 1}, /* data to write */
                             {1, 1, 1},
                             {1, 1, 1}};

    /* Variables used in extending and writing to the extended portion of dataset */
    hsize_t size[2];
    hsize_t offset[2];
    hsize_t dimsext[2]    = {7, 3}; /* extend dimensions */
    int     dataext[7][3] = {{2, 3, 4}, {2, 3, 4}, {2, 3, 4}, {2, 3, 4}, {2, 3, 4}, {2, 3, 4}, {2, 3, 4}};

    /* Variables used in reading data back */
    hsize_t chunk_dimsr[2];
    hsize_t dimsr[2];
    hsize_t i, j;
    int     rdata[10][3];
    herr_t  status_n;
    int     rank, rank_chunk;

    /* Create the data space with unlimited dimensions. */
    dataspace = H5Screate_simple(RANK, dims, maxdims);

    /* Create a new file. If file exists its contents will be overwritten. */
    file = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Modify dataset creation properties, i.e. enable chunking  */
    prop   = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_chunk(prop, RANK, chunk_dims);

    /* Create a new dataset within the file using chunk
       creation properties.  */
    dataset = H5Dcreate2(file, DATASETNAME, H5T_NATIVE_INT, dataspace, H5P_DEFAULT, prop, H5P_DEFAULT);

    /* Write data to dataset */
    status = H5Dwrite(dataset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, data);

    /* Extend the dataset. Dataset becomes 10 x 3  */
    size[0] = dims[0] + dimsext[0];
    size[1] = dims[1];
    status  = H5Dset_extent(dataset, size);

    /* Select a hyperslab in extended portion of dataset  */
    filespace = H5Dget_space(dataset);
    offset[0] = 3;
    offset[1] = 0;
    status    = H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, dimsext, NULL);

    /* Define memory space */
    memspace = H5Screate_simple(RANK, dimsext, NULL);

    /* Write the data to the extended portion of dataset  */
    status = H5Dwrite(dataset, H5T_NATIVE_INT, memspace, filespace, H5P_DEFAULT, dataext);

    /* Close resources */
    status = H5Dclose(dataset);
    status = H5Pclose(prop);
    status = H5Sclose(dataspace);
    status = H5Sclose(memspace);
    status = H5Sclose(filespace);
    status = H5Fclose(file);

    /********************************************
     * Re-open the file and read the data back. *
     ********************************************/

    file    = H5Fopen(FILENAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dataset = H5Dopen2(file, DATASETNAME, H5P_DEFAULT);

    filespace = H5Dget_space(dataset);
    rank      = H5Sget_simple_extent_ndims(filespace);
    status_n  = H5Sget_simple_extent_dims(filespace, dimsr, NULL);

    prop = H5Dget_create_plist(dataset);

    if (H5D_CHUNKED == H5Pget_layout(prop))
        rank_chunk = H5Pget_chunk(prop, rank, chunk_dimsr);

    memspace = H5Screate_simple(rank, dimsr, NULL);
    status   = H5Dread(dataset, H5T_NATIVE_INT, memspace, filespace, H5P_DEFAULT, rdata);

    printf("\n");
    printf("Dataset: \n");
    for (j = 0; j < dimsr[0]; j++) {
        for (i = 0; i < dimsr[1]; i++)
            printf("%d ", rdata[j][i]);
        printf("\n");
    }

    status = H5Pclose(prop);
    status = H5Dclose(dataset);
    status = H5Sclose(filespace);
    status = H5Sclose(memspace);
    status = H5Fclose(file);
}
```

### `HDF5Examples/C/TUTR/h5_extend_write.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *   This example shows how to work with extendible dataset.
 *   In the current version of the library dataset MUST be
 *   chunked.
 *
 */

#include "hdf5.h"

#define H5FILE_NAME "SDSextendible.h5"
#define DATASETNAME "ExtendibleArray"
#define RANK        2
#define NX          10
#define NY          5

int
main(void)
{
    hid_t   file; /* handles */
    hid_t   dataspace, dataset;
    hid_t   filespace;
    hid_t   cparms;
    hsize_t dims[2] = {3, 3};  /*
                                * dataset dimensions
                                * at the creation time
                                */
    hsize_t dims1[2] = {3, 3}; /* data1 dimensions */
    hsize_t dims2[2] = {7, 1}; /* data2 dimensions */
    hsize_t dims3[2] = {2, 2}; /* data3 dimensions */

    hsize_t maxdims[2]    = {H5S_UNLIMITED, H5S_UNLIMITED};
    hsize_t chunk_dims[2] = {2, 5};
    hsize_t size[2];
    hsize_t offset[2];

    herr_t status;

    int data1[3][3] = {{1, 1, 1}, /* data to write */
                       {1, 1, 1},
                       {1, 1, 1}};

    int data2[7] = {2, 2, 2, 2, 2, 2, 2};

    int data3[2][2] = {{3, 3}, {3, 3}};
    int fillvalue   = 0;

    /*
     * Create the data space with unlimited dimensions.
     */
    dataspace = H5Screate_simple(RANK, dims, maxdims);

    /*
     * Create a new file. If file exists its contents will be overwritten.
     */
    file = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Modify dataset creation properties, i.e. enable chunking.
     */
    cparms = H5Pcreate(H5P_DATASET_CREATE);
    status = H5Pset_chunk(cparms, RANK, chunk_dims);
    status = H5Pset_fill_value(cparms, H5T_NATIVE_INT, &fillvalue);

    /*
     * Create a new dataset within the file using cparms
     * creation properties.
     */
    dataset = H5Dcreate2(file, DATASETNAME, H5T_NATIVE_INT, dataspace, H5P_DEFAULT, cparms, H5P_DEFAULT);

    /*
     * Extend the dataset. This call assures that dataset is at least 3 x 3.
     */
    size[0] = 3;
    size[1] = 3;
    status  = H5Dset_extent(dataset, size);

    /*
     * Select a hyperslab.
     */
    filespace = H5Dget_space(dataset);
    offset[0] = 0;
    offset[1] = 0;
    status    = H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, dims1, NULL);

    /*
     * Write the data to the hyperslab.
     */
    status = H5Dwrite(dataset, H5T_NATIVE_INT, dataspace, filespace, H5P_DEFAULT, data1);

    /*
     * Extend the dataset. Dataset becomes 10 x 3.
     */
    dims[0] = dims1[0] + dims2[0];
    size[0] = dims[0];
    size[1] = dims[1];
    status  = H5Dset_extent(dataset, size);

    /*
     * Select a hyperslab.
     */
    filespace = H5Dget_space(dataset);
    offset[0] = 3;
    offset[1] = 0;
    status    = H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, dims2, NULL);

    /*
     * Define memory space
     */
    dataspace = H5Screate_simple(RANK, dims2, NULL);

    /*
     * Write the data to the hyperslab.
     */
    status = H5Dwrite(dataset, H5T_NATIVE_INT, dataspace, filespace, H5P_DEFAULT, data2);

    /*
     * Extend the dataset. Dataset becomes 10 x 5.
     */
    dims[1] = dims1[1] + dims3[1];
    size[0] = dims[0];
    size[1] = dims[1];
    status  = H5Dset_extent(dataset, size);

    /*
     * Select a hyperslab
     */
    filespace = H5Dget_space(dataset);
    offset[0] = 0;
    offset[1] = 3;
    status    = H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, dims3, NULL);

    /*
     * Define memory space.
     */
    dataspace = H5Screate_simple(RANK, dims3, NULL);

    /*
     * Write the data to the hyperslab.
     */
    status = H5Dwrite(dataset, H5T_NATIVE_INT, dataspace, filespace, H5P_DEFAULT, data3);

    /*
     * Resulting dataset
     *
     *   1 1 1 3 3
     *   1 1 1 3 3
     *   1 1 1 0 0
     *	 2 0 0 0 0
     *	 2 0 0 0 0
     *	 2 0 0 0 0
     *	 2 0 0 0 0
     *	 2 0 0 0 0
     *	 2 0 0 0 0
     *	 2 0 0 0 0
     */
    /*
     * Close/release resources.
     */
    H5Dclose(dataset);
    H5Sclose(dataspace);
    H5Sclose(filespace);
    H5Pclose(cparms);
    H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_extlink.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/* This program demonstrates how to create and use "external links" in
 * HDF5.
 *
 * External links point from one HDF5 file to an object (Group, Dataset, or
 * committed Datatype) in another file.
 */

#include "hdf5.h"
#include <string.h>

#define SOURCE_FILE "extlink_source.h5"
#define TARGET_FILE "extlink_target.h5"

#define PREFIX_SOURCE_FILE "extlink_prefix_source.h5"

#define SOFT_LINK_FILE    "soft_link.h5"
#define SOFT_LINK_NAME    "soft_link_to_group"
#define UD_SOFT_LINK_NAME "ud_soft_link"
#define TARGET_GROUP      "target_group"

#define UD_SOFT_CLASS 65

#define HARD_LINK_FILE    "hard_link.h5"
#define HARD_LINK_NAME    "hard_link_to_group"
#define UD_HARD_LINK_NAME "ud_hard_link"

#define UD_HARD_CLASS 66

#define PLIST_LINK_PROP "plist_link_prop"
#define UD_PLIST_CLASS  66

/* Basic external link example
 *
 * Creates two files and uses an external link to access an object in the
 * second file from the first file.
 */
static void
extlink_example(void)
{
    hid_t source_file_id, targ_file_id;
    hid_t group_id, group2_id;

    /* Create two files, a source and a target */
    source_file_id = H5Fcreate(SOURCE_FILE, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    targ_file_id   = H5Fcreate(TARGET_FILE, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create a group in the target file for the external link to point to. */
    group_id = H5Gcreate2(targ_file_id, "target_group", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /* Close the group and the target file */
    H5Gclose(group_id);

    /* Create an external link in the source file pointing to the target group.
     * We could instead have created the external link first, then created the
     * group it points to; the order doesn't matter.
     */
    H5Lcreate_external(TARGET_FILE, "target_group", source_file_id, "ext_link", H5P_DEFAULT, H5P_DEFAULT);

    /* Now we can use the external link to create a new group inside the
     * target group (even though the target file is closed!).  The external
     * link works just like a soft link.
     */
    group_id = H5Gcreate2(source_file_id, "ext_link/new_group", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /* The group is inside the target file and we can access it normally.
     * Here, group_id and group2_id point to the same group inside the
     * target file.
     */
    group2_id = H5Gopen2(targ_file_id, "target_group/new_group", H5P_DEFAULT);

    /* Don't forget to close the IDs we opened. */
    H5Gclose(group2_id);
    H5Gclose(group_id);

    H5Fclose(targ_file_id);
    H5Fclose(source_file_id);

    /* The link from the source file to the target file will work as long as
     * the target file can be found.  If the target file is moved, renamed,
     * or deleted in the filesystem, HDF5 won't be able to find it and the
     * external link will "dangle."
     */
}

/* External link prefix example
 *
 * Uses a group access property list to set a "prefix" for the filenames
 * accessed through an external link.
 *
 * Group access property lists inherit from link access property lists;
 * the external link prefix property is actually a property of LAPLs.
 *
 * This example requires a "red" directory and a "blue" directory to exist
 * where it is run (so to run this example on Unix, first mkdir red and mkdir
 * blue).
 */
static void
extlink_prefix_example(void)
{
    hid_t source_file_id, red_file_id, blue_file_id;
    hid_t group_id, group2_id;
    hid_t gapl_id;

    /* Create three files, a source and two targets.  The targets will have
     * the same name, but one will be located in the red directory and one will
     * be located in the blue directory */
    source_file_id = H5Fcreate(PREFIX_SOURCE_FILE, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    red_file_id    = H5Fcreate("red/prefix_target.h5", H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    blue_file_id   = H5Fcreate("blue/prefix_target.h5", H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* This test needs a red and a blue directory in the filesystem. If they're not present,
     * trying to create the files above will fail.
     */
    if (red_file_id < 0 || blue_file_id < 0)
        printf("This test requires directories named 'red' and 'blue' to exist. Did you forget to create "
               "them?\n");

    /* Create an external link in the source file pointing to the root group of
     * a file named prefix_target.h5.  This file doesn't exist in the current
     * directory, but the files in the red and blue directories both have this
     * name.
     */
    H5Lcreate_external("prefix_target.h5", "/", source_file_id, "ext_link", H5P_DEFAULT, H5P_DEFAULT);

    /* If we tried to traverse the external link now, we would fail (since the
     * file it points to doesn't exist).  Instead, we'll create a group access
     * property list that will provide a prefix path to the external link.
     * Group access property lists inherit the properties of link access
     * property lists.
     */
    gapl_id = H5Pcreate(H5P_GROUP_ACCESS);
    H5Pset_elink_prefix(gapl_id, "red/");

    /* Now if we traverse the external link, HDF5 will look for an external
     * file named red/prefix_target.h5, which exists.
     * To pass the group access property list, we need to use H5Gopen2.
     */
    group_id = H5Gopen2(source_file_id, "ext_link", gapl_id);

    /* Now we can use the open group ID to create a new group inside the
     * "red" file.
     */
    group2_id = H5Gcreate2(group_id, "pink", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /* Close both groups. */
    H5Gclose(group2_id);
    H5Gclose(group_id);

    /* If we change the prefix, the same external link can find a file in the blue
     * directory.
     */
    H5Pset_elink_prefix(gapl_id, "blue/");
    group_id  = H5Gopen2(source_file_id, "ext_link", gapl_id);
    group2_id = H5Gcreate2(group_id, "sky blue", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /* Close both groups. */
    H5Gclose(group2_id);
    H5Gclose(group_id);

    /* Each file has had a group created inside it using the same external link. */
    group_id  = H5Gopen2(red_file_id, "pink", H5P_DEFAULT);
    group2_id = H5Gopen2(blue_file_id, "sky blue", H5P_DEFAULT);

    /* Clean up our open IDs */
    H5Gclose(group2_id);
    H5Gclose(group_id);
    H5Pclose(gapl_id);
    H5Fclose(blue_file_id);
    H5Fclose(red_file_id);
    H5Fclose(source_file_id);

    /* User-defined links can expand on the ability to pass in parameters
     * using an access property list; for instance, a user-defined link
     * might function like an external link but allow the full filename to be
     * passed in through the access property list.
     */
}

/* Soft Link example
 *
 * Create a new class of user-defined links that behave like HDF5's built-in
 * soft links.
 *
 * This isn't very useful by itself (HDF5's soft links already do the same
 * thing), but it can serve as an example for how to reference objects by
 * name.
 */

/* We need to define the callback function that the soft link will use.
 * It is defined after the example below.
 * To keep the example simple, these links don't have a query callback.
 * In general, link classes should always be query-able.
 * We might also have wanted to supply a creation callback that checks
 * that a path was supplied in the udata.
 */
static hid_t UD_soft_traverse(const char *link_name, hid_t cur_group, const void *udata, size_t udata_size,
                              hid_t lapl_id, hid_t dxpl_id);

static void
soft_link_example(void)
{
    hid_t file_id;
    hid_t group_id;
    /* Define the link class that we'll use to register "user-defined soft
     * links" using the callbacks we defined above.
     * A link class can have NULL for any callback except its traverse
     * callback.
     */
    const H5L_class_t UD_soft_class[1] = {{
        H5L_LINK_CLASS_T_VERS,     /* Version number for this struct.
                                    * This field is always H5L_LINK_CLASS_T_VERS */
        (H5L_type_t)UD_SOFT_CLASS, /* Link class id number. This can be any
                                    * value between H5L_TYPE_UD_MIN (64) and
                                    * H5L_TYPE_MAX (255). It should be a
                                    * value that isn't already being used by
                                    * another kind of link. We'll use 65. */
        "UD_soft_link",            /* Link class name for debugging  */
        NULL,                      /* Creation callback              */
        NULL,                      /* Move callback                  */
        NULL,                      /* Copy callback                  */
        UD_soft_traverse,          /* The actual traversal function  */
        NULL,                      /* Deletion callback              */
        NULL                       /* Query callback                 */
    }};

    /* First, create a file and an object within the file for the link to
     * point to.
     */
    file_id  = H5Fcreate(SOFT_LINK_FILE, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    group_id = H5Gcreate2(file_id, TARGET_GROUP, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    H5Gclose(group_id);

    /* This is how we create a normal soft link to the group.
     */
    H5Lcreate_soft(TARGET_GROUP, file_id, SOFT_LINK_NAME, H5P_DEFAULT, H5P_DEFAULT);

    /* To do the same thing using a user-defined link, we first have to
     * register the link class we defined.
     */
    H5Lregister(UD_soft_class);

    /* Now create a user-defined link.  We give it the path to the group
     * as its udata.1
     */
    H5Lcreate_ud(file_id, UD_SOFT_LINK_NAME, (H5L_type_t)UD_SOFT_CLASS, TARGET_GROUP,
                 strlen(TARGET_GROUP) + 1, H5P_DEFAULT, H5P_DEFAULT);

    /* We can access the group through the UD soft link like we would through
     * a normal soft link. This link will still dangle if the object's
     * original name is changed or unlinked.
     */
    group_id = H5Gopen2(file_id, UD_SOFT_LINK_NAME, H5P_DEFAULT);

    /* The group is now open normally.  Don't forget to close it! */
    H5Gclose(group_id);

    H5Fclose(file_id);
}

/* UD_soft_traverse
 * The actual traversal function simply needs to open the correct object by
 * name and return its ID.
 */

static hid_t
UD_soft_traverse(const char *link_name, hid_t cur_group, const void *udata, size_t udata_size, hid_t lapl_id,
                 hid_t dxpl_id)
{
    const char *target = (const char *)udata;
    hid_t       ret_value;

    /* Pass the udata straight through to HDF5. If it's invalid, let HDF5
     * return an error.
     */
    ret_value = H5Oopen(cur_group, target, lapl_id);
    return ret_value;
}

/* Hard Link example
 *
 * Create a new class of user-defined links that behave like HDF5's built-in
 * hard links.
 *
 * This isn't very useful by itself (HDF5's hard links already do the same
 * thing), but it can serve as an example for how to reference objects by
 * address.
 */

/* We need to define the callback functions that the hard link will use.
 * These are defined after the example below.
 * To keep the example simple, these links don't have a query callback.
 * Generally, real link classes should always be query-able.
 */
static herr_t UD_hard_create(const char *link_name, hid_t loc_group, const void *udata, size_t udata_size,
                             hid_t lcpl_id);
static herr_t UD_hard_delete(const char *link_name, hid_t loc_group, const void *udata, size_t udata_size);
static hid_t  UD_hard_traverse(const char *link_name, hid_t cur_group, const void *udata, size_t udata_size,
                               hid_t lapl_id, hid_t dxpl_id);

static void
hard_link_example(void)
{
    hid_t       file_id;
    hid_t       group_id;
    H5L_info2_t li;
    /* Define the link class that we'll use to register "user-defined hard
     * links" using the callbacks we defined above.
     * A link class can have NULL for any callback except its traverse
     * callback.
     */
    const H5L_class_t UD_hard_class[1] = {{
        H5L_LINK_CLASS_T_VERS,     /* Version number for this struct.
                                    * This field is always H5L_LINK_CLASS_T_VERS */
        (H5L_type_t)UD_HARD_CLASS, /* Link class id number. This can be any
                                    * value between H5L_TYPE_UD_MIN (64) and
                                    * H5L_TYPE_MAX (255). It should be a
                                    * value that isn't already being used by
                                    * another kind of link. We'll use 66. */
        "UD_hard_link",            /* Link class name for debugging  */
        UD_hard_create,            /* Creation callback              */
        NULL,                      /* Move callback                  */
        NULL,                      /* Copy callback                  */
        UD_hard_traverse,          /* The actual traversal function  */
        UD_hard_delete,            /* Deletion callback              */
        NULL                       /* Query callback                 */
    }};

    /* First, create a file and an object within the file for the link to
     * point to.
     */
    file_id  = H5Fcreate(HARD_LINK_FILE, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    group_id = H5Gcreate2(file_id, TARGET_GROUP, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    H5Gclose(group_id);

    /* This is how we create a normal hard link to the group. This
     * creates a second "name" for the group.
     */
    H5Lcreate_hard(file_id, TARGET_GROUP, file_id, HARD_LINK_NAME, H5P_DEFAULT, H5P_DEFAULT);

    /* To do the same thing using a user-defined link, we first have to
     * register the link class we defined.
     */
    H5Lregister(UD_hard_class);

    /* Since hard links link by object address, we'll need to retrieve
     * the target group's address. We do this by calling H5Lget_info
     * on a hard link to the object.
     */
    H5Lget_info2(file_id, TARGET_GROUP, &li, H5P_DEFAULT);

    /* Now create a user-defined link.  We give it the group's address
     * as its udata.
     */
    H5Lcreate_ud(file_id, UD_HARD_LINK_NAME, (H5L_type_t)UD_HARD_CLASS, &(li.u.token), sizeof(H5O_token_t),
                 H5P_DEFAULT, H5P_DEFAULT);

    /* The UD hard link has now incremented the group's reference count
     * like a normal hard link would.  This means that we can unlink the
     * other two links to that group and it won't be deleted until the
     * UD hard link is deleted.
     */
    H5Ldelete(file_id, TARGET_GROUP, H5P_DEFAULT);
    H5Ldelete(file_id, HARD_LINK_NAME, H5P_DEFAULT);

    /* The group is still accessible through the UD hard link. If this were
     * a soft link instead, the object would have been deleted when the last
     * hard link to it was unlinked. */
    group_id = H5Gopen2(file_id, UD_HARD_LINK_NAME, H5P_DEFAULT);

    /* The group is now open normally.  Don't forget to close it! */
    H5Gclose(group_id);

    /* Removing the user-defined hard link will delete the group. */
    H5Ldelete(file_id, UD_HARD_LINK_NAME, H5P_DEFAULT);

    H5Fclose(file_id);
}

/* Callbacks for User-defined hard links. */
/* UD_hard_create
 * The most important thing this callback does is to increment the reference
 * count on the target object. Without this step, the object could be
 * deleted while this link still pointed to it, resulting in possible data
 * corruption!
 * The create callback also checks the arguments used to create this link.
 * If this function returns a negative value, the call to H5Lcreate_ud()
 * will also return failure and the link will not be created.
 */
static herr_t
UD_hard_create(const char *link_name, hid_t loc_group, const void *udata, size_t udata_size, hid_t lcpl_id)
{
    H5O_token_t token;
    hid_t       target_obj = H5I_INVALID_HID;
    herr_t      ret_value  = 0;

    /* Make sure that the address passed in looks valid */
    if (udata_size != sizeof(H5O_token_t)) {
        ret_value = -1;
        goto done;
    }

    token = *((const H5O_token_t *)udata);

    //! [H5Oopen_by_token_snip]

    /* Open the object this link points to so that we can increment
     * its reference count. This also ensures that the token passed
     * in points to a real object (although this check is not perfect!) */
    target_obj = H5Oopen_by_token(loc_group, token);

    //! [H5Oopen_by_token_snip]

    if (target_obj < 0) {
        ret_value = -1;
        goto done;
    }

    /* Increment the reference count of the target object */
    if (H5Oincr_refcount(target_obj) < 0) {
        ret_value = -1;
        goto done;
    }

done:
    /* Close the target object if we opened it */
    if (target_obj >= 0)
        H5Oclose(target_obj);
    return ret_value;
}

/* UD_hard_delete
 * Since the creation function increments the object's reference count, it's
 * important to decrement it again when the link is deleted.
 */
static herr_t
UD_hard_delete(const char *link_name, hid_t loc_group, const void *udata, size_t udata_size)
{
    H5O_token_t token;
    hid_t       target_obj = H5I_INVALID_HID;
    herr_t      ret_value  = 0;

    /* Sanity check; we have already verified the udata's size in the creation
     * callback.
     */
    if (udata_size != sizeof(H5O_token_t)) {
        ret_value = -1;
        goto done;
    }

    token = *((const H5O_token_t *)udata);

    /* Open the object this link points to */
    target_obj = H5Oopen_by_token(loc_group, token);
    if (target_obj < 0) {
        ret_value = -1;
        goto done;
    }

    /* Decrement the reference count of the target object */
    if (H5Odecr_refcount(target_obj) < 0) {
        ret_value = -1;
        goto done;
    }

done:
    /* Close the target object if we opened it */
    if (target_obj >= 0)
        H5Oclose(target_obj);
    return ret_value;
}

/* UD_hard_traverse
 * The actual traversal function simply needs to open the correct object and
 * return its ID.
 */
static hid_t
UD_hard_traverse(const char *link_name, hid_t cur_group, const void *udata, size_t udata_size, hid_t lapl_id,
                 hid_t dxpl_id)
{
    H5O_token_t token;
    hid_t       ret_value = H5I_INVALID_HID;

    /* Sanity check; we have already verified the udata's size in the creation
     * callback.
     */
    if (udata_size != sizeof(H5O_token_t))
        return H5I_INVALID_HID;

    token = *((const H5O_token_t *)udata);

    /* Open the object by token. If H5Oopen_by_token fails, ret_value will
     * be negative to indicate that the traversal function failed.
     */
    ret_value = H5Oopen_by_token(cur_group, token);

    return ret_value;
}

/* Plist example
 *
 * Create a new class of user-defined links that open objects within a file
 * based on a value passed in through a link access property list.
 *
 * Group, dataset, and datatype access property lists all inherit from link
 * access property lists, so they can be used instead of LAPLs.
 */

/* We need to define the callback functions that this link type will use.
 * These are defined after the example below.
 * These links have no udata, so they don't need a query function.
 */
static hid_t UD_plist_traverse(const char *link_name, hid_t cur_group, const void *udata, size_t udata_size,
                               hid_t lapl_id, hid_t dxpl_id);

static void
plist_link_example(void)
{
    hid_t file_id;
    hid_t group_id, group2_id;
    hid_t gapl_id;
    char *path = NULL;

    /* Define the link class that we'll use to register "plist
     * links" using the callback we defined above.
     * A link class can have NULL for any callback except its traverse
     * callback.
     */
    const H5L_class_t UD_plist_class[1] = {{
        H5L_LINK_CLASS_T_VERS,      /* Version number for this struct.
                                     * This field is always H5L_LINK_CLASS_T_VERS */
        (H5L_type_t)UD_PLIST_CLASS, /* Link class id number. This can be any
                                     * value between H5L_TYPE_UD_MIN (64) and
                                     * H5L_TYPE_MAX (255). It should be a
                                     * value that isn't already being used by
                                     * another kind of link. We'll use 67. */
        "UD_plist_link",            /* Link class name for debugging  */
        NULL,                       /* Creation callback              */
        NULL,                       /* Move callback                  */
        NULL,                       /* Copy callback                  */
        UD_plist_traverse,          /* The actual traversal function  */
        NULL,                       /* Deletion callback              */
        NULL                        /* Query callback                 */
    }};

    /* First, create a file and two objects within the file for the link to
     * point to.
     */
    file_id  = H5Fcreate(HARD_LINK_FILE, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    group_id = H5Gcreate2(file_id, "group_1", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    H5Gclose(group_id);
    group_id = H5Gcreate2(file_id, "group_1/group_2", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    H5Gclose(group_id);

    /* Register "plist links" and create one.  It has no udata at all. */
    H5Lregister(UD_plist_class);
    H5Lcreate_ud(file_id, "plist_link", (H5L_type_t)UD_PLIST_CLASS, NULL, 0, H5P_DEFAULT, H5P_DEFAULT);

    /* Create a group access property list to pass in the target for the
     * plist link.
     */
    gapl_id = H5Pcreate(H5P_GROUP_ACCESS);

    /* There is no HDF5 API for setting the property that controls these
     * links, so we have to add the property manually
     */
    H5Pinsert2(gapl_id, PLIST_LINK_PROP, sizeof(const char *), &(path), NULL, NULL, NULL, NULL, NULL, NULL);

    /* Set the property to point to the first group. */
    path = "group_1";
    H5Pset(gapl_id, PLIST_LINK_PROP, &path);

    /* Open the first group through the plist link using the GAPL we just
     * created */
    group_id = H5Gopen2(file_id, "plist_link", gapl_id);

    /* If we change the value set on the property list, it will change where
     * the plist link points.
     */
    path = "group_1/group_2";
    H5Pset(gapl_id, PLIST_LINK_PROP, &path);
    group2_id = H5Gopen2(file_id, "plist_link", gapl_id);

    /* group_id points to group_1 and group2_id points to group_2, both opened
     * through the same link.
     * Using more than one of this type of link could quickly become confusing,
     * since they will all use the same property list; however, there is
     * nothing to prevent the links from changing the property list in their
     * traverse callbacks.
     */

    /* Clean up */
    H5Pclose(gapl_id);
    H5Gclose(group_id);
    H5Gclose(group2_id);
    H5Fclose(file_id);
}

/* Traversal callback for User-defined plist links. */
/* UD_plist_traverse
 * Open a path passed in through the property list.
 */
static hid_t
UD_plist_traverse(const char *link_name, hid_t cur_group, const void *udata, size_t udata_size, hid_t lapl_id,
                  hid_t dxpl_id)
{
    char *path;
    hid_t ret_value = H5I_INVALID_HID;

    /* If the link property isn't set or can't be found, traversal fails. */
    if (H5Pexist(lapl_id, PLIST_LINK_PROP) < 0)
        goto error;

    if (H5Pget(lapl_id, PLIST_LINK_PROP, &path) < 0)
        goto error;

    /* Open the object by address. If H5Oopen_by_addr fails, ret_value will
     * be negative to indicate that the traversal function failed.
     */
    ret_value = H5Oopen(cur_group, path, lapl_id);

    return ret_value;

error:
    return H5I_INVALID_HID;
}

/* Main function
 *
 * Invokes the example functions.
 */
int
main(void)
{
    printf("Testing basic external links.\n");
    extlink_example();

    printf("Testing external link prefixes.\n");
    extlink_prefix_example();

    printf("Testing user-defined soft links.\n");
    soft_link_example();

    printf("Testing user-defined hard links.\n");
    hard_link_example();

    printf("Testing user-defined property list links.\n");
    plist_link_example();

    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_group.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 * This program creates a group in the file and two datasets in the group.
 * Hard link to the group object is created and one of the datasets is accessed
 * under new name.
 * Iterator functions are used to find information about the objects
 * in the root group and in the created group.
 */

#include "hdf5.h"

#define H5FILE_NAME "group.h5"
#define RANK        2

static herr_t file_info(hid_t loc_id, const char *name, const H5L_info2_t *linfo,
                        void *opdata); /* Link iteration operator function */
static herr_t group_info(hid_t loc_id, const char *name, const H5L_info2_t *linfo,
                         void *opdata); /* Link iteration operator function */
int
main(void)
{

    hid_t file;
    hid_t grp;
    hid_t dataset, dataspace;
    hid_t plist;

    herr_t  status;
    hsize_t dims[2];
    hsize_t cdims[2];

    int idx_f, idx_g;

    /*
     * Create a file.
     */
    file = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create a group in the file.
     */
    grp = H5Gcreate2(file, "/Data", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataset "Compressed Data" in the group using absolute
     * name. Dataset creation property list is modified to use
     * GZIP compression with the compression effort set to 6.
     * Note that compression can be used only when dataset is chunked.
     */
    dims[0]   = 1000;
    dims[1]   = 20;
    cdims[0]  = 20;
    cdims[1]  = 20;
    dataspace = H5Screate_simple(RANK, dims, NULL);
    plist     = H5Pcreate(H5P_DATASET_CREATE);
    H5Pset_chunk(plist, 2, cdims);
    H5Pset_deflate(plist, 6);
    dataset =
        H5Dcreate2(file, "/Data/Compressed_Data", H5T_NATIVE_INT, dataspace, H5P_DEFAULT, plist, H5P_DEFAULT);
    /*
     * Close the first dataset .
     */
    H5Sclose(dataspace);
    H5Dclose(dataset);

    /*
     * Create the second dataset.
     */
    dims[0]   = 500;
    dims[1]   = 20;
    dataspace = H5Screate_simple(RANK, dims, NULL);
    dataset   = H5Dcreate2(file, "/Data/Float_Data", H5T_NATIVE_FLOAT, dataspace, H5P_DEFAULT, H5P_DEFAULT,
                           H5P_DEFAULT);

    /*
     *Close the second dataset and file.
     */
    H5Sclose(dataspace);
    H5Dclose(dataset);
    H5Pclose(plist);
    H5Gclose(grp);
    H5Fclose(file);

    /*
     * Now reopen the file and group in the file.
     */
    file = H5Fopen(H5FILE_NAME, H5F_ACC_RDWR, H5P_DEFAULT);
    grp  = H5Gopen2(file, "Data", H5P_DEFAULT);

    /*
     * Access "Compressed_Data" dataset in the group.
     */
    dataset = H5Dopen2(grp, "Compressed_Data", H5P_DEFAULT);
    if (dataset < 0)
        printf(" Dataset 'Compressed-Data' is not found. \n");
    printf("\"/Data/Compressed_Data\" dataset is open \n");

    /*
     * Close the dataset.
     */
    status = H5Dclose(dataset);

    /*
     * Create hard link to the Data group.
     */
    status = H5Lcreate_hard(file, "Data", H5L_SAME_LOC, "Data_new", H5P_DEFAULT, H5P_DEFAULT);

    /*
     * We can access "Compressed_Data" dataset using created
     * hard link "Data_new".
     */
    dataset = H5Dopen2(file, "/Data_new/Compressed_Data", H5P_DEFAULT);
    if (dataset < 0)
        printf(" Dataset is not found. \n");
    printf("\"/Data_new/Compressed_Data\" dataset is open \n");

    /*
     * Close the dataset.
     */
    status = H5Dclose(dataset);

    /*
     * Use iterator to see the names of the objects in the root group.
     */
    idx_f = H5Literate2(file, H5_INDEX_NAME, H5_ITER_INC, NULL, file_info, NULL);

    /*
     * Unlink  name "Data" and use iterator to see the names
     * of the objects in the file root direvtory.
     */
    if (H5Ldelete(file, "Data", H5P_DEFAULT) < 0)
        printf(" H5Ldelete failed \n");
    else
        printf("\"Data\" is unlinked \n");

    idx_f = H5Literate2(file, H5_INDEX_NAME, H5_ITER_INC, NULL, file_info, NULL);

    /*
     * Use iterator to see the names of the objects in the group
     * /Data_new.
     */
    idx_g = H5Literate_by_name2(grp, "/Data_new", H5_INDEX_NAME, H5_ITER_INC, NULL, group_info, NULL,
                                H5P_DEFAULT);

    /*
     * Close the file.
     */

    H5Gclose(grp);
    H5Fclose(file);

    return 0;
}

/*
 * Operator function.
 */
static herr_t
file_info(hid_t loc_id, const char *name, const H5L_info2_t *linfo, void *opdata)
{
    /* avoid compiler warnings */
    (void)loc_id;
    (void)opdata;
    (void)linfo;

    /*
     * Display group name. The name is passed to the function by
     * the Library. Some magic :-)
     */
    printf("\nName : %s\n", name);

    return 0;
}

/*
 * Operator function.
 */
static herr_t
group_info(hid_t loc_id, const char *name, const H5L_info2_t *linfo, void *opdata)
{
    hid_t       did; /* dataset identifier  */
    hid_t       tid; /* datatype identifier */
    H5T_class_t t_class;
    hid_t       pid; /* data_property identifier */
    hsize_t     chunk_dims_out[2];
    int         rank_chunk;

    /* avoid warnings */
    (void)opdata;
    (void)linfo;

    /*
     * Open the datasets using their names.
     */
    did = H5Dopen2(loc_id, name, H5P_DEFAULT);

    /*
     * Display dataset name.
     */
    printf("\nName : %s\n", name);

    /*
     * Display dataset information.
     */
    tid = H5Dget_type(did);         /* get datatype*/
    pid = H5Dget_create_plist(did); /* get creation property list */

    /*
     * Check if dataset is chunked.
     */
    if (H5D_CHUNKED == H5Pget_layout(pid)) {
        /*
         * get chunking information: rank and dimensions.
         */
        rank_chunk = H5Pget_chunk(pid, 2, chunk_dims_out);
        printf("chunk rank %d, dimensions %lu x %lu\n", rank_chunk, (unsigned long)(chunk_dims_out[0]),
               (unsigned long)(chunk_dims_out[1]));
    }
    else {
        t_class = H5Tget_class(tid);
        if (t_class < 0) {
            puts(" Invalid datatype.\n");
        }
        else {
            if (t_class == H5T_INTEGER)
                puts(" Datatype is 'H5T_NATIVE_INTEGER'.\n");
            if (t_class == H5T_FLOAT)
                puts(" Datatype is 'H5T_NATIVE_FLOAT'.\n");
            if (t_class == H5T_STRING)
                puts(" Datatype is 'H5T_NATIVE_STRING'.\n");
            if (t_class == H5T_BITFIELD)
                puts(" Datatype is 'H5T_NATIVE_BITFIELD'.\n");
            if (t_class == H5T_OPAQUE)
                puts(" Datatype is 'H5T_NATIVE_OPAQUE'.\n");
            if (t_class == H5T_COMPOUND)
                puts(" Datatype is 'H5T_NATIVE_COMPOUND'.\n");
        }
    }

    H5Dclose(did);
    H5Pclose(pid);
    H5Tclose(tid);
    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_interm_group.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 * This program checks if group exists in a file and creates it including
 * all intermediate groups.
 */

#include "hdf5.h"

#define H5FILE_NAME "interm_group.h5"

int
main(void)
{

    hid_t      file;
    hid_t      g1_id, g2_id, g3_id;
    hid_t      grp_crt_plist;
    H5G_info_t g2_info;
    char       name[3];

    herr_t status;
    int    i;

    /*
     * Create a file using the default properties.
     */
    file = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create a group in the file.
     */
    g1_id = H5Gcreate2(file, "/G1", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    H5Gclose(g1_id);
    H5Fclose(file);

    /*
     * Now reopen the file and group in the file.
     */
    file = H5Fopen(H5FILE_NAME, H5F_ACC_RDWR, H5P_DEFAULT);

    /*
     * Check if group /G1 exists in the file.
     */
    if (H5Lexists(file, "/G1", H5P_DEFAULT) != false)
        printf("Group /G1 exists in the file\n");

    /*
     * Check that group G2/G3 exists in /G1 and if not create it using
     * intermediate group creation property.
     */
    g1_id = H5Gopen2(file, "/G1", H5P_DEFAULT);
    /* Next commented call causes error stack to be printed out; the next one
     * works fine; is it a bug or a feature? EIP 04-25-07
     */
    /*  if (H5Lexists(g1_id, "G2/G3", H5P_DEFAULT) !=true) { */
    if (H5Lexists(g1_id, "G2", H5P_DEFAULT) != true) {

        grp_crt_plist = H5Pcreate(H5P_LINK_CREATE);

        /* Set flag for intermediate group creation */
        status = H5Pset_create_intermediate_group(grp_crt_plist, true);
        g3_id  = H5Gcreate2(g1_id, "G2/G3", grp_crt_plist, H5P_DEFAULT, H5P_DEFAULT);
        H5Gclose(g3_id);
    }
    H5Gclose(g1_id);

    /* Now check if group /G1/G2 exists in the file, then open it and print
     * its members names
     */
    if (H5Lexists(file, "/G1/G2", H5P_DEFAULT)) {

        g2_id  = H5Gopen2(file, "/G1/G2", H5P_DEFAULT);
        status = H5Gget_info(g2_id, &g2_info);
        printf("Group /G1/G2 has %d member(s)\n", (int)g2_info.nlinks);

        for (i = 0; i < (int)g2_info.nlinks; i++) {
            H5Lget_name_by_idx(g2_id, ".", H5_INDEX_NAME, H5_ITER_NATIVE, (hsize_t)i, name, 3, H5P_DEFAULT);
            printf("Object's name is %s\n", name);
        }
        H5Gclose(g2_id);
    }
    H5Fclose(file);
    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_mount.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 * This program shows the concept of "mounting files".
 * Program creates one file with group G in it, and another
 * file with dataset D. Then second file is mounted in the first one
 * under the "mounting point" G. Dataset D is accessed in the first file
 * under name /G/D and data is printed out.
 */

#include "hdf5.h"

#define FILENAME1 "mount1.h5"
#define FILENAME2 "mount2.h5"

#define RANK 2
#define NX   4
#define NY   5

int
main(void)
{

    hid_t fid1, fid2, gid; /* Files and group identifiers */
    hid_t did, tid, sid;   /* Dataset and datatype identifiers */

    herr_t  status;
    hsize_t dims[] = {NX, NY}; /* Dataset dimensions */

    int i, j;
    int bm[NX][NY], bm_out[NX][NY]; /* Data buffers */

    /*
     * Initialization of buffer matrix "bm"
     */
    for (i = 0; i < NX; i++)
        for (j = 0; j < NY; j++)
            bm[i][j] = i + j;

    /*
     * Create first file and a group in it.
     */
    fid1 = H5Fcreate(FILENAME1, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    gid  = H5Gcreate2(fid1, "/G", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Close group and file
     */
    H5Gclose(gid);
    H5Fclose(fid1);

    /*
     * Create second file and dataset "D" in it.
     */
    fid2    = H5Fcreate(FILENAME2, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    dims[0] = NX;
    dims[1] = NY;
    sid     = H5Screate_simple(RANK, dims, NULL);
    did     = H5Dcreate2(fid2, "D", H5T_NATIVE_INT, sid, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Write data to the dataset.
     */
    status = H5Dwrite(did, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, bm);

    /*
     * Close all identifiers.
     */
    H5Sclose(sid);
    H5Dclose(did);
    H5Fclose(fid2);

    /*
     * Reopen both files
     */
    fid1 = H5Fopen(FILENAME1, H5F_ACC_RDONLY, H5P_DEFAULT);
    fid2 = H5Fopen(FILENAME2, H5F_ACC_RDONLY, H5P_DEFAULT);

    /*
     * Mount second file under G in the first file.
     */
    H5Fmount(fid1, "/G", fid2, H5P_DEFAULT);

    /*
     * Access dataset D in the first file under /G/D name.
     */
    did    = H5Dopen2(fid1, "/G/D", H5P_DEFAULT);
    tid    = H5Dget_type(did);
    status = H5Dread(did, tid, H5S_ALL, H5S_ALL, H5P_DEFAULT, bm_out);

    /*
     * Print out the data.
     */
    for (i = 0; i < NX; i++) {
        for (j = 0; j < NY; j++)
            printf("  %d", bm_out[i][j]);
        printf("\n");
    }

    /*
     * Close all identifiers
     */
    H5Tclose(tid);
    H5Dclose(did);

    /*
     * Unmounting second file
     */
    H5Funmount(fid1, "/G");

    /*
     * Close both files
     */
    H5Fclose(fid1);
    H5Fclose(fid2);

    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_rdwt.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to write and read data in an existing
 *  dataset.  It is used in the HDF5 Tutorial.
 */

#include "hdf5.h"

#define FILENAME "dset.h5"

int
main(void)
{

    hid_t  file_id, dataset_id; /* identifiers */
    herr_t status;
    int    i, j, dset_data[4][6];

    /* Initialize the dataset. */
    for (i = 0; i < 4; i++)
        for (j = 0; j < 6; j++)
            dset_data[i][j] = i * 6 + j + 1;

    /* Open an existing file. */
    file_id = H5Fopen(FILENAME, H5F_ACC_RDWR, H5P_DEFAULT);

    /* Open an existing dataset. */
    dataset_id = H5Dopen2(file_id, "/dset", H5P_DEFAULT);

    /* Write the dataset. */
    status = H5Dwrite(dataset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, dset_data);

    status = H5Dread(dataset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, dset_data);

    /* Close the dataset. */
    status = H5Dclose(dataset_id);

    /* Close the file. */
    status = H5Fclose(file_id);
}
```

### `HDF5Examples/C/TUTR/h5_read.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *   This example reads hyperslab from the SDS.h5 file
 *   created by h5_write.c program into two-dimensional
 *   plane of the three-dimensional array.
 *   Information about dataset in the SDS.h5 file is obtained.
 */

#include "hdf5.h"

#define H5FILE_NAME "SDS.h5"
#define DATASETNAME "IntArray"
#define NX_SUB      3 /* hyperslab dimensions */
#define NY_SUB      4
#define NX          7 /* output buffer dimensions */
#define NY          7
#define NZ          3
#define RANK        2
#define RANK_OUT    3

int
main(void)
{
    hid_t       file, dataset; /* handles */
    hid_t       datatype, dataspace;
    hid_t       memspace;
    H5T_class_t t_class; /* data type class */
    H5T_order_t order;   /* data order */
    size_t      size;    /*
                          * size of the data element
                          * stored in file
                          */
    hsize_t dimsm[3];    /* memory space dimensions */
    hsize_t dims_out[2]; /* dataset dimensions */
    herr_t  status;

    int data_out[NX][NY][NZ]; /* output buffer */

    hsize_t count[2];      /* size of the hyperslab in the file */
    hsize_t offset[2];     /* hyperslab offset in the file */
    hsize_t count_out[3];  /* size of the hyperslab in memory */
    hsize_t offset_out[3]; /* hyperslab offset in memory */
    int     i, j, k, status_n, rank;

    for (j = 0; j < NX; j++) {
        for (i = 0; i < NY; i++) {
            for (k = 0; k < NZ; k++)
                data_out[j][i][k] = 0;
        }
    }

    /*
     * Open the file and the dataset.
     */
    file    = H5Fopen(H5FILE_NAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dataset = H5Dopen2(file, DATASETNAME, H5P_DEFAULT);

    /*
     * Get datatype and dataspace handles and then query
     * dataset class, order, size, rank and dimensions.
     */
    datatype = H5Dget_type(dataset); /* datatype handle */
    t_class  = H5Tget_class(datatype);
    if (t_class == H5T_INTEGER)
        printf("Data set has INTEGER type \n");
    order = H5Tget_order(datatype);
    if (order == H5T_ORDER_LE)
        printf("Little endian order \n");

    size = H5Tget_size(datatype);
    printf(" Data size is %d \n", (int)size);

    dataspace = H5Dget_space(dataset); /* dataspace handle */
    rank      = H5Sget_simple_extent_ndims(dataspace);
    status_n  = H5Sget_simple_extent_dims(dataspace, dims_out, NULL);
    printf("rank %d, dimensions %lu x %lu \n", rank, (unsigned long)(dims_out[0]),
           (unsigned long)(dims_out[1]));

    /*
     * Define hyperslab in the dataset.
     */
    offset[0] = 1;
    offset[1] = 2;
    count[0]  = NX_SUB;
    count[1]  = NY_SUB;
    status    = H5Sselect_hyperslab(dataspace, H5S_SELECT_SET, offset, NULL, count, NULL);

    /*
     * Define the memory dataspace.
     */
    dimsm[0] = NX;
    dimsm[1] = NY;
    dimsm[2] = NZ;
    memspace = H5Screate_simple(RANK_OUT, dimsm, NULL);

    /*
     * Define memory hyperslab.
     */
    offset_out[0] = 3;
    offset_out[1] = 0;
    offset_out[2] = 0;
    count_out[0]  = NX_SUB;
    count_out[1]  = NY_SUB;
    count_out[2]  = 1;
    status        = H5Sselect_hyperslab(memspace, H5S_SELECT_SET, offset_out, NULL, count_out, NULL);

    /*
     * Read data from hyperslab in the file into the hyperslab in
     * memory and display.
     */
    status = H5Dread(dataset, H5T_NATIVE_INT, memspace, dataspace, H5P_DEFAULT, data_out);
    for (j = 0; j < NX; j++) {
        for (i = 0; i < NY; i++)
            printf("%d ", data_out[j][i][0]);
        printf("\n");
    }
    /*
     * 0 0 0 0 0 0 0
     * 0 0 0 0 0 0 0
     * 0 0 0 0 0 0 0
     * 3 4 5 6 0 0 0
     * 4 5 6 7 0 0 0
     * 5 6 7 8 0 0 0
     * 0 0 0 0 0 0 0
     */

    /*
     * Close/release resources.
     */
    H5Tclose(datatype);
    H5Dclose(dataset);
    H5Sclose(dataspace);
    H5Sclose(memspace);
    H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_ref2reg_deprec.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
/*
    This program shows how to create, store and dereference references
    to the dataset regions.

    It creates a file and writes a two dimensional integer dataset
    to it. Then it creates a dataset to store region references in. It
    stores references to a hyperslab and 3 points selected (for the
    integer dataset previously created).

    It then reopens the references dataset, reads and dereferences the
    region references, and then reads and displays the selected hyperslab
    and selected elements data from the integer dataset.
*/

#include "hdf5.h"

#define filename  "REF_REG.h5"
#define dsetnamev "MATRIX"
#define dsetnamer "REGION_REFERENCES"

int
main(void)
{
    hid_t           file_id;  /* file identifier */
    hid_t           space_id; /* dataspace identifiers */
    hid_t           spacer_id;
    hid_t           dsetv_id; /*dataset identifiers*/
    hid_t           dsetr_id;
    hsize_t         dims[2]  = {2, 9};
    hsize_t         dimsr[1] = {2};
    int             rank     = 2;
    int             rankr    = 1;
    herr_t          status;
    hdset_reg_ref_t ref[2];
    hdset_reg_ref_t ref_out[2];
    int             data[2][9]     = {{1, 1, 2, 3, 3, 4, 5, 5, 6}, {1, 2, 2, 3, 4, 4, 5, 6, 6}};
    int             data_out[2][9] = {{0, 0, 0, 0, 0, 0, 0, 0, 0}, {0, 0, 0, 0, 0, 0, 0, 0, 0}};
    hsize_t         start[2];
    hsize_t         count[2];
    hsize_t         coord[2][3] = {{0, 0, 1}, {6, 0, 8}};
    unsigned        num_points  = 3;
    int             i, j;
    size_t          name_size1, name_size2;
    char            buf1[10], buf2[10];

    /*
     * Create file with default file access and file creation properties.
     */
    file_id = H5Fcreate(filename, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create dataspace for datasets.
     */
    space_id  = H5Screate_simple(rank, dims, NULL);
    spacer_id = H5Screate_simple(rankr, dimsr, NULL);

    /*
     * Create integer dataset.
     */
    dsetv_id =
        H5Dcreate2(file_id, dsetnamev, H5T_NATIVE_INT, space_id, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Write data to the dataset.
     */
    status = H5Dwrite(dsetv_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, data);
    status = H5Dclose(dsetv_id);

    /*
     * Dataset with references.
     */
    dsetr_id =
        H5Dcreate2(file_id, dsetnamer, H5T_STD_REF_DSETREG, spacer_id, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create a reference to the hyperslab.
     */
    start[0] = 0;
    start[1] = 3;
    count[0] = 2;
    count[1] = 3;
    status   = H5Sselect_hyperslab(space_id, H5S_SELECT_SET, start, NULL, count, NULL);
    status   = H5Rcreate(&ref[0], file_id, dsetnamev, H5R_DATASET_REGION, space_id);

    /*
     * Create a reference to elements selection.
     */
    status = H5Sselect_none(space_id);
    status = H5Sselect_elements(space_id, H5S_SELECT_SET, num_points, (const hsize_t *)coord);
    status = H5Rcreate(&ref[1], file_id, dsetnamev, H5R_DATASET_REGION, space_id);

    /*
     * Write dataset with the references.
     */
    status = H5Dwrite(dsetr_id, H5T_STD_REF_DSETREG, H5S_ALL, H5S_ALL, H5P_DEFAULT, ref);

    /*
     * Close all objects.
     */
    status = H5Sclose(space_id);
    status = H5Sclose(spacer_id);
    status = H5Dclose(dsetr_id);
    status = H5Fclose(file_id);

    /*
     * Reopen the file to read selections back.
     */
    file_id = H5Fopen(filename, H5F_ACC_RDWR, H5P_DEFAULT);

    /*
     * Reopen the dataset with object references and read references
     * to the buffer.
     */
    dsetr_id = H5Dopen2(file_id, dsetnamer, H5P_DEFAULT);

    status = H5Dread(dsetr_id, H5T_STD_REF_DSETREG, H5S_ALL, H5S_ALL, H5P_DEFAULT, ref_out);

    /*
     * Dereference the first reference.
     */
    dsetv_id = H5Rdereference2(dsetr_id, H5P_DEFAULT, H5R_DATASET_REGION, &ref_out[0]);
    /*
     * Get name of the dataset the first region reference points to
     * using H5Rget_name
     */
    name_size1 = H5Rget_name(dsetr_id, H5R_DATASET_REGION, &ref_out[0], (char *)buf1, 10);
    printf(" Dataset's name (returned by H5Rget_name) the reference points to is %s, name length is %d\n",
           buf1, (int)name_size1);
    /*
     * Get name of the dataset the first region reference points to
     * using H5Iget_name
     */
    name_size2 = H5Iget_name(dsetv_id, (char *)buf2, 10);
    printf(" Dataset's name (returned by H5Iget_name) the reference points to is %s, name length is %d\n",
           buf2, (int)name_size2);

    space_id = H5Rget_region(dsetr_id, H5R_DATASET_REGION, &ref_out[0]);

    /*
     * Read and display hyperslab selection from the dataset.
     */

    status = H5Dread(dsetv_id, H5T_NATIVE_INT, H5S_ALL, space_id, H5P_DEFAULT, data_out);
    printf("Selected hyperslab: ");
    for (i = 0; i <= 1; i++) {
        printf("\n");
        for (j = 0; j <= 8; j++)
            printf("%d ", data_out[i][j]);
    }
    printf("\n");

    /*
     * Close dataspace and the dataset.
     */
    status = H5Sclose(space_id);
    status = H5Dclose(dsetv_id);

    /*
     * Initialize data_out array again to get point selection.
     */
    for (i = 0; i <= 1; i++)
        for (j = 0; j <= 8; j++)
            data_out[i][j] = 0;

    /*
     * Dereference the second reference.
     */
    dsetv_id = H5Rdereference2(dsetr_id, H5P_DEFAULT, H5R_DATASET_REGION, &ref_out[1]);
    space_id = H5Rget_region(dsetv_id, H5R_DATASET_REGION, &ref_out[1]);

    /*
     * Read selected data from the dataset.
     */

    status = H5Dread(dsetv_id, H5T_NATIVE_INT, H5S_ALL, space_id, H5P_DEFAULT, data_out);
    printf("Selected points: ");
    for (i = 0; i <= 1; i++) {
        printf("\n");
        for (j = 0; j <= 8; j++)
            printf("%d ", data_out[i][j]);
    }
    printf("\n");

    /*
     * Close dataspace and the dataset.
     */
    status = H5Sclose(space_id);
    status = H5Dclose(dsetv_id);
    status = H5Dclose(dsetr_id);
    status = H5Fclose(file_id);

    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_ref_compat.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 * The example below illustrates the use of the new API with a file that was
 * written using the old-style reference API, showing how one can take
 * advantage of the automatic type conversion from old reference type to new
 * reference type.
 */

#include <stdlib.h>

#include "hdf5.h"
#include <assert.h>

#define H5FILE_NAME "refer_deprec.h5"

#define NDIMS    1 /* Number of dimensions */
#define BUF_SIZE 4 /* Size of example buffer */
#define NREFS    1 /* Number of references */

int
main(void)
{
    hid_t   file1, dset1, space1;
    hsize_t dset1_dims[NDIMS] = {BUF_SIZE};
    int     dset_buf[BUF_SIZE];

    hid_t      dset2, space2;
    hsize_t    dset2_dims[NDIMS]  = {NREFS};
    hobj_ref_t ref_buf[NREFS]     = {0};
    H5R_ref_t  new_ref_buf[NREFS] = {0};
    H5O_type_t obj_type;
    int        i;

    for (i = 0; i < BUF_SIZE; i++)
        dset_buf[i] = i;

    /* Create file with one dataset and close it */
    file1 = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    space1 = H5Screate_simple(NDIMS, dset1_dims, NULL);
    dset1  = H5Dcreate2(file1, "dataset1", H5T_NATIVE_INT, space1, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    H5Dwrite(dset1, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, dset_buf);
    H5Dclose(dset1);
    H5Sclose(space1);

    /**
     * Create reference to dataset1 with deprecated API
     * (reminder: there is no destroy call for those references)
     */
    H5Rcreate(&ref_buf[0], file1, "dataset1", H5R_OBJECT, H5I_INVALID_HID);

    /* Store reference in separate dataset using deprecated reference type */
    space2 = H5Screate_simple(NDIMS, dset2_dims, NULL);
    dset2  = H5Dcreate2(file1, "references", H5T_STD_REF_OBJ, space2, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    H5Dwrite(dset2, H5T_STD_REF_OBJ, H5S_ALL, H5S_ALL, H5P_DEFAULT, ref_buf);
    H5Dclose(dset2);
    H5Sclose(space2);
    H5Fclose(file1);

    /* Read reference from file using new reference type */
    file1 = H5Fopen(H5FILE_NAME, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset2 = H5Dopen2(file1, "references", H5P_DEFAULT);
    H5Dread(dset2, H5T_STD_REF, H5S_ALL, H5S_ALL, H5P_DEFAULT, new_ref_buf);
    H5Dclose(dset2);

    /* Access reference and read dataset data through new API */
    assert(H5Rget_type((const H5R_ref_t *)&new_ref_buf[0]) == H5R_OBJECT2);
    H5Rget_obj_type3(&new_ref_buf[0], H5P_DEFAULT, &obj_type);
    assert(obj_type == H5O_TYPE_DATASET);
    dset1 = H5Ropen_object(&new_ref_buf[0], H5P_DEFAULT, H5P_DEFAULT);
    H5Dread(dset1, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, dset_buf);
    H5Dclose(dset1);
    H5Rdestroy(&new_ref_buf[0]);

    for (i = 0; i < BUF_SIZE; i++)
        assert(dset_buf[i] == i);
    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_ref_extern.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 * The example below illustrates the use of the new API with files that are
 * opened read-only. Created references to the objects in that file are
 * stored into a separate file, and accessed from that file, without the user
 * explicitly opening the original file that was referenced.
 */

#include <stdlib.h>

#include "hdf5.h"
#include <assert.h>

#define H5FILE_NAME1 "refer_extern1.h5"
#define H5FILE_NAME2 "refer_extern2.h5"

#define NDIMS    1 /* Number of dimensions */
#define BUF_SIZE 4 /* Size of example buffer */
#define NREFS    1 /* Number of references */

int
main(void)
{
    hid_t   file1, dset1, space1;
    hsize_t dset1_dims[NDIMS] = {BUF_SIZE};
    int     dset_buf[BUF_SIZE];

    hid_t      file2, dset2, space2;
    hsize_t    dset2_dims[NDIMS] = {NREFS};
    H5R_ref_t  ref_buf[NREFS]    = {0};
    H5O_type_t obj_type;
    int        i;

    for (i = 0; i < BUF_SIZE; i++)
        dset_buf[i] = i;

    /* Create file with one dataset and close it */
    file1  = H5Fcreate(H5FILE_NAME1, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    space1 = H5Screate_simple(NDIMS, dset1_dims, NULL);
    dset1  = H5Dcreate2(file1, "dataset1", H5T_NATIVE_INT, space1, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    H5Dwrite(dset1, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, dset_buf);
    H5Dclose(dset1);
    H5Sclose(space1);
    H5Fclose(file1);

    /* Create reference to dataset1 in "refer_extern1.h5" */
    file1 = H5Fopen(H5FILE_NAME1, H5F_ACC_RDONLY, H5P_DEFAULT);
    H5Rcreate_object(file1, "dataset1", H5P_DEFAULT, &ref_buf[0]);
    H5Fclose(file1);

    /* Store reference in dataset in separate file "refer_extern2.h5" */
    file2  = H5Fcreate(H5FILE_NAME2, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    space2 = H5Screate_simple(NDIMS, dset2_dims, NULL);
    dset2  = H5Dcreate2(file2, "references", H5T_STD_REF, space2, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
    H5Dwrite(dset2, H5T_STD_REF, H5S_ALL, H5S_ALL, H5P_DEFAULT, ref_buf);
    H5Dclose(dset2);
    H5Sclose(space2);
    H5Fclose(file2);
    H5Rdestroy(&ref_buf[0]);

    /* Read reference back from "refer_extern2.h5" */
    file2 = H5Fopen(H5FILE_NAME2, H5F_ACC_RDONLY, H5P_DEFAULT);
    dset2 = H5Dopen2(file2, "references", H5P_DEFAULT);
    H5Dread(dset2, H5T_STD_REF, H5S_ALL, H5S_ALL, H5P_DEFAULT, ref_buf);
    H5Dclose(dset2);
    H5Fclose(file2);

    /* Access reference and read dataset data without opening original file */
    assert(H5Rget_type((const H5R_ref_t *)&ref_buf[0]) == H5R_OBJECT2);
    H5Rget_obj_type3(&ref_buf[0], H5P_DEFAULT, &obj_type);
    assert(obj_type == H5O_TYPE_DATASET);
    dset1 = H5Ropen_object(&ref_buf[0], H5P_DEFAULT, H5P_DEFAULT);
    H5Dread(dset1, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, dset_buf);
    H5Dclose(dset1);
    H5Rdestroy(&ref_buf[0]);

    for (i = 0; i < BUF_SIZE; i++)
        assert(dset_buf[i] == i);

    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_reference_deprec.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *       This program illustrates how references to objects can be used.
 *       Program creates a dataset and a group in a file. It also creates
 *       second dataset, and references to the first dataset and the group
 *       are stored in it.
 *       Program reopens the file and reads dataset with the references.
 *       References are used to open the objects. Information about the
 *       objects is displayed.
 */

#include <stdlib.h>

#include "hdf5.h"

#define H5FILE_NAME "refere.h5"

int
main(void)
{
    hid_t      fid;   /* File, group, datasets, datatypes */
    hid_t      gid_a; /* and  dataspaces identifiers   */
    hid_t      did_b, sid_b, tid_b;
    hid_t      did_r, tid_r, sid_r;
    H5O_type_t obj_type;
    herr_t     status;

    hobj_ref_t *wbuf; /* buffer to write to disk */
    hobj_ref_t *rbuf; /* buffer to read from disk */

    hsize_t dim_r[1];
    hsize_t dim_b[2];

    /*
     *  Create a file using default properties.
     */
    fid = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     *  Create  group "A" in the file.
     */
    gid_a = H5Gcreate2(fid, "A", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     *  Create dataset "B" in the file.
     */
    dim_b[0] = 2;
    dim_b[1] = 6;
    sid_b    = H5Screate_simple(2, dim_b, NULL);
    did_b    = H5Dcreate2(fid, "B", H5T_NATIVE_FLOAT, sid_b, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     *  Create dataset "R" to store references to the objects "A" and "B".
     */
    dim_r[0] = 2;
    sid_r    = H5Screate_simple(1, dim_r, NULL);
    tid_r    = H5Tcopy(H5T_STD_REF_OBJ);
    did_r    = H5Dcreate2(fid, "R", tid_r, sid_r, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     *  Allocate write and read buffers.
     */
    wbuf = (hobj_ref_t *)malloc(sizeof(hobj_ref_t) * 2);
    rbuf = (hobj_ref_t *)malloc(sizeof(hobj_ref_t) * 2);

    /*
     *  Create references to the group "A" and dataset "B"
     *  and store them in the wbuf.
     */
    H5Rcreate(&wbuf[0], fid, "A", H5R_OBJECT, (hid_t)-1);
    H5Rcreate(&wbuf[1], fid, "B", H5R_OBJECT, (hid_t)-1);

    /*
     *  Write dataset R using default transfer properties.
     */
    status = H5Dwrite(did_r, H5T_STD_REF_OBJ, H5S_ALL, H5S_ALL, H5P_DEFAULT, wbuf);

    /*
     *  Close all objects.
     */
    H5Gclose(gid_a);

    H5Sclose(sid_b);
    H5Dclose(did_b);

    H5Tclose(tid_r);
    H5Sclose(sid_r);
    H5Dclose(did_r);

    H5Fclose(fid);

    /*
     * Reopen the file.
     */
    fid = H5Fopen(H5FILE_NAME, H5F_ACC_RDWR, H5P_DEFAULT);

    /*
     *  Open and read dataset "R".
     */
    did_r  = H5Dopen2(fid, "R", H5P_DEFAULT);
    status = H5Dread(did_r, H5T_STD_REF_OBJ, H5S_ALL, H5S_ALL, H5P_DEFAULT, rbuf);

    /*
     * Find the type of referenced objects.
     */
    status = H5Rget_obj_type2(did_r, H5R_OBJECT, &rbuf[0], &obj_type);
    if (obj_type == H5O_TYPE_GROUP)
        printf("First dereferenced object is a group. \n");

    status = H5Rget_obj_type2(did_r, H5R_OBJECT, &rbuf[1], &obj_type);
    if (obj_type == H5O_TYPE_DATASET)
        printf("Second dereferenced object is a dataset. \n");

    /*
     *  Get datatype of the dataset "B"
     */
    did_b = H5Rdereference2(did_r, H5P_DEFAULT, H5R_OBJECT, &rbuf[1]);
    tid_b = H5Dget_type(did_b);
    if (H5Tequal(tid_b, H5T_NATIVE_FLOAT))
        printf("Datatype of the dataset is H5T_NATIVE_FLOAT.\n");
    printf("\n");

    /*
     * Close all objects and free memory buffers.
     */
    H5Dclose(did_r);
    H5Dclose(did_b);
    H5Tclose(tid_b);
    H5Fclose(fid);
    free(rbuf);
    free(wbuf);

    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_select.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This program shows how the H5Sselect_hyperslab and H5Sselect_elements
 *  functions are used to write selected data from memory to the file.
 *  Program takes 48 elements from the linear buffer and writes them into
 *  the matrix using 3x2 blocks, (4,3) stride and (2,4) count.
 *  Then four elements  of the matrix are overwritten with the new values and
 *  file is closed. Program reopens the file and selects the union of two
 *  hyperslabs in the dataset in the file. Then it reads the selection into the
 *  memory dataset preserving the shape of the selection.
 */

#include "hdf5.h"

#define H5FILE_NAME "Select.h5"

#define MSPACE1_RANK 1  /* Rank of the first dataset in memory */
#define MSPACE1_DIM  50 /* Dataset size in memory */

#define MSPACE2_RANK 1 /* Rank of the second dataset in memory */
#define MSPACE2_DIM  4 /* Dataset size in memory */

#define FSPACE_RANK 2 /* Dataset rank as it is stored in the file */
#define FSPACE_DIM1                                                                                          \
    8 /* Dimension sizes of the dataset as it is                                                             \
         stored in the file */
#define FSPACE_DIM2 12

/* We will read dataset back from the file
   to the dataset in memory with these
   dataspace parameters. */
#define MSPACE_RANK 2
#define MSPACE_DIM1 8
#define MSPACE_DIM2 9

#define NPOINTS                                                                                              \
    4 /* Number of points that will be selected                                                              \
         and overwritten */
int
main(void)
{

    hid_t file, dataset;        /* File and dataset identifiers */
    hid_t mid1, mid2, mid, fid; /* Dataspace identifiers */
    hid_t plist;                /* Dataset property list identifier */

    hsize_t dim1[] = {MSPACE1_DIM}; /* Dimension size of the first dataset
                                       (in memory) */
    hsize_t dim2[] = {MSPACE2_DIM}; /* Dimension size of the second dataset
                                       (in memory */
    hsize_t fdim[] = {FSPACE_DIM1, FSPACE_DIM2};
    /* Dimension sizes of the dataset (on disk) */
    hsize_t mdim[] = {MSPACE_DIM1, MSPACE_DIM2}; /* Dimension sizes of the
                                                    dataset in memory when we
                                                    read selection from the
                                                    dataset on the disk */

    hsize_t start[2];  /* Start of hyperslab */
    hsize_t stride[2]; /* Stride of hyperslab */
    hsize_t count[2];  /* Block count */
    hsize_t block[2];  /* Block sizes */

    hsize_t coord[NPOINTS][FSPACE_RANK]; /* Array to store selected points
                                             from the file dataspace */
    herr_t   ret;
    unsigned i, j;
    int      fillvalue = 0; /* Fill value for the dataset */

    int matrix_out[MSPACE_DIM1][MSPACE_DIM2]; /* Buffer to read from the
                                                 dataset */
    int vector[MSPACE1_DIM];
    int values[] = {53, 59, 61, 67}; /* New values to be written */

    /*
     * Buffers' initialization.
     */
    vector[0] = vector[MSPACE1_DIM - 1] = -1;
    for (i = 1; i < MSPACE1_DIM - 1; i++)
        vector[i] = i;

    /*
     * Create a file.
     */
    file = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Create property list for a dataset and set up fill values.
     */
    plist = H5Pcreate(H5P_DATASET_CREATE);
    ret   = H5Pset_fill_value(plist, H5T_NATIVE_INT, &fillvalue);

    /*
     * Create dataspace for the dataset in the file.
     */
    fid = H5Screate_simple(FSPACE_RANK, fdim, NULL);

    /*
     * Create dataset in the file. Notice that creation
     * property list plist is used.
     */
    dataset = H5Dcreate2(file, "Matrix in file", H5T_NATIVE_INT, fid, H5P_DEFAULT, plist, H5P_DEFAULT);

    /*
     * Select hyperslab for the dataset in the file, using 3x2 blocks,
     * (4,3) stride and (2,4) count starting at the position (0,1).
     */
    start[0]  = 0;
    start[1]  = 1;
    stride[0] = 4;
    stride[1] = 3;
    count[0]  = 2;
    count[1]  = 4;
    block[0]  = 3;
    block[1]  = 2;
    ret       = H5Sselect_hyperslab(fid, H5S_SELECT_SET, start, stride, count, block);

    /*
     * Create dataspace for the first dataset.
     */
    mid1 = H5Screate_simple(MSPACE1_RANK, dim1, NULL);

    /*
     * Select hyperslab.
     * We will use 48 elements of the vector buffer starting at the second element.
     * Selected elements are 1 2 3 . . . 48
     */
    start[0]  = 1;
    stride[0] = 1;
    count[0]  = 48;
    block[0]  = 1;
    ret       = H5Sselect_hyperslab(mid1, H5S_SELECT_SET, start, stride, count, block);

    /*
     * Write selection from the vector buffer to the dataset in the file.
     *
     * File dataset should look like this:
     *                    0  1  2  0  3  4  0  5  6  0  7  8
     *                    0  9 10  0 11 12  0 13 14  0 15 16
     *                    0 17 18  0 19 20  0 21 22  0 23 24
     *                    0  0  0  0  0  0  0  0  0  0  0  0
     *                    0 25 26  0 27 28  0 29 30  0 31 32
     *                    0 33 34  0 35 36  0 37 38  0 39 40
     *                    0 41 42  0 43 44  0 45 46  0 47 48
     *                    0  0  0  0  0  0  0  0  0  0  0  0
     */
    ret = H5Dwrite(dataset, H5T_NATIVE_INT, mid1, fid, H5P_DEFAULT, vector);

    /*
     * Reset the selection for the file dataspace fid.
     */
    ret = H5Sselect_none(fid);

    /*
     * Create dataspace for the second dataset.
     */
    mid2 = H5Screate_simple(MSPACE2_RANK, dim2, NULL);

    /*
     * Select sequence of NPOINTS points in the file dataspace.
     */
    coord[0][0] = 0;
    coord[0][1] = 0;
    coord[1][0] = 3;
    coord[1][1] = 3;
    coord[2][0] = 3;
    coord[2][1] = 5;
    coord[3][0] = 5;
    coord[3][1] = 6;

    ret = H5Sselect_elements(fid, H5S_SELECT_SET, NPOINTS, (const hsize_t *)coord);

    /*
     * Write new selection of points to the dataset.
     */
    ret = H5Dwrite(dataset, H5T_NATIVE_INT, mid2, fid, H5P_DEFAULT, values);

    /*
     * File dataset should look like this:
     *                   53  1  2  0  3  4  0  5  6  0  7  8
     *                    0  9 10  0 11 12  0 13 14  0 15 16
     *                    0 17 18  0 19 20  0 21 22  0 23 24
     *                    0  0  0 59  0 61  0  0  0  0  0  0
     *                    0 25 26  0 27 28  0 29 30  0 31 32
     *                    0 33 34  0 35 36 67 37 38  0 39 40
     *                    0 41 42  0 43 44  0 45 46  0 47 48
     *                    0  0  0  0  0  0  0  0  0  0  0  0
     *
     */

    /*
     * Close memory file and memory dataspaces.
     */
    ret = H5Sclose(mid1);
    ret = H5Sclose(mid2);
    ret = H5Sclose(fid);

    /*
     * Close dataset.
     */
    ret = H5Dclose(dataset);

    /*
     * Close the file.
     */
    ret = H5Fclose(file);

    /*
     * Open the file.
     */
    file = H5Fopen(H5FILE_NAME, H5F_ACC_RDONLY, H5P_DEFAULT);

    /*
     * Open the dataset.
     */
    dataset = H5Dopen2(file, "Matrix in file", H5P_DEFAULT);

    /*
     * Get dataspace of the open dataset.
     */
    fid = H5Dget_space(dataset);

    /*
     * Select first hyperslab for the dataset in the file. The following
     * elements are selected:
     *                     10  0 11 12
     *                     18  0 19 20
     *                      0 59  0 61
     *
     */
    start[0]  = 1;
    start[1]  = 2;
    block[0]  = 1;
    block[1]  = 1;
    stride[0] = 1;
    stride[1] = 1;
    count[0]  = 3;
    count[1]  = 4;
    ret       = H5Sselect_hyperslab(fid, H5S_SELECT_SET, start, stride, count, block);

    /*
     * Add second selected hyperslab to the selection.
     * The following elements are selected:
     *                    19 20  0 21 22
     *                     0 61  0  0  0
     *                    27 28  0 29 30
     *                    35 36 67 37 38
     *                    43 44  0 45 46
     *                     0  0  0  0  0
     * Note that two hyperslabs overlap. Common elements are:
     *                                              19 20
     *                                               0 61
     */
    start[0]  = 2;
    start[1]  = 4;
    block[0]  = 1;
    block[1]  = 1;
    stride[0] = 1;
    stride[1] = 1;
    count[0]  = 6;
    count[1]  = 5;
    ret       = H5Sselect_hyperslab(fid, H5S_SELECT_OR, start, stride, count, block);

    /*
     * Create memory dataspace.
     */
    mid = H5Screate_simple(MSPACE_RANK, mdim, NULL);

    /*
     * Select two hyperslabs in memory. Hyperslabs has the same
     * size and shape as the selected hyperslabs for the file dataspace.
     */
    start[0]  = 0;
    start[1]  = 0;
    block[0]  = 1;
    block[1]  = 1;
    stride[0] = 1;
    stride[1] = 1;
    count[0]  = 3;
    count[1]  = 4;
    ret       = H5Sselect_hyperslab(mid, H5S_SELECT_SET, start, stride, count, block);

    start[0]  = 1;
    start[1]  = 2;
    block[0]  = 1;
    block[1]  = 1;
    stride[0] = 1;
    stride[1] = 1;
    count[0]  = 6;
    count[1]  = 5;
    ret       = H5Sselect_hyperslab(mid, H5S_SELECT_OR, start, stride, count, block);

    /*
     * Initialize data buffer.
     */
    for (i = 0; i < MSPACE_DIM1; i++) {
        for (j = 0; j < MSPACE_DIM2; j++)
            matrix_out[i][j] = 0;
    }
    /*
     * Read data back to the buffer matrix_out.
     */
    ret = H5Dread(dataset, H5T_NATIVE_INT, mid, fid, H5P_DEFAULT, matrix_out);

    /*
     * Display the result. Memory dataset is:
     *
     *                    10  0 11 12  0  0  0  0  0
     *                    18  0 19 20  0 21 22  0  0
     *                     0 59  0 61  0  0  0  0  0
     *                     0  0 27 28  0 29 30  0  0
     *                     0  0 35 36 67 37 38  0  0
     *                     0  0 43 44  0 45 46  0  0
     *                     0  0  0  0  0  0  0  0  0
     *                     0  0  0  0  0  0  0  0  0
     */
    for (i = 0; i < MSPACE_DIM1; i++) {
        for (j = 0; j < MSPACE_DIM2; j++)
            printf("%3d  ", matrix_out[i][j]);
        printf("\n");
    }

    /*
     * Close memory file and memory dataspaces.
     */
    ret = H5Sclose(mid);
    ret = H5Sclose(fid);

    /*
     * Close dataset.
     */
    ret = H5Dclose(dataset);

    /*
     * Close property list
     */
    ret = H5Pclose(plist);

    /*
     * Close the file.
     */
    ret = H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/TUTR/h5_shared_mesg.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This program illustrates the usage of HDF5's implicit message sharing
 *  feature, which can be used to save space when the same messages are
 *  used many times in a file.
 *
 *  This example creates a standard file using file creation property lists
 *  to control which messages are shared.  Messages that can be shared are
 *  datatypes, dataspaces, attributes, fill values, and filter pipelines.
 *
 */

#include <stdlib.h>

#include "hdf5.h"

#define NUM_DATASETS 40
const char *DSETNAME[] = {"dataset0",  "dataset1",  "dataset2",  "dataset3",  "dataset4",  "dataset5",
                          "dataset6",  "dataset7",  "dataset8",  "dataset9",  "dataset10", "dataset11",
                          "dataset12", "dataset13", "dataset14", "dataset15", "dataset16", "dataset17",
                          "dataset18", "dataset19", "dataset20", "dataset21", "dataset22", "dataset23",
                          "dataset24", "dataset25", "dataset26", "dataset27", "dataset28", "dataset29",
                          "dataset30", "dataset31", "dataset32", "dataset33", "dataset34", "dataset35",
                          "dataset36", "dataset37", "dataset38", "dataset39", NULL};

herr_t create_standard_file(const char *filename, hid_t fcpl);

/*-------------------------------------------------------------------------
 * Function:    main
 *
 * Purpose:     Enables shared messages using File Creation Property Lists
 *              and creates files using these settings.
 *
 *-------------------------------------------------------------------------
 */
int
main(void)
{
    hid_t  fcpl_id;
    herr_t ret;

    /* Create a file creation property list */
    fcpl_id = H5Pcreate(H5P_FILE_CREATE);
    if (fcpl_id < 0)
        goto error;

    /* The file creation property list is the default list right now.
     * Create a file using it (this is the same as creating a file with
     * H5P_DEFAULT).  Implicit shared messages will be disabled.
     */
    ret = create_standard_file("default_file.h5", fcpl_id);
    if (ret < 0)
        goto error;

    /* There are five kinds of messages that can be shared: datatypes,
     * dataspaces, attributes, fill values, and filter pipelines.
     * Shared messages are stored in up to five "indexes," where each
     * index can contain one or more types of message.  Using more indexes
     * will result in more overhead for sharing, but can also provide
     * more "tunability" and may affect caching performance.
     */
    /* To begin with, use only one index. */
    ret = H5Pset_shared_mesg_nindexes(fcpl_id, 1);
    if (ret < 0)
        goto error;

    /* Each index has a "minimum message size" for a message of that
     * type to be shared.  Since sharing a message creates some overhead,
     * this is to prevent this overhead for very small messages when little
     * space would be saved by sharing them anyway.
     * If the content of the file isn't known beforehand, it's probably best
     * to set the minimum size "high"; over 100 or 200 bytes.  If the content
     * of the file is known, this value can be used to trade space saved for
     * performance lost.  The smaller this value is, the more messages will
     * be shared, so the more overhead will be incurred.
     * This value is in bytes.  A shared message involves about 30 bytes of
     * overhead.  Note that even messages that are only written once will
     * require this overhead (since they "might" be shared in the future),
     * so setting the minimum size too low may result in a file actually growing
     * in size.
     * For this example case, we'll set the minimum sharing size to be small
     * since we know that every message the "standard" file uses will be
     * repeated many times.
     */
    /* The other property that each index has is the kinds of messages that
     * it holds.  For the simple case, we'll put every message that could be
     * shared in this single index.
     */
    ret = H5Pset_shared_mesg_index(fcpl_id, 0, H5O_SHMESG_ALL_FLAG, 40);
    if (ret < 0)
        goto error;

    /* The other property that can be set for shared messages is the
     * list/B-tree cutoff for the indexes.
     * Each shared message index beins life as a simple list of messages
     * and becomes a B-tree when "too many" messages are written to it.
     * This keeps the indexes simple when only a few messages are shared,
     * but allows them to scale for many messages.  If many messages are
     * deleted from the B-tree, it scales back down into a list.
     * A "reasonable" setting for maximum list size and minimum btree size
     * depends on what kinds of messages will be stored in the file.
     * These numbers are the same for all indexes in a file.
     * We'll guess at some numbers, though we could just as easily have kept
     * the default values.  The first value is the maximum list size, the
     * second the minimum B-tree size.
     */
    ret = H5Pset_shared_mesg_phase_change(fcpl_id, 30, 20);
    if (ret < 0)
        goto error;

    /* Now create a file with this property list.  After the FCPL is used,
     * everything is automatic; messages will be shared and this will be
     * completely transparent to the user.  Even if the file is closed
     * and re-opened, this settings will be saved and applied to messages
     * written later.
     */
    ret = create_standard_file("one_index_file.h5", fcpl_id);
    if (ret < 0)
        goto error;

    /* Now try some variations on this.  The FCPL hasn't been closed, so
     * we don't need to re-create it.
     * For instance, if we set the index to only share very large
     * messages, none of the messages we write will qualify and the file
     * will be about the same size as a normal file (with just a little extra
     * overhead).
     */
    ret = H5Pset_shared_mesg_index(fcpl_id, 0, H5O_SHMESG_ALL_FLAG, 1000);
    if (ret < 0)
        goto error;

    ret = create_standard_file("only_huge_mesgs_file.h5", fcpl_id);
    if (ret < 0)
        goto error;

    /* Or, suppose we only wanted to shared dataspaces and
     * attributes (which might make sense if we were going to use committed
     * datatypes).  We could change the flags on the index:
     */
    ret = H5Pset_shared_mesg_index(fcpl_id, 0, H5O_SHMESG_SDSPACE_FLAG | H5O_SHMESG_ATTR_FLAG, 40);
    if (ret < 0)
        goto error;

    ret = create_standard_file("only_dspaces_and_attrs_file.h5", fcpl_id);
    if (ret < 0)
        goto error;

    /* We could create a second index and put attributes in it to separate them
     * from datatypes and dataspaces (and then run some performance metrics to
     * see whether this improved caching performance).
     */
    ret = H5Pset_shared_mesg_nindexes(fcpl_id, 2);
    if (ret < 0)
        goto error;
    ret = H5Pset_shared_mesg_index(fcpl_id, 0, H5O_SHMESG_DTYPE_FLAG | H5O_SHMESG_SDSPACE_FLAG, 40);
    if (ret < 0)
        goto error;
    ret = H5Pset_shared_mesg_index(fcpl_id, 1, H5O_SHMESG_ATTR_FLAG, 40);
    if (ret < 0)
        goto error;

    ret = create_standard_file("separate_indexes_file.h5", fcpl_id);
    if (ret < 0)
        goto error;

    /* We can try twiddling the "phase change" values and see what it does to
     * the file size.  Since there's only a few different messages (two
     * datatypes, two dataspaces, and one attribute), using smaller lists will
     * save some space.
     */
    ret = H5Pset_shared_mesg_nindexes(fcpl_id, 1);
    if (ret < 0)
        goto error;
    ret = H5Pset_shared_mesg_index(fcpl_id, 0, H5O_SHMESG_ALL_FLAG, 40);
    if (ret < 0)
        goto error;

    ret = H5Pset_shared_mesg_phase_change(fcpl_id, 5, 0);
    if (ret < 0)
        goto error;

    ret = create_standard_file("small_lists_file.h5", fcpl_id);
    if (ret < 0)
        goto error;

    /* Or we could create indexes that are never lists, but are created as
     * B-trees.  We do this by setting the "maximum list size" to zero.
     */
    ret = H5Pset_shared_mesg_phase_change(fcpl_id, 0, 0);
    if (ret < 0)
        goto error;

    ret = create_standard_file("btrees_file.h5", fcpl_id);
    if (ret < 0)
        goto error;

    /* Obviously there are a lot more permutations of these options possible.
     * Performance will often be a tradeoff of speed for space, but will
     * depend a great deal on the specific application.  If performance is
     * important, the best thing to do is to play with these settings to find
     * the ones that work best for you.
     * Please let The HDF Group (help@hdfgroup.org) know what you find!
     */

    /* Close the property list */
    ret = H5Pclose(fcpl_id);
    if (ret < 0)
        goto error;
    return 0;

error:
    return -1;
}

/*-------------------------------------------------------------------------
 * Function:    create_standard_file
 *
 * Purpose:     A helper function for the example.  Creates an HDF5 file
 *              with many repeated messages using the file creation
 *              property list FCPL.
 *
 *              This function only uses datatypes, dataspaces, and
 *              attributes.  Fill values and filter pipelines can also
 *              be shared in the same way (i.e., by enabling sharing in
 *              the FCPL and writing the same message more than once).
 *-------------------------------------------------------------------------
 */
herr_t
create_standard_file(const char *filename, hid_t fcpl_id)
{
    hid_t   file_id = -1;
    hid_t   type_id = -1, temp_type_id = -1;
    hsize_t dims[]        = {10, 9, 8, 7, 6, 5, 4, 3, 2, 1};
    hid_t   space_id      = -1;
    hid_t   attr_type_id  = -1;
    hid_t   attr_space_id = -1;
    int     attr_data[]   = {1, 2, 3, 4, 5, 6, 7, 8, 9, 0};
    hid_t   dset_id       = -1;
    hid_t   attr_id       = -1;
    int     x;
    herr_t  ret;

    /* Create the file */
    file_id = H5Fcreate(filename, H5F_ACC_TRUNC, fcpl_id, H5P_DEFAULT);
    if (file_id < 0)
        goto error;

    /* Create the datatype we'll be using.  Generally, sharing messages
     * is most useful when the message is complex and takes more space on
     * disk, so this type will be an array type rather than an atomic type.
     * However, any type can be shared.
     */
    temp_type_id = H5Tarray_create2(H5T_NATIVE_INT, 2, dims);
    if (temp_type_id < 0)
        goto error;
    type_id = H5Tarray_create2(temp_type_id, 2, dims);
    if (type_id < 0)
        goto error;
    ret = H5Tclose(temp_type_id);
    if (ret < 0)
        goto error;

    /* Create the dataspace we'll be using.
     * Again, create a more complex dataspace so that more space will
     * be saved when we share it.
     */
    space_id = H5Screate_simple(10, dims, dims);
    if (space_id < 0)
        goto error;

    /* Create a datatype and dataspace for the attributes we'll be creating.
     * The datatype will be a single integer, and each attribute will hold
     * 10 integers.
     */
    attr_type_id = H5Tcopy(H5T_NATIVE_INT);
    if (attr_type_id < 0)
        goto error;
    attr_space_id = H5Screate_simple(1, dims, dims);
    if (attr_space_id < 0)
        goto error;

    /* Begin using the messages many times.  Do this by creating datasets
     * that use this datatype, dataspace, and have this attribute.
     */
    for (x = 0; x < NUM_DATASETS; ++x) {
        /* Create a dataset */
        dset_id = H5Dcreate2(file_id, DSETNAME[x], type_id, space_id, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
        if (dset_id < 0)
            goto error;

        /* Create an attribute on the dataset */
        attr_id = H5Acreate2(dset_id, "attr_name", attr_type_id, attr_space_id, H5P_DEFAULT, H5P_DEFAULT);
        if (attr_id < 0)
            goto error;

        /* Write data to the attribute */
        ret = H5Awrite(attr_id, H5T_NATIVE_INT, attr_data);
        if (ret < 0)
            goto error;

        ret = H5Aclose(attr_id);
        if (ret < 0)
            goto error;
        ret = H5Dclose(dset_id);
        if (ret < 0)
            goto error;
    }

    /* Close all open IDs */
    ret = H5Tclose(attr_type_id);
    if (ret < 0)
        goto error;
    ret = H5Sclose(attr_space_id);
    if (ret < 0)
        goto error;
    ret = H5Tclose(type_id);
    if (ret < 0)
        goto error;
    ret = H5Sclose(space_id);
    if (ret < 0)
        goto error;
    ret = H5Fclose(file_id);
    if (ret < 0)
        goto error;

    return 0;

error:
    return -1;
}
```

### `HDF5Examples/C/TUTR/h5_subset.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to read/write a subset of data (a slab)
 *  from/to a dataset in an HDF5 file.  It is used in the HDF5 Tutorial.
 */

#include "hdf5.h"

#define FILENAME    "subset.h5"
#define DATASETNAME "IntArray"
#define RANK        2

#define DIM0_SUB 3 /* subset dimensions */
#define DIM1_SUB 4

#define DIM0 8 /* size of dataset */
#define DIM1 10

int
main(void)
{
    hsize_t dims[2], dimsm[2];
    int     data[DIM0][DIM1];          /* data to write */
    int     sdata[DIM0_SUB][DIM1_SUB]; /* subset to write */
    int     rdata[DIM0][DIM1];         /* buffer for read */

    hid_t file_id, dataset_id; /* handles */
    hid_t dataspace_id, memspace_id;

    herr_t status;

    hsize_t count[2];  /* size of subset in the file */
    hsize_t offset[2]; /* subset offset in the file */
    hsize_t stride[2];
    hsize_t block[2];
    int     i, j;

    /*****************************************************************
     * Create a new file with default creation and access properties.*
     * Then create a dataset and write data to it and close the file *
     * and dataset.                                                  *
     *****************************************************************/

    file_id = H5Fcreate(FILENAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    dims[0]      = DIM0;
    dims[1]      = DIM1;
    dataspace_id = H5Screate_simple(RANK, dims, NULL);

    dataset_id =
        H5Dcreate2(file_id, DATASETNAME, H5T_STD_I32BE, dataspace_id, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    for (j = 0; j < DIM0; j++) {
        for (i = 0; i < DIM1; i++)
            if (i < (DIM1 / 2))
                data[j][i] = 1;
            else
                data[j][i] = 2;
    }

    status = H5Dwrite(dataset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, data);

    printf("\nData Written to File:\n");
    for (i = 0; i < DIM0; i++) {
        for (j = 0; j < DIM1; j++)
            printf(" %i", data[i][j]);
        printf("\n");
    }
    status = H5Sclose(dataspace_id);
    status = H5Dclose(dataset_id);
    status = H5Fclose(file_id);

    /*****************************************************
     * Reopen the file and dataset and write a subset of *
     * values to the dataset.
     *****************************************************/

    file_id    = H5Fopen(FILENAME, H5F_ACC_RDWR, H5P_DEFAULT);
    dataset_id = H5Dopen2(file_id, DATASETNAME, H5P_DEFAULT);

    /* Specify size and shape of subset to write. */

    offset[0] = 1;
    offset[1] = 2;

    count[0] = DIM0_SUB;
    count[1] = DIM1_SUB;

    stride[0] = 1;
    stride[1] = 1;

    block[0] = 1;
    block[1] = 1;

    /* Create memory space with size of subset. Get file dataspace
       and select subset from file dataspace. */

    dimsm[0]    = DIM0_SUB;
    dimsm[1]    = DIM1_SUB;
    memspace_id = H5Screate_simple(RANK, dimsm, NULL);

    dataspace_id = H5Dget_space(dataset_id);
    status       = H5Sselect_hyperslab(dataspace_id, H5S_SELECT_SET, offset, stride, count, block);

    /* Write a subset of data to the dataset, then read the
       entire dataset back from the file.  */

    printf("\nWrite subset to file specifying:\n");
    printf("    offset=1x2 stride=1x1 count=3x4 block=1x1\n");
    for (j = 0; j < DIM0_SUB; j++) {
        for (i = 0; i < DIM1_SUB; i++)
            sdata[j][i] = 5;
    }

    status = H5Dwrite(dataset_id, H5T_NATIVE_INT, memspace_id, dataspace_id, H5P_DEFAULT, sdata);

    status = H5Dread(dataset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, rdata);

    printf("\nData in File after Subset is Written:\n");
    for (i = 0; i < DIM0; i++) {
        for (j = 0; j < DIM1; j++)
            printf(" %i", rdata[i][j]);
        printf("\n");
    }

    status = H5Sclose(memspace_id);
    status = H5Sclose(dataspace_id);
    status = H5Dclose(dataset_id);
    status = H5Fclose(file_id);
}
```

### `HDF5Examples/C/TUTR/h5_write.c`

```c
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example writes data to the HDF5 file.
 *  Data conversion is performed during write operation.
 */

#include "hdf5.h"

#define H5FILE_NAME "SDS.h5"
#define DATASETNAME "IntArray"
#define NX          5 /* dataset dimensions */
#define NY          6
#define RANK        2

int
main(void)
{
    hid_t   file, dataset;       /* file and dataset handles */
    hid_t   datatype, dataspace; /* handles */
    hsize_t dimsf[2];            /* dataset dimensions */
    herr_t  status;
    int     data[NX][NY]; /* data to write */
    int     i, j;

    /*
     * Data  and output buffer initialization.
     */
    for (j = 0; j < NX; j++)
        for (i = 0; i < NY; i++)
            data[j][i] = i + j;
    /*
     * 0 1 2 3 4 5
     * 1 2 3 4 5 6
     * 2 3 4 5 6 7
     * 3 4 5 6 7 8
     * 4 5 6 7 8 9
     */

    /*
     * Create a new file using H5F_ACC_TRUNC access,
     * default file creation properties, and default file
     * access properties.
     */
    file = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Describe the size of the array and create the data space for fixed
     * size dataset.
     */
    dimsf[0]  = NX;
    dimsf[1]  = NY;
    dataspace = H5Screate_simple(RANK, dimsf, NULL);

    /*
     * Define datatype for the data in the file.
     * We will store little endian INT numbers.
     */
    datatype = H5Tcopy(H5T_NATIVE_INT);
    status   = H5Tset_order(datatype, H5T_ORDER_LE);

    /*
     * Create a new dataset within the file using defined dataspace and
     * datatype and default dataset creation properties.
     */
    dataset = H5Dcreate2(file, DATASETNAME, datatype, dataspace, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /*
     * Write the data to the dataset using default transfer properties.
     */
    status = H5Dwrite(dataset, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, data);

    /*
     * Close/release resources.
     */
    H5Sclose(dataspace);
    H5Tclose(datatype);
    H5Dclose(dataset);
    H5Fclose(file);

    return 0;
}
```

### `HDF5Examples/C/test-pc.sh`

```bash
#! /bin/sh
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.

# This file is for use of h5cc created with the CMake process.
# Environment variable, HDF5_HOME is expected to be set.
# $1 is the path name of the source directory.
# $2 is the path name of the build directory.
# $3 is the current path name.

top_srcdir=$1
top_builddir=$2
currentpath=$3
verbose=yes
nerrors=0

return_val=0

echo "Current build directory: $top_builddir$currentpath"
# Loop through all subdirectories
for dir in */; do
  if [ -d "$dir" ] && [ ! -L "$dir" ]; then
    # Check if test-pc.sh exists
    if [ -f "$dir/test-pc.sh" ];
    then
        echo "Entering directory: $dir"
        (
            mkdir -p "$top_builddir/$currentpath/$dir"
            cd "$dir"
            ./test-pc.sh $top_srcdir $top_builddir $currentpath/$dir # Execute script in the subdirectory
            status=$?
            exit $status
        )
        return_val=$?
        if test $return_val -ne 0
        then
          echo "Exiting directory: $dir with $return_val tests FAILED"
        else
          echo "Exiting directory: $dir Passed"
        fi
        nerrors=`expr $return_val + $nerrors`
    fi
  fi
done

echo "$nerrors tests failed in $currentpath"
exit $nerrors
```

### `HDF5Examples/CMakeLists.txt`

```
# Top-level CMake configuration for HDF5 Examples
# This file sets up build options, dependencies, and subdirectories for C, C++,
# Fortran, Java, and filter examples.
# It also handles MPI/parallel support, version parsing, and testing configuration.
#
# The option naming convention is H5EXAMPLE_* for examples options and HDF5_* for HDF5
# library features.

cmake_minimum_required (VERSION 3.26)
project (H5EXAMPLES C)

#-----------------------------------------------------------------------------
# Define some CMake variables for use later in the project
#-----------------------------------------------------------------------------
set (H5EXAMPLE_C_SRC_DIR          ${H5EXAMPLES_SOURCE_DIR}/C)
set (H5EXAMPLE_CXX_SRC_DIR        ${H5EXAMPLES_SOURCE_DIR}/CXX)
set (H5EXAMPLE_F90_SRC_DIR        ${H5EXAMPLES_SOURCE_DIR}/FORTRAN)
set (H5EXAMPLE_JAVA_DIR           ${H5EXAMPLES_SOURCE_DIR}/JAVA)
#-----------------------------------------------------------------------------
# Basic HDF5Examples stuff here
#-----------------------------------------------------------------------------
if (NOT EXAMPLES_EXTERNALLY_CONFIGURED)
  set (H5EXAMPLE_CONFIG_DIR      ${H5EXAMPLES_SOURCE_DIR}/config)
  set (H5EXAMPLE_RESOURCES_DIR   ${H5EXAMPLES_SOURCE_DIR}/config/cmake)

  include (TestBigEndian)
  if (NOT WINDOWS)
    TEST_BIG_ENDIAN (H5EXAMPLE_WORDS_BIGENDIAN)
  endif ()

  include (${H5EXAMPLE_CONFIG_DIR}/HDFMacros.cmake)
  include (${H5EXAMPLE_RESOURCES_DIR}/HDFExampleMacros.cmake)
  set (CMAKE_JAVA_INCLUDE_PATH "")

  SET_HDF_BUILD_TYPE()

  BASIC_SETTINGS (EX)

  #-----------------------------------------------------------------------------
  # HDF5 support
  #-----------------------------------------------------------------------------
  HDF5_SUPPORT (TRUE)
  APIVersion(${HDF5_VERSION} H5_LIBVER_DIR)

  #-----------------------------------------------------------------------------
  # Python support check
  #-----------------------------------------------------------------------------
  PYTHON_SUPPORT ()
endif ()
message (STATUS "HDF5 link libs: ${H5EXAMPLE_HDF5_LINK_LIBS}")
message (STATUS "HDF5 H5_LIBVER_DIR: ${H5_LIBVER_DIR} HDF5_VERSION_MAJOR: ${HDF5_VERSION_MAJOR}")

#-----------------------------------------------------------------------------
# Option to build JAVA examples
#-----------------------------------------------------------------------------
option (H5EXAMPLE_BUILD_JAVA "Build JAVA support" OFF)
if (H5EXAMPLE_BUILD_JAVA)
  find_package (Java)

  include (UseJava)
endif ()

#-----------------------------------------------------------------------------
# parse the full version number from H5public.h and include in H5_VERS_INFO
#-----------------------------------------------------------------------------
if(NOT DEFINED _h5public_h_contents)
  find_file (_h5public_h H5public.h ${H5EXAMPLE_HDF5_INCLUDE_DIRS})
  file (READ ${_h5public_h} _h5public_h_contents)

  string (REGEX REPLACE ".*#define[ \t]+H5_VERS_MAJOR[ \t]+([0-9]*).*$"
      "\\1" H5_VERS_MAJOR ${_h5public_h_contents})
  string (REGEX REPLACE ".*#define[ \t]+H5_VERS_MINOR[ \t]+([0-9]*).*$"
      "\\1" H5_VERS_MINOR ${_h5public_h_contents})
  string (REGEX REPLACE ".*#define[ \t]+H5_VERS_RELEASE[ \t]+([0-9]*).*$"
      "\\1" H5_VERS_RELEASE ${_h5public_h_contents})
  string (REGEX REPLACE ".*#define[ \t]+H5_VERS_SUBRELEASE[ \t]+\"([0-9A-Za-z._-]*)\".*$"
      "\\1" H5_VERS_SUBRELEASE ${_h5public_h_contents})
endif ()

if (WIN32)
  set(CMAKE_TEST_LIB_DIRECTORY "${HDF5_TOOLS_DIR}")
else ()
  set(CMAKE_TEST_LIB_DIRECTORY "${HDF5_LIBRARY_PATH}")
endif ()

#-----------------------------------------------------------------------------
# Option to Enable MPI Parallel
#-----------------------------------------------------------------------------
set (CMAKE_MODULE_PATH ${H5EXAMPLE_CONFIG_DIR} ${H5EXAMPLE_RESOURCES_DIR} ${CMAKE_MODULE_PATH})
option (H5EXAMPLE_ENABLE_PARALLEL "Enable parallel build (requires MPI)" OFF)
if (H5EXAMPLE_ENABLE_PARALLEL)
  find_package(MPI REQUIRED)
  if (MPI_C_FOUND)
    set (H5_HAVE_PARALLEL 1)
    # MPI checks, only do these if MPI_C_FOUND is true, otherwise they always fail
    # and once set, they are cached as false and not regenerated
    set (CMAKE_REQUIRED_LIBRARIES "${MPI_C_LIBRARIES}" )
    # Used by Fortran + MPI
    include (${CMAKE_ROOT}/Modules/CheckSymbolExists.cmake)
    CHECK_SYMBOL_EXISTS (MPI_Comm_c2f "${MPI_C_INCLUDE_DIRS}/mpi.h"  H5_HAVE_MPI_MULTI_LANG_Comm)
    CHECK_SYMBOL_EXISTS (MPI_Info_c2f "${MPI_C_INCLUDE_DIRS}/mpi.h"  H5_HAVE_MPI_MULTI_LANG_Info)
  else ()
    message (WARNING "Parallel libraries not found")
  endif ()
endif ()

# Parallel IO usage requires MPI to be Linked and Included
if (H5_HAVE_PARALLEL)
  set (H5EXAMPLE_HDF5_LINK_LIBS ${H5EXAMPLE_HDF5_LINK_LIBS} ${MPI_C_LIBRARIES})
  if (MPI_C_LINK_FLAGS)
    set (CMAKE_EXE_LINKER_FLAGS "${MPI_C_LINK_FLAGS} ${CMAKE_EXE_LINKER_FLAGS}")
  endif ()
  INCLUDE_DIRECTORIES (${MPI_C_INCLUDE_DIRS})
endif ()

set_directory_properties(PROPERTIES INCLUDE_DIRECTORIES 
    "${H5EXAMPLE_HDF5_INCLUDE_DIRS}"
)

#-----------------------------------------------------------------------------
# Dashboard and Testing Settings
#-----------------------------------------------------------------------------
option (H5EXAMPLE_BUILD_TESTING "Build HDF5 Example Testing" OFF)
if (H5EXAMPLE_BUILD_TESTING)
  set (DART_TESTING_TIMEOUT 1200 CACHE STRING
      "Timeout in seconds for each test (default 1200=20minutes)"
  )
  enable_testing ()
  include (CTest)
  include (${PROJECT_SOURCE_DIR}/CTestConfig.cmake)
  configure_file (${H5EXAMPLE_CONFIG_DIR}/CTestCustom.cmake ${PROJECT_BINARY_DIR}/CTestCustom.ctest @ONLY)
endif ()

if (${H5_LIBVER_DIR} GREATER 16)
  #-----------------------------------------------------------------------------
  # Option to build Fortran examples
  # Make sure this appears before the CONFIGURE_FILE step
  # so that fortran name mangling is detected before writing H5pubconf.h
  #-----------------------------------------------------------------------------
  # Set default name mangling : overridden by Fortran detection in fortran dir
  set (H5_FC_FUNC  "H5_FC_FUNC(name,NAME) name ## _")
  set (H5_FC_FUNC_ "H5_FC_FUNC_(name,NAME) name ## _")
  if (EXISTS "${H5EXAMPLES_SOURCE_DIR}/FORTRAN" AND IS_DIRECTORY "${H5EXAMPLES_SOURCE_DIR}/FORTRAN")
    option (H5EXAMPLE_BUILD_FORTRAN "Build examples FORTRAN support" OFF)
    if (H5EXAMPLE_BUILD_FORTRAN AND HDF5_PROVIDES_FORTRAN)
      set (H5EXAMPLE_LINK_Fortran_LIBS ${H5EXAMPLE_HDF5_LINK_LIBS})

      # Parallel IO usage requires MPI to be Linked and Included
      if (H5_HAVE_PARALLEL)
        set (H5EXAMPLE_LINK_Fortran_LIBS ${H5EXAMPLE_LINK_Fortran_LIBS} ${MPI_Fortran_LIBRARIES})
        if (MPI_Fortran_LINK_FLAGS)
          set (CMAKE_Fortran_EXE_LINKER_FLAGS "${MPI_Fortran_LINK_FLAGS} ${CMAKE_EXE_LINKER_FLAGS}")
        endif ()
      endif ()

      configure_file (${H5EXAMPLE_F90_SRC_DIR}/H5D/h5_version.h.in ${PROJECT_BINARY_DIR}/FORTRAN/H5D/h5_version.h @ONLY)
      configure_file (${H5EXAMPLE_F90_SRC_DIR}/H5D/h5_version.h.in ${PROJECT_BINARY_DIR}/FORTRAN/H5G/h5_version.h @ONLY)
    else ()
      set (H5EXAMPLE_BUILD_FORTRAN OFF CACHE BOOL "Build examples FORTRAN support" FORCE)
    endif ()
  else ()
    set (H5EXAMPLE_BUILD_FORTRAN OFF CACHE BOOL "Build examples FORTRAN support" FORCE)
  endif ()

  if (${H5_LIBVER_DIR} GREATER 18)
    #-----------------------------------------------------------------------------
    # Option to build JAVA examples
    #-----------------------------------------------------------------------------
    if (EXISTS "${H5EXAMPLES_SOURCE_DIR}/JAVA" AND IS_DIRECTORY "${H5EXAMPLES_SOURCE_DIR}/JAVA")
      option (H5EXAMPLE_BUILD_JAVA "Build examples JAVA support" OFF)
    else ()
      set (H5EXAMPLE_BUILD_JAVA OFF CACHE BOOL "Build examples JAVA support" FORCE)
    endif ()
  else ()
    set (H5EXAMPLE_BUILD_JAVA OFF CACHE BOOL "Build examples JAVA support" FORCE)
  endif ()

  #-----------------------------------------------------------------------------
  # Build the HL Examples
  #-----------------------------------------------------------------------------
  option (H5EXAMPLE_BUILD_HL "Build examples HIGH Level support" OFF)

  #-----------------------------------------------------------------------------
  # Build the CPP Examples
  #-----------------------------------------------------------------------------
  if (EXISTS "${H5EXAMPLES_SOURCE_DIR}/CXX" AND IS_DIRECTORY "${H5EXAMPLES_SOURCE_DIR}/CXX")
    option (H5EXAMPLE_BUILD_CXX "Build examples C++ support" OFF)
  else ()
    set (H5EXAMPLE_BUILD_CXX OFF CACHE BOOL "Build examples C++ support" FORCE)
  endif ()

  #-----------------------------------------------------------------------------
  # Option to build filter examples
  #-----------------------------------------------------------------------------
  if (EXISTS "${H5EXAMPLES_SOURCE_DIR}/C/H5FLT" AND IS_DIRECTORY "${H5EXAMPLES_SOURCE_DIR}/C/H5FLT")
    option (H5EXAMPLE_BUILD_FILTERS "Build examples PLUGIN filter support" OFF)
    if (H5EXAMPLE_BUILD_FILTERS AND HDF5_PROVIDES_PLUGIN_SUPPORT)
      if(DEFINED ENV{HDF5_PLUGIN_PATH})
        message (STATUS "ENV PATH=$ENV{HDF5_PLUGIN_PATH}")
        set (H5EXAMPLE_HDF5_PLUGIN_PATH $ENV{HDF5_PLUGIN_PATH})
      else ()
        if(NOT DEFINED H5EXAMPLE_HDF5_PLUGIN_PATH)
          message (STATUS "LIBRARY PATH=${HDF5_LIBRARY_PATH}/plugin")
          set (H5EXAMPLE_HDF5_PLUGIN_PATH ${HDF5_LIBRARY_PATH}/plugin)
        endif ()
      endif ()
      message (STATUS "H5EXAMPLE_HDF5_PLUGIN_PATH=${H5EXAMPLE_HDF5_PLUGIN_PATH}")
    else ()
      set (H5EXAMPLE_BUILD_FILTERS OFF CACHE BOOL "Build examples PLUGIN filter support" FORCE)
    endif ()
  else ()
    set (H5EXAMPLE_BUILD_FILTERS OFF CACHE BOOL "Build examples PLUGIN filter support" FORCE)
  endif ()
else ()
  set (H5EXAMPLE_BUILD_HL OFF CACHE BOOL "Build examples High Level support" FORCE)
  set (H5EXAMPLE_BUILD_FORTRAN OFF CACHE BOOL "Build examples FORTRAN support" FORCE)
  set (H5EXAMPLE_BUILD_JAVA OFF CACHE BOOL "Build examples JAVA support" FORCE)
  set (H5EXAMPLE_BUILD_CXX OFF CACHE BOOL "Build examples C++ support" FORCE)
  set (H5EXAMPLE_BUILD_FILTERS OFF CACHE BOOL "Build examples PLUGIN filter support" FORCE)
endif ()

#-----------------------------------------------------------------------------
# Build examples
#-----------------------------------------------------------------------------
add_subdirectory (C)
if (H5EXAMPLE_BUILD_FORTRAN AND HDF5_PROVIDES_FORTRAN)
  add_subdirectory (FORTRAN)
endif ()
if (H5EXAMPLE_BUILD_JAVA AND HDF5_PROVIDES_JAVA)
  add_subdirectory (JAVA)
endif ()
if (H5EXAMPLE_BUILD_CXX AND HDF5_PROVIDES_CPP_LIB)
  add_subdirectory (CXX)
endif ()
if (H5EXAMPLE_BUILD_PYTHON)
  add_subdirectory (PYTHON)
endif ()
```

### `HDF5Examples/CMakePresets.json`

```json
{
  "version": 6,
  "include": [
    "config/cmake-presets/hidden-presets.json"
  ],
  "configurePresets": [
    {
      "name": "ci-base-examples",
      "hidden": true,
      "cacheVariables": {
        "CPACK_PACKAGE_VERSION": "2.0.4",
        "HDF5_NAMESPACE": {"type": "STRING", "value": "hdf5::"},
        "HDF5_PACKAGE_NAME": {"type": "STRING", "value": "hdf5"},
        "H5EXAMPLE_BUILD_TESTING": "ON"
      }
    },
    {
      "name": "ci-StdHL",
      "hidden": true,
      "cacheVariables": {
        "H5EXAMPLE_BUILD_HL": "ON"
      }
    },
    {
      "name": "ci-StdJava",
      "hidden": true,
      "cacheVariables": {
        "H5EXAMPLE_BUILD_JAVA": "ON"
      }
    },
    {
      "name": "ci-StdFortran",
      "hidden": true,
      "cacheVariables": {
        "H5EXAMPLE_BUILD_FORTRAN": "ON"
      }
    },
    {
      "name": "ci-StdPlugins",
      "hidden": true,
      "cacheVariables": {
        "H5EXAMPLE_BUILD_FILTERS": "ON"
      }
    },
    {
      "name": "ci-StdShar",
      "hidden": true,
      "inherits": ["ci-base", "ci-base-examples"],
      "cacheVariables": {
        "BUILD_SHARED_LIBS": "ON",
        "USE_SHARED_LIBS": "ON"
      }
    },
    {
      "name": "ci-StdShar-MSVC",
      "description": "MSVC Standard Config for x64 (Release)",
      "inherits": [
        "ci-x64-Release-MSVC",
        "ci-StdJava",
        "ci-StdFortran",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-Clang",
      "description": "Clang Standard Config for x64 (Release)",
      "inherits": [
        "ci-x64-Release-Clang",
        "ci-StdJava",
        "ci-StdFortran",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-GNUC",
      "description": "GNUC Standard Config for x64 (Release)",
      "inherits": [
        "ci-x64-Release-GNUC",
        "ci-StdJava",
        "ci-StdFortran",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-macos-Clang",
      "description": "Clang Standard Config for macos (Release)",
      "inherits": [
        "ci-macos-arm64-Release-Clang",
        "ci-StdJava",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-macos-GNUC",
      "description": "GNUC Standard Config for macos (Release)",
      "inherits": [
        "ci-macos-arm64-Release-GNUC",
        "ci-StdJava",
        "ci-StdShar"
      ]
    },
    {
      "name": "ci-StdShar-Intel",
      "description": "Intel Standard Config for x64 (Release)",
      "inherits": [
        "ci-x64-Release-Intel",
        "ci-StdJava",
        "ci-StdFortran",
        "ci-StdShar"
      ]
    }
  ],
  "buildPresets": [
    {
      "name": "ci-StdShar-MSVC",
      "description": "MSVC Standard Build for x64 (Release)",
      "configurePreset": "ci-StdShar-MSVC",
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-StdShar-Clang",
      "description": "Clang Standard Build for x64 (Release)",
      "configurePreset": "ci-StdShar-Clang",
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-StdShar-GNUC",
      "description": "GNUC Standard Build for x64 (Release)",
      "configurePreset": "ci-StdShar-GNUC",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-macos-Clang",
      "description": "Clang Standard Build for macos-arm64 (Release)",
      "configurePreset": "ci-StdShar-macos-Clang",
      "inherits": [
        "ci-macos-arm64-Release-Clang"
      ]
    },
    {
      "name": "ci-StdShar-macos-GNUC",
      "description": "GNUC Standard Build for macos-arm64 (Release)",
      "configurePreset": "ci-StdShar-macos-GNUC",
      "verbose": true,
      "inherits": [
        "ci-macos-arm64-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-Intel",
      "description": "Intel Standard Build for x64 (Release)",
      "configurePreset": "ci-StdShar-Intel",
      "verbose": true,
      "inherits": [
        "ci-x64-Release-Intel"
      ]
    }
  ],
  "testPresets": [
    {
      "name": "ci-StdShar-MSVC",
      "configurePreset": "ci-StdShar-MSVC",
      "inherits": [
        "ci-x64-Release-MSVC"
      ]
    },
    {
      "name": "ci-StdShar-Clang",
      "configurePreset": "ci-StdShar-Clang",
      "inherits": [
        "ci-x64-Release-Clang"
      ]
    },
    {
      "name": "ci-StdShar-macos-Clang",
      "configurePreset": "ci-StdShar-macos-Clang",
      "inherits": [
        "ci-macos-arm64-Release-Clang"
      ],
      "execution": {
        "noTestsAction": "error",
        "timeout": 180,
        "jobs": 2
      }
    },
    {
      "name": "ci-StdShar-macos-GNUC",
      "configurePreset": "ci-StdShar-macos-GNUC",
      "inherits": [
        "ci-macos-arm64-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-GNUC",
      "configurePreset": "ci-StdShar-GNUC",
      "inherits": [
        "ci-x64-Release-GNUC"
      ]
    },
    {
      "name": "ci-StdShar-win-Intel",
      "configurePreset": "ci-StdShar-Intel",
      "inherits": [
        "ci-x64-Release-Intel"
      ],
      "filter": {
        "exclude": {
          "name": "H5DUMP-tfloatsattrs"
        }
      },
      "condition": {
        "type": "equals",
        "lhs": "${hostSystemName}",
        "rhs": "Windows"
      }
    },
    {
      "name": "ci-StdShar-Intel",
      "configurePreset": "ci-StdShar-Intel",
      "inherits": [
        "ci-x64-Release-Intel"
      ]
    }
  ],
  "workflowPresets": [
    {
      "name": "ci-StdShar-MSVC",
      "steps": [
        {"type": "configure", "name": "ci-StdShar-MSVC"},
        {"type": "build", "name": "ci-StdShar-MSVC"},
        {"type": "test", "name": "ci-StdShar-MSVC"}
      ]
    },
    {
      "name": "ci-StdShar-Clang",
      "steps": [
        {"type": "configure", "name": "ci-StdShar-Clang"},
        {"type": "build", "name": "ci-StdShar-Clang"},
        {"type": "test", "name": "ci-StdShar-Clang"}
      ]
    },
    {
      "name": "ci-StdShar-macos-Clang",
      "steps": [
        {"type": "configure", "name": "ci-StdShar-macos-Clang"},
        {"type": "build", "name": "ci-StdShar-macos-Clang"},
        {"type": "test", "name": "ci-StdShar-macos-Clang"}
      ]
    },
    {
      "name": "ci-StdShar-GNUC",
      "steps": [
        {"type": "configure", "name": "ci-StdShar-GNUC"},
        {"type": "build", "name": "ci-StdShar-GNUC"},
        {"type": "test", "name": "ci-StdShar-GNUC"}
      ]
    },
    {
      "name": "ci-StdShar-macos-GNUC",
      "steps": [
        {"type": "configure", "name": "ci-StdShar-macos-GNUC"},
        {"type": "build", "name": "ci-StdShar-macos-GNUC"},
        {"type": "test", "name": "ci-StdShar-macos-GNUC"}
      ]
    },
    {
      "name": "ci-StdShar-Intel",
      "steps": [
        {"type": "configure", "name": "ci-StdShar-Intel"},
        {"type": "build", "name": "ci-StdShar-Intel"},
        {"type": "test", "name": "ci-StdShar-Intel"}
      ]
    },
    {
      "name": "ci-StdShar-win-Intel",
      "steps": [
        {"type": "configure", "name": "ci-StdShar-Intel"},
        {"type": "build", "name": "ci-StdShar-Intel"},
        {"type": "test", "name": "ci-StdShar-win-Intel"}
      ]
    }
  ]
}
```

### `HDF5Examples/CTestConfig.cmake`

```cmake
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.
#
## This file should be placed in the root directory of your project.
## Then modify the CMakeLists.txt file in the root directory of your
## project to incorporate the testing dashboard.
## # The following are required to use Dart and the CDash dashboard
##   ENABLE_TESTING()
##   INCLUDE(CTest)
set (CTEST_PROJECT_NAME "HDF5Examples")
set (CTEST_NIGHTLY_START_TIME "18:00:00 CST")

set (CTEST_DROP_METHOD "https")
if (CTEST_DROP_SITE_INIT)
  set (CTEST_DROP_SITE "${CTEST_DROP_SITE_INIT}")
else ()
  set (CTEST_DROP_SITE "cdash.hdfgroup.org")
endif ()
if (CTEST_DROP_LOCATION_INIT)
  set (CTEST_DROP_LOCATION "${CTEST_DROP_LOCATION_INIT}")
else ()
  set (CTEST_DROP_LOCATION "/submit.php?project=HDF5Examples")
endif ()
set (CTEST_DROP_SITE_CDASH TRUE)

set (UPDATE_TYPE git)
set (VALGRIND_COMMAND "/usr/bin/valgrind")
set (VALGRIND_COMMAND_OPTIONS "-v --tool=memcheck --leak-check=full --track-fds=yes --num-callers=50 --show-reachable=yes --track-origins=yes --malloc-fill=0xff --free-fill=0xfe")
set (CTEST_MEMORYCHECK_COMMAND "/usr/bin/valgrind")
set (CTEST_MEMORYCHECK_COMMAND_OPTIONS "-v --tool=memcheck --leak-check=full --track-fds=yes --num-callers=50 --show-reachable=yes --track-origins=yes --malloc-fill=0xff --free-fill=0xfe")

set (CTEST_TESTING_TIMEOUT 1200)
set (DART_TESTING_TIMEOUT 1200)
```

### `HDF5Examples/CXX/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDFCXX_EXAMPLES CXX)

add_subdirectory (H5D)
add_subdirectory (TUTR)

#-- Add High Level Examples
if (H5EXAMPLE_BUILD_HL AND HDF5_PROVIDES_HL_LIB)
  add_subdirectory (HL)
endif ()
```

### `HDF5Examples/CXX/H5D/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_CXX_H5D CXX)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (C_sourcefiles.cmake)

if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8")
  foreach (example_name ${examples})
    add_executable (${EXAMPLE_VARNAME}_cpp_ex_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.cpp)
    target_compile_options (${EXAMPLE_VARNAME}_cpp_ex_${example_name}
        PRIVATE
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
    )
    if (H5_HAVE_PARALLEL)
      target_include_directories (${EXAMPLE_VARNAME}_cpp_ex_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
    endif ()
    target_link_libraries (${EXAMPLE_VARNAME}_cpp_ex_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
  endforeach ()
endif ()

if (H5EXAMPLE_BUILD_TESTING)
  set (${EXAMPLE_VARNAME}_cpp_ex_CLEANFILES
      Group.h5
      SDS.h5
      SDScompound.h5
      SDSextendible.h5
      Select.h5
  )
  add_test (
      NAME ${EXAMPLE_VARNAME}_cpp_ex-clear-objects
      COMMAND ${CMAKE_COMMAND} -E remove ${${EXAMPLE_VARNAME}_cpp_ex_CLEANFILES}
  )
  set_tests_properties (${EXAMPLE_VARNAME}_cpp_ex-clear-objects PROPERTIES
      FIXTURES_SETUP clear_${EXAMPLE_VARNAME}_cpp_ex
      WORKING_DIRECTORY ${PROJECT_BINARY_DIR}
  )
  add_test (
      NAME ${EXAMPLE_VARNAME}_cpp_ex-clean-objects
      COMMAND ${CMAKE_COMMAND} -E remove ${${EXAMPLE_VARNAME}_cpp_ex_CLEANFILES}
  )
  set_tests_properties (${EXAMPLE_VARNAME}_cpp_ex-clean-objects PROPERTIES
      FIXTURES_CLEANUP clear_${EXAMPLE_VARNAME}_cpp_ex
      WORKING_DIRECTORY ${PROJECT_BINARY_DIR}
  )

  macro (ADD_H5_TEST testname)
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_cpp_ex_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_cpp_ex_${testname}>)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_cpp_ex_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_cpp_ex_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_SKIP_COMPARE=TRUE"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
    endif ()
    set_tests_properties (${EXAMPLE_VARNAME}_cpp_ex_${testname} PROPERTIES
        FIXTURES_REQUIRED clear_${EXAMPLE_VARNAME}_cpp_ex
        ENVIRONMENT "${CROSSCOMPILING_PATH}"
    )
    if (last_test)
      set_tests_properties (${EXAMPLE_VARNAME}_cpp_ex_${testname} PROPERTIES DEPENDS ${last_test})
    endif ()
    set (last_test "${EXAMPLE_VARNAME}_cpp_ex_${testname}")
  endmacro ()

  foreach (example_name ${examples})
    ADD_H5_TEST (${example_name})
  endforeach ()
endif ()
```

### `HDF5Examples/CXX/H5D/C_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (examples
    create
    readdata
    writedata
    compound
    extend_ds
    chunks
    h5group
)
```

### `HDF5Examples/CXX/H5D/chunks.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *   This example shows how to read data from a chunked dataset.
 *   We will read from the file created by extend.cpp
 */

#include <iostream>
using std::cout;
using std::endl;

#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("SDSextendible.h5");
const H5std_string DATASET_NAME("ExtendibleArray");
const int          NX    = 10;
const int          NY    = 5;
const int          RANK  = 2;
const int          RANKC = 1;

int
main(void)
{
    hsize_t i, j;

    // Try block to detect exceptions raised by any of the calls inside it
    try {
        /*
         * Turn off the auto-printing when failure occurs so that we can
         * handle the errors appropriately
         */
        Exception::dontPrint();

        /*
         * Open the file and the dataset.
         */
        H5File  file(FILE_NAME, H5F_ACC_RDONLY);
        DataSet dataset = file.openDataSet(DATASET_NAME);

        /*
         * Get filespace for rank and dimension
         */
        DataSpace filespace = dataset.getSpace();

        /*
         * Get number of dimensions in the file dataspace
         */
        int rank = filespace.getSimpleExtentNdims();

        /*
         * Get and print the dimension sizes of the file dataspace
         */
        hsize_t dims[2]; // dataset dimensions
        rank = filespace.getSimpleExtentDims(dims);
        cout << "dataset rank = " << rank << ", dimensions " << (unsigned long)(dims[0]) << " x "
             << (unsigned long)(dims[1]) << endl;

        /*
         * Define the memory space to read dataset.
         */
        DataSpace mspace1(RANK, dims);

        /*
         * Read dataset back and display.
         */
        int data_out[NX][NY]; // buffer for dataset to be read
        dataset.read(data_out, PredType::NATIVE_INT, mspace1, filespace);

        cout << "\n";
        cout << "Dataset: \n";
        for (j = 0; j < dims[0]; j++) {
            for (i = 0; i < dims[1]; i++)
                cout << data_out[j][i] << " ";
            cout << endl;
        }

        /*
         *          dataset rank 2, dimensions 10 x 5
         *          chunk rank 2, dimensions 2 x 5

         *          Dataset:
         *          1 1 1 3 3
         *          1 1 1 3 3
         *          1 1 1 0 0
         *          2 0 0 0 0
         *          2 0 0 0 0
         *          2 0 0 0 0
         *          2 0 0 0 0
         *          2 0 0 0 0
         *          2 0 0 0 0
         *          2 0 0 0 0
         */

        /*
         * Read the third column from the dataset.
         * First define memory dataspace, then define hyperslab
         * and read it into column array.
         */
        hsize_t col_dims[1];
        col_dims[0] = 10;
        DataSpace mspace2(RANKC, col_dims);

        /*
         * Define the column (hyperslab) to read.
         */
        hsize_t offset[2] = {0, 2};
        hsize_t count[2]  = {10, 1};
        int     column[10]; // buffer for column to be read

        /*
         * Define hyperslab and read.
         */
        filespace.selectHyperslab(H5S_SELECT_SET, count, offset);
        dataset.read(column, PredType::NATIVE_INT, mspace2, filespace);

        cout << endl;
        cout << "Third column: " << endl;
        for (i = 0; i < 10; i++)
            cout << column[i] << endl;

        /*
         *          Third column:
         *          1
         *          1
         *          1
         *          0
         *          0
         *          0
         *          0
         *          0
         *          0
         *          0
         */

        /*
         * Get creation properties list.
         */
        DSetCreatPropList cparms = dataset.getCreatePlist();

        /*
         * Check if dataset is chunked.
         */
        hsize_t chunk_dims[2];
        int     rank_chunk;
        if (H5D_CHUNKED == cparms.getLayout()) {
            /*
             * Get chunking information: rank and dimensions
             */
            rank_chunk = cparms.getChunk(2, chunk_dims);
            cout << "chunk rank " << rank_chunk << "dimensions " << (unsigned long)(chunk_dims[0]) << " x "
                 << (unsigned long)(chunk_dims[1]) << endl;

            /*
             * Define the memory space to read a chunk.
             */
            DataSpace mspace3(rank_chunk, chunk_dims);

            /*
             * Define chunk in the file (hyperslab) to read.
             */
            offset[0] = 2;
            offset[1] = 0;
            count[0]  = chunk_dims[0];
            count[1]  = chunk_dims[1];
            filespace.selectHyperslab(H5S_SELECT_SET, count, offset);

            /*
             * Read chunk back and display.
             */
            int chunk_out[2][5]; // buffer for chunk to be read
            dataset.read(chunk_out, PredType::NATIVE_INT, mspace3, filespace);
            cout << endl;
            cout << "Chunk:" << endl;
            for (j = 0; j < chunk_dims[0]; j++) {
                for (i = 0; i < chunk_dims[1]; i++)
                    cout << chunk_out[j][i] << " ";
                cout << endl;
            }
            /*
             *   Chunk:
             *   1 1 1 0 0
             *   2 0 0 0 0
             */
        }
    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataSpaceIException error) {
        error.printErrorStack();
        return -1;
    }
    return 0;
}
```

### `HDF5Examples/CXX/H5D/compound.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 * This example shows how to create a compound datatype,
 * write an array which has the compound datatype to the file,
 * and read back fields' subsets.
 */

#include <iostream>
using std::cout;
using std::endl;

#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("SDScompound.h5");
const H5std_string DATASET_NAME("ArrayOfStructures");
const H5std_string MEMBER1("a_name");
const H5std_string MEMBER2("b_name");
const H5std_string MEMBER3("c_name");
const int          LENGTH = 10;
const int          RANK   = 1;

int
main(void)
{
    /* First structure  and dataset*/
    typedef struct s1_t {
        int    a;
        float  b;
        double c;
    } s1_t;

    /* Second structure (subset of s1_t)  and dataset*/
    typedef struct s2_t {
        double c;
        int    a;
    } s2_t;

    // Try block to detect exceptions raised by any of the calls inside it
    try {
        /*
         * Initialize the data
         */
        int  i;
        s1_t s1[LENGTH];
        for (i = 0; i < LENGTH; i++) {
            s1[i].a = i;
            s1[i].b = i * i;
            s1[i].c = 1. / (i + 1);
        }

        /*
         * Turn off the auto-printing when failure occurs so that we can
         * handle the errors appropriately
         */
        Exception::dontPrint();

        /*
         * Create the data space.
         */
        hsize_t   dim[] = {LENGTH}; /* Dataspace dimensions */
        DataSpace space(RANK, dim);

        /*
         * Create the file.
         */
        H5File *file = new H5File(FILE_NAME, H5F_ACC_TRUNC);

        /*
         * Create the memory datatype.
         */
        CompType mtype1(sizeof(s1_t));
        mtype1.insertMember(MEMBER1, HOFFSET(s1_t, a), PredType::NATIVE_INT);
        mtype1.insertMember(MEMBER3, HOFFSET(s1_t, c), PredType::NATIVE_DOUBLE);
        mtype1.insertMember(MEMBER2, HOFFSET(s1_t, b), PredType::NATIVE_FLOAT);

        /*
         * Create the dataset.
         */
        DataSet *dataset;
        dataset = new DataSet(file->createDataSet(DATASET_NAME, mtype1, space));

        /*
         * Write data to the dataset;
         */
        dataset->write(s1, mtype1);

        /*
         * Release resources
         */
        delete dataset;
        delete file;

        /*
         * Open the file and the dataset.
         */
        file    = new H5File(FILE_NAME, H5F_ACC_RDONLY);
        dataset = new DataSet(file->openDataSet(DATASET_NAME));

        /*
         * Create a datatype for s2
         */
        CompType mtype2(sizeof(s2_t));

        mtype2.insertMember(MEMBER3, HOFFSET(s2_t, c), PredType::NATIVE_DOUBLE);
        mtype2.insertMember(MEMBER1, HOFFSET(s2_t, a), PredType::NATIVE_INT);

        /*
         * Read two fields c and a from s1 dataset. Fields in the file
         * are found by their names "c_name" and "a_name".
         */
        s2_t s2[LENGTH];
        dataset->read(s2, mtype2);

        /*
         * Display the fields
         */
        cout << endl << "Field c : " << endl;
        for (i = 0; i < LENGTH; i++)
            cout << s2[i].c << " ";
        cout << endl;

        cout << endl << "Field a : " << endl;
        for (i = 0; i < LENGTH; i++)
            cout << s2[i].a << " ";
        cout << endl;

        /*
         * Create a datatype for s3.
         */
        CompType mtype3(sizeof(float));

        mtype3.insertMember(MEMBER2, 0, PredType::NATIVE_FLOAT);

        /*
         * Read field b from s1 dataset. Field in the file is found by its name.
         */
        float s3[LENGTH]; // Third "structure" - used to read float field of s1
        dataset->read(s3, mtype3);

        /*
         * Display the field
         */
        cout << endl << "Field b : " << endl;
        for (i = 0; i < LENGTH; i++)
            cout << s3[i] << " ";
        cout << endl;

        /*
         * Release resources
         */
        delete dataset;
        delete file;
    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataSpaceIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataTypeIException error) {
        error.printErrorStack();
        return -1;
    }

    return 0;
}
```

### `HDF5Examples/CXX/H5D/create.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example writes a dataset to a new HDF5 file.
 */

#include <iostream>

#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("SDS.h5");
const H5std_string DATASET_NAME("IntArray");
const int          NX   = 5; // dataset dimensions
const int          NY   = 6;
const int          RANK = 2;

int
main(void)
{
    /*
     * Data initialization.
     */
    int i, j;
    int data[NX][NY]; // buffer for data to write
    for (j = 0; j < NX; j++) {
        for (i = 0; i < NY; i++)
            data[j][i] = i + j;
    }
    /*
     * 0 1 2 3 4 5
     * 1 2 3 4 5 6
     * 2 3 4 5 6 7
     * 3 4 5 6 7 8
     * 4 5 6 7 8 9
     */

    // Try block to detect exceptions raised by any of the calls inside it
    try {
        /*
         * Turn off the auto-printing when failure occurs so that we can
         * handle the errors appropriately
         */
        Exception::dontPrint();

        /*
         * Create a new file using H5F_ACC_TRUNC access,
         * default file creation properties, and default file
         * access properties.
         */
        H5File file(FILE_NAME, H5F_ACC_TRUNC);

        /*
         * Define the size of the array and create the data space for fixed
         * size dataset.
         */
        hsize_t dimsf[2]; // dataset dimensions
        dimsf[0] = NX;
        dimsf[1] = NY;
        DataSpace dataspace(RANK, dimsf);

        /*
         * Define datatype for the data in the file.
         * We will store little endian INT numbers.
         */
        IntType datatype(PredType::NATIVE_INT);
        datatype.setOrder(H5T_ORDER_LE);

        /*
         * Create a new dataset within the file using defined dataspace and
         * datatype and default dataset creation properties.
         */
        DataSet dataset = file.createDataSet(DATASET_NAME, datatype, dataspace);

        /*
         * Write the data to the dataset using default memory space, file
         * space, and transfer properties.
         */
        dataset.write(data, PredType::NATIVE_INT);
    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataSpaceIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataTypeIException error) {
        error.printErrorStack();
        return -1;
    }

    return 0; // successfully terminated
}
```

### `HDF5Examples/CXX/H5D/extend_ds.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *   This example shows how to work with extendible dataset.
 *   In the current version of the library dataset MUST be
 *   chunked.
 *
 */

#include <iostream>
#include <string>

using std::cout;
using std::endl;

#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("SDSextendible.h5");
const H5std_string DATASET_NAME("ExtendibleArray");
const int          NX   = 10;
const int          NY   = 5;
const int          RANK = 2;

int
main(void)
{
    /*
     * Try block to detect exceptions raised by any of the calls inside it
     */
    try {
        /*
         * Turn off the auto-printing when failure occurs so that we can
         * handle the errors appropriately
         */
        Exception::dontPrint();

        /*
         * Create the data space with unlimited dimensions.
         */
        hsize_t   dims[2]    = {3, 3}; // dataset dimensions at creation
        hsize_t   maxdims[2] = {H5S_UNLIMITED, H5S_UNLIMITED};
        DataSpace mspace1(RANK, dims, maxdims);

        /*
         * Create a new file. If file exists its contents will be overwritten.
         */
        H5File file(FILE_NAME, H5F_ACC_TRUNC);

        /*
         * Modify dataset creation properties, i.e. enable chunking.
         */
        DSetCreatPropList cparms;

        hsize_t chunk_dims[2] = {2, 5};
        cparms.setChunk(RANK, chunk_dims);

        /*
         * Set fill value for the dataset
         */
        int fill_val = 0;
        cparms.setFillValue(PredType::NATIVE_INT, &fill_val);

        /*
         * Create a new dataset within the file using cparms
         * creation properties.
         */
        DataSet dataset = file.createDataSet(DATASET_NAME, PredType::NATIVE_INT, mspace1, cparms);

        /*
         * Extend the dataset. This call assures that dataset is at least 3 x 3.
         */
        hsize_t size[2];
        size[0] = 3;
        size[1] = 3;
        dataset.extend(size);

        /*
         * Select a hyperslab.
         */
        DataSpace fspace1 = dataset.getSpace();
        hsize_t   offset[2];
        offset[0]        = 0;
        offset[1]        = 0;
        hsize_t dims1[2] = {3, 3}; /* data1 dimensions */
        fspace1.selectHyperslab(H5S_SELECT_SET, dims1, offset);

        /*
         * Write the data to the hyperslab.
         */
        int data1[3][3] = {{1, 1, 1}, /* data to write */
                           {1, 1, 1},
                           {1, 1, 1}};
        dataset.write(data1, PredType::NATIVE_INT, mspace1, fspace1);

        /*
         * Extend the dataset. Dataset becomes 10 x 3.
         */
        hsize_t dims2[2] = {7, 1}; /* data2 dimensions */
        dims[0]          = dims1[0] + dims2[0];
        size[0]          = dims[0];
        size[1]          = dims[1];
        dataset.extend(size);

        /*
         * Select a hyperslab.
         */
        DataSpace fspace2 = dataset.getSpace();
        offset[0]         = 3;
        offset[1]         = 0;
        fspace2.selectHyperslab(H5S_SELECT_SET, dims2, offset);

        /*
         * Define memory space
         */
        DataSpace mspace2(RANK, dims2);

        /*
         * Write the data to the hyperslab.
         */
        int data2[7] = {2, 2, 2, 2, 2, 2, 2};
        dataset.write(data2, PredType::NATIVE_INT, mspace2, fspace2);

        /*
         * Extend the dataset. Dataset becomes 10 x 5.
         */
        hsize_t dims3[2] = {2, 2}; /* data3 dimensions */
        dims[1]          = dims1[1] + dims3[1];
        size[0]          = dims[0];
        size[1]          = dims[1];
        dataset.extend(size);

        /*
         * Select a hyperslab
         */
        DataSpace fspace3 = dataset.getSpace();
        offset[0]         = 0;
        offset[1]         = 3;
        fspace3.selectHyperslab(H5S_SELECT_SET, dims3, offset);

        /*
         * Define memory space.
         */
        DataSpace mspace3(RANK, dims3);

        /*
         * Write the data to the hyperslab.
         */
        int data3[2][2] = {{3, 3}, {3, 3}};
        dataset.write(data3, PredType::NATIVE_INT, mspace3, fspace3);

        /*
         * Read the data from this dataset and display it.
         */
        int i, j;
        int data_out[NX][NY];
        for (i = 0; i < NX; i++) {
            for (j = 0; j < NY; j++)
                data_out[i][j] = 0;
        }
        dataset.read(data_out, PredType::NATIVE_INT);
        /*
         * Resulting dataset
         *
         *         1 1 1 3 3
         *         1 1 1 3 3
         *         1 1 1 0 0
         *         2 0 0 0 0
         *         2 0 0 0 0
         *         2 0 0 0 0
         *         2 0 0 0 0
         *         2 0 0 0 0
         *         2 0 0 0 0
         *         2 0 0 0 0
         */
        /*
         * Display the result.
         */
        for (i = 0; i < NX; i++) {
            for (j = 0; j < NY; j++)
                cout << data_out[i][j] << "  ";
            cout << endl;
        }
    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataSpaceIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataTypeIException error) {
        error.printErrorStack();
        return -1;
    }
    return 0;
}
```

### `HDF5Examples/CXX/H5D/h5group.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 * This example creates a group in the file and dataset in the group.
 * Hard link to the group object is created and the dataset is accessed
 * under different names.
 * Iterator function is used to find the object names in the root group.
 * Note that the C++ API iterator function is not completed yet, thus
 * the C version is used in this example.
 */

#include <iostream>
using std::cout;
using std::endl;

#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("Group.h5");
const int          RANK = 2;

// Operator function
extern "C" herr_t file_info(hid_t loc_id, const char *name, const H5L_info2_t *linfo, void *opdata);

int
main(void)
{

    hsize_t dims[2];
    hsize_t cdims[2];

    // Try block to detect exceptions raised by any of the calls inside it
    try {
        /*
         * Turn off the auto-printing when failure occurs so that we can
         * handle the errors appropriately
         */
        Exception::dontPrint();

        /*
         * Create the named file, truncating the existing one if any,
         * using default create and access property lists.
         */
        H5File *file = new H5File(FILE_NAME, H5F_ACC_TRUNC);

        /*
         * Create a group in the file
         */
        Group *group = new Group(file->createGroup("/Data"));

        /*
         * Create dataset "Compressed Data" in the group using absolute
         * name. Dataset creation property list is modified to use
         * GZIP compression with the compression effort set to 6.
         * Note that compression can be used only when dataset is chunked.
         */
        dims[0]                     = 1000;
        dims[1]                     = 20;
        cdims[0]                    = 20;
        cdims[1]                    = 20;
        DataSpace        *dataspace = new DataSpace(RANK, dims); // create new dspace
        DSetCreatPropList ds_creatplist;                         // create dataset creation prop list
        ds_creatplist.setChunk(2, cdims);                        // then modify it for compression
        ds_creatplist.setDeflate(6);

        /*
         * Create the first dataset.
         */
        DataSet *dataset = new DataSet(
            file->createDataSet("/Data/Compressed_Data", PredType::NATIVE_INT, *dataspace, ds_creatplist));

        /*
         * Close the first dataset.
         */
        delete dataset;
        delete dataspace;

        /*
         * Create the second dataset.
         */
        dims[0]   = 500;
        dims[1]   = 20;
        dataspace = new DataSpace(RANK, dims); // create second dspace
        dataset   = new DataSet(file->createDataSet("/Data/Float_Data", PredType::NATIVE_FLOAT, *dataspace));

        delete dataset;
        delete dataspace;
        delete group;
        delete file;

        /*
         * Now reopen the file and group in the file.
         */
        file  = new H5File(FILE_NAME, H5F_ACC_RDWR);
        group = new Group(file->openGroup("Data"));

        /*
         * Access "Compressed_Data" dataset in the group.
         */
        try { // to determine if the dataset exists in the group
            dataset = new DataSet(group->openDataSet("Compressed_Data"));
        }
        catch (GroupIException not_found_error) {
            cout << " Dataset is not found." << endl;
        }
        cout << "dataset \"/Data/Compressed_Data\" is open" << endl;

        /*
         * Close the dataset.
         */
        delete dataset;

        /*
         * Create hard link to the Data group.
         */
        file->link(H5L_TYPE_HARD, "Data", "Data_new");

        /*
         * We can access "Compressed_Data" dataset using created
         * hard link "Data_new".
         */
        try { // to determine if the dataset exists in the file
            dataset = new DataSet(file->openDataSet("/Data_new/Compressed_Data"));
        }
        catch (FileIException not_found_error) {
            cout << " Dataset is not found." << endl;
        }
        cout << "dataset \"/Data_new/Compressed_Data\" is open" << endl;

        /*
         * Close the dataset.
         */
        delete dataset;

        /*
         * Use iterator to see the names of the objects in the file
         * root directory.
         */
        cout << endl << "Iterating over elements in the file" << endl;
        herr_t idx = H5Literate2(file->getId(), H5_INDEX_NAME, H5_ITER_INC, NULL, file_info, NULL);
        cout << endl;

        /*
         * Unlink  name "Data" and use iterator to see the names
         * of the objects in the file root direvtory.
         */
        cout << "Unlinking..." << endl;
        try { // attempt to unlink the dataset
            file->unlink("Data");
        }
        catch (FileIException unlink_error) {
            cout << " unlink failed." << endl;
        }
        cout << "\"Data\" is unlinked" << endl;

        cout << endl << "Iterating over elements in the file again" << endl;
        idx = H5Literate2(file->getId(), H5_INDEX_NAME, H5_ITER_INC, NULL, file_info, NULL);
        cout << endl;

        /*
         * Close the group and file.
         */
        delete group;
        delete file;
    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataSpaceIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the Attribute operations
    catch (AttributeIException error) {
        error.printErrorStack();
        return -1;
    }
    return 0;
}

/*
 * Operator function.
 */
herr_t
file_info(hid_t loc_id, const char *name, const H5L_info2_t *linfo, void *opdata)
{
    hid_t group;

    /*
     * Open the group using its name.
     */
    group = H5Gopen2(loc_id, name, H5P_DEFAULT);

    /*
     * Display group name.
     */
    cout << "Name : " << name << endl;

    H5Gclose(group);
    return 0;
}
```

### `HDF5Examples/CXX/H5D/readdata.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

//
//      This example reads hyperslab from the SDS.h5 file into
//      two-dimensional plane of a three-dimensional array.  Various
//      information about the dataset in the SDS.h5 file is obtained.
//

#include <iostream>
using std::cout;
using std::endl;

#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("SDS.h5");
const H5std_string DATASET_NAME("IntArray");
const int          NX_SUB   = 3; // hyperslab dimensions
const int          NY_SUB   = 4;
const int          NX       = 7; // output buffer dimensions
const int          NY       = 7;
const int          NZ       = 3;
const int          RANK_OUT = 3;

int
main(void)
{
    /*
     * Output buffer initialization.
     */
    int i, j, k;
    int data_out[NX][NY][NZ]; /* output buffer */
    for (j = 0; j < NX; j++) {
        for (i = 0; i < NY; i++) {
            for (k = 0; k < NZ; k++)
                data_out[j][i][k] = 0;
        }
    }

    /*
     * Try block to detect exceptions raised by any of the calls inside it
     */
    try {
        /*
         * Turn off the auto-printing when failure occurs so that we can
         * handle the errors appropriately
         */
        Exception::dontPrint();

        /*
         * Open the specified file and the specified dataset in the file.
         */
        H5File  file(FILE_NAME, H5F_ACC_RDONLY);
        DataSet dataset = file.openDataSet(DATASET_NAME);

        /*
         * Get the class of the datatype that is used by the dataset.
         */
        H5T_class_t type_class = dataset.getTypeClass();

        /*
         * Get class of datatype and print message if it's an integer.
         */
        if (type_class == H5T_INTEGER) {
            cout << "Data set has INTEGER type" << endl;

            /*
             * Get the integer datatype
             */
            IntType intype = dataset.getIntType();

            /*
             * Get order of datatype and print message if it's a little endian.
             */
            H5std_string order_string;
            (void)intype.getOrder(order_string);
            cout << order_string << endl;

            /*
             * Get size of the data element stored in file and print it.
             */
            size_t size = intype.getSize();
            cout << "Data size is " << size << endl;
        }

        /*
         * Get dataspace of the dataset.
         */
        DataSpace dataspace = dataset.getSpace();

        /*
         * Get the number of dimensions in the dataspace.
         */
        int rank = dataspace.getSimpleExtentNdims();

        /*
         * Get the dimension size of each dimension in the dataspace and
         * display them.
         */
        hsize_t dims_out[2];
        (void)dataspace.getSimpleExtentDims(dims_out, NULL);
        cout << "rank " << rank << ", dimensions " << (unsigned long)(dims_out[0]) << " x "
             << (unsigned long)(dims_out[1]) << endl;

        /*
         * Define hyperslab in the dataset; implicitly giving strike and
         * block NULL.
         */
        hsize_t offset[2]; // hyperslab offset in the file
        hsize_t count[2];  // size of the hyperslab in the file
        offset[0] = 1;
        offset[1] = 2;
        count[0]  = NX_SUB;
        count[1]  = NY_SUB;
        dataspace.selectHyperslab(H5S_SELECT_SET, count, offset);

        /*
         * Define the memory dataspace.
         */
        hsize_t dimsm[3]; /* memory space dimensions */
        dimsm[0] = NX;
        dimsm[1] = NY;
        dimsm[2] = NZ;
        DataSpace memspace(RANK_OUT, dimsm);

        /*
         * Define memory hyperslab.
         */
        hsize_t offset_out[3]; // hyperslab offset in memory
        hsize_t count_out[3];  // size of the hyperslab in memory
        offset_out[0] = 3;
        offset_out[1] = 0;
        offset_out[2] = 0;
        count_out[0]  = NX_SUB;
        count_out[1]  = NY_SUB;
        count_out[2]  = 1;
        memspace.selectHyperslab(H5S_SELECT_SET, count_out, offset_out);

        /*
         * Read data from hyperslab in the file into the hyperslab in
         * memory and display the data.
         */
        dataset.read(data_out, PredType::NATIVE_INT, memspace, dataspace);

        for (j = 0; j < NX; j++) {
            for (i = 0; i < NY; i++)
                cout << data_out[j][i][0] << " ";
            cout << endl;
        }
        /*
         * 0 0 0 0 0 0 0
         * 0 0 0 0 0 0 0
         * 0 0 0 0 0 0 0
         * 3 4 5 6 0 0 0
         * 4 5 6 7 0 0 0
         * 5 6 7 8 0 0 0
         * 0 0 0 0 0 0 0
         */
    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataSpaceIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataTypeIException error) {
        error.printErrorStack();
        return -1;
    }

    return 0; // successfully terminated
}
```

### `HDF5Examples/CXX/H5D/writedata.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This program shows how the select_hyperslab and select_elements
 *  functions are used to write selected data from memory to the file.
 *  Program takes 48 elements from the linear buffer and writes them into
 *  the matrix using 3x2 blocks, (4,3) stride and (2,4) count.
 *  Then four elements  of the matrix are overwritten with the new values and
 *  file is closed. Program reopens the file and reads and displays the result.
 */

#include <iostream>
using std::cout;
using std::endl;

#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("Select.h5");
const H5std_string DATASET_NAME("Matrix in file");
const int          MSPACE1_RANK = 1;  // Rank of the first dataset in memory
const int          MSPACE1_DIM  = 50; // Dataset size in memory
const int          MSPACE2_RANK = 1;  // Rank of the second dataset in memory
const int          MSPACE2_DIM  = 4;  // Dataset size in memory
const int          FSPACE_RANK  = 2;  // Dataset rank as it is stored in the file
const int          FSPACE_DIM1  = 8;  // Dimension sizes of the dataset as it is
const int          FSPACE_DIM2  = 12; //      stored in the file
const int          MSPACE_RANK  = 2;  // Rank of the first dataset in memory
const int          MSPACE_DIM1  = 8;  // We will read dataset back from the file
const int          MSPACE_DIM2  = 9;  //      to the dataset in memory with these
                                      //      dataspace parameters
const int NPOINTS = 4;                // Number of points that will be selected
                                      //      and overwritten

int
main(void)
{
    int i, j; // loop indices */

    /*
     * Try block to detect exceptions raised by any of the calls inside it
     */
    try {
        /*
         * Turn off the auto-printing when failure occurs so that we can
         * handle the errors appropriately
         */
        Exception::dontPrint();

        /*
         * Create a file.
         */
        H5File *file = new H5File(FILE_NAME, H5F_ACC_TRUNC);

        /*
         * Create property list for a dataset and set up fill values.
         */
        int               fillvalue = 0; /* Fill value for the dataset */
        DSetCreatPropList plist;
        plist.setFillValue(PredType::NATIVE_INT, &fillvalue);

        /*
         * Create dataspace for the dataset in the file.
         */
        hsize_t   fdim[] = {FSPACE_DIM1, FSPACE_DIM2}; // dim sizes of ds (on disk)
        DataSpace fspace(FSPACE_RANK, fdim);

        /*
         * Create dataset and write it into the file.
         */
        DataSet *dataset =
            new DataSet(file->createDataSet(DATASET_NAME, PredType::NATIVE_INT, fspace, plist));

        /*
         * Select hyperslab for the dataset in the file, using 3x2 blocks,
         * (4,3) stride and (2,4) count starting at the position (0,1).
         */
        hsize_t start[2];  // Start of hyperslab
        hsize_t stride[2]; // Stride of hyperslab
        hsize_t count[2];  // Block count
        hsize_t block[2];  // Block sizes
        start[0]  = 0;
        start[1]  = 1;
        stride[0] = 4;
        stride[1] = 3;
        count[0]  = 2;
        count[1]  = 4;
        block[0]  = 3;
        block[1]  = 2;
        fspace.selectHyperslab(H5S_SELECT_SET, count, start, stride, block);

        /*
         * Create dataspace for the first dataset.
         */
        hsize_t dim1[] = {MSPACE1_DIM}; /* Dimension size of the first dataset
                                          (in memory) */
        DataSpace mspace1(MSPACE1_RANK, dim1);

        /*
         * Select hyperslab.
         * We will use 48 elements of the vector buffer starting at the
         * second element.  Selected elements are 1 2 3 . . . 48
         */
        start[0]  = 1;
        stride[0] = 1;
        count[0]  = 48;
        block[0]  = 1;
        mspace1.selectHyperslab(H5S_SELECT_SET, count, start, stride, block);

        /*
         * Write selection from the vector buffer to the dataset in the file.
         *
         * File dataset should look like this:
         *                    0  1  2  0  3  4  0  5  6  0  7  8
         *                    0  9 10  0 11 12  0 13 14  0 15 16
         *                    0 17 18  0 19 20  0 21 22  0 23 24
         *                    0  0  0  0  0  0  0  0  0  0  0  0
         *                    0 25 26  0 27 28  0 29 30  0 31 32
         *                    0 33 34  0 35 36  0 37 38  0 39 40
         *                    0 41 42  0 43 44  0 45 46  0 47 48
         *                    0  0  0  0  0  0  0  0  0  0  0  0
         */
        int vector[MSPACE1_DIM]; // vector buffer for dset

        /*
         * Buffer initialization.
         */
        vector[0] = vector[MSPACE1_DIM - 1] = -1;
        for (i = 1; i < MSPACE1_DIM - 1; i++)
            vector[i] = i;

        dataset->write(vector, PredType::NATIVE_INT, mspace1, fspace);

        /*
         * Reset the selection for the file dataspace fid.
         */
        fspace.selectNone();

        /*
         * Create dataspace for the second dataset.
         */
        hsize_t dim2[] = {MSPACE2_DIM}; /* Dimension size of the second dataset
                                          (in memory */
        DataSpace mspace2(MSPACE2_RANK, dim2);

        /*
         * Select sequence of NPOINTS points in the file dataspace.
         */
        hsize_t coord[NPOINTS][FSPACE_RANK]; /* Array to store selected points
                                                from the file dataspace */
        coord[0][0] = 0;
        coord[0][1] = 0;
        coord[1][0] = 3;
        coord[1][1] = 3;
        coord[2][0] = 3;
        coord[2][1] = 5;
        coord[3][0] = 5;
        coord[3][1] = 6;

        fspace.selectElements(H5S_SELECT_SET, NPOINTS, (const hsize_t *)coord);

        /*
         * Write new selection of points to the dataset.
         */
        int values[] = {53, 59, 61, 67}; /* New values to be written */
        dataset->write(values, PredType::NATIVE_INT, mspace2, fspace);

        /*
         * File dataset should look like this:
         *                   53  1  2  0  3  4  0  5  6  0  7  8
         *                    0  9 10  0 11 12  0 13 14  0 15 16
         *                    0 17 18  0 19 20  0 21 22  0 23 24
         *                    0  0  0 59  0 61  0  0  0  0  0  0
         *                    0 25 26  0 27 28  0 29 30  0 31 32
         *                    0 33 34  0 35 36 67 37 38  0 39 40
         *                    0 41 42  0 43 44  0 45 46  0 47 48
         *                    0  0  0  0  0  0  0  0  0  0  0  0
         *
         */

        /*
         * Close the dataset and the file.
         */
        delete dataset;
        delete file;

        /*
         * Open the file.
         */
        file = new H5File(FILE_NAME, H5F_ACC_RDONLY);

        /*
         * Open the dataset.
         */
        dataset = new DataSet(file->openDataSet(DATASET_NAME));

        /*
         * Get dataspace of the dataset.
         */
        fspace = dataset->getSpace();

        /*
         * Select first hyperslab for the dataset in the file. The following
         * elements are selected:
         *                     10  0 11 12
         *                     18  0 19 20
         *                      0 59  0 61
         *
         */
        start[0]  = 1;
        start[1]  = 2;
        block[0]  = 1;
        block[1]  = 1;
        stride[0] = 1;
        stride[1] = 1;
        count[0]  = 3;
        count[1]  = 4;
        fspace.selectHyperslab(H5S_SELECT_SET, count, start, stride, block);

        /*
         * Add second selected hyperslab to the selection.
         * The following elements are selected:
         *                    19 20  0 21 22
         *                     0 61  0  0  0
         *                    27 28  0 29 30
         *                    35 36 67 37 38
         *                    43 44  0 45 46
         *                     0  0  0  0  0
         * Note that two hyperslabs overlap. Common elements are:
         *                                              19 20
         *                                               0 61
         */
        start[0]  = 2;
        start[1]  = 4;
        block[0]  = 1;
        block[1]  = 1;
        stride[0] = 1;
        stride[1] = 1;
        count[0]  = 6;
        count[1]  = 5;
        fspace.selectHyperslab(H5S_SELECT_OR, count, start, stride, block);

        /*
         * Create memory dataspace.
         */
        hsize_t mdim[] = {MSPACE_DIM1, MSPACE_DIM2}; /* Dimension sizes of the
                                                   dataset in memory when we
                                                   read selection from the
                                                   dataset on the disk */
        DataSpace mspace(MSPACE_RANK, mdim);

        /*
         * Select two hyperslabs in memory. Hyperslabs has the same
         * size and shape as the selected hyperslabs for the file dataspace.
         */
        start[0]  = 0;
        start[1]  = 0;
        block[0]  = 1;
        block[1]  = 1;
        stride[0] = 1;
        stride[1] = 1;
        count[0]  = 3;
        count[1]  = 4;
        mspace.selectHyperslab(H5S_SELECT_SET, count, start, stride, block);
        start[0]  = 1;
        start[1]  = 2;
        block[0]  = 1;
        block[1]  = 1;
        stride[0] = 1;
        stride[1] = 1;
        count[0]  = 6;
        count[1]  = 5;
        mspace.selectHyperslab(H5S_SELECT_OR, count, start, stride, block);

        /*
         * Initialize data buffer.
         */
        int matrix_out[MSPACE_DIM1][MSPACE_DIM2];
        for (i = 0; i < MSPACE_DIM1; i++)
            for (j = 0; j < MSPACE_DIM2; j++)
                matrix_out[i][j] = 0;

        /*
         * Read data back to the buffer matrix.
         */
        dataset->read(matrix_out, PredType::NATIVE_INT, mspace, fspace);

        /*
         * Display the result.  Memory dataset is:
         *
         *                    10  0 11 12  0  0  0  0  0
         *                    18  0 19 20  0 21 22  0  0
         *                     0 59  0 61  0  0  0  0  0
         *                     0  0 27 28  0 29 30  0  0
         *                     0  0 35 36 67 37 38  0  0
         *                     0  0 43 44  0 45 46  0  0
         *                     0  0  0  0  0  0  0  0  0
         *                     0  0  0  0  0  0  0  0  0
         */
        for (i = 0; i < MSPACE_DIM1; i++) {
            for (j = 0; j < MSPACE_DIM2; j++)
                cout << matrix_out[i][j] << "  ";
            cout << endl;
        }

        /*
         * Close the dataset and the file.
         */
        delete dataset;
        delete file;
    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataSpaceIException error) {
        error.printErrorStack();
        return -1;
    }

    return 0;
}
```

### `HDF5Examples/CXX/HL/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_CXX_HL CXX)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (C_sourcefiles.cmake)

if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8")
  foreach (example_name ${common_examples})
    add_executable (${EXAMPLE_VARNAME}_cpp_ex_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.cpp)
    target_compile_options (${EXAMPLE_VARNAME}_cpp_ex_${example_name}
        PRIVATE
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
    )
    if (H5_HAVE_PARALLEL)
      target_include_directories (${EXAMPLE_VARNAME}_cpp_ex_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
    endif ()
    target_link_libraries (${EXAMPLE_VARNAME}_cpp_ex_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
  endforeach ()
endif ()

if (H5EXAMPLE_BUILD_TESTING)
  set (${EXAMPLE_VARNAME}_cpp_ex_CLEANFILES
      packet_table.h5
  )
  add_test (
      NAME ${EXAMPLE_VARNAME}_cpp_ex-clear-objects
      COMMAND ${CMAKE_COMMAND} -E remove ${${EXAMPLE_VARNAME}_cpp_ex_CLEANFILES}
  )
  set_tests_properties (${EXAMPLE_VARNAME}_cpp_ex-clear-objects PROPERTIES
      FIXTURES_SETUP clear_${EXAMPLE_VARNAME}_cpp_ex
      WORKING_DIRECTORY ${PROJECT_BINARY_DIR}
  )
  add_test (
      NAME ${EXAMPLE_VARNAME}_cpp_ex-clean-objects
      COMMAND ${CMAKE_COMMAND} -E remove ${${EXAMPLE_VARNAME}_cpp_ex_CLEANFILES}
  )
  set_tests_properties (${EXAMPLE_VARNAME}_cpp_ex-clean-objects PROPERTIES
      FIXTURES_CLEANUP clear_${EXAMPLE_VARNAME}_cpp_ex
      WORKING_DIRECTORY ${PROJECT_BINARY_DIR}
  )

  macro (ADD_H5_TEST testname)
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_cpp_ex_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_cpp_ex_${testname}>)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_cpp_ex_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_cpp_ex_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_SKIP_COMPARE=TRUE"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
    endif ()
    set_tests_properties (${EXAMPLE_VARNAME}_cpp_ex_${testname} PROPERTIES
        FIXTURES_REQUIRED clear_${EXAMPLE_VARNAME}_cpp_ex
        ENVIRONMENT "${CROSSCOMPILING_PATH}"
    )
    if (last_test)
      set_tests_properties (${EXAMPLE_VARNAME}_cpp_ex_${testname} PROPERTIES DEPENDS ${last_test})
    endif ()
    set (last_test "${EXAMPLE_VARNAME}_cpp_ex_${testname}")
  endmacro ()

  foreach (example_name ${common_examples})
    ADD_H5_TEST (${example_name})
  endforeach ()
endif ()
```

### `HDF5Examples/CXX/HL/C_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (common_examples
    packet_table_FL
)
```

### `HDF5Examples/CXX/HL/packet_table_FL.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

#include "H5PacketTable.h"

/*-------------------------------------------------------------------------
 * Packet Table Fixed-Length Example
 *
 * Example program that creates a packet table and performs
 * writes and reads.
 *
 *-------------------------------------------------------------------------
 */

const char *FILE_NAME("PTcppexampleFL.h5");
const char *PT_NAME("/examplePacketTable");

int
main(void)
{
    herr_t  err;     /* Return value from function calls */
    hid_t   fileID;  /* HDF5 identifier for file */
    hid_t   plistID; /* HDF5 identifier for property list to use compression */
    hsize_t count;   /* Number of records in table */
    int     x;       /* Temporary counter variable */

    /* Buffers to hold data */
    int readBuffer[5];
    int writeBuffer[5];

    /* Initialize buffers */
    for (x = 0; x < 5; x++) {
        writeBuffer[x] = x;
        readBuffer[x]  = -1;
    }

    /* Create a new HDF5 file */
    fileID = H5Fcreate(FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    if (fileID < 0)
        fprintf(stderr, "Couldn't create file.\n");

    /* Prepare property list to set compression, randomly use deflate
       with compression level 5. */
    plistID = H5Pcreate(H5P_DATASET_CREATE);
    err     = H5Pset_deflate(plistID, 5);
    if (err < 0)
        fprintf(stderr, "Error setting compression level.");

    /* Create a fixed-length packet table. */
    FL_PacketTable ptable(fileID, plistID, PT_NAME, H5T_NATIVE_INT, 100);
    if (!ptable.IsValid())
        fprintf(stderr, "Unable to create packet table.");

    /* Append five packets to the packet table, one at a time */
    for (x = 0; x < 5; x++) {
        err = ptable.AppendPacket(&(writeBuffer[x]));
        if (err < 0)
            fprintf(stderr, "Error adding record.");
    }

    /* Get the number of packets in the packet table.  This should be five. */
    count = ptable.GetPacketCount(err);
    if (err < 0)
        fprintf(stderr, "Error getting packet count.");

    printf("Number of packets in packet table after five appends: %" PRIuHSIZE "\n", count);

    /* Initialize packet table's "current record" */
    ptable.ResetIndex();

    /* Iterate through packets, read each one back */
    for (x = 0; x < 5; x++) {
        err = ptable.GetNextPacket(&(readBuffer[x]));
        if (err < 0)
            fprintf(stderr, "Error reading record.");

        printf("Packet %d's value is %d.\n", x, readBuffer[x]);
    }

    /* The packet table will close automatically when its object goes */
    /* out of scope.  */

    err = H5Fclose(fileID);
    if (err < 0)
        fprintf(stderr, "Failed to close file.\n");

    return 0;
}
```

### `HDF5Examples/CXX/TUTR/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_CXX_TUTR CXX)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (C_sourcefiles.cmake)

if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.8")
  foreach (example_name ${examples})
    add_executable (${EXAMPLE_VARNAME}_cpp_ex_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.cpp)
    target_compile_options (${EXAMPLE_VARNAME}_cpp_ex_${example_name}
        PRIVATE
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
    )
    if (H5_HAVE_PARALLEL)
      target_include_directories (${EXAMPLE_VARNAME}_cpp_ex_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
    endif ()
    target_link_libraries (${EXAMPLE_VARNAME}_cpp_ex_${example_name} ${H5EXAMPLE_HDF5_LINK_LIBS})
  endforeach ()
endif ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_cpp_ex_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
    )
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_cpp_ex_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_cpp_ex_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_cpp_ex_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_cpp_ex_${testname}-clearall)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_cpp_ex_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_cpp_ex_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_SKIP_COMPARE=TRUE"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_cpp_ex_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_cpp_ex_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
    endif ()
    if (last_test)
      set_tests_properties (${EXAMPLE_VARNAME}_cpp_ex_${testname} PROPERTIES DEPENDS ${last_test})
    endif ()
    set (last_test "${EXAMPLE_VARNAME}_cpp_ex_${testname}")
  endmacro ()

  foreach (example_name ${examples})
    ADD_H5_TEST (${example_name})
  endforeach ()
endif ()
```

### `HDF5Examples/CXX/TUTR/C_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (examples
    h5tutr_cmprss
    h5tutr_crtdat
    h5tutr_crtatt
    h5tutr_crtgrpar
    h5tutr_crtgrp
    h5tutr_crtgrpd
    h5tutr_extend
    h5tutr_rdwt
    h5tutr_subset
)
```

### `HDF5Examples/CXX/TUTR/h5tutr_cmprss.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to create a compressed dataset.
 *  It is used in the HDF5 Tutorial.
 */

#include <iostream>
using std::cout;
using std::endl;

#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("h5tutr_cmprss.h5");
const H5std_string DATASET_NAME("Compressed_Data");
const int          DIM0 = 100;
const int          DIM1 = 20;

int
main(void)
{
    hsize_t dims[2]       = {DIM0, DIM1}; // dataset dimensions
    hsize_t chunk_dims[2] = {20, 20};     // chunk dimensions
    int     i, j, buf[DIM0][DIM1];

    // Try block to detect exceptions raised by any of the calls inside it
    try {
        // Turn off the auto-printing when failure occurs so that we can
        // handle the errors appropriately
        Exception::dontPrint();

        // Create a new file using the default property lists.
        H5File file(FILE_NAME, H5F_ACC_TRUNC);

        // Create the data space for the dataset.
        DataSpace *dataspace = new DataSpace(2, dims);

        // Modify dataset creation property to enable chunking
        DSetCreatPropList *plist = new DSetCreatPropList;
        plist->setChunk(2, chunk_dims);

        // Set ZLIB (DEFLATE) Compression using level 6.
        // To use SZIP compression comment out this line.
        plist->setDeflate(6);

        // Uncomment these lines to set SZIP Compression
        // unsigned szip_options_mask = H5_SZIP_NN_OPTION_MASK;
        // unsigned szip_pixels_per_block = 16;
        // plist->setSzip(szip_options_mask, szip_pixels_per_block);

        // Create the dataset.
        DataSet *dataset =
            new DataSet(file.createDataSet(DATASET_NAME, PredType::STD_I32BE, *dataspace, *plist));

        for (i = 0; i < DIM0; i++)
            for (j = 0; j < DIM1; j++)
                buf[i][j] = i + j;

        // Write data to dataset.
        dataset->write(buf, PredType::NATIVE_INT);

        // Close objects and file.  Either approach will close the HDF5 item.
        delete dataspace;
        delete dataset;
        delete plist;
        file.close();

        // -----------------------------------------------
        // Re-open the file and dataset, retrieve filter
        // information for dataset and read the data back.
        // -----------------------------------------------

        int          rbuf[DIM0][DIM1];
        int          numfilt;
        size_t       nelmts = {1}, namelen = {1};
        unsigned     flags, filter_info, cd_values[1], idx;
        char         name[1];
        H5Z_filter_t filter_type;

        // Open the file and the dataset in the file.
        file.openFile(FILE_NAME, H5F_ACC_RDONLY);
        dataset = new DataSet(file.openDataSet(DATASET_NAME));

        // Get the create property list of the dataset.
        plist = new DSetCreatPropList(dataset->getCreatePlist());

        // Get the number of filters associated with the dataset.
        numfilt = plist->getNfilters();
        cout << "Number of filters associated with dataset: " << numfilt << endl;

        for (idx = 0; idx < numfilt; idx++) {
            nelmts = 0;

            filter_type = plist->getFilter(idx, flags, nelmts, cd_values, namelen, name, filter_info);

            cout << "Filter Type: ";

            switch (filter_type) {
                case H5Z_FILTER_DEFLATE:
                    cout << "H5Z_FILTER_DEFLATE" << endl;
                    break;
                case H5Z_FILTER_SZIP:
                    cout << "H5Z_FILTER_SZIP" << endl;
                    break;
                default:
                    cout << "Other filter type included." << endl;
            }
        }

        // Read data.
        dataset->read(rbuf, PredType::NATIVE_INT);

        delete plist;
        delete dataset;
        file.close(); // can be skipped

    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataSpaceIException error) {
        error.printErrorStack();
        return -1;
    }

    return 0; // successfully terminated
}
```

### `HDF5Examples/CXX/TUTR/h5tutr_crtatt.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to create an attribute attached to a
 *  dataset. It is used in the HDF5 Tutorial.
 */

#include <iostream>
#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("h5tutr_dset.h5");
const H5std_string DATASET_NAME("dset");
const H5std_string ATTR_NAME("Units");

const int DIM1 = 2;

int
main(void)
{
    int     attr_data[2] = {100, 200};
    hsize_t dims[1]      = {DIM1};

    // Try block to detect exceptions raised by any of the calls inside it
    try {
        // Turn off the auto-printing when failure occurs so that we can
        // handle the errors appropriately
        Exception::dontPrint();

        // Open an existing file and dataset.
        H5File  file(FILE_NAME, H5F_ACC_RDWR);
        DataSet dataset = file.openDataSet(DATASET_NAME);

        // Create the data space for the attribute.
        DataSpace attr_dataspace = DataSpace(1, dims);

        // Create a dataset attribute.
        Attribute attribute = dataset.createAttribute(ATTR_NAME, PredType::STD_I32BE, attr_dataspace);

        // Write the attribute data.
        attribute.write(PredType::NATIVE_INT, attr_data);

    } // end of try block

    // catch failure caused by the H5File operations
    catch (DataSpaceIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the H5File operations
    catch (AttributeIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    return 0; // successfully terminated
}
```

### `HDF5Examples/CXX/TUTR/h5tutr_crtdat.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to create a dataset that is a 4 x 6
 *  array. It is used in the HDF5 Tutorial.
 */

#include <iostream>
#include <string>

#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("h5tutr_dset.h5");
const H5std_string DATASET_NAME("dset");
const int          NX   = 4; // dataset dimensions
const int          NY   = 6;
const int          RANK = 2;

int
main(void)
{
    // Try block to detect exceptions raised by any of the calls inside it
    try {
        // Turn off the auto-printing when failure occurs so that we can
        // handle the errors appropriately
        Exception::dontPrint();

        // Create a new file using the default property lists.
        H5File file(FILE_NAME, H5F_ACC_TRUNC);

        // Create the data space for the dataset.
        hsize_t dims[2]; // dataset dimensions
        dims[0] = NX;
        dims[1] = NY;
        DataSpace dataspace(RANK, dims);

        // Create the dataset.
        DataSet dataset = file.createDataSet(DATASET_NAME, PredType::STD_I32BE, dataspace);

    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataSpaceIException error) {
        error.printErrorStack();
        return -1;
    }

    return 0; // successfully terminated
}
```

### `HDF5Examples/CXX/TUTR/h5tutr_crtgrp.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to create and close a group.
 *  It is used in the HDF5 Tutorial.
 */

#include <iostream>
using std::cout;
using std::endl;

#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("h5tutr_group.h5");

int
main(void)
{
    // Try block to detect exceptions raised by any of the calls inside it
    try {
        // Turn off the auto-printing when failure occurs so that we can
        // handle the errors appropriately
        Exception::dontPrint();

        // Create a new file using default property lists.
        H5File file(FILE_NAME, H5F_ACC_TRUNC);

        // Create a group named "/MygGroup" in the file
        Group group(file.createGroup("/MyGroup"));

        // File and group will be closed as their instances go out of scope.

    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }
    // catch failure caused by the Group operations
    catch (GroupIException error) {
        error.printErrorStack();
        return -1;
    }

    return 0;
}
```

### `HDF5Examples/CXX/TUTR/h5tutr_crtgrpar.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates the creation of groups using absolute and
 *  relative names. It is used in the HDF5 Tutorial.
 */

#include <iostream>
using std::cout;
using std::endl;

#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("h5tutr_groups.h5");

int
main(void)
{

    // Try block to detect exceptions raised by any of the calls inside it
    try {

        // Turn off the auto-printing when failure occurs so that we can
        // handle the errors appropriately.

        Exception::dontPrint();

        // Create a new file using default properties.

        H5File file(FILE_NAME, H5F_ACC_TRUNC);

        // Create group "MyGroup" in the root group using an absolute name.

        Group group1(file.createGroup("/MyGroup"));

        // Create group "Group_A" in group "MyGroup" using an
        // absolute name.

        Group group2(file.createGroup("/MyGroup/Group_A"));

        // Create group "Group_B" in group "MyGroup" using a
        // relative name.

        Group group3(group1.createGroup("Group_B"));

        // Close the groups and file.

        group1.close();
        group2.close();
        group3.close();
        file.close();

    } // end of try block

    // catch failure caused by the File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the Group operations
    catch (GroupIException error) {
        error.printErrorStack();
        return -1;
    }

    return 0;
}
```

### `HDF5Examples/CXX/TUTR/h5tutr_crtgrpd.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to create a dataset in a group.
 *  It is used in the HDF5 Tutorial.
 */

#include <iostream>
using std::cout;
using std::endl;

#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("h5tutr_groups.h5");
const H5std_string DATASET_NAME1("/MyGroup/dset1");
const H5std_string DATASET_NAME2("dset2");
const int          RANK   = 2;
const int          D1DIM1 = 3;
const int          D1DIM2 = 3;
const int          D2DIM1 = 2;
const int          D2DIM2 = 10;

int
main(void)
{
    int dset1_data[D1DIM1][D1DIM2], dset2_data[D2DIM1][D2DIM2]; // data buffers
    int i, j;

    // Try block to catch exceptions raised by any of the calls inside it
    try {
        // Turn off the auto-printing when failure occurs so that we can
        // handle the errors appropriately
        Exception::dontPrint();

        // Initialize the first dataset.
        for (i = 0; i < D1DIM1; i++)
            for (j = 0; j < D1DIM2; j++)
                dset1_data[i][j] = j + 1;

        //  Initialize the second dataset.
        for (i = 0; i < D2DIM1; i++)
            for (j = 0; j < D2DIM2; j++)
                dset2_data[i][j] = j + 1;

        // Open an existing file and dataset.
        H5File file(FILE_NAME, H5F_ACC_RDWR);

        // Create the data space for the first dataset.  Note the use of
        // pointer for the instance 'dataspace'.  It can be deleted and
        // used again later for another data space.  An HDF5 identifier is
        // closed by the destructor or the method 'close()'.
        hsize_t dims[RANK]; // dataset dimensions
        dims[0]              = D1DIM1;
        dims[1]              = D1DIM2;
        DataSpace *dataspace = new DataSpace(RANK, dims);

        // Create the dataset in group "MyGroup".  Same note as for the
        // dataspace above.
        DataSet *dataset = new DataSet(file.createDataSet(DATASET_NAME1, PredType::STD_I32BE, *dataspace));

        // Write the data to the dataset using default memory space, file
        // space, and transfer properties.
        dataset->write(dset1_data, PredType::NATIVE_INT);

        // Close the current dataset and data space.
        delete dataset;
        delete dataspace;

        // Create the data space for the second dataset.
        dims[0]   = D2DIM1;
        dims[1]   = D2DIM2;
        dataspace = new DataSpace(RANK, dims);

        // Create group "Group_A" in group "MyGroup".
        Group group(file.openGroup("/MyGroup/Group_A"));

        // Create the second dataset in group "Group_A".
        dataset = new DataSet(group.createDataSet(DATASET_NAME2, PredType::STD_I32BE, *dataspace));

        // Write the data to the dataset using default memory space, file
        // space, and transfer properties.
        dataset->write(dset2_data, PredType::NATIVE_INT);

        // Close all objects.
        delete dataspace;
        delete dataset;
        group.close();

    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }
    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataSpaceIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the Group operations
    catch (GroupIException error) {
        error.printErrorStack();
        return -1;
    }

    return 0;
}
```

### `HDF5Examples/CXX/TUTR/h5tutr_extend.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to create a dataset that is a 4 x 6
 *  array. It is used in the HDF5 Tutorial.
 */

#include <iostream>
using std::cout;
using std::endl;

#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("h5tutr_extend.h5");
const H5std_string DATASETNAME("ExtendibleArray");

int
main(void)
{
    hsize_t dims[2]       = {3, 3}; // dataset dimensions at creation
    hsize_t maxdims[2]    = {H5S_UNLIMITED, H5S_UNLIMITED};
    hsize_t chunk_dims[2] = {2, 5};
    int     data[3][3]    = {{1, 1, 1}, // data to write
                             {1, 1, 1},
                             {1, 1, 1}};

    // Variables used in extending and writing to the extended portion of dataset

    hsize_t size[2];
    hsize_t offset[2];
    hsize_t dimsext[2]    = {7, 3}; // extend dimensions
    int     dataext[7][3] = {{2, 3, 4}, {2, 3, 4}, {2, 3, 4}, {2, 3, 4}, {2, 3, 4}, {2, 3, 4}, {2, 3, 4}};

    // Try block to detect exceptions raised by any of the calls inside it
    try {
        // Turn off the auto-printing when failure occurs so that we can
        // handle the errors appropriately
        Exception::dontPrint();

        // Create a new file using the default property lists.
        H5File file(FILE_NAME, H5F_ACC_TRUNC);

        // Create the data space for the dataset.  Note the use of pointer
        // for the instance 'dataspace'.  It can be deleted and used again
        // later for another dataspace.  An HDF5 identifier can be closed
        // by the destructor or the method 'close()'.
        DataSpace *dataspace = new DataSpace(2, dims, maxdims);

        // Modify dataset creation property to enable chunking
        DSetCreatPropList prop;
        prop.setChunk(2, chunk_dims);

        // Create the chunked dataset.  Note the use of pointer.
        DataSet *dataset =
            new DataSet(file.createDataSet(DATASETNAME, PredType::STD_I32BE, *dataspace, prop));

        // Write data to dataset.
        dataset->write(data, PredType::NATIVE_INT);

        // Extend the dataset. Dataset becomes 10 x 3.
        size[0] = dims[0] + dimsext[0];
        size[1] = dims[1];
        dataset->extend(size);

        // Select a hyperslab in extended portion of the dataset.
        DataSpace *filespace = new DataSpace(dataset->getSpace());
        offset[0]            = 3;
        offset[1]            = 0;
        filespace->selectHyperslab(H5S_SELECT_SET, dimsext, offset);

        // Define memory space.
        DataSpace *memspace = new DataSpace(2, dimsext, NULL);

        // Write data to the extended portion of the dataset.
        dataset->write(dataext, PredType::NATIVE_INT, *memspace, *filespace);

        // Close all objects and file.
        prop.close();
        delete filespace;
        delete memspace;
        delete dataspace;
        delete dataset;
        file.close();

        // ---------------------------------------
        // Re-open the file and read the data back
        // ---------------------------------------

        int     rdata[10][3];
        int     i, j, rank;
        hsize_t chunk_dimsr[2], dimsr[2];

        // Open the file and dataset.
        file.openFile(FILE_NAME, H5F_ACC_RDONLY);
        dataset = new DataSet(file.openDataSet(DATASETNAME));

        // Get the dataset's dataspace and creation property list.
        filespace = new DataSpace(dataset->getSpace());
        prop      = dataset->getCreatePlist();

        // Get information to obtain memory dataspace.
        rank = filespace->getSimpleExtentNdims();
        (void)filespace->getSimpleExtentDims(dimsr);

        if (H5D_CHUNKED == prop.getLayout()) {
            int rank_chunk = prop.getChunk(rank, chunk_dimsr);
            cout << "rank chunk = " << rank_chunk << endl;
        }

        memspace = new DataSpace(rank, dimsr, NULL);
        dataset->read(rdata, PredType::NATIVE_INT, *memspace, *filespace);

        cout << endl;
        for (j = 0; j < dimsr[0]; j++) {
            for (i = 0; i < dimsr[1]; i++)
                cout << " " << rdata[j][i];
            cout << endl;
        }

        // Close all objects and file.
        prop.close();
        delete filespace;
        delete memspace;
        delete dataset;
        file.close();

    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataSpaceIException error) {
        error.printErrorStack();
        return -1;
    }

    return 0; // successfully terminated
}
```

### `HDF5Examples/CXX/TUTR/h5tutr_rdwt.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to write to and read from an existing
 *  dataset. It is used in the HDF5 Tutorial.
 */

#include <iostream>
#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("h5tutr_dset.h5");
const H5std_string DATASET_NAME("dset");
const int          DIM0 = 4; // dataset dimensions
const int          DIM1 = 6;

int
main(void)
{

    // Data initialization.

    int i, j;
    int data[DIM0][DIM1]; // buffer for data to write

    for (j = 0; j < DIM0; j++)
        for (i = 0; i < DIM1; i++)
            data[j][i] = i * 6 + j + 1;

    // Try block to detect exceptions raised by any of the calls inside it
    try {
        // Turn off the auto-printing when failure occurs so that we can
        // handle the errors appropriately
        Exception::dontPrint();

        // Open an existing file and dataset.
        H5File  file(FILE_NAME, H5F_ACC_RDWR);
        DataSet dataset = file.openDataSet(DATASET_NAME);

        // Write the data to the dataset using default memory space, file
        // space, and transfer properties.
        dataset.write(data, PredType::NATIVE_INT);

    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    return 0; // successfully terminated
}
```

### `HDF5Examples/CXX/TUTR/h5tutr_subset.cpp`

```cpp
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/*
 *  This example illustrates how to read/write a subset of data (a slab)
 *  from/to a dataset in an HDF5 file. It is used in the HDF5 Tutorial.
 */

#include <iostream>
using std::cout;
using std::endl;

#include <string>
#include "H5Cpp.h"
using namespace H5;

const H5std_string FILE_NAME("h5tutr_subset.h5");
const H5std_string DATASET_NAME("IntArray");

const int RANK     = 2;
const int DIM0_SUB = 3; // subset dimensions
const int DIM1_SUB = 4;
const int DIM0     = 8; // size of dataset
const int DIM1     = 10;

int
main(void)
{
    int i, j;
    int data[DIM0][DIM1], sdata[DIM0_SUB][DIM1_SUB], rdata[DIM0][DIM1];

    // Try block to detect exceptions raised by any of the calls inside it
    try {
        // Turn off the auto-printing when failure occurs so that we can
        // handle the errors appropriately
        Exception::dontPrint();

        // ---------------------------------------------------
        // Create a new file using the default property lists.
        // Then create a dataset and write data to it.
        // Close the file and dataset.
        // ---------------------------------------------------

        H5File file(FILE_NAME, H5F_ACC_TRUNC);

        hsize_t dims[2];
        dims[0]             = DIM0;
        dims[1]             = DIM1;
        DataSpace dataspace = DataSpace(RANK, dims);

        DataSet dataset(file.createDataSet(DATASET_NAME, PredType::STD_I32BE, dataspace));

        for (j = 0; j < DIM0; j++) {
            for (i = 0; i < DIM1; i++)
                if (i < (DIM1 / 2))
                    data[j][i] = 1;
                else
                    data[j][i] = 2;
        }

        dataset.write(data, PredType::NATIVE_INT);

        cout << endl << "Data Written to File:" << endl;
        for (j = 0; j < DIM0; j++) {
            for (i = 0; i < DIM1; i++)
                cout << " " << data[j][i];
            cout << endl;
        }

        dataspace.close();
        dataset.close();
        file.close();

        // ---------------------------------------------------
        // Reopen the file and dataset and write a subset of
        // values to the dataset.
        // ---------------------------------------------------

        hsize_t offset[2], count[2], stride[2], block[2];
        hsize_t dimsm[2];

        file.openFile(FILE_NAME, H5F_ACC_RDWR);
        dataset = file.openDataSet(DATASET_NAME);

        // Specify size and shape of subset to write.

        offset[0] = 1;
        offset[1] = 2;

        count[0] = DIM0_SUB;
        count[1] = DIM1_SUB;

        stride[0] = 1;
        stride[1] = 1;

        block[0] = 1;
        block[1] = 1;

        // Define Memory Dataspace. Get file dataspace and select
        // a subset from the file dataspace.

        dimsm[0] = DIM0_SUB;
        dimsm[1] = DIM1_SUB;

        DataSpace memspace(RANK, dimsm, NULL);

        dataspace = dataset.getSpace();
        dataspace.selectHyperslab(H5S_SELECT_SET, count, offset, stride, block);

        // Write a subset of data to the dataset, then read the
        // entire dataset back from the file.

        cout << endl << "Write subset to file specifying: " << endl;
        cout << "  offset=1x2 stride=1x1 count=3x4 block=1x1" << endl;
        for (j = 0; j < DIM0_SUB; j++) {
            for (i = 0; i < DIM1_SUB; i++)
                sdata[j][i] = 5;
        }

        dataset.write(sdata, PredType::NATIVE_INT, memspace, dataspace);
        dataset.read(rdata, PredType::NATIVE_INT);

        cout << endl << "Data in File after Subset is Written:" << endl;
        for (i = 0; i < DIM0; i++) {
            for (j = 0; j < DIM1; j++)
                cout << " " << rdata[i][j];
            cout << endl;
        }
        cout << endl;

        // It is not necessary to close these objects because close() will
        // be called when the object instances are going out of scope.
        dataspace.close();
        memspace.close();
        dataset.close();
        file.close();

    } // end of try block

    // catch failure caused by the H5File operations
    catch (FileIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSet operations
    catch (DataSetIException error) {
        error.printErrorStack();
        return -1;
    }

    // catch failure caused by the DataSpace operations
    catch (DataSpaceIException error) {
        error.printErrorStack();
        return -1;
    }

    return 0; // successfully terminated
}
```

### `HDF5Examples/CXX/test-pc.sh`

```bash
#! /bin/sh
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.

# This file is for use of h5cc created with the CMake process.
# Environment variable, HDF5_HOME is expected to be set.
# $1 is the path name of the source directory.
# $2 is the path name of the build directory.
# $3 is the current path name.

top_srcdir=$1
top_builddir=$2
currentpath=$3
verbose=yes
nerrors=0

return_val=0

echo "Current build directory: $top_builddir$currentpath"
# Loop through all subdirectories
for dir in */; do
  if [ -d "$dir" ] && [ ! -L "$dir" ]; then
    # Check if test-pc.sh exists
    if [ -f "$dir/test-pc.sh" ];
    then
        echo "Entering directory: $dir"
        (
            mkdir -p "$top_builddir/$currentpath/$dir"
            cd "$dir"
            ./test-pc.sh $top_srcdir $top_builddir $currentpath/$dir # Execute script in the subdirectory
            status=$?
            exit $status
        )
        return_val=$?
        if test $return_val -ne 0
        then
          echo "Exiting directory: $dir with $return_val tests FAILED"
        else
          echo "Exiting directory: $dir Passed"
        fi
        nerrors=`expr $return_val + $nerrors`
    fi
  fi
done

echo "$nerrors tests failed in $currentpath"
exit $nerrors
```

### `HDF5Examples/FORTRAN/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
PROJECT (HDF5Examples_F90 Fortran)

#-----------------------------------------------------------------------------
# Build the Fortran Examples
#-----------------------------------------------------------------------------
add_subdirectory (${PROJECT_SOURCE_DIR}/TUTR)
add_subdirectory (${PROJECT_SOURCE_DIR}/H5D)
add_subdirectory (${PROJECT_SOURCE_DIR}/H5G)
add_subdirectory (${PROJECT_SOURCE_DIR}/H5T)

if (H5_HAVE_PARALLEL AND HDF5_PROVIDES_PARALLEL)
  add_subdirectory (${PROJECT_SOURCE_DIR}/H5PAR)
endif ()

#-- Add High Level Examples
if (H5EXAMPLE_BUILD_HL AND HDF5_PROVIDES_HL_LIB)
  add_subdirectory (HL)
endif ()
```

### `HDF5Examples/FORTRAN/H5D/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_FORTRAN_H5D Fortran)

# --------------------------------------------------------------------
# Notes: When creating examples they should be prefixed
# with "f90_". This allows for easier filtering of the examples.
# --------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Setup include Directories
#-----------------------------------------------------------------------------
set_directory_properties (PROPERTIES INCLUDE_DIRECTORIES
    "${CMAKE_Fortran_MODULE_DIRECTORY}${H5EXAMPLE_MOD_EXT};${HDF5_F90_BINARY_DIR};${PROJECT_BINARY_DIR};${CMAKE_LIBRARY_OUTPUT_DIRECTORY}"
)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (Fortran_sourcefiles.cmake)

foreach (example_name ${common_examples})
  add_executable (${EXAMPLE_VARNAME}_f90_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.F90)
  target_compile_options (${EXAMPLE_VARNAME}_f90_${example_name}
      PRIVATE
          "-DH5_LIBVER_DIR=${H5_LIBVER_DIR}"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  if (H5_HAVE_PARALLEL)
    target_include_directories (${EXAMPLE_VARNAME}_f90_${example_name} PUBLIC ${MPI_Fortran_INCLUDE_DIRS})
  endif ()
  target_link_libraries (${EXAMPLE_VARNAME}_f90_${example_name} ${H5EXAMPLE_LINK_Fortran_LIBS})
  set_target_properties (${EXAMPLE_VARNAME}_f90_${example_name} PROPERTIES LINKER_LANGUAGE Fortran)
  if (H5EXAMPLE_BUILD_TESTING)
    if (${example_name} STREQUAL "h5ex_d_alloc")
      if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
        add_custom_command (
            TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
            POST_BUILD
            COMMAND    ${CMAKE_COMMAND}
            ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
        )
      else ()
        add_custom_command (
            TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
            POST_BUILD
            COMMAND    ${CMAKE_COMMAND}
            ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
        )
      endif ()
    else ()
      add_custom_command (
          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
          POST_BUILD
          COMMAND    ${CMAKE_COMMAND}
          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
      )
    endif ()
  endif ()
endforeach ()

if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
  foreach (example_name ${1_10_examples})
    add_executable (${EXAMPLE_VARNAME}_f90_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.F90)
    target_compile_options(${EXAMPLE_VARNAME}_f90_${example_name}
        PRIVATE
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
    )
    if (H5_HAVE_PARALLEL)
      target_include_directories (${EXAMPLE_VARNAME}_f90_${example_name} PUBLIC ${MPI_Fortran_INCLUDE_DIRS})
    endif ()
    target_link_libraries (${EXAMPLE_VARNAME}_f90_${example_name} ${H5EXAMPLE_LINK_Fortran_LIBS})
    set_target_properties (${EXAMPLE_VARNAME}_f90_${example_name} PROPERTIES LINKER_LANGUAGE Fortran)
    if (H5EXAMPLE_BUILD_TESTING)
      add_custom_command (
          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
          POST_BUILD
          COMMAND    ${CMAKE_COMMAND}
          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
      )
    endif ()
  endforeach ()
endif ()


#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
#  foreach (example_name ${1_12_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
#  foreach (example_name ${1_14_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
#  foreach (example_name ${2_0_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()

if (HDF5_PROVIDES_TOOLS)
  foreach (example_name ${common_examples})
    if (${example_name} STREQUAL "h5ex_d_nbit")
      if (HDF5_VERSION_MAJOR VERSION_EQUAL "1.8" AND HDF5_VERSION_STRING VERSION_LESS_EQUAL "1.8.22")
        add_custom_command (
            TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
            POST_BUILD
            COMMAND    ${CMAKE_COMMAND}
            ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}22.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
        )
      elseif (HDF5_VERSION_MAJOR VERSION_EQUAL "1.10" AND HDF5_VERSION_STRING VERSION_LESS_EQUAL "1.10.7")
          add_custom_command (
              TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
              POST_BUILD
              COMMAND    ${CMAKE_COMMAND}
              ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}07.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
          )
      else ()
        add_custom_command (
            TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
            POST_BUILD
            COMMAND    ${CMAKE_COMMAND}
            ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
        )
      endif ()
    else ()
      add_custom_command (
          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
          POST_BUILD
          COMMAND    ${CMAKE_COMMAND}
          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      )
    endif ()
  endforeach ()

  if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
    foreach (example_name ${1_10_examples})
      if (${example_name} STREQUAL "h5ex_d_nbit")
        if (HDF5_VERSION_MAJOR VERSION_EQUAL "1.10" AND HDF5_VERSION_STRING VERSION_LESS_EQUAL "1.10.7")
          add_custom_command (
              TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
              POST_BUILD
              COMMAND    ${CMAKE_COMMAND}
              ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}07.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
          )
        else ()
          add_custom_command (
              TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
              POST_BUILD
              COMMAND    ${CMAKE_COMMAND}
              ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
          )
        endif ()
      elseif (${example_name} STREQUAL "h5ex_d_alloc")
        if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
          add_custom_command (
              TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
              POST_BUILD
              COMMAND    ${CMAKE_COMMAND}
              ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}07.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
          )
        endif ()
      else ()
        if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
          add_custom_command (
              TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
              POST_BUILD
              COMMAND    ${CMAKE_COMMAND}
              ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
          )
        endif ()
        #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
        #  add_custom_command (
        #      TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
        #      POST_BUILD
        #      COMMAND    ${CMAKE_COMMAND}
        #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
        #  )
        #endif ()
        #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
        #  add_custom_command (
        #      TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
        #      POST_BUILD
        #      COMMAND    ${CMAKE_COMMAND}
        #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
        #  )
        #endif ()
        #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
        #  add_custom_command (
        #      TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
        #      POST_BUILD
        #      COMMAND    ${CMAKE_COMMAND}
        #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
        #  )
        #endif ()
      endif ()
    endforeach ()

#    foreach (example_name ${1_12_examples})
#    endforeach ()
#    foreach (example_name ${1_14_examples})
#    endforeach ()
#    foreach (example_name ${2_0_examples})
#    endforeach ()
  endif ()
endif ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_f90_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
    )
    if (HDF5_USING_ANALYSIS_TOOL)
      add_test (NAME ${EXAMPLE_VARNAME}_f90_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_f90_${testname}-clearall)
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}-clean
          COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_f90_${testname})
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      if (last_test)
        set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES
            DEPENDS ${last_test}
        )
      endif ()
      set (last_test "${EXAMPLE_VARNAME}_f90_${testname}")
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${ARGN};${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-f90_${testname} PROPERTIES
            DEPENDS ${last_test}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
        set (last_test "${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}")
      endif ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}-clean
          COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES DEPENDS ${last_test})
    endif ()
  endmacro ()

  foreach (example_name ${common_examples} ${1_10_examples})
    if (${example_name} STREQUAL "h5ex_d_transform")
      ADD_H5_TEST (${example_name} -n)
    else ()
#      if (${example_name} STREQUAL "h5ex_d_alloc" AND ${H5_LIBVER_DIR} GREATER 112)
        ADD_H5_TEST (${example_name})
#      else ()
#        message (STATUS " Skipping f90_${example_name} test")
#      endif ()
    endif ()
  endforeach ()
endif ()
```

### `HDF5Examples/FORTRAN/H5D/Fortran_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (examples)

set (common_examples
    h5ex_d_alloc
    h5ex_d_checksum
    h5ex_d_chunk
    h5ex_d_compact
    h5ex_d_extern
    h5ex_d_fillval
    h5ex_d_hyper
    h5ex_d_rdwr
    h5ex_d_unlimmod
    h5ex_d_nbit
#    h5ex_d_sofloat
    h5ex_d_soint
    h5ex_d_transform
)

if (HDF5_PROVIDES_ZLIB_SUPPORT)
  set (common_examples ${common_examples}
      h5ex_d_gzip
  )
endif ()

if (HDF5_PROVIDES_SZIP_SUPPORT)
  set (common_examples ${common_examples}
      h5ex_d_szip
  )
endif ()

set (1_10_examples
)
```

### `HDF5Examples/FORTRAN/H5D/test-pc.sh`

```bash
#! /bin/sh
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.

# This file is for use of h5cc created with the CMake process.
# Environment variable, HDF5_HOME is expected to be set.
# $1 is the path name of the source directory.
# $2 is the path name of the build directory.
# $3 is the current path name.

top_srcdir=$1
top_builddir=$2
currentpath=$3
verbose=yes
nerrors=0

echo "Current build directory: $top_builddir/$currentpath"

# HDF5 compile commands, assuming they are in your $PATH.
H5FC=$HDF5_HOME/bin/h5fc
LD_LIBRARY_PATH=$HDF5_HOME/lib
export LD_LIBRARY_PATH

if ! test -f $H5FC; then
    echo "Set paths for H5FC and LD_LIBRARY_PATH in test.sh"
    echo "Set environment variable HDF5_HOME to the hdf5 install dir"
    echo "h5fc was not found at $H5FC"
    exit $EXIT_FAILURE
fi

H5DUMP=`echo $H5FC | sed -e 's/\/[^/]*$/\/h5dump/'`;
H5_LIBVER=$($H5FC -showconfig | grep -i "HDF5 Version:" | sed 's/^.* //g' | sed 's/[-].*//g')
H5_APIVER=$($H5FC -showconfig | grep -i "Default API mapping:" | sed 's/^.* //g' | sed 's/v//g' | sed 's/1/1_/')

H5_MAJORVER=$(echo $H5_LIBVER | cut -f1 -d'.'  | sed -E 's/\./_/g')
H5_MINORVER=$(echo $H5_LIBVER | cut -f2 -d'.'  | sed -E 's/\./_/g')
H5_RELEASEVER=$(echo $H5_LIBVER | cut -f3 -d'.'  | sed -E 's/\./_/g')
H5_LIBVER_DIR=$H5_MAJORVER$H5_MINORVER

# Shell commands used in Makefiles
RM="rm -rf"
DIFF="diff -c"
CMP="cmp -s"
GREP='grep'
CP="cp -p"  # Use -p to preserve mode,ownership,timestamps
DIRNAME='dirname'
LS='ls'
AWK='awk'

# setup plugin path
ENVCMD="env HDF5_PLUGIN_PATH=$LD_LIBRARY_PATH/plugin"

TESTDIR=$top_builddir/$currentpath


case `echo "testing\c"; echo 1,2,3`,`echo -n testing; echo 1,2,3` in
  *c*,-n*) ECHO_N= ECHO_C='
' ;;
  *c*,*  ) ECHO_N=-n ECHO_C= ;;
  *)       ECHO_N= ECHO_C='\c' ;;
esac
ECHO_N="echo $ECHO_N"


exout() {
    cd $TESTDIR
    "$@"
}

dumpout() {
    cd $TESTDIR
    $H5DUMP "$@"
}

compileout() {
    cd $TESTDIR
    $H5FC "$@"
}

# compare current version, required version.
# returns if cur_ver < req_ver is true.
version_compare() {
  version_lt=0
  if [ ! "$(printf '%s\n' "$1" "$2" | sort -V | head -n1)" = "$2" ]; then
          version_lt=1
  fi
}

# require h5_version.h generated
# topics="chunk compact extern"

topics="alloc \
  checksum \
  fillval \
  gzip \
  hyper \
  rdwr \
  soint \
  szip \
  unlimmod"

FORTRAN_2003_CONDITIONAL_F="@FORTRAN_2003_CONDITIONAL_F@"

if [ "$FORTRAN_2003_CONDITIONAL_F" = "Xyes" ]; then
   topics="$topics rdwr_kind"
fi

return_val=0

#Remove external data file from h5ex_d_extern
rm -f h5ex_d_extern.data

for topic in $topics
do
    compileout $top_srcdir/$currentpath/h5ex_d_$topic.F90 -o h5ex_d_$topic
done

for topic in $topics
do
    fname=h5ex_d_$topic
    $ECHO_N "Testing FORTRAN/H5D/$fname...$ECHO_C"
    exout ./$fname >tmp.test
    status=$?
    if test $status -eq 1
    then
        echo "  Unsupported feature"
        status=0
    else
        if [ "$topic" = "alloc" ]; then
            # Check if the only difference is the size of the unallocated space. This
            # was fixed later in HDF5 to be of zero size.
            status=0
            diff $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.tst > $TESTDIR/tmp.diff
            if [ $? -ne 0 ]; then
               NumOfFinds=`grep -c "0 bytes" tmp.diff | wc -l`
               rm -f $TESTDIR/tmp.diff
               if [ "$NumOfFinds" -gt "1" ]; then
                   status=1
               fi
            fi
        else
            cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.tst
            status=$?
        fi
        status=$?
        if test $status -ne 0
        then
            echo "  FAILED!"
        else
          dumpout $fname.h5 >tmp.test
          rm -f $TESTDIR/$fname.h5
          cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.ddl
          status=$?
          if test $status -ne 0
          then 
             # test to see if the only difference is because of big-endian and little-endian
             diff $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.ddl > $TESTDIR/tmp.diff
             echo " "
             NumOfFinds=`grep -c "DATATYPE" tmp.diff`
             NumOfFinds=`expr $NumOfFinds \* 2`
             NumOfLines=`wc -l <tmp.diff`
             rm -f $TESTDIR/tmp.diff
             if test $NumOfLines -gt $NumOfFinds 
             then
                echo "  FAILED!"
             else
                echo "  *Inconsequential difference* ... Passed"
                status=0
             fi
          else
              echo "  Passed"
          fi
        fi
        return_val=`expr $status + $return_val`
    fi
done

#######Non-standard tests#######
USE_ALT=""
### Set default tfiles directory for tests
nbitdir="18"
version_compare "$H5_LIBVER" "1.8.23"
# check if HDF5 version is < 1.8.23
if [ "$version_lt" = 1 ]; then
    USE_ALT="22"
else
# check if HDF5 version is < 1.10.8
  version_compare "$H5_LIBVER" "1.10.8"
  if [ "$version_lt" = 1 ]; then
    USE_ALT="07"
    nbitdir="110"
  fi
fi

topics18="nbit"
for topic in $topics18
do
    compileout $top_srcdir/$currentpath/h5ex_d_$topic.F90 -o h5ex_d_$topic
done

for topic in $topics18
do
    fname=h5ex_d_$topic
    $ECHO_N "Testing C/H5D/$fname...$ECHO_C"
    exout ./$fname >tmp.test
    status=$?
    if test $status -eq 1
    then
        echo "  Unsupported feature"
        status=0
    else
        if [ "$fname" = "h5ex_d_nbit" ];
        then
            tdir=$nbitdir
            if [ "$USE_ALT" = "" ];
            then
                ### set USE_ALT=07 if not set above
                USE_ALT="07"
            fi
        else
            tdir=18
            ### unset USE_ALT for the other topics
            USE_ALT=""
        fi
        cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.tst
        status=$?
        if test $status -ne 0
        then
            echo "  FAILED!"
        else
          if [ "$fname" = "h5ex_d_transform" ];
          then
              targ="-n"
          else
              targ=""
          fi
          dumpout $targ $fname.h5 >tmp.test
          rm -f $TESTDIR/$fname.h5
          cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/$tdir/$fname$USE_ALT.ddl
          status=$?
          if test $status -ne 0
          then
              echo "  FAILED!"
          else
              echo "  Passed"
          fi
        fi
        return_val=`expr $status + $return_val`
    fi
done


rm -f $TESTDIR/tmp.test
echo "$return_val tests failed in FORTRAN/H5D/"
exit $return_val
```

### `HDF5Examples/FORTRAN/H5G/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_FORTRAN_H5G Fortran)

# --------------------------------------------------------------------
# Notes: When creating examples they should be prefixed
# with "f90_". This allows for easier filtering of the examples.
# --------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Setup include Directories
#-----------------------------------------------------------------------------
set_directory_properties (PROPERTIES INCLUDE_DIRECTORIES
    "${CMAKE_Fortran_MODULE_DIRECTORY}${H5EXAMPLE_MOD_EXT};${HDF5_F90_BINARY_DIR};${PROJECT_BINARY_DIR};${CMAKE_LIBRARY_OUTPUT_DIRECTORY}"
)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (Fortran_sourcefiles.cmake)

foreach (example_name ${common_examples})
  add_executable (${EXAMPLE_VARNAME}_f90_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.F90)
  target_compile_options (${EXAMPLE_VARNAME}_f90_${example_name}
      PRIVATE
          "-DH5_LIBVER_DIR=${H5_LIBVER_DIR}"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  if (H5_HAVE_PARALLEL)
    target_include_directories (${EXAMPLE_VARNAME}_f90_${example_name} PUBLIC ${MPI_Fortran_INCLUDE_DIRS})
  endif ()
  target_link_libraries (${EXAMPLE_VARNAME}_f90_${example_name} ${H5EXAMPLE_LINK_Fortran_LIBS})
  set_target_properties (${EXAMPLE_VARNAME}_f90_${example_name} PROPERTIES LINKER_LANGUAGE Fortran)
  if (H5EXAMPLE_BUILD_TESTING)
    if (NOT ${example_name} STREQUAL "h5ex_g_create" AND NOT ${example_name} STREQUAL "h5ex_g_compact")
      add_custom_command (
          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
          POST_BUILD
          COMMAND    ${CMAKE_COMMAND}
          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
      )
    endif ()
  endif ()
endforeach ()

#if (H5EXAMPLE_ENABLE_F2003)
#  foreach (example_name ${f03examples})
#    add_executable (${EXAMPLE_VARNAME}_f90_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
#    target_compile_options(${EXAMPLE_VARNAME}_f90_${example_name}
#        PRIVATE
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
#    )
#    if (H5_HAVE_PARALLEL)
#      target_include_directories (${EXAMPLE_VARNAME}_f90_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
#    endif ()
#    target_link_libraries (${EXAMPLE_VARNAME}_f90_${example_name} ${H5EXAMPLE_LINK_Fortran_LIBS})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/F03/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()

#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
#  foreach (example_name ${1_10_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
#  foreach (example_name ${1_12_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
#  foreach (example_name ${1_14_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
#  foreach (example_name ${2_0_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()

if (HDF5_PROVIDES_TOOLS)
  foreach (example_name ${common_examples})
    if (${example_name} STREQUAL "h5ex_g_create")
      add_custom_command (
          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
          POST_BUILD
          COMMAND    ${CMAKE_COMMAND}
          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      )
    endif ()
  endforeach ()

  add_custom_command (
      TARGET     ${EXAMPLE_VARNAME}_f90_h5ex_g_compact
      POST_BUILD
      COMMAND    ${CMAKE_COMMAND}
      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/h5ex_g_compact1.ddl ${PROJECT_BINARY_DIR}/h5ex_g_compact1.ddl
  )
  add_custom_command (
      TARGET     ${EXAMPLE_VARNAME}_f90_h5ex_g_compact
      POST_BUILD
      COMMAND    ${CMAKE_COMMAND}
      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/h5ex_g_compact2.ddl ${PROJECT_BINARY_DIR}/h5ex_g_compact2.ddl
  )
endif ()

#if (H5EXAMPLE_ENABLE_F2003)
#  foreach (example_name ${f03examples})
#    add_custom_command (
#        TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#        POST_BUILD
#        COMMAND    ${CMAKE_COMMAND}
#        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/F03/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
#    )
#  endforeach ()

#  foreach (example_name ${1_10_examples})
#  endforeach ()
#  foreach (example_name ${1_12_examples})
#  endforeach ()
#  foreach (example_name ${1_14_examples})
#  endforeach ()
#  foreach (example_name ${2_0_examples})
#  endforeach ()
#endif ()

if (H5EXAMPLE_BUILD_TESTING)
  set (exfiles
      h5ex_g_iterate
      h5ex_g_traverse
      h5ex_g_visit
  )
  foreach (example ${exfiles})
    add_custom_command (
        TARGET     ${EXAMPLE_VARNAME}_f90_${example}
        POST_BUILD
        COMMAND    ${CMAKE_COMMAND}
        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/${example}.h5 ${PROJECT_BINARY_DIR}/${example}.h5
    )
  endforeach ()

  macro (ADD_DUMP_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_f90_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
    )
    if (HDF5_USING_ANALYSIS_TOOL)
      add_test (NAME ${EXAMPLE_VARNAME}_f90_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_f90_${testname}-clearall)
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}-clean
          COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_f90_${testname}
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_SKIP_COMPARE=TRUE"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      if (last_test)
        set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES DEPENDS ${last_test})
      endif ()
      set (last_test "${EXAMPLE_VARNAME}_f90_${testname}")
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${ARGN};${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-f90_${testname} PROPERTIES
            DEPENDS ${last_test}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
        set (last_test "${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}")
      endif ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}-clean
          COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES DEPENDS ${last_test})
    endif ()
  endmacro ()

  macro (ADD_H5_DUMP_TEST testname)
    if (HDF5_USING_ANALYSIS_TOOL)
      add_test (NAME ${EXAMPLE_VARNAME}_f90_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_f90_${testname}-clearall)
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}-clean
          COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_f90_${testname})
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES}
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      if (last_test)
        set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES DEPENDS ${last_test})
      endif ()
      set (last_test "${EXAMPLE_VARNAME}_f90_${testname}")
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}
           COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${ARGN};${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-f90_${testname} PROPERTIES
            DEPENDS ${last_test}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
        set (last_test "${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}")
      endif ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}-clean
          COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES DEPENDS ${last_test})
    endif ()
  endmacro ()

  macro (ADD_H5_DUMP2_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_f90_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove
            ${testname}.h5
            ${testname}1.h5
            ${testname}2.h5
    )
    if (HDF5_USING_ANALYSIS_TOOL)
      add_test (NAME ${EXAMPLE_VARNAME}_f90_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_f90_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}-clean
          COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_f90_${testname})
    else ()
      if (${ARGN} STREQUAL "NULL")
        add_test (
            NAME ${EXAMPLE_VARNAME}_f90_${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>"
                -D "TEST_ARGS:STRING="
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_EXPECT=0"
                -D "TEST_SKIP_COMPARE=TRUE"
                -D "TEST_OUTPUT=${testname}.out"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
      else ()
        add_test (
            NAME ${EXAMPLE_VARNAME}_f90_${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>"
                -D "TEST_ARGS:STRING="
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_EXPECT=0"
                -D "TEST_OUTPUT=${testname}.out"
                -D "TEST_REFERENCE=${testname}.tst"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
      endif ()
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_f90_${testname}-clearall
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}1
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${testname}1.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}1.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}1.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}1 PROPERTIES
            DEPENDS ${EXAMPLE_VARNAME}_f90_${testname}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}2
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${testname}2.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}2.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}2.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}2 PROPERTIES
            DEPENDS ${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}1
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
      endif ()
    endif ()
  endmacro ()

  macro (ADD_H5_CMP_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_f90_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.out.tmp
    )
    if (HDF5_USING_ANALYSIS_TOOL)
      add_test (NAME ${EXAMPLE_VARNAME}_f90_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>"
              -D "TEST_ARGS:STRING=${ARGN}"
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
    endif ()
    set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES
        DEPENDS ${EXAMPLE_VARNAME}_f90_${testname}-clearall
        ENVIRONMENT "${CROSSCOMPILING_PATH}"
    )
    add_test (
        NAME ${EXAMPLE_VARNAME}_f90_${testname}-clean
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.out.tmp
    )
    set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_f90_${testname})
  endmacro ()

  ADD_H5_DUMP2_TEST (h5ex_g_compact NULL)
  ADD_DUMP_TEST (h5ex_g_create)
  ADD_H5_CMP_TEST (h5ex_g_corder)
  ADD_H5_CMP_TEST (h5ex_g_phase)

  if (HDF5_VERSION_STRING VERSION_GREATER_EQUAL "1.10.0")
    ADD_H5_CMP_TEST (h5ex_g_intermediate)
    ADD_H5_CMP_TEST (h5ex_g_iterate)
    ADD_H5_CMP_TEST (h5ex_g_visit)
    #if (HDF5_VERSION_STRING VERSION_GREATER_EQUAL "1.14.3")
	    #ADD_H5_CMP_TEST (h5ex_g_traverse)
    #endif()
  else ()
    if (H5EXAMPLE_ENABLE_F2003)
      ADD_H5_CMP_TEST (h5ex_g_intermediate)
      ADD_H5_CMP_TEST (h5ex_g_iterate)
      #  ADD_H5_CMP_TEST (h5ex_g_traverse)
      ADD_H5_CMP_TEST (h5ex_g_visit)
    endif ()
  endif ()

endif ()
```

### `HDF5Examples/FORTRAN/H5G/Fortran_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (examples)

set (common_examples
    h5ex_g_compact
    h5ex_g_corder
    h5ex_g_phase
    h5ex_g_create
)
if (HDF5_VERSION_STRING VERSION_GREATER_EQUAL "1.10.0")
  set (common_examples
    ${common_examples}
    h5ex_g_intermediate
    h5ex_g_iterate
    h5ex_g_visit
  )
  if (HDF5_VERSION_STRING VERSION_GREATER_EQUAL "1.14.3")
    set (common_examples
      ${common_examples}
      h5ex_g_traverse
    )
  endif()
else ()
  if (H5EXAMPLE_ENABLE_F2003)
    set (common_examples
      ${common_examples}
      h5ex_g_intermediate
      h5ex_g_iterate
      h5ex_g_traverse
      h5ex_g_visit
    )
  endif ()
endif ()
```

### `HDF5Examples/FORTRAN/H5G/test-pc.sh`

```bash
#! /bin/sh
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.

# This file is for use of h5cc created with the CMake process.
# Environment variable, HDF5_HOME is expected to be set.
# $1 is the path name of the source directory.
# $2 is the path name of the build directory.
# $3 is the current path name.

top_srcdir=$1
top_builddir=$2
currentpath=$3
verbose=yes
nerrors=0

echo "Current build directory: $top_builddir/$currentpath"

# HDF5 compile commands, assuming they are in your $PATH.
H5FC=$HDF5_HOME/bin/h5fc
LD_LIBRARY_PATH=$HDF5_HOME/lib
export LD_LIBRARY_PATH

if ! test -f $H5FC; then
    echo "Set paths for H5FC and LD_LIBRARY_PATH in test.sh"
    echo "Set environment variable HDF5_HOME to the hdf5 install dir"
    echo "h5fc was not found at $H5FC"
    exit $EXIT_FAILURE
fi

H5DUMP=`echo $H5FC | sed -e 's/\/[^/]*$/\/h5dump/'`;
H5_LIBVER=$($H5FC -showconfig | grep -i "HDF5 Version:" | sed 's/^.* //g' | sed 's/[-].*//g')
H5_APIVER=$($H5FC -showconfig | grep -i "Default API mapping:" | sed 's/^.* //g' | sed 's/v//g' | sed 's/1/1_/')

H5_MAJORVER=$(echo $H5_LIBVER | cut -f1 -d'.'  | sed -E 's/\./_/g')
H5_MINORVER=$(echo $H5_LIBVER | cut -f2 -d'.'  | sed -E 's/\./_/g')
H5_RELEASEVER=$(echo $H5_LIBVER | cut -f3 -d'.'  | sed -E 's/\./_/g')
H5_LIBVER_DIR=$H5_MAJORVER$H5_MINORVER

# Shell commands used in Makefiles
RM="rm -rf"
DIFF="diff -c"
CMP="cmp -s"
GREP='grep'
CP="cp -p"  # Use -p to preserve mode,ownership,timestamps
DIRNAME='dirname'
LS='ls'
AWK='awk'

# setup plugin path
ENVCMD="env HDF5_PLUGIN_PATH=$LD_LIBRARY_PATH/plugin"

TESTDIR=$top_builddir/$currentpath


case `echo "testing\c"; echo 1,2,3`,`echo -n testing; echo 1,2,3` in
  *c*,-n*) ECHO_N= ECHO_C='
' ;;
  *c*,*  ) ECHO_N=-n ECHO_C= ;;
  *)       ECHO_N= ECHO_C='\c' ;;
esac
ECHO_N="echo $ECHO_N"


exout() {
    cd $TESTDIR
    "$@"
}

dumpout() {
    cd $TESTDIR
    $H5DUMP "$@"
}

compileout() {
    cd $TESTDIR
    $H5FC "$@"
}

return_val=0

compileout $top_srcdir/$currentpath/h5ex_g_create.F90 -o h5ex_g_create

$ECHO_N "Testing FORTRAN/H5G/h5ex_g_create...$ECHO_C"
./h5ex_g_create
dumpout h5ex_g_create.h5 >tmp.test
rm -f $TESTDIR/h5ex_g_create.h5
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/h5ex_g_create.ddl
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
    echo "  Passed"
fi
return_val=`expr $status + $return_val`

compileout $top_srcdir/$currentpath/h5ex_g_compact.F90 -o h5ex_g_compact

$ECHO_N "Testing FORTRAN/H5G/h5ex_g_compact...$ECHO_C"
./h5ex_g_compact >/dev/null
dumpout h5ex_g_compact1.h5 >tmp.test
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/h5ex_g_compact1.ddl
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
  dumpout h5ex_g_compact2.h5 >tmp.test
  cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/h5ex_g_compact2.ddl
  status=$?
  if test $status -ne 0
  then
      echo "  FAILED!"
  else
      echo "  Passed"
  fi
fi
return_val=`expr $status + $return_val`
rm -f $TESTDIR/h5ex_g_compact1.h5
rm -f $TESTDIR/h5ex_g_compact2.h5

compileout $top_srcdir/$currentpath/h5ex_g_phase.F90 -o h5ex_g_phase

$ECHO_N "Testing FORTRAN/H5G/h5ex_g_phase...$ECHO_C"
exout ./h5ex_g_phase >tmp.test
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/h5ex_g_phase.tst
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
    echo "  Passed"
fi
return_val=`expr $status + $return_val`
rm -f $TESTDIR/h5ex_g_phase.h5

compileout $top_srcdir/$currentpath/h5ex_g_corder.F90 -o h5ex_g_corder

$ECHO_N "Testing FORTRAN/H5G/h5ex_g_corder...$ECHO_C"
exout ./h5ex_g_corder >tmp.test
cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/h5ex_g_corder.tst
status=$?
if test $status -ne 0
then
    echo "  FAILED!"
else
    echo "  Passed"
fi
return_val=`expr $status + $return_val`
rm -f $TESTDIR/h5ex_g_corder.h5


rm -f $TESTDIR/tmp.test
echo "$return_val tests failed in /FORTRAN/H5G/"
exit $return_val
```

### `HDF5Examples/FORTRAN/H5PAR/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_FORTRAN_H5PAR Fortran)

# --------------------------------------------------------------------
# Notes: When creating examples they should be prefixed
# with "f90_". This allows for easier filtering of the examples.
# --------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Setup include Directories
#-----------------------------------------------------------------------------
set_directory_properties(PROPERTIES INCLUDE_DIRECTORIES
  "${CMAKE_Fortran_MODULE_DIRECTORY}${H5EXAMPLE_MOD_EXT};${HDF5_F90_BINARY_DIR};${PROJECT_BINARY_DIR};${CMAKE_LIBRARY_OUTPUT_DIRECTORY}"
)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (Fortran_sourcefiles.cmake)

foreach (example_name ${examples})
  add_executable (${EXAMPLE_VARNAME}_f90_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.F90)
  target_compile_options(${EXAMPLE_VARNAME}_f90_${example_name}
      PRIVATE
          "-DH5_LIBVER_DIR=${H5_LIBVER_DIR}"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  target_include_directories (${EXAMPLE_VARNAME}_f90_${example_name} PUBLIC ${MPI_Fortran_INCLUDE_DIRS})
  target_link_libraries (${EXAMPLE_VARNAME}_f90_${example_name} ${H5EXAMPLE_LINK_Fortran_LIBS})
  set_target_properties (${EXAMPLE_VARNAME}_f90_${example_name} PROPERTIES LINKER_LANGUAGE Fortran)
endforeach ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_GREP_TEST testname mumprocs)
    add_test (
        NAME MPI_TEST_${EXAMPLE_VARNAME}_f90_${testname}-clearall
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${testname}.h5
    )
    add_test (NAME MPI_TEST_${EXAMPLE_VARNAME}_f90_${testname} COMMAND "${CMAKE_COMMAND}"
        -D "TEST_PROGRAM=${MPIEXEC_EXECUTABLE};${MPIEXEC_NUMPROC_FLAG};${mumprocs};${MPIEXEC_PREFLAGS};$<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>;${MPIEXEC_POSTFLAGS}"
        -D "TEST_ARGS:STRING="
        -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
        -D "TEST_EXPECT=0"
        -D "TEST_SKIP_COMPARE=TRUE"
        -D "TEST_OUTPUT=${testname}.out"
        -D "TEST_GREP_COMPARE=TRUE"
        -D "TEST_REFERENCE:STRING=PHDF5 example finished with no errors"
        -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
        -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
    )
    set_tests_properties (MPI_TEST_${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES DEPENDS MPI_TEST_${EXAMPLE_VARNAME}_f90_${testname}-clearall)
    add_test (
        NAME MPI_TEST_${EXAMPLE_VARNAME}_f90_${testname}-clean
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${testname}.h5
    )
    set_tests_properties (MPI_TEST_${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES DEPENDS MPI_TEST_${EXAMPLE_VARNAME}_f90_${testname})
    set (last_test "MPI_TEST_${EXAMPLE_VARNAME}_f90_${testname}-clean")
  endmacro ()

  # Ensure that 24 is a multiple of the number of processes.
  # The number 24 corresponds to SPACE1_DIM1 and SPACE1_DIM2 defined in ph5example.c
  math(EXPR NUMPROCS "24 / ((24 + ${MPIEXEC_MAX_NUMPROCS} - 1) / ${MPIEXEC_MAX_NUMPROCS})")
  foreach (example_name ${examples})
    if (${example_name} STREQUAL "ph5_f90_hyperslab_by_row")
      ADD_GREP_TEST (${example_name} 2)
    elseif (${example_name} STREQUAL "ph5_f90_hyperslab_by_chunk" OR ${example_name} STREQUAL "ph5_f90_hyperslab_by_pattern")
      ADD_GREP_TEST (${example_name} 4)
    else ()
      ADD_GREP_TEST (${example_name} ${NUMPROCS})
    endif ()
  endforeach ()

endif ()
```

### `HDF5Examples/FORTRAN/H5PAR/Fortran_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (examples
  ph5_f90_dataset
  ph5_f90_file_create
  ph5_f90_hyperslab_by_row
  ph5_f90_hyperslab_by_col
  ph5_f90_hyperslab_by_pattern
  ph5_f90_hyperslab_by_chunk
)

if (HDF5_PROVIDES_SUBFILING_VFD)
  set (examples ${examples}
    ph5_f90_subfiling
  )
endif()
if (HDF5_VERSION_STRING VERSION_GREATER_EQUAL "1.14.4")
  set (examples ${examples}
    ph5_f90_filtered_writes_no_sel
  )
endif()
```

### `HDF5Examples/FORTRAN/H5T/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_FORTRAN_H5T Fortran)

#-----------------------------------------------------------------------------
# Setup include Directories
#-----------------------------------------------------------------------------
set_directory_properties (PROPERTIES INCLUDE_DIRECTORIES
    "${CMAKE_Fortran_MODULE_DIRECTORY}${H5EXAMPLE_MOD_EXT};${HDF5_F90_BINARY_DIR};${PROJECT_BINARY_DIR};${CMAKE_LIBRARY_OUTPUT_DIRECTORY}"
)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (Fortran_sourcefiles.cmake)

#if (H5EXAMPLE_ENABLE_F2003)
  foreach (example_name ${f03_examples})
    add_executable (${EXAMPLE_VARNAME}_f90_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.F90)
    target_compile_options(${EXAMPLE_VARNAME}_f90_${example_name}
        PRIVATE
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
    )
    if (H5_HAVE_PARALLEL)
      target_include_directories (${EXAMPLE_VARNAME}_f90_${example_name} PUBLIC ${MPI_Fortran_INCLUDE_DIRS})
    endif ()
    target_link_libraries (${EXAMPLE_VARNAME}_f90_${example_name} ${H5EXAMPLE_LINK_Fortran_LIBS})
    set_target_properties (${EXAMPLE_VARNAME}_f90_${example_name} PROPERTIES LINKER_LANGUAGE Fortran)
    if (H5EXAMPLE_BUILD_TESTING)
      add_custom_command (
          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
          POST_BUILD
          COMMAND    ${CMAKE_COMMAND}
          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
      )
    endif ()
  endforeach ()
#endif ()

foreach (example_name ${common_examples})
  add_executable (${EXAMPLE_VARNAME}_f90_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.F90)
  target_compile_options(${EXAMPLE_VARNAME}_f90_${example_name}
      PRIVATE
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  if (H5_HAVE_PARALLEL)
    target_include_directories (${EXAMPLE_VARNAME}_f90_${example_name} PUBLIC ${MPI_Fortran_INCLUDE_DIRS})
  endif ()
  target_link_libraries (${EXAMPLE_VARNAME}_f90_${example_name} ${H5EXAMPLE_LINK_Fortran_LIBS})
  set_target_properties (${EXAMPLE_VARNAME}_f90_${example_name} PROPERTIES LINKER_LANGUAGE Fortran)
  if (H5EXAMPLE_BUILD_TESTING)
    add_custom_command (
        TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
        POST_BUILD
        COMMAND    ${CMAKE_COMMAND}
        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
    )
  endif ()
endforeach ()

#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
#  foreach (example_name ${1_10_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
#  foreach (example_name ${1_12_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
#  foreach (example_name ${1_14_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
#  foreach (example_name ${2_0_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()

if (HDF5_PROVIDES_TOOLS)
  foreach (example_name ${common_examples})
    add_custom_command (
        TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
        POST_BUILD
        COMMAND    ${CMAKE_COMMAND}
        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
    )
  endforeach ()

  #if (H5EXAMPLE_ENABLE_F2003)
    foreach (example_name ${f03_examples})
      if (NOT ${example_name} STREQUAL "h5ex_t_convert_F03")
        if (${example_name} STREQUAL "h5ex_t_vlen_F03" OR ${example_name} STREQUAL "h5ex_t_vlenatt_F03")
          if (HDF5_VERSION_STRING VERSION_GREATER_EQUAL "1.14.3")
            add_custom_command (
                TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                POST_BUILD
                COMMAND    ${CMAKE_COMMAND}
                ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
            )
          else ()
            add_custom_command (
                TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                POST_BUILD
                COMMAND    ${CMAKE_COMMAND}
                ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
            )
          endif ()
        elseif ((${example_name} STREQUAL "h5ex_t_objref_F03" OR ${example_name} STREQUAL "h5ex_t_objrefatt_F03") OR (${example_name} STREQUAL "h5ex_t_regref_F03" OR ${example_name} STREQUAL "h5ex_t_regrefatt_F03"))
          if (${EXAMPLE_VARNAME}_USE_16_API OR ${EXAMPLE_VARNAME}_USE_18_API OR ${EXAMPLE_VARNAME}_USE_110_API)
            if (HDF5_VERSION_MAJOR VERSION_EQUAL "1.8")
              if (HDF5_VERSION_STRING VERSION_LESS_EQUAL "1.8.21")
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}21.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              else ()
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              endif ()
            elseif (HDF5_VERSION_MAJOR VERSION_EQUAL "1.10")
              if (HDF5_VERSION_STRING VERSION_LESS_EQUAL "1.10.6")
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}06.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              else ()
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              endif ()
            elseif (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
            else ()
              add_custom_command (
                  TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                  POST_BUILD
                  COMMAND    ${CMAKE_COMMAND}
                  ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
              )
            endif ()
          else ()
            if (HDF5_VERSION_MAJOR VERSION_EQUAL "1.8")
              if (HDF5_VERSION_STRING VERSION_LESS_EQUAL "1.8.21")
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}21.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              else ()
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              endif ()
            elseif (HDF5_VERSION_MAJOR VERSION_EQUAL "1.10")
              if (HDF5_VERSION_STRING VERSION_LESS_EQUAL "1.10.6")
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}06.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              else ()
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
              endif ()
            elseif (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
                add_custom_command (
                    TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                    POST_BUILD
                    COMMAND    ${CMAKE_COMMAND}
                    ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
                )
            else ()
              add_custom_command (
                  TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
                  POST_BUILD
                  COMMAND    ${CMAKE_COMMAND}
                  ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
              )
            endif ()
          endif ()
        else ()
          add_custom_command (
              TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
              POST_BUILD
              COMMAND    ${CMAKE_COMMAND}
              ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/18/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
          )
        endif ()
      endif ()
    endforeach ()
  #endif ()

#  foreach (example_name ${1_10_examples})
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
      #if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
      #  add_custom_command (
      #      TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
      #      POST_BUILD
      #      COMMAND    ${CMAKE_COMMAND}
      #      ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
      #  )
      #endif ()
#  endforeach ()
#  foreach (example_name ${1_12_examples})
#  endforeach ()
#  foreach (example_name ${1_14_examples})
#  endforeach ()
#  foreach (example_name ${2_0_examples})
#  endforeach ()
endif ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_f90_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
    )
    if (HDF5_USING_ANALYSIS_TOOL)
      add_test (NAME ${EXAMPLE_VARNAME}_f90_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_f90_${testname}-clearall)
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}-clean
          COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_f90_${testname}
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      if (last_test)
        set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES DEPENDS ${last_test})
      endif ()
      set (last_test "${EXAMPLE_VARNAME}_f90_${testname}")
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${ARGN};${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-f90_${testname} PROPERTIES
            DEPENDS ${last_test}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
        set (last_test "${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}")
      endif ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}-clean
          COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES DEPENDS ${last_test})
    endif ()
  endmacro ()

  macro (ADD_H5_CMP_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_f90_${testname}-clearall
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
    )
    if (HDF5_USING_ANALYSIS_TOOL)
      add_test (NAME ${EXAMPLE_VARNAME}_f90_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>"
              -D "TEST_ARGS:STRING=${ARGN}"
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
    endif ()
    set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES
        DEPENDS ${EXAMPLE_VARNAME}_f90_${testname}-clearall
        ENVIRONMENT "${CROSSCOMPILING_PATH}"
    )
    add_test (
        NAME ${EXAMPLE_VARNAME}_f90_${testname}-clean
        COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
    )
    set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_f90_${testname})
  endmacro ()

  macro (TEST_EXAMPLE example)
    if (${example} STREQUAL "h5ex_t_cpxcmpd_F03" OR ${example} STREQUAL "h5ex_t_cpxcmpdatt_F03")
      ADD_H5_TEST (${example} -n)
    elseif (${example} STREQUAL "h5ex_t_convert_F03")
      ADD_H5_CMP_TEST (${example})
    else ()
      ADD_H5_TEST (${example})
    endif ()
  endmacro ()

  #if (H5EXAMPLE_ENABLE_F2003)
    foreach (example_name ${f03_examples} ${common_examples})
      TEST_EXAMPLE (${example_name})
    endforeach ()
  #endif ()
endif ()
```

### `HDF5Examples/FORTRAN/H5T/Fortran_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (f03_examples
    h5ex_t_array_F03
    h5ex_t_arrayatt_F03
    h5ex_t_bit_F03
    h5ex_t_bitatt_F03
    h5ex_t_cmpd_F03
    h5ex_t_cmpdatt_F03
    h5ex_t_enum_F03
    h5ex_t_enumatt_F03
    h5ex_t_float_F03
    h5ex_t_floatatt_F03
    h5ex_t_int_F03
    h5ex_t_intatt_F03
    h5ex_t_objref_F03
    h5ex_t_objrefatt_F03
    h5ex_t_opaque_F03
    h5ex_t_opaqueatt_F03
    h5ex_t_regref_F03
    h5ex_t_regrefatt_F03
    h5ex_t_string_F03
    h5ex_t_stringC_F03
    h5ex_t_stringCatt_F03
    h5ex_t_vlen_F03
    h5ex_t_vlenatt_F03
#    h5ex_t_vlstring_F03
#    h5ex_t_vlstringatt_F03
#    h5ex_t_cpxcmpd_F03
#    h5ex_t_cpxcmpdatt_F03
#    h5ex_t_commit_F03
#    h5ex_t_convert_F03
)
set (common_examples
    h5ex_t_vlstring
#    h5ex_t_vlstringatt
#    h5ex_t_cpxcmpd
#    h5ex_t_cpxcmpdatt
#    h5ex_t_commit
#    h5ex_t_convert
)
```

### `HDF5Examples/FORTRAN/H5T/test-pc.sh`

```bash
#! /bin/sh
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.

# This file is for use of h5cc created with the CMake process.
# Environment variable, HDF5_HOME is expected to be set.
# $1 is the path name of the source directory.
# $2 is the path name of the build directory.
# $3 is the current path name.

top_srcdir=$1
top_builddir=$2
currentpath=$3
verbose=yes
nerrors=0

echo "Current build directory: $top_builddir/$currentpath"

# HDF5 compile commands, assuming they are in your $PATH.
H5FC=$HDF5_HOME/bin/h5fc
LD_LIBRARY_PATH=$HDF5_HOME/lib
export LD_LIBRARY_PATH

if ! test -f $H5FC; then
    echo "Set paths for H5FC and LD_LIBRARY_PATH in test.sh"
    echo "Set environment variable HDF5_HOME to the hdf5 install dir"
    echo "h5fc was not found at $H5FC"
    exit $EXIT_FAILURE
fi

H5DUMP=`echo $H5FC | sed -e 's/\/[^/]*$/\/h5dump/'`;
H5_LIBVER=$($H5FC -showconfig | grep -i "HDF5 Version:" | sed 's/^.* //g' | sed 's/[-].*//g')
H5_APIVER=$($H5FC -showconfig | grep -i "Default API mapping:" | sed 's/^.* //g' | sed 's/v//g' | sed 's/1/1_/')

H5_MAJORVER=$(echo $H5_LIBVER | cut -f1 -d'.'  | sed -E 's/\./_/g')
H5_MINORVER=$(echo $H5_LIBVER | cut -f2 -d'.'  | sed -E 's/\./_/g')
H5_RELEASEVER=$(echo $H5_LIBVER | cut -f3 -d'.'  | sed -E 's/\./_/g')
H5_LIBVER_DIR=$H5_MAJORVER$H5_MINORVER

# Shell commands used in Makefiles
RM="rm -rf"
DIFF="diff -c"
CMP="cmp -s"
GREP='grep'
CP="cp -p"  # Use -p to preserve mode,ownership,timestamps
DIRNAME='dirname'
LS='ls'
AWK='awk'

# setup plugin path
ENVCMD="env HDF5_PLUGIN_PATH=$LD_LIBRARY_PATH/plugin"

TESTDIR=$top_builddir/$currentpath


case `echo "testing\c"; echo 1,2,3`,`echo -n testing; echo 1,2,3` in
  *c*,-n*) ECHO_N= ECHO_C='
' ;;
  *c*,*  ) ECHO_N=-n ECHO_C= ;;
  *)       ECHO_N= ECHO_C='\c' ;;
esac
ECHO_N="echo $ECHO_N"


exout() {
    cd $TESTDIR
    "$@"
}

dumpout() {
    cd $TESTDIR
    $H5DUMP "$@"
}

compileout() {
    cd $TESTDIR
    $H5FC "$@"
}

# compare current version, required version.
# returns if cur_ver < req_ver is true.
version_compare() {
  version_lt=0
  if [ ! "$(printf '%s\n' "$1" "$2" | sort -V | head -n1)" = "$2" ]; then
          version_lt=1
  fi
}

FORTRAN_2003_CONDITIONAL_F="@FORTRAN_2003_CONDITIONAL_F@"

topics="vlstring"

if [ "$FORTRAN_2003_CONDITIONAL_F" = "Xyes" ]; then
    topics="arrayatt_F03 array_F03 bitatt_F03 bit_F03 cmpdatt_F03 cmpd_F03 \
            Cstring_F03 enumatt_F03 enum_F03 floatatt_F03 float_F03 \
            intatt_F03 int_F03 opaqueatt_F03 opaque_F03 \
            string_F03 $topics"
fi

return_val=0

for topic in $topics
do
    compileout $top_srcdir/$currentpath/h5ex_t_$topic.F90 -o h5ex_t_$topic
done

for topic in $topics
do
    fname=h5ex_t_$topic
    $ECHO_N "Testing FORTRAN/H5T/$fname...$ECHO_C"
    exout ./$fname >tmp.test
    cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.tst
    status=$?
    if test $status -ne 0
    then
        echo "  FAILED!"
    else
        if [ "$fname" = "h5ex_t_cpxcmpd_F03" -o "$fname" = "h5ex_t_cpxcmpdatt_F03" ];
        then
            targ="-n"
        else
            targ=""
        fi
        dumpout $targ $fname.h5 >tmp.test
        rm -f $fname.h5
        cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.ddl
        status=$?
        if test $status -ne 0
        then
            echo "  FAILED!"
        else
            echo "  Passed"
        fi
    fi
    return_val=`expr $status + $return_val`
done


#######Non-standard tests#######

USE_ALT=""
if [ "$H5_LIBVER_DIR" = "110" ]; then
   # check if HDF5 version is < 1.10.7
   version_compare "$H5_LIBVER" "1.10.7"
   if [ "$version_lt" = 1 ]; then
      USE_ALT="06"
   fi
else
  if [ "$H5_LIBVER_DIR" = "18" ]; then
   # check if HDF5 version is < 1.8.22
   version_compare "$H5_LIBVER" "1.8.22"
   if [ "$version_lt" = 1 ]; then
      USE_ALT="21"
   fi
  fi
fi

if [ "$FORTRAN_2003_CONDITIONAL_F" = "Xyes" ]; then
    topics="objrefatt_F03 objref_F03 regrefatt_F03 regref_F03"
else
    topics=""
fi

for topic in $topics
do
    compileout $top_srcdir/$currentpath/h5ex_t_$topic.F90 -o h5ex_t_$topic
done

for topic in $topics
do
    fname=h5ex_t_$topic
    $ECHO_N "Testing FORTRAN/H5T/$fname...$ECHO_C"
    exout ./$fname >tmp.test
    cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.tstc
    status=$?
    if test $status -ne 0
    then
        echo "  FAILED!"
    else
        dumpout $fname.h5 >tmp.test
        rm -f $TESTDIR/$fname.h5
        version_compare "$H5_LIBVER" "1.10.0"
        if [ "$version_lt" = 1 ]; then
            cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname$USE_ALT.ddl
        else
            version_compare "$H5_LIBVER" "1.12.0"
            if [ "$version_lt" = 1 ]; then
               version_compare "$H5_LIBVER" "1.10.7"
               if [ "$version_lt" = 1 ]; then
                  cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/110/$fname$USE_ALT.ddl
               else
                  cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.ddl
               fi
            else
                cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/112/$fname.ddl
            fi
        fi
        status=$?
        if test $status -ne 0
        then
            echo "  FAILED!"
        else
            echo "  Passed"
        fi
    fi
    return_val=`expr $status + $return_val`
done

topics=""
version_compare "$H5_LIBVER" "1.10.0"
if [ "$version_lt" = 0 ]; then
  topics=" vlenatt_F03 vlen_F03"
fi

for topic in $topics
do
    compileout $top_srcdir/$currentpath/h5ex_t_$topic.F90 -o h5ex_t_$topic
done

for topic in $topics
do
    fname=h5ex_t_$topic
    $ECHO_N "Testing C/H5T/$fname...$ECHO_C"
    exout ./$fname >tmp.test
    cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.tst
    status=$?
    if test $status -ne 0
    then
        echo "  FAILED!"
    else
        dumpout $fname.h5 >tmp.test
        rm -f $TESTDIR/$fname.h5
        version_compare "$H5_LIBVER" "1.14.3"
        if [ "$version_lt" = 1 ]; then
            cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.ddl
        else
            cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/114/$fname.ddl
        fi
        status=$?
        if test $status -ne 0
        then
            echo "  FAILED!"
        else
            echo "  Passed"
        fi
    fi
    return_val=`expr $status + $return_val`
done

#compileout $top_srcdir/$currentpath/h5ex_t_convert.F90 -o h5ex_t_convert

#fname=h5ex_t_convert
#$ECHO_N "Testing FORTRAN/H5T/$fname...$ECHO_C"
#exout ./$fname >tmp.test
#cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/18/$fname.test
#status=$?
#if test $status -ne 0
#then
#    echo "  FAILED!"
#else
#    echo "  Passed"
#fi
#return_val=`expr $status + $return_val`


rm -f $TESTDIR/tmp.test
echo "$return_val tests failed in /FORTRAN/H5T/"
exit $return_val
```

### `HDF5Examples/FORTRAN/HL/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_FORTRAN_HL Fortran)

# --------------------------------------------------------------------
# Notes: When creating examples they should be prefixed
# with "f90_". This allows for easier filtering of the examples.
# --------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Setup include Directories
#-----------------------------------------------------------------------------
set_directory_properties (PROPERTIES INCLUDE_DIRECTORIES
    "${CMAKE_Fortran_MODULE_DIRECTORY}${H5EXAMPLE_MOD_EXT};${HDF5_F90_BINARY_DIR};${PROJECT_BINARY_DIR};${CMAKE_LIBRARY_OUTPUT_DIRECTORY}"
)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (Fortran_sourcefiles.cmake)

foreach (example_name ${f03_examples})
  add_executable (${EXAMPLE_VARNAME}_f90_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.F90)
  target_compile_options(${EXAMPLE_VARNAME}_f90_${example_name}
      PRIVATE
          "-DH5_LIBVER_DIR=${H5_LIBVER_DIR}"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  if (H5_HAVE_PARALLEL)
    target_include_directories (${EXAMPLE_VARNAME}_f90_${example_name} PUBLIC ${MPI_Fortran_INCLUDE_DIRS})
  endif ()
  target_link_libraries (${EXAMPLE_VARNAME}_f90_${example_name} ${H5EXAMPLE_LINK_Fortran_LIBS})
  set_target_properties (${EXAMPLE_VARNAME}_f90_${example_name} PROPERTIES LINKER_LANGUAGE Fortran)
  if (H5EXAMPLE_BUILD_TESTING)
    add_custom_command (
        TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
        POST_BUILD
        COMMAND    ${CMAKE_COMMAND}
        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
    )
  endif ()
endforeach ()

#if (H5EXAMPLE_ENABLE_F2003)
#  foreach (example_name ${f03examples})
#    add_executable (${EXAMPLE_VARNAME}_f90_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.c)
#    target_compile_options(${EXAMPLE_VARNAME}_f90_${example_name}
#        PRIVATE
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
#            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
#    )
#    if (H5_HAVE_PARALLEL)
#      target_include_directories (${EXAMPLE_VARNAME}_f90_${example_name} PUBLIC ${MPI_C_INCLUDE_DIRS})
#    endif ()
#    target_link_libraries (${EXAMPLE_VARNAME}_f90_${example_name} ${H5EXAMPLE_LINK_Fortran_LIBS})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/F03/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()

#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.10")
#  foreach (example_name ${1_10_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.12")
#  foreach (example_name ${1_12_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/112/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "1.14")
#  foreach (example_name ${1_14_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/114/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()
#if (HDF5_VERSION_MAJOR VERSION_GREATER_EQUAL "2.0")
#  foreach (example_name ${2_0_examples})
#    if (H5EXAMPLE_BUILD_TESTING)
#      add_custom_command (
#          TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#          POST_BUILD
#          COMMAND    ${CMAKE_COMMAND}
#          ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/200/${example_name}.tst ${PROJECT_BINARY_DIR}/${example_name}.tst
#      )
#    endif ()
#  endforeach ()
#endif ()

#if (HDF5_PROVIDES_TOOLS)
#  foreach (example_name ${f03_examples})
#    add_custom_command (
#        TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
#        POST_BUILD
#        COMMAND    ${CMAKE_COMMAND}
#        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
#    )
#  endforeach ()
#endif ()

#if (H5EXAMPLE_ENABLE_F2003)
  foreach (example_name ${f03examples})
    add_custom_command (
        TARGET     ${EXAMPLE_VARNAME}_f90_${example_name}
        POST_BUILD
        COMMAND    ${CMAKE_COMMAND}
        ARGS       -E copy_if_different ${PROJECT_SOURCE_DIR}/tfiles/${example_name}.ddl ${PROJECT_BINARY_DIR}/${example_name}.ddl
    )
  endforeach ()

#  foreach (example_name ${1_10_examples})
#  endforeach ()
#  foreach (example_name ${1_12_examples})
#  endforeach ()
#  foreach (example_name ${1_14_examples})
#  endforeach ()
#  foreach (example_name ${2_0_examples})
#  endforeach ()
#endif ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_DUMP_TEST testname)
    add_test (
        NAME ${EXAMPLE_VARNAME}_f90_${testname}-clearall
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${testname}.h5
    )
    if (HDF5_ENABLE_USING_MEMCHECKER)
      add_test (NAME ${EXAMPLE_VARNAME}_f90_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>)
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_f90_${testname}-clearall)
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}-clean
          COMMAND    ${CMAKE_COMMAND}
              -E remove
              ${testname}.h5
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES
          DEPENDS ${EXAMPLE_VARNAME}_f90_${testname}
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_f90_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_REFERENCE=${testname}.tst"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES
          ENVIRONMENT "${CROSSCOMPILING_PATH}"
      )
      if (last_test)
        set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname} PROPERTIES DEPENDS ${last_test})
      endif ()
      set (last_test "${EXAMPLE_VARNAME}_f90_${testname}")
      if (HDF5_PROVIDES_TOOLS)
        add_test (
            NAME ${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}
            COMMAND "${CMAKE_COMMAND}"
                -D "TEST_PROGRAM=${H5EXAMPLE_HDF5_DUMP_EXECUTABLE}"
                -D "TEST_ARGS:STRING=${ARGN};${testname}.h5"
                -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
                -D "TEST_OUTPUT=${testname}.ddl.out"
                -D "TEST_EXPECT=0"
                -D "TEST_SKIP_COMPARE=TRUE"
                #-D "TEST_REFERENCE=${testname}.ddl"
                -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
                -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
        )
        set_tests_properties (${EXAMPLE_VARNAME}_H5DUMP-f90_${testname} PROPERTIES
            DEPENDS ${last_test}
            ENVIRONMENT "${CROSSCOMPILING_PATH}"
        )
        set (last_test "${EXAMPLE_VARNAME}_H5DUMP-f90_${testname}")
      endif ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_${testname}-clean
          COMMAND ${CMAKE_COMMAND} -E remove ${testname}.h5
      )
      set_tests_properties (${EXAMPLE_VARNAME}_f90_${testname}-clean PROPERTIES DEPENDS ${last_test})
    endif ()
  endmacro ()

  foreach (example_name ${f03_examples})
    ADD_DUMP_TEST (${example_name})
  endforeach ()

endif ()
```

### `HDF5Examples/FORTRAN/HL/Fortran_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (f03_examples
    exlite ex_ds1
)
```

### `HDF5Examples/FORTRAN/HL/test-pc.sh`

```bash
#! /bin/sh
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.

# This file is for use of h5cc created with the CMake process.
# Environment variable, HDF5_HOME is expected to be set.
# $1 is the path name of the source directory.
# $2 is the path name of the build directory.
# $3 is the current path name.

top_srcdir=$1
top_builddir=$2
currentpath=$3
verbose=yes
nerrors=0

echo "Current build directory: $top_builddir/$currentpath"

# HDF5 compile commands, assuming they are in your $PATH.
H5FC=$HDF5_HOME/bin/h5fc
LD_LIBRARY_PATH=$HDF5_HOME/lib
export LD_LIBRARY_PATH

if ! test -f $H5FC; then
    echo "Set paths for H5FC and LD_LIBRARY_PATH in test.sh"
    echo "Set environment variable HDF5_HOME to the hdf5 install dir"
    echo "h5fc was not found at $H5FC"
    exit $EXIT_FAILURE
fi

H5DUMP=`echo $H5FC | sed -e 's/\/[^/]*$/\/h5dump/'`;
H5_LIBVER=$($H5FC -showconfig | grep -i "HDF5 Version:" | sed 's/^.* //g' | sed 's/[-].*//g')
H5_APIVER=$($H5FC -showconfig | grep -i "Default API mapping:" | sed 's/^.* //g' | sed 's/v//g' | sed 's/1/1_/')

H5_MAJORVER=$(echo $H5_LIBVER | cut -f1 -d'.'  | sed -E 's/\./_/g')
H5_MINORVER=$(echo $H5_LIBVER | cut -f2 -d'.'  | sed -E 's/\./_/g')
H5_RELEASEVER=$(echo $H5_LIBVER | cut -f3 -d'.'  | sed -E 's/\./_/g')
H5_LIBVER_DIR=$H5_MAJORVER$H5_MINORVER

# Shell commands used in Makefiles
RM="rm -rf"
DIFF="diff -c"
CMP="cmp -s"
GREP='grep'
CP="cp -p"  # Use -p to preserve mode,ownership,timestamps
DIRNAME='dirname'
LS='ls'
AWK='awk'

# setup plugin path
ENVCMD="env HDF5_PLUGIN_PATH=$LD_LIBRARY_PATH/plugin"

TESTDIR=$top_builddir/$currentpath


case `echo "testing\c"; echo 1,2,3`,`echo -n testing; echo 1,2,3` in
  *c*,-n*) ECHO_N= ECHO_C='
' ;;
  *c*,*  ) ECHO_N=-n ECHO_C= ;;
  *)       ECHO_N= ECHO_C='\c' ;;
esac
ECHO_N="echo $ECHO_N"


exout() {
    cd $TESTDIR
    "$@"
}

dumpout() {
    cd $TESTDIR
    $H5DUMP "$@"
}

compileout() {
    cd $TESTDIR
    $H5FC "$@"
}

# compare current version, required version.
# returns if cur_ver < req_ver is true.
version_compare() {
  version_lt=0
  if [ ! "$(printf '%s\n' "$1" "$2" | sort -V | head -n1)" = "$2" ]; then
          version_lt=1
  fi
}
topics="exlite ex_ds1"

return_val=0

for topic in $topics
do
    compileout $top_srcdir/$currentpath/$topic.F90 -o $topic
done

for topic in $topics
do
    fname=$topic
    $ECHO_N "Testing FORTRAN/HL/$fname...$ECHO_C"
    exout ./$fname >tmp.test
    cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/$fname.tst
    status=$?
    if test $status -ne 0
    then
        echo "  FAILED!"
    else
        dumpout $fname.h5 >tmp.test
        rm -f $fname.h5
        if [ !"$fname" = "h5ex_ds1" ]; then
          cmp -s $TESTDIR/tmp.test $top_srcdir/$currentpath/tfiles/$fname.ddl
        fi
        status=$?
        if test $status -ne 0
        then
            echo "  FAILED!"
        else
            echo "  Passed"
        fi
    fi
    return_val=`expr $status + $return_val`
done


rm -f $TESTDIR/tmp.test
echo "$return_val tests failed in /FORTRAN/HL/"
exit $return_val
```

### `HDF5Examples/FORTRAN/TUTR/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_FORTRAN_TUTR Fortran)

#-----------------------------------------------------------------------------
# Setup include Directories
#-----------------------------------------------------------------------------
set_directory_properties(PROPERTIES INCLUDE_DIRECTORIES
    "${CMAKE_Fortran_MODULE_DIRECTORY}${H5EXAMPLE_MOD_EXT};${HDF5_F90_BINARY_DIR};${PROJECT_BINARY_DIR};${CMAKE_LIBRARY_OUTPUT_DIRECTORY}"
)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (Fortran_sourcefiles.cmake)

#if (H5EXAMPLE_ENABLE_F2003)
  foreach (example_name ${f03_examples})
    add_executable (${EXAMPLE_VARNAME}_f90_tutr_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.f90)
    target_compile_options(${EXAMPLE_VARNAME}_f90_tutr_${example_name}
        PRIVATE
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
            "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
    )
    if (H5_HAVE_PARALLEL)
      target_include_directories (${EXAMPLE_VARNAME}_f90_tutr_${example_name} PUBLIC ${MPI_Fortran_INCLUDE_DIRS})
    endif ()
    target_link_libraries (${EXAMPLE_VARNAME}_f90_tutr_${example_name} ${H5EXAMPLE_LINK_Fortran_LIBS})
    set_target_properties (${EXAMPLE_VARNAME}_f90_tutr_${example_name} PROPERTIES LINKER_LANGUAGE Fortran)
  endforeach ()
#endif ()

foreach (example_name ${common_examples})
  add_executable (${EXAMPLE_VARNAME}_f90_tutr_${example_name} ${PROJECT_SOURCE_DIR}/${example_name}.f90)
  target_compile_options(${EXAMPLE_VARNAME}_f90_tutr_${example_name}
      PRIVATE
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_16_API}>:-DH5_USE_16_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_18_API}>:-DH5_USE_18_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_110_API}>:-DH5_USE_110_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_112_API}>:-DH5_USE_112_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_114_API}>:-DH5_USE_114_API>"
          "$<$<BOOL:${${EXAMPLE_VARNAME}_USE_200_API}>:-DH5_USE_200_API>"
  )
  if (H5_HAVE_PARALLEL)
    target_include_directories (${EXAMPLE_VARNAME}_f90_tutr_${example_name} PUBLIC ${MPI_Fortran_INCLUDE_DIRS})
  endif ()
  target_link_libraries (${EXAMPLE_VARNAME}_f90_tutr_${example_name} ${H5EXAMPLE_LINK_Fortran_LIBS})
  set_target_properties (${EXAMPLE_VARNAME}_f90_tutr_${example_name} PROPERTIES LINKER_LANGUAGE Fortran)
endforeach ()

if (H5EXAMPLE_BUILD_TESTING)
  set (${EXAMPLE_VARNAME}_f90_tutr_CLEANFILES
      compound.h5
      copy1.h5
      copy2.h5
      dsetf.h5
      extend.h5
      FORTRAN.h5
      groupf.h5
      groupsf.h5
      h5_cmprss.h5
      mount1.h5
      mount2.h5
      sdsf.h5
      subset.h5
      SDScompound.h5
      test.h5
  )

  # Remove any output file left over from previous test run
  add_test (
      NAME ${EXAMPLE_VARNAME}_f90_tutr-clear-objects
      COMMAND ${CMAKE_COMMAND} -E remove ${${EXAMPLE_VARNAME}_f90_tutr_CLEANFILES}
  )
  set_tests_properties (${EXAMPLE_VARNAME}_f90_tutr-clear-objects PROPERTIES
      FIXTURES_SETUP clear_${EXAMPLE_VARNAME}_f90_tutr
      WORKING_DIRECTORY ${PROJECT_BINARY_DIR}
  )
  add_test (
      NAME ${EXAMPLE_VARNAME}_f90_tutr-clean-objects
      COMMAND ${CMAKE_COMMAND} -E remove ${${EXAMPLE_VARNAME}_f90_tutr_CLEANFILES}
  )
  set_tests_properties (${EXAMPLE_VARNAME}_f90_tutr-clean-objects PROPERTIES
      FIXTURES_CLEANUP clear_${EXAMPLE_VARNAME}_f90_tutr
      WORKING_DIRECTORY ${PROJECT_BINARY_DIR}
  )
  macro (ADD_H5_TEST testname)
    if (HDF5_USING_ANALYSIS_TOOL)
      add_test (NAME ${EXAMPLE_VARNAME}_f90_tutr_${testname} COMMAND $<TARGET_FILE:${EXAMPLE_VARNAME}_f90_tutr_${testname}>)
    else ()
      add_test (
          NAME ${EXAMPLE_VARNAME}_f90_tutr_${testname}
          COMMAND "${CMAKE_COMMAND}"
              -D "TEST_PROGRAM=$<TARGET_FILE:${EXAMPLE_VARNAME}_f90_tutr_${testname}>"
              -D "TEST_ARGS:STRING="
              -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
              -D "TEST_EXPECT=0"
              -D "TEST_SKIP_COMPARE=TRUE"
              -D "TEST_OUTPUT=${testname}.out"
              -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
              -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
      )
    endif ()
    set_tests_properties (${EXAMPLE_VARNAME}_f90_tutr_${testname} PROPERTIES
        FIXTURES_REQUIRED clear_${EXAMPLE_VARNAME}_f90_tutr
        ENVIRONMENT "${CROSSCOMPILING_PATH}"
    )
    if (last_test)
      set_tests_properties (${EXAMPLE_VARNAME}_f90_tutr_${testname} PROPERTIES DEPENDS ${last_test})
    endif ()
    set (last_test "${EXAMPLE_VARNAME}_f90_tutr_${testname}")
  endmacro ()

  #if (H5EXAMPLE_ENABLE_F2003)
    foreach (example_name ${f03_examples} ${common_examples})
      ADD_H5_TEST (${example_name})
    endforeach ()
  #endif ()
endif ()
```

### `HDF5Examples/FORTRAN/TUTR/Fortran_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (common_examples
    h5_cmprss
    h5_crtdat
    h5_rdwt
    h5_crtatt
    h5_crtgrp
    h5_crtgrpar
    h5_crtgrpd
    h5_extend
    h5_subset
    hyperslab
    selectele
    refobjexample
    refregexample
    mountexample
    compound
)

set (f03_examples
    rwdset_fortran2003
    nested_derived_type
    compound_fortran2003
    compound_complex_fortran2003
)
```

### `HDF5Examples/FORTRAN/test-pc.sh`

```bash
#! /bin/sh
#
# Copyright by The HDF Group.
# All rights reserved.
#
# This file is part of HDF5.  The full HDF5 copyright notice, including
# terms governing use, modification, and redistribution, is contained in
# the LICENSE file, which can be found at the root of the source code
# distribution tree, or in https://www.hdfgroup.org/licenses.
# If you do not have access to either file, you may request a copy from
# help@hdfgroup.org.

# This file is for use of h5cc created with the CMake process.
# Environment variable, HDF5_HOME is expected to be set.
# $1 is the path name of the source directory.
# $2 is the path name of the build directory.
# $3 is the current path name.

top_srcdir=$1
top_builddir=$2
currentpath=$3
verbose=yes
nerrors=0

return_val=0

echo "Current build directory: $top_builddir$currentpath"
# Loop through all subdirectories
for dir in */; do
  if [ -d "$dir" ] && [ ! -L "$dir" ]; then
    # Check if test-pc.sh exists
    if [ -f "$dir/test-pc.sh" ];
    then
        echo "Entering directory: $dir"
        (
            mkdir -p "$top_builddir/$currentpath/$dir"
            cd "$dir"
            ./test-pc.sh $top_srcdir $top_builddir $currentpath/$dir # Execute script in the subdirectory
            status=$?
            exit $status
        )
        return_val=$?
        if test $return_val -ne 0
        then
          echo "Exiting directory: $dir with $return_val tests FAILED"
        else
          echo "Exiting directory: $dir Passed"
        fi
        nerrors=`expr $return_val + $nerrors`
    fi
  fi
done

echo "$nerrors tests failed in $currentpath"
exit $nerrors
```

### `HDF5Examples/JAVA/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDFJAVA_EXAMPLES Java)

set_directory_properties(PROPERTIES INCLUDE_DIRECTORIES 
    "${HDFJAVA_LIB_DIR};${JAVA_INCLUDE_PATH};${JAVA_INCLUDE_PATH2}"
)

if (NOT HDF5_PROVIDES_JNI AND ${H5_LIBVER_DIR} GREATER 114)
  message (STATUS "HDF5 Java examples FFM support when using libver >= v1.14")
  add_subdirectory (H5D)
  add_subdirectory (H5T)
  add_subdirectory (H5G)
  add_subdirectory (TUTR)
endif ()

if (HDF5_PROVIDES_JAVA_COMPAT OR HDF5_PROVIDES_JNI)
  message (STATUS "HDF5 Java examples compat support")
  add_subdirectory (compat)
endif ()
```

### `HDF5Examples/JAVA/H5D/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_JAVA_H5D Java)

set (CMAKE_VERBOSE_MAKEFILE 1)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (Java_sourcefiles.cmake)

if (WIN32)
  set (CMAKE_JAVA_INCLUDE_FLAG_SEP ";")
else ()
  set (CMAKE_JAVA_INCLUDE_FLAG_SEP ":")
endif ()

set (CMAKE_JAVA_INCLUDE_PATH ".")
foreach (CMAKE_JINCLUDE_PATH ${HDF5_JAVA_INCLUDE_DIRS})
  set (CMAKE_JAVA_INCLUDE_PATH "${CMAKE_JAVA_INCLUDE_PATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${CMAKE_JINCLUDE_PATH}")
endforeach ()

set (CMAKE_JAVA_CLASSPATH ".")
foreach (CMAKE_INCLUDE_PATH ${HDF5_JAVA_INCLUDE_DIRS})
  set (CMAKE_JAVA_CLASSPATH "${CMAKE_JAVA_CLASSPATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${CMAKE_INCLUDE_PATH}")
endforeach ()

foreach (HCP_JAR ${HDF5_JAVA_INCLUDE_DIRS})
  get_filename_component (_HCP_FILE ${HCP_JAR} NAME)
  set (HDFJAVA_CLASSJARS "${_HCP_FILE} ${HDFJAVA_CLASSJARS}")
endforeach ()

foreach (example ${HDF_JAVA_EXAMPLES})
  get_filename_component (example_name ${example} NAME_WE)
  file (WRITE ${PROJECT_BINARY_DIR}/${example_name}_Manifest.txt
  "Main-Class: ${example_name}
Class-Path: ${HDFJAVA_CLASSJARS}
"
  )
  add_jar (${EXAMPLE_VARNAME}J_${example_name}
      SOURCES ${example}
      MANIFEST ${PROJECT_BINARY_DIR}/${example_name}_Manifest.txt
  )
  get_target_property (${EXAMPLE_VARNAME}J_${example_name}_JAR_FILE ${EXAMPLE_VARNAME}J_${example_name} JAR_FILE)
  get_target_property (${EXAMPLE_VARNAME}J_${example_name}_CLASSPATH ${EXAMPLE_VARNAME}J_${example_name} CLASSDIR)
  if (H5EXAMPLE_JAVA_LIBRARIES)
    add_dependencies (${EXAMPLE_VARNAME}J_${example_name} ${H5EXAMPLE_JAVA_LIBRARIES})
  endif ()
endforeach ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST resultfile resultcode)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${resultfile}
        COMMAND "${CMAKE_COMMAND}"
            -D "TEST_JAVA=${CMAKE_Java_RUNTIME};${CMAKE_Java_RUNTIME_FLAGS}"
            -D "TEST_PROGRAM=${resultfile}"
            -D "TEST_ARGS:STRING=${ARGN};${CMD_ARGS}"
            -D "TEST_CLASSPATH:STRING=${CMAKE_JAVA_CLASSPATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${${EXAMPLE_VARNAME}J_${resultfile}_JAR_FILE}"
            -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
            -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
            -D "TEST_OUTPUT=${PROJECT_BINARY_DIR}/${resultfile}.out"
            -D "TEST_REFERENCE=${resultfile}.txt"
            -D "TEST_EXPECT=${resultcode}"
            -D "TEST_SKIP_COMPARE=TRUE"
            -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
    )
    if (NOT "${last_test}" STREQUAL "")
      set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${resultfile} PROPERTIES DEPENDS ${last_test})
    endif ()
    set (last_test "${EXAMPLE_VARNAME}_jnative-h5-${resultfile}")
  endmacro ()

  foreach (example ${HDF_JAVA_EXAMPLES})
    get_filename_component (example_name ${example} NAME_WE)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clearall-objects
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${PROJECT_BINARY_DIR}/${example_name}.h5
            ${PROJECT_BINARY_DIR}/${example_name}.out
            ${PROJECT_BINARY_DIR}/${example_name}.out.err
    )
    if (NOT "${last_test}" STREQUAL "")
      set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clearall-objects PROPERTIES DEPENDS ${last_test})
    endif ()
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-copy-objects
        COMMAND    ${CMAKE_COMMAND}
            -E copy_if_different
            ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.txt
            ${PROJECT_BINARY_DIR}/${example_name}.txt
    )
    set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${example_name}-copy-objects PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clearall-objects)
    set (last_test "${EXAMPLE_VARNAME}_jnative-h5-${example_name}-copy-objects")
    ADD_H5_TEST (${example_name} 0)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clean-objects
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${PROJECT_BINARY_DIR}/${example_name}.h5
            ${PROJECT_BINARY_DIR}/${example_name}.out
            ${PROJECT_BINARY_DIR}/${example_name}.out.err
    )
    set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clean-objects PROPERTIES DEPENDS ${last_test})
  endforeach ()

endif ()
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_Alloc.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to set the space allocation time
  for a dataset.  The program first creates two datasets,
  one with the default allocation time (late) and one with
  early allocation time, and displays whether each has been
  allocated and their allocation size.  Next, it writes data
  to the datasets, and again displays whether each has been
  allocated and their allocation size.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_D_Alloc {
    private static String FILENAME     = "H5Ex_D_Alloc.h5";
    private static String DATASETNAME1 = "DS1";
    private static String DATASETNAME2 = "DS2";
    private static final int DIM_X     = 4;
    private static final int DIM_Y     = 7;
    private static final int FILLVAL   = 99;
    private static final int RANK      = 2;

    // Values for the status of space allocation
    enum H5D_space_status {
        H5D_SPACE_STATUS_ERROR(-1),
        H5D_SPACE_STATUS_NOT_ALLOCATED(0),
        H5D_SPACE_STATUS_PART_ALLOCATED(1),
        H5D_SPACE_STATUS_ALLOCATED(2);
        private static final Map<Integer, H5D_space_status> lookup = new HashMap<Integer, H5D_space_status>();

        static
        {
            for (H5D_space_status s : EnumSet.allOf(H5D_space_status.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5D_space_status(int space_status) { this.code = space_status; }

        public int getCode() { return this.code; }

        public static H5D_space_status get(int code) { return lookup.get(code); }
    }

    private static void allocation(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id1  = H5I_INVALID_HID();
        long dataset_id2  = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];
        int space_status  = 0;
        long storage_size = 0;

        // Initialize the dataset.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = FILLVAL;

        // Create a file using default properties.
        try {
            MemorySegment filename = arena.allocateFrom(FILENAME);
            file_id                = H5Fcreate(filename, H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
            filespace_id          = H5Screate_simple(RANK, dimsSeg, MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list, and set the chunk size.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Set the allocation time to "early". This way we can be sure
        // that reading from the dataset immediately after creation will
        // return the fill value.
        try {
            if (dcpl_id >= 0)
                H5Pset_alloc_time(dcpl_id, H5D_ALLOC_TIME_EARLY());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        System.out.println("Creating datasets...");
        System.out.println(DATASETNAME1 + " has allocation time H5D_ALLOC_TIME_LATE");
        System.out.println(DATASETNAME2 + " has allocation time H5D_ALLOC_TIME_EARLY");
        System.out.println();

        // Create the dataset using the dataset default creation property list.
        try {
            if ((file_id >= 0) && (filespace_id >= 0)) {
                MemorySegment datasetname1 = arena.allocateFrom(DATASETNAME1);
                dataset_id1 = H5Dcreate2(file_id, datasetname1, H5T_NATIVE_INT_g(), filespace_id,
                                         H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset using the dataset creation property list.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0)) {
                MemorySegment datasetname2 = arena.allocateFrom(DATASETNAME2);
                dataset_id2 = H5Dcreate2(file_id, datasetname2, H5T_NATIVE_INT_g(), filespace_id,
                                         H5P_DEFAULT(), dcpl_id, H5P_DEFAULT());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print space status and storage size for dset1.
        try {
            if (dataset_id1 >= 0) {
                MemorySegment statusSeg = arena.allocate(ValueLayout.JAVA_INT);
                H5Dget_space_status(dataset_id1, statusSeg);
                space_status = statusSeg.get(ValueLayout.JAVA_INT, 0);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (dataset_id1 >= 0)
                storage_size = H5Dget_storage_size(dataset_id1);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        String the_space = " ";
        if (H5D_space_status.get(space_status) != H5D_space_status.H5D_SPACE_STATUS_ALLOCATED)
            the_space += "not ";
        System.out.println("Space for " + DATASETNAME1 + " has" + the_space + "been allocated.");
        System.out.println("Storage size for " + DATASETNAME1 + " is: " + storage_size + " bytes.");

        // Retrieve and print space status and storage size for dset2.
        try {
            if (dataset_id2 >= 0) {
                MemorySegment statusSeg = arena.allocate(ValueLayout.JAVA_INT);
                H5Dget_space_status(dataset_id2, statusSeg);
                space_status = statusSeg.get(ValueLayout.JAVA_INT, 0);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (dataset_id2 >= 0)
                storage_size = H5Dget_storage_size(dataset_id2);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        the_space = " ";
        if (H5D_space_status.get(space_status) != H5D_space_status.H5D_SPACE_STATUS_ALLOCATED)
            the_space += "not ";
        System.out.println("Space for " + DATASETNAME2 + " has" + the_space + "been allocated.");
        System.out.println("Storage size for " + DATASETNAME2 + " is: " + storage_size + " bytes.");
        System.out.println();

        System.out.println("Writing data...");
        System.out.println();

        // Write the data to the datasets.
        try {
            if (dataset_id1 >= 0) {
                // Flatten 2D array to 1D for MemorySegment
                int[] flatData = new int[DIM_X * DIM_Y];
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        flatData[i * DIM_Y + j] = dset_data[i][j];
                    }
                }
                // Copy to MemorySegment
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);
                for (int i = 0; i < flatData.length; i++) {
                    dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
                }
                H5Dwrite(dataset_id1, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (dataset_id2 >= 0) {
                // Flatten 2D array to 1D for MemorySegment
                int[] flatData = new int[DIM_X * DIM_Y];
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        flatData[i * DIM_Y + j] = dset_data[i][j];
                    }
                }
                // Copy to MemorySegment
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);
                for (int i = 0; i < flatData.length; i++) {
                    dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
                }
                H5Dwrite(dataset_id2, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print space status and storage size for dset1.
        try {
            if (dataset_id1 >= 0) {
                MemorySegment statusSeg = arena.allocate(ValueLayout.JAVA_INT);
                H5Dget_space_status(dataset_id1, statusSeg);
                space_status = statusSeg.get(ValueLayout.JAVA_INT, 0);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (dataset_id1 >= 0)
                storage_size = H5Dget_storage_size(dataset_id1);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        the_space = " ";
        if (H5D_space_status.get(space_status) != H5D_space_status.H5D_SPACE_STATUS_ALLOCATED)
            the_space += "not ";
        System.out.println("Space for " + DATASETNAME1 + " has" + the_space + "been allocated.");
        System.out.println("Storage size for " + DATASETNAME1 + " is: " + storage_size + " bytes.");

        // Retrieve and print space status and storage size for dset2.
        try {
            if (dataset_id2 >= 0) {
                MemorySegment statusSeg = arena.allocate(ValueLayout.JAVA_INT);
                H5Dget_space_status(dataset_id2, statusSeg);
                space_status = statusSeg.get(ValueLayout.JAVA_INT, 0);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (dataset_id2 >= 0)
                storage_size = H5Dget_storage_size(dataset_id2);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        the_space = " ";
        if (H5D_space_status.get(space_status) != H5D_space_status.H5D_SPACE_STATUS_ALLOCATED)
            the_space += "not ";
        System.out.println("Space for " + DATASETNAME2 + " has" + the_space + "been allocated.");
        System.out.println("Storage size for " + DATASETNAME2 + " is: " + storage_size + " bytes.");
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id1 >= 0)
                H5Dclose(dataset_id1);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id2 >= 0)
                H5Dclose(dataset_id2);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            H5Ex_D_Alloc.allocation(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_Checksum.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write data to a dataset
  using the Fletcher32 checksum filter.  The program first
  checks if the Fletcher32 filter is available, then if it
  is it writes integers to a dataset using Fletcher32, then
  closes the file.  Next, it reopens the file, reads back
  the data, checks if the filter detected an error and
  outputs the type of filter and the maximum value in the
  dataset to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_D_Checksum {
    private static String FILENAME    = "H5Ex_D_Checksum.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 32;
    private static final int DIM_Y    = 64;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 8;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;

    // Values for the status of space allocation
    enum H5Z_filter {
        H5Z_FILTER_ERROR(-1),
        H5Z_FILTER_NONE(0),
        H5Z_FILTER_DEFLATE(1),
        H5Z_FILTER_SHUFFLE(2),
        H5Z_FILTER_FLETCHER32(3),
        H5Z_FILTER_SZIP(4),
        H5Z_FILTER_NBIT(5),
        H5Z_FILTER_SCALEOFFSET(6),
        H5Z_FILTER_RESERVED(256),
        H5Z_FILTER_MAX(65535);
        private static final Map<Integer, H5Z_filter> lookup = new HashMap<Integer, H5Z_filter>();

        static
        {
            for (H5Z_filter s : EnumSet.allOf(H5Z_filter.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5Z_filter(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5Z_filter get(int code) { return lookup.get(code); }
    }

    private static boolean checkFletcher32Filter()
    {
        try {
            int available = H5Zfilter_avail(H5Z_filter.H5Z_FILTER_FLETCHER32.getCode());
            if (available == 0) {
                System.out.println("N-Bit filter not available.");
                return false;
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            // Note: checkFletcher32Filter doesn't have Arena, using Arena.ofConfined() locally
            try (Arena arena = Arena.ofConfined()) {
                MemorySegment filterInfoSeg = arena.allocate(ValueLayout.JAVA_INT);
                H5Zget_filter_info(H5Z_FILTER_FLETCHER32(), filterInfoSeg);
                int filter_info = filterInfoSeg.get(ValueLayout.JAVA_INT, 0);
                if (((filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED()) == 0) ||
                    ((filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED()) == 0)) {
                    System.out.println("N-Bit filter not available for encoding and decoding.");
                    return false;
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        return true;
    }

    private static void writeChecksum(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        long[] chunk_dims = {CHUNK_X, CHUNK_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            filespace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list, add the N-Bit filter.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
            if (dcpl_id >= 0) {
                H5Pset_fletcher32(dcpl_id);
                // Set the chunk size.
                H5Pset_chunk(dcpl_id, NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, chunk_dims));
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        filespace_id, H5P_DEFAULT(), dcpl_id, H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten 2D array for FFM
                int[] flatData = new int[DIM_X * DIM_Y];
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        flatData[i * DIM_Y + j] = dset_data[i][j];
                    }
                }
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);
                for (int i = 0; i < flatData.length; i++) {
                    dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
                }
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readChecksum(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print the filter type. Here we only retrieve the
        // first filter because we know that we only added one filter.
        try {
            if (dcpl_id >= 0) {
                // Allocate MemorySegments for output parameters
                MemorySegment flagsSeg   = arena.allocate(ValueLayout.JAVA_INT);
                MemorySegment cdNeltsSeg = arena.allocate(ValueLayout.JAVA_LONG);
                cdNeltsSeg.set(ValueLayout.JAVA_LONG, 0, 10L); // max cd_values
                MemorySegment cdValuesSeg     = arena.allocate(ValueLayout.JAVA_INT, 10);
                MemorySegment nameSegment     = arena.allocate(256);
                MemorySegment filterConfigSeg = arena.allocate(ValueLayout.JAVA_INT);

                int filter_type = H5Pget_filter2(dcpl_id, 0, flagsSeg, cdNeltsSeg, cdValuesSeg, 256,
                                                 nameSegment, filterConfigSeg);
                System.out.print("Filter type is: ");
                switch (H5Z_filter.get(filter_type)) {
                case H5Z_FILTER_DEFLATE:
                    System.out.println("H5Z_FILTER_DEFLATE");
                    break;
                case H5Z_FILTER_SHUFFLE:
                    System.out.println("H5Z_FILTER_SHUFFLE");
                    break;
                case H5Z_FILTER_FLETCHER32:
                    System.out.println("H5Z_FILTER_FLETCHER32");
                    break;
                case H5Z_FILTER_SZIP:
                    System.out.println("H5Z_FILTER_SZIP");
                    break;
                default:
                    System.out.println("H5Z_FILTER_ERROR");
                }
                System.out.println();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);
                int status =
                    H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Check if the read was successful. Normally we do not perform
                // error checking in these examples for the sake of clarity, but in
                // this case we will make an exception because this is how the
                // fletcher32 checksum filter reports data errors.
                if (status < 0) {
                    System.out.print("Dataset read failed!");
                    try {
                        if (dcpl_id >= 0)
                            H5Pclose(dcpl_id);
                        if (dataset_id >= 0)
                            H5Dclose(dataset_id);
                        if (file_id >= 0)
                            H5Fclose(file_id);
                    }
                    catch (Exception e) {
                        e.printStackTrace();
                    }
                    return;
                }
                // Unflatten to 2D array
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Find the maximum value in the dataset, to verify that it was read
        // correctly.
        int max = dset_data[0][0];
        for (int indx = 0; indx < DIM_X; indx++) {
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                if (max < dset_data[indx][jndx])
                    max = dset_data[indx][jndx];
        }
        // Print the maximum value.
        System.out.println("Maximum value in " + DATASETNAME + " is: " + max);

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            if (H5Ex_D_Checksum.checkFletcher32Filter()) {
                H5Ex_D_Checksum.writeChecksum(arena);
                H5Ex_D_Checksum.readChecksum(arena);
            }
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_Chunk.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to create a chunked dataset.  The
  program first writes integers in a hyperslab selection to
  a chunked dataset with dataspace dimensions of DIM_XxDIM_Y
  and chunk size of CHUNK_XxCHUNK_Y, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs it to the screen.  Finally it reads the data again
  using a different hyperslab selection, and outputs
  the result to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_D_Chunk {
    private static String FILENAME    = "H5Ex_D_Chunk.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 6;
    private static final int DIM_Y    = 8;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 4;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;

    // Values for the status of space allocation
    enum H5D_layout {
        H5D_LAYOUT_ERROR(-1),
        H5D_COMPACT(0),
        H5D_CONTIGUOUS(1),
        H5D_CHUNKED(2),
        H5D_VIRTUAL(3),
        H5D_NLAYOUTS(4);
        private static final Map<Integer, H5D_layout> lookup = new HashMap<Integer, H5D_layout>();

        static
        {
            for (H5D_layout s : EnumSet.allOf(H5D_layout.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5D_layout(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5D_layout get(int code) { return lookup.get(code); }
    }

    private static void writeChunk(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        long[] chunk_dims = {CHUNK_X, CHUNK_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data to "1", to make it easier to see the selections.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = 1;

        // Print the data to the screen.
        System.out.println("Original Data:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // Create a new file using default properties.
        try {
            MemorySegment filename = arena.allocateFrom(FILENAME);
            file_id                = H5Fcreate(filename, H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
            filespace_id          = H5Screate_simple(RANK, dimsSeg, MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Set the chunk size.
        try {
            if (dcpl_id >= 0) {
                MemorySegment chunkDimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, chunk_dims);
                H5Pset_chunk(dcpl_id, NDIMS, chunkDimsSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the chunked dataset.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0)) {
                MemorySegment datasetname = arena.allocateFrom(DATASETNAME);
                dataset_id = H5Dcreate2(file_id, datasetname, H5T_STD_I32LE_g(), filespace_id, H5P_DEFAULT(),
                                        dcpl_id, H5P_DEFAULT());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Define and select the first part of the hyperslab selection.
        long[] start  = {0, 0};
        long[] stride = {3, 3};
        long[] count  = {2, 3};
        long[] block  = {2, 2};
        try {
            if ((filespace_id >= 0)) {
                MemorySegment startSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, start);
                MemorySegment strideSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, stride);
                MemorySegment countSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, count);
                MemorySegment blockSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, block);
                H5Sselect_hyperslab(filespace_id, H5S_SELECT_SET(), startSeg, strideSeg, countSeg, blockSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        // Define and select the second part of the hyperslab selection,
        // which is subtracted from the first selection by the use of
        // H5S_SELECT_NOTB
        block[0] = 1;
        block[1] = 1;
        try {
            if ((filespace_id >= 0)) {
                MemorySegment startSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, start);
                MemorySegment strideSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, stride);
                MemorySegment countSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, count);
                MemorySegment blockSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, block);
                H5Sselect_hyperslab(filespace_id, H5S_SELECT_NOTB(), startSeg, strideSeg, countSeg, blockSeg);

                // Write the data to the dataset.
                if (dataset_id >= 0) {
                    // Flatten 2D array for FFM
                    int[] flatData = new int[DIM_X * DIM_Y];
                    for (int i = 0; i < DIM_X; i++) {
                        for (int j = 0; j < DIM_Y; j++) {
                            flatData[i * DIM_Y + j] = dset_data[i][j];
                        }
                    }
                    MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);
                    for (int i = 0; i < flatData.length; i++) {
                        dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
                    }
                    H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), filespace_id, H5P_DEFAULT(), dataSeg);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readChunk(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open an existing file.
        try {
            MemorySegment filename = arena.allocateFrom(FILENAME);
            file_id                = H5Fopen(filename, H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0) {
                MemorySegment datasetname = arena.allocateFrom(DATASETNAME);
                dataset_id                = H5Dopen2(file_id, datasetname, H5P_DEFAULT());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Print the storage layout.
        try {
            if (dcpl_id >= 0) {
                int layout_type = H5Pget_layout(dcpl_id);
                System.out.print("Storage layout for " + DATASETNAME + " is: ");
                switch (H5D_layout.get(layout_type)) {
                case H5D_COMPACT:
                    System.out.println("H5D_COMPACT");
                    break;
                case H5D_CONTIGUOUS:
                    System.out.println("H5D_CONTIGUOUS");
                    break;
                case H5D_CHUNKED:
                    System.out.println("H5D_CHUNKED");
                    break;
                case H5D_VIRTUAL:
                    System.out.println("H5D_VIRTUAL");
                    break;
                case H5D_LAYOUT_ERROR:
                    break;
                case H5D_NLAYOUTS:
                    break;
                default:
                    break;
                }
                System.out.println();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten to 2D array
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Data as written to disk by hyberslabs:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // Initialize the read array.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = 0;

        // Define and select the hyperslab to use for reading.
        try {
            if (dataset_id >= 0) {
                filespace_id = H5Dget_space(dataset_id);

                long[] start  = {0, 1};
                long[] stride = {4, 4};
                long[] count  = {2, 2};
                long[] block  = {2, 3};

                if (filespace_id >= 0) {
                    MemorySegment startSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, start);
                    MemorySegment strideSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, stride);
                    MemorySegment countSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, count);
                    MemorySegment blockSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, block);
                    H5Sselect_hyperslab(filespace_id, H5S_SELECT_SET(), startSeg, strideSeg, countSeg,
                                        blockSeg);

                    // Read the data using the previously defined hyperslab.
                    if ((dataset_id >= 0) && (filespace_id >= 0)) {
                        MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);
                        H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), filespace_id, H5P_DEFAULT(),
                                dataSeg);
                        // Unflatten to 2D array
                        for (int i = 0; i < DIM_X; i++) {
                            for (int j = 0; j < DIM_Y; j++) {
                                dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                            }
                        }
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Data as read from disk by hyberslab:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            H5Ex_D_Chunk.writeChunk(arena);
            H5Ex_D_Chunk.readChunk(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_Compact.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write data to a compact
  dataset.  The program first writes integers to a compact
  dataset with dataspace dimensions of DIM_XxDIM_Y, then
  closes the file.  Next, it reopens the file, reads back
  the data, and outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_D_Compact {
    private static String FILENAME    = "H5Ex_D_Compact.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 4;
    private static final int DIM_Y    = 7;
    private static final int RANK     = 2;

    // Values for the status of space allocation
    enum H5D_layout {
        H5D_LAYOUT_ERROR(-1),
        H5D_COMPACT(0),
        H5D_CONTIGUOUS(1),
        H5D_CHUNKED(2),
        H5D_VIRTUAL(3),
        H5D_NLAYOUTS(4);
        private static final Map<Integer, H5D_layout> lookup = new HashMap<Integer, H5D_layout>();

        static
        {
            for (H5D_layout s : EnumSet.allOf(H5D_layout.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5D_layout(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5D_layout get(int code) { return lookup.get(code); }
    }

    private static void writeCompact(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            MemorySegment filename = arena.allocateFrom(FILENAME);
            file_id                = H5Fcreate(filename, H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
            filespace_id          = H5Screate_simple(RANK, dimsSeg, MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Set the layout to compact.
        try {
            if (dcpl_id >= 0)
                H5Pset_layout(dcpl_id, H5D_layout.H5D_COMPACT.getCode());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset. We will use all default properties for this example.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0)) {
                MemorySegment datasetname = arena.allocateFrom(DATASETNAME);
                dataset_id = H5Dcreate2(file_id, datasetname, H5T_STD_I32LE_g(), filespace_id, H5P_DEFAULT(),
                                        dcpl_id, H5P_DEFAULT());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten 2D array for FFM
                int[] flatData = new int[DIM_X * DIM_Y];
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        flatData[i * DIM_Y + j] = dset_data[i][j];
                    }
                }
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);
                for (int i = 0; i < flatData.length; i++) {
                    dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
                }
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readCompact(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open file and dataset using the default properties.
        try {
            MemorySegment filename = arena.allocateFrom(FILENAME);
            file_id                = H5Fopen(filename, H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0) {
                MemorySegment datasetname = arena.allocateFrom(DATASETNAME);
                dataset_id                = H5Dopen2(file_id, datasetname, H5P_DEFAULT());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Print the storage layout.
        try {
            if (dcpl_id >= 0) {
                int layout_type = H5Pget_layout(dcpl_id);
                System.out.print("Storage layout for " + DATASETNAME + " is: ");
                switch (H5D_layout.get(layout_type)) {
                case H5D_COMPACT:
                    System.out.println("H5D_COMPACT");
                    break;
                case H5D_CONTIGUOUS:
                    System.out.println("H5D_CONTIGUOUS");
                    break;
                case H5D_CHUNKED:
                    System.out.println("H5D_CHUNKED");
                    break;
                case H5D_VIRTUAL:
                    System.out.println("H5D_VIRTUAL");
                    break;
                case H5D_LAYOUT_ERROR:
                    break;
                case H5D_NLAYOUTS:
                    break;
                default:
                    break;
                }
                System.out.println();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten to 2D array
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Data for " + DATASETNAME + " is: ");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            H5Ex_D_Compact.writeCompact(arena);
            H5Ex_D_Compact.readCompact(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_External.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write data to an
  external dataset.  The program first writes integers to an
  external dataset with dataspace dimensions of DIM_XxDIM_Y,
  then closes the file.  Next, it reopens the file, reads
  back the data, and outputs the name of the external data
  file and the data to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_D_External {
    private static String FILENAME         = "H5Ex_D_External.h5";
    private static String EXTERNALNAME     = "H5Ex_D_External.data";
    private static String DATASETNAME      = "DS1";
    private static final int DIM_X         = 4;
    private static final int DIM_Y         = 7;
    private static final int RANK          = 2;
    private static final int NAME_BUF_SIZE = 32;

    private static void writeExternal(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize the dataset.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            filespace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // set the external file.
        try {
            if (dcpl_id >= 0)
                H5Pset_external(dcpl_id, arena.allocateFrom(EXTERNALNAME), 0, H5F_UNLIMITED());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the HDF5Constants.dataset.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        filespace_id, H5P_DEFAULT(), dcpl_id, H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the dataset.
        try {
            // Flatten 2D array for FFM
            int[] flatData = new int[DIM_X * DIM_Y];
            for (int i = 0; i < DIM_X; i++) {
                for (int j = 0; j < DIM_Y; j++) {
                    flatData[i * DIM_Y + j] = dset_data[i][j];
                }
            }
            MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);
            for (int i = 0; i < flatData.length; i++) {
                dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
            }
            H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readExternal(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        int[][] dset_data = new int[DIM_X][DIM_Y];
        String[] Xname    = new String[1];

        // Open file using the default properties.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDWR(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open dataset using the default properties.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print the name of the external file.
        String externalFileName = "";
        try {
            if (dcpl_id >= 0) {
                MemorySegment namelenSeg = arena.allocate(ValueLayout.JAVA_LONG);
                MemorySegment nameSeg    = arena.allocate(256);
                MemorySegment offsetSeg  = arena.allocate(ValueLayout.JAVA_LONG);
                MemorySegment sizeSeg    = arena.allocate(ValueLayout.JAVA_LONG);
                H5Pget_external(dcpl_id, 0, 256, nameSeg, offsetSeg, sizeSeg);
                externalFileName = nameSeg.getString(0);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        System.out.println(DATASETNAME + " is stored in file: " + externalFileName);

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten to 2D array
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println(DATASETNAME + ":");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // Close the dataset.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            H5Ex_D_External.writeExternal(arena);
            H5Ex_D_External.readExternal(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_FillValue.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to set the fill value for a
  dataset.  The program first sets the fill value to
  FILLVAL, creates a dataset with dimensions of DIM_XxDIM_Y,
  reads from the uninitialized dataset, and outputs the
  contents to the screen.  Next, it writes integers to the
  dataset, reads the data back, and outputs it to the
  screen.  Finally it extends the dataset, reads from it,
  and outputs the result to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_D_FillValue {
    private static String FILENAME    = "H5Ex_D_FillValue.h5";
    private static String DATASETNAME = "ExtendibleArray";
    private static final int DIM_X    = 4;
    private static final int DIM_Y    = 7;
    private static final int EDIM_X   = 6;
    private static final int EDIM_Y   = 10;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 4;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;
    private static final int FILLVAL  = 99;

    private static void fillValue(Arena arena)
    {
        long file_id             = H5I_INVALID_HID();
        long dcpl_id             = H5I_INVALID_HID();
        long dataspace_id        = H5I_INVALID_HID();
        long dataset_id          = H5I_INVALID_HID();
        long[] dims              = {DIM_X, DIM_Y};
        long[] extdims           = {EDIM_X, EDIM_Y};
        long[] chunk_dims        = {CHUNK_X, CHUNK_Y};
        long[] maxdims           = {H5S_UNLIMITED(), H5S_UNLIMITED()};
        int[][] write_dset_data  = new int[DIM_X][DIM_Y];
        int[][] read_dset_data   = new int[DIM_X][DIM_Y];
        int[][] extend_dset_data = new int[EDIM_X][EDIM_Y];

        // Initialize the dataset.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                write_dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            MemorySegment filename = arena.allocateFrom(FILENAME);
            file_id                = H5Fcreate(filename, H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace with unlimited dimensions.
        try {
            MemorySegment dimsSeg    = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
            MemorySegment maxdimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, maxdims);
            dataspace_id             = H5Screate_simple(RANK, dimsSeg, maxdimsSeg);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Set the chunk size.
        try {
            if (dcpl_id >= 0) {
                MemorySegment chunkDimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, chunk_dims);
                H5Pset_chunk(dcpl_id, NDIMS, chunkDimsSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Set the fill value for the dataset
        try {
            if (dcpl_id >= 0) {
                int[] fill_value           = {FILLVAL};
                MemorySegment fillValueSeg = arena.allocateFrom(ValueLayout.JAVA_INT, fill_value);
                H5Pset_fill_value(dcpl_id, H5T_NATIVE_INT_g(), fillValueSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Set the allocation time to "early". This way we can be sure
        // that reading from the dataset immediately after creation will
        // return the fill value.
        try {
            if (dcpl_id >= 0)
                H5Pset_alloc_time(dcpl_id, H5D_ALLOC_TIME_EARLY());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset using the dataset creation property list.
        try {
            if ((file_id >= 0) && (dataspace_id >= 0) && (dcpl_id >= 0)) {
                MemorySegment datasetname = arena.allocateFrom(DATASETNAME);
                dataset_id = H5Dcreate2(file_id, datasetname, H5T_STD_I32LE_g(), dataspace_id, H5P_DEFAULT(),
                                        dcpl_id, H5P_DEFAULT());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read values from the dataset, which has not been written to yet.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten to 2D array
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        read_dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Dataset before being written to:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(read_dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten 2D array for FFM
                int[] flatData = new int[DIM_X * DIM_Y];
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        flatData[i * DIM_Y + j] = write_dset_data[i][j];
                    }
                }
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);
                for (int i = 0; i < flatData.length; i++) {
                    dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
                }
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data back.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten to 2D array
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        read_dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Dataset after being written to:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(read_dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // Extend the dataset.
        try {
            if (dataset_id >= 0) {
                MemorySegment extdimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, extdims);
                H5Dset_extent(dataset_id, extdimsSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read from the extended dataset.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, EDIM_X * EDIM_Y);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten to 2D array
                for (int i = 0; i < EDIM_X; i++) {
                    for (int j = 0; j < EDIM_Y; j++) {
                        extend_dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * EDIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Dataset after extension:");
        for (int indx = 0; indx < EDIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < EDIM_Y; jndx++)
                System.out.print(extend_dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            H5Ex_D_FillValue.fillValue(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_Gzip.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write data to a dataset
  using gzip compression (also called zlib or deflate).  The
  program first checks if gzip compression is available,
  then if it is it writes integers to a dataset using gzip,
  then closes the file.  Next, it reopens the file, reads
  back the data, and outputs the type of compression and the
  maximum value in the dataset to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_D_Gzip {
    private static String FILENAME    = "H5Ex_D_Gzip.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 32;
    private static final int DIM_Y    = 64;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 8;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;

    // Values for the status of space allocation
    enum H5Z_filter {
        H5Z_FILTER_ERROR(H5Z_FILTER_ERROR()),
        H5Z_FILTER_NONE(H5Z_FILTER_NONE()),
        H5Z_FILTER_DEFLATE(H5Z_FILTER_DEFLATE()),
        H5Z_FILTER_SHUFFLE(H5Z_FILTER_SHUFFLE()),
        H5Z_FILTER_FLETCHER32(H5Z_FILTER_FLETCHER32()),
        H5Z_FILTER_SZIP(H5Z_FILTER_SZIP()),
        H5Z_FILTER_NBIT(H5Z_FILTER_NBIT()),
        H5Z_FILTER_SCALEOFFSET(H5Z_FILTER_SCALEOFFSET()),
        H5Z_FILTER_RESERVED(H5Z_FILTER_RESERVED()),
        H5Z_FILTER_MAX(H5Z_FILTER_MAX());
        private static final Map<Integer, H5Z_filter> lookup = new HashMap<Integer, H5Z_filter>();

        static
        {
            for (H5Z_filter s : EnumSet.allOf(H5Z_filter.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5Z_filter(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5Z_filter get(int code) { return lookup.get(code); }
    }

    private static boolean checkGzipFilter()
    {
        try {
            int available = H5Zfilter_avail(H5Z_FILTER_DEFLATE());
            if (available == 0) {
                System.out.println("gzip filter not available.");
                return false;
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            try (Arena arena = Arena.ofConfined()) {
                MemorySegment filterInfoSeg = arena.allocate(ValueLayout.JAVA_INT);
                H5Zget_filter_info(H5Z_FILTER_DEFLATE(), filterInfoSeg);
                int filter_info = filterInfoSeg.get(ValueLayout.JAVA_INT, 0);
                if (((filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED()) == 0) ||
                    ((filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED()) == 0)) {
                    System.out.println("gzip filter not available for encoding and decoding.");
                    return false;
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        return true;
    }

    private static void writeGzip(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        long[] chunk_dims = {CHUNK_X, CHUNK_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            filespace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list, add the gzip compression
        // filter.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
            if (dcpl_id >= 0) {
                H5Pset_deflate(dcpl_id, 9);
                // Set the chunk size.
                H5Pset_chunk(dcpl_id, NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, chunk_dims));
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        filespace_id, H5P_DEFAULT(), dcpl_id, H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten 2D array for FFM

                int[] flatData = new int[DIM_X * DIM_Y];

                for (int i = 0; i < DIM_X; i++) {

                    for (int j = 0; j < DIM_Y; j++) {

                        flatData[i * DIM_Y + j] = dset_data[i][j];
                    }
                }

                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);

                for (int i = 0; i < flatData.length; i++) {

                    dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
                }

                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readGzip(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print the filter type. Here we only retrieve the
        // first filter because we know that we only added one filter.
        try {
            if (dcpl_id >= 0) {
                // FFM requires MemorySegment parameters
                MemorySegment flagsSeg   = arena.allocate(ValueLayout.JAVA_INT);
                MemorySegment cdNeltsSeg = arena.allocate(ValueLayout.JAVA_LONG);
                cdNeltsSeg.set(ValueLayout.JAVA_LONG, 0, 10L);
                MemorySegment cdValuesSeg     = arena.allocate(ValueLayout.JAVA_INT, 10);
                MemorySegment nameSegment     = arena.allocate(256);
                MemorySegment filterConfigSeg = arena.allocate(ValueLayout.JAVA_INT);
                int filter_type = H5Pget_filter2(dcpl_id, 0, flagsSeg, cdNeltsSeg, cdValuesSeg, 256,
                                                 nameSegment, filterConfigSeg);
                System.out.print("Filter type is: ");
                switch (H5Z_filter.get(filter_type)) {
                case H5Z_FILTER_DEFLATE:
                    System.out.println("H5Z_FILTER_DEFLATE");
                    break;
                case H5Z_FILTER_SHUFFLE:
                    System.out.println("H5Z_FILTER_SHUFFLE");
                    break;
                case H5Z_FILTER_FLETCHER32:
                    System.out.println("H5Z_FILTER_FLETCHER32");
                    break;
                case H5Z_FILTER_SZIP:
                    System.out.println("H5Z_FILTER_SZIP");
                    break;
                case H5Z_FILTER_NBIT:
                    System.out.println("H5Z_FILTER_NBIT");
                    break;
                case H5Z_FILTER_SCALEOFFSET:
                    System.out.println("H5Z_FILTER_SCALEOFFSET");
                    break;
                default:
                    System.out.println("H5Z_FILTER_ERROR");
                }
                System.out.println();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);

                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);

                // Unflatten to 2D array

                for (int i = 0; i < DIM_X; i++) {

                    for (int j = 0; j < DIM_Y; j++) {

                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Find the maximum value in the dataset, to verify that it was read
        // correctly.
        int max = dset_data[0][0];
        for (int indx = 0; indx < DIM_X; indx++) {
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                if (max < dset_data[indx][jndx])
                    max = dset_data[indx][jndx];
        }
        // Print the maximum value.
        System.out.println("Maximum value in " + DATASETNAME + " is: " + max);

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            if (H5Ex_D_Gzip.checkGzipFilter()) {
                H5Ex_D_Gzip.writeGzip(arena);
                H5Ex_D_Gzip.readGzip(arena);
            }
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_Hyperslab.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write data to a
  dataset by hyberslabs.  The program first writes integers
  in a hyperslab selection to a dataset with dataspace
  dimensions of DIM_XxDIM_Y, then closes the file.  Next, it
  reopens the file, reads back the data, and outputs it to
  the screen.  Finally it reads the data again using a
  different hyperslab selection, and outputs the result to
  the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_D_Hyperslab {
    private static String FILENAME    = "H5Ex_D_Hyperslab.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 6;
    private static final int DIM_Y    = 8;
    private static final int RANK     = 2;

    private static void writeHyperslab(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data to "1", to make it easier to see the selections.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = 1;

        // Print the data to the screen.
        System.out.println("Original Data:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            filespace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset. We will use all default properties for this example.
        try {
            if ((file_id >= 0) && (filespace_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        filespace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Define and select the first part of the hyperslab selection.
        long[] start  = {0, 0};
        long[] stride = {3, 3};
        long[] count  = {2, 3};
        long[] block  = {2, 2};
        try {
            if ((filespace_id >= 0)) {
                MemorySegment startSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, start);
                MemorySegment strideSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, stride);
                MemorySegment countSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, count);
                MemorySegment blockSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, block);
                H5Sselect_hyperslab(filespace_id, H5S_SELECT_SET(), startSeg, strideSeg, countSeg, blockSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        // Define and select the second part of the hyperslab selection,
        // which is subtracted from the first selection by the use of
        // H5S_SELECT_NOTB
        block[0] = 1;
        block[1] = 1;
        try {
            if ((filespace_id >= 0)) {
                MemorySegment startSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, start);
                MemorySegment strideSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, stride);
                MemorySegment countSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, count);
                MemorySegment blockSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, block);
                H5Sselect_hyperslab(filespace_id, H5S_SELECT_NOTB(), startSeg, strideSeg, countSeg, blockSeg);

                // Write the data to the dataset.
                if (dataset_id >= 0) {
                    // Flatten 2D array for FFM
                    int[] flatData = new int[DIM_X * DIM_Y];
                    for (int i = 0; i < DIM_X; i++) {
                        for (int j = 0; j < DIM_Y; j++) {
                            flatData[i * DIM_Y + j] = dset_data[i][j];
                        }
                    }
                    MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);
                    for (int i = 0; i < flatData.length; i++) {
                        dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
                    }
                    H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), filespace_id, H5P_DEFAULT(), dataSeg);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readHyperslab(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten to 2D array
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Data as written to disk by hyberslabs:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // Initialize the read array.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = 0;

        // Define and select the hyperslab to use for reading.
        try {
            if (dataset_id >= 0) {
                filespace_id = H5Dget_space(dataset_id);

                long[] start  = {0, 1};
                long[] stride = {4, 4};
                long[] count  = {2, 2};
                long[] block  = {2, 3};

                if (filespace_id >= 0) {
                    MemorySegment startSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, start);
                    MemorySegment strideSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, stride);
                    MemorySegment countSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, count);
                    MemorySegment blockSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, block);
                    H5Sselect_hyperslab(filespace_id, H5S_SELECT_SET(), startSeg, strideSeg, countSeg,
                                        blockSeg);

                    // Read the data using the previously defined hyperslab.
                    if ((dataset_id >= 0) && (filespace_id >= 0)) {
                        MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);
                        H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), filespace_id, H5P_DEFAULT(),
                                dataSeg);
                        // Unflatten to 2D array
                        for (int i = 0; i < DIM_X; i++) {
                            for (int j = 0; j < DIM_Y; j++) {
                                dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                            }
                        }
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Data as read from disk by hyberslab:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            H5Ex_D_Hyperslab.writeHyperslab(arena);
            H5Ex_D_Hyperslab.readHyperslab(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_Nbit.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
 This example shows how to read and write data to a dataset
 using the N-Bit filter.  The program first checks if the
 N-Bit filter is available, then if it is it writes integers
 to a dataset using N-Bit, then closes the file. Next, it
 reopens the file, reads back the data, and outputs the type
 of filter and the maximum value in the dataset to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_D_Nbit {
    private static String FILENAME    = "H5Ex_D_Nbit.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 32;
    private static final int DIM_Y    = 64;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 8;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;

    // Values for the status of space allocation
    enum H5Z_filter {
        H5Z_FILTER_ERROR(H5Z_FILTER_ERROR()),
        H5Z_FILTER_NONE(H5Z_FILTER_NONE()),
        H5Z_FILTER_DEFLATE(H5Z_FILTER_DEFLATE()),
        H5Z_FILTER_SHUFFLE(H5Z_FILTER_SHUFFLE()),
        H5Z_FILTER_FLETCHER32(H5Z_FILTER_FLETCHER32()),
        H5Z_FILTER_SZIP(H5Z_FILTER_SZIP()),
        H5Z_FILTER_NBIT(H5Z_FILTER_NBIT()),
        H5Z_FILTER_SCALEOFFSET(H5Z_FILTER_SCALEOFFSET()),
        H5Z_FILTER_RESERVED(H5Z_FILTER_RESERVED()),
        H5Z_FILTER_MAX(H5Z_FILTER_MAX());
        private static final Map<Integer, H5Z_filter> lookup = new HashMap<Integer, H5Z_filter>();

        static
        {
            for (H5Z_filter s : EnumSet.allOf(H5Z_filter.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5Z_filter(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5Z_filter get(int code) { return lookup.get(code); }
    }

    private static boolean checkNbitFilter()
    {
        try {
            // Check if N-Bit compression is available and can be used for both compression and decompression.
            int available = H5Zfilter_avail(H5Z_FILTER_NBIT());
            if (available == 0) {
                System.out.println("N-Bit filter not available.");
                return false;
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try (Arena arena = Arena.ofConfined()) {
            MemorySegment filterInfoSeg = arena.allocate(ValueLayout.JAVA_INT);
            H5Zget_filter_info(H5Z_FILTER_NBIT(), filterInfoSeg);
            int filter_info = filterInfoSeg.get(ValueLayout.JAVA_INT, 0);
            if (((filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED()) == 0) ||
                ((filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED()) == 0)) {
                System.out.println("N-Bit filter not available for encoding and decoding.");
                return false;
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        return true;
    }

    private static void writeData(Arena arena) throws Exception
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dtype_id     = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        long[] chunk_dims = {CHUNK_X, CHUNK_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        try {
            // Create a new file using the default properties.
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());

            // Create dataspace. Setting maximum size to NULL sets the maximum
            // size to be the current size.
            filespace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);

            // Create the datatype to use with the N-Bit filter. It has an uncompressed size of 32 bits,
            // but will have a size of 16 bits after being packed by the N-Bit filter.
            dtype_id = H5Tcopy(H5T_STD_I32LE_g());
            H5Tset_precision(dtype_id, 16);
            H5Tset_offset(dtype_id, 5);

            // Create the dataset creation property list, add the N-Bit filter and set the chunk size.
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
            H5Pset_nbit(dcpl_id);
            H5Pset_chunk(dcpl_id, NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, chunk_dims));

            // Create the dataset.
            dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), dtype_id, filespace_id,
                                    H5P_DEFAULT(), dcpl_id, H5P_DEFAULT());

            // Write the data to the dataset.
            // Flatten 2D array for FFM

            int[] flatData = new int[DIM_X * DIM_Y];

            for (int i = 0; i < DIM_X; i++) {

                for (int j = 0; j < DIM_Y; j++) {

                    flatData[i * DIM_Y + j] = dset_data[i][j];
                }
            }

            MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);

            for (int i = 0; i < flatData.length; i++) {

                dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
            }

            H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        finally {
            // Close and release resources.
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
            if (dtype_id >= 0)
                H5Tclose(dtype_id);
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
            if (file_id >= 0)
                H5Fclose(file_id);
        }
    }

    private static void readData(Arena arena) throws Exception
    {
        long file_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print the filter type. Here we only retrieve the
        // first filter because we know that we only added one filter.
        try {
            if (dcpl_id >= 0) {
                // FFM requires MemorySegment parameters
                MemorySegment flagsSeg   = arena.allocate(ValueLayout.JAVA_INT);
                MemorySegment cdNeltsSeg = arena.allocate(ValueLayout.JAVA_LONG);
                cdNeltsSeg.set(ValueLayout.JAVA_LONG, 0, 10L);
                MemorySegment cdValuesSeg     = arena.allocate(ValueLayout.JAVA_INT, 10);
                MemorySegment nameSegment     = arena.allocate(256);
                MemorySegment filterConfigSeg = arena.allocate(ValueLayout.JAVA_INT);
                int filter_type = H5Pget_filter2(dcpl_id, 0, flagsSeg, cdNeltsSeg, cdValuesSeg, 256,
                                                 nameSegment, filterConfigSeg);
                System.out.print("Filter type is: ");
                switch (H5Z_filter.get(filter_type)) {
                case H5Z_FILTER_DEFLATE:
                    System.out.println("H5Z_FILTER_DEFLATE");
                    break;
                case H5Z_FILTER_SHUFFLE:
                    System.out.println("H5Z_FILTER_SHUFFLE");
                    break;
                case H5Z_FILTER_FLETCHER32:
                    System.out.println("H5Z_FILTER_FLETCHER32");
                    break;
                case H5Z_FILTER_SZIP:
                    System.out.println("H5Z_FILTER_SZIP");
                    break;
                case H5Z_FILTER_NBIT:
                    System.out.println("H5Z_FILTER_NBIT");
                    break;
                case H5Z_FILTER_SCALEOFFSET:
                    System.out.println("H5Z_FILTER_SCALEOFFSET");
                    break;
                default:
                    System.out.println("H5Z_FILTER_ERROR");
                }
                System.out.println();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten to 2D array
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Find the maximum value in the dataset, to verify that it was read
        // correctly.
        int max = dset_data[0][0];
        for (int indx = 0; indx < DIM_X; indx++) {
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                if (max < dset_data[indx][jndx])
                    max = dset_data[indx][jndx];
        }
        // Print the maximum value.
        System.out.println("Maximum value in " + DATASETNAME + " is: " + max);

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            /*
             * Check if N-Bit compression is available and can be used for both compression and decompression.
             * Normally we do not perform error checking in these examples for the sake of clarity, but in
             * this case we will make an exception because this filter is an optional part of the hdf5
             * library.
             */
            if (H5Ex_D_Nbit.checkNbitFilter()) {
                try {
                    H5Ex_D_Nbit.writeData(arena);
                    H5Ex_D_Nbit.readData(arena);
                }
                catch (Exception ex) {
                    ex.printStackTrace();
                }
            }
        }
        catch (Exception ex) {
            ex.printStackTrace();
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_ReadWrite.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************

  This example shows how to read and write data to a
  dataset using the FFM (Foreign Function & Memory) API.
  The program first writes integers to a dataset with
  dataspace dimensions of DIM_XxDIM_Y, then closes the
  file.  Next, it reopens the file, reads back the data,
  and outputs it to the screen.

  This is a pure FFM example showing:
  - Arena-based memory management
  - MemorySegment for strings and arrays
  - Direct FFM API calls (no H5 wrapper)

 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_D_ReadWrite {
    private static String FILENAME    = "H5Ex_D_ReadWrite.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 4;
    private static final int DIM_Y    = 7;
    private static final int RANK     = 2;

    private static void WriteDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            MemorySegment filename = arena.allocateFrom(FILENAME);
            file_id                = H5Fcreate(filename, H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
            filespace_id          = H5Screate_simple(RANK, dimsSeg, MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset. We will use all default properties for this example.
        try {
            if ((file_id >= 0) && (filespace_id >= 0)) {
                MemorySegment datasetname = arena.allocateFrom(DATASETNAME);
                dataset_id = H5Dcreate2(file_id, datasetname, H5T_STD_I32LE_g(), filespace_id, H5P_DEFAULT(),
                                        H5P_DEFAULT(), H5P_DEFAULT());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        // FFM requires explicit conversion of 2D array to MemorySegment.
        try {
            if (dataset_id >= 0) {
                // Flatten 2D array to 1D for MemorySegment
                int[] flatData = new int[DIM_X * DIM_Y];
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        flatData[i * DIM_Y + j] = dset_data[i][j];
                    }
                }

                // Copy flattened data to MemorySegment
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);
                for (int i = 0; i < flatData.length; i++) {
                    dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
                }

                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open file using the default properties.
        try {
            MemorySegment filename = arena.allocateFrom(FILENAME);
            file_id                = H5Fopen(filename, H5F_ACC_RDWR(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open dataset using the default properties.
        try {
            if (file_id >= 0) {
                MemorySegment datasetname = arena.allocateFrom(DATASETNAME);
                dataset_id                = H5Dopen2(file_id, datasetname, H5P_DEFAULT());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        // FFM requires explicit conversion from MemorySegment to 2D array.
        try {
            if (dataset_id >= 0) {
                // Allocate MemorySegment for reading
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);

                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);

                // Copy from MemorySegment and unflatten to 2D array
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println(DATASETNAME + ":");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // Close the dataset.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        // Arena manages all native memory allocations
        // All allocations are automatically freed when arena closes
        try (Arena arena = Arena.ofConfined()) {
            H5Ex_D_ReadWrite.WriteDataset(arena);
            H5Ex_D_ReadWrite.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_Shuffle.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write data to a dataset
  using the shuffle filter with gzip compression.  The
  program first checks if the shuffle and gzip filters are
  available, then if they are it writes integers to a
  dataset using shuffle+gzip, then closes the file.  Next,
  it reopens the file, reads back the data, and outputs the
  types of filters and the maximum value in the dataset to
  the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_D_Shuffle {
    private static String FILENAME    = "H5Ex_D_Shuffle.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 32;
    private static final int DIM_Y    = 64;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 8;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;

    // Values for the status of space allocation
    enum H5Z_filter {
        H5Z_FILTER_ERROR(H5Z_FILTER_ERROR()),
        H5Z_FILTER_NONE(H5Z_FILTER_NONE()),
        H5Z_FILTER_DEFLATE(H5Z_FILTER_DEFLATE()),
        H5Z_FILTER_SHUFFLE(H5Z_FILTER_SHUFFLE()),
        H5Z_FILTER_FLETCHER32(H5Z_FILTER_FLETCHER32()),
        H5Z_FILTER_SZIP(H5Z_FILTER_SZIP()),
        H5Z_FILTER_NBIT(H5Z_FILTER_NBIT()),
        H5Z_FILTER_SCALEOFFSET(H5Z_FILTER_SCALEOFFSET()),
        H5Z_FILTER_RESERVED(H5Z_FILTER_RESERVED()),
        H5Z_FILTER_MAX(H5Z_FILTER_MAX());

        private static final Map<Integer, H5Z_filter> lookup = new HashMap<Integer, H5Z_filter>();

        static
        {
            for (H5Z_filter s : EnumSet.allOf(H5Z_filter.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5Z_filter(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5Z_filter get(int code) { return lookup.get(code); }
    }

    private static boolean checkGzipFilter()
    {
        try {
            int available = H5Zfilter_avail(H5Z_FILTER_DEFLATE());
            if (available == 0) {
                System.out.println("gzip filter not available.");
                return false;
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            try (Arena arena = Arena.ofConfined()) {
                MemorySegment filterInfoSeg = arena.allocate(ValueLayout.JAVA_INT);
                H5Zget_filter_info(H5Z_FILTER_DEFLATE(), filterInfoSeg);
                int filter_info = filterInfoSeg.get(ValueLayout.JAVA_INT, 0);
                if (((filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED()) == 0) ||
                    ((filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED()) == 0)) {
                    System.out.println("gzip filter not available for encoding and decoding.");
                    return false;
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        return true;
    }

    private static boolean checkShuffleFilter()
    {
        try {
            int available = H5Zfilter_avail(H5Z_FILTER_SHUFFLE());
            if (available == 0) {
                System.out.println("Shuffle filter not available.");
                return false;
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            try (Arena arena = Arena.ofConfined()) {
                MemorySegment filterInfoSeg = arena.allocate(ValueLayout.JAVA_INT);
                H5Zget_filter_info(H5Z_FILTER_SHUFFLE(), filterInfoSeg);
                int filter_info = filterInfoSeg.get(ValueLayout.JAVA_INT, 0);
                if (((filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED()) == 0) ||
                    ((filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED()) == 0)) {
                    System.out.println("Shuffle filter not available for encoding and decoding.");
                    return false;
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        return true;
    }

    private static void writeShuffle(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        long[] chunk_dims = {CHUNK_X, CHUNK_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            filespace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list, add the shuffle
        // filter and the gzip compression filter.
        // The order in which the filters are added here is significant -
        // we will see much greater results when the shuffle is applied
        // first. The order in which the filters are added to the property
        // list is the order in which they will be invoked when writing
        // data.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
            if (dcpl_id >= 0) {
                H5Pset_shuffle(dcpl_id);
                H5Pset_deflate(dcpl_id, 9);
                // Set the chunk size.
                H5Pset_chunk(dcpl_id, NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, chunk_dims));
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        filespace_id, H5P_DEFAULT(), dcpl_id, H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten the 2D array to 1D for MemorySegment
                int[] flatData = new int[DIM_X * DIM_Y];
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        flatData[i * DIM_Y + j] = dset_data[i][j];
                    }
                }
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_INT, flatData);
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readShuffle(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the number of filters, and retrieve and print the
        // type of each.
        try {
            if (dcpl_id >= 0) {
                int nfilters = H5Pget_nfilters(dcpl_id);
                for (int indx = 0; indx < nfilters; indx++) {
                    // FFM requires MemorySegment parameters
                    MemorySegment flagsSeg   = arena.allocate(ValueLayout.JAVA_INT);
                    MemorySegment cdNeltsSeg = arena.allocate(ValueLayout.JAVA_LONG);
                    cdNeltsSeg.set(ValueLayout.JAVA_LONG, 0, 10L);
                    MemorySegment cdValuesSeg     = arena.allocate(ValueLayout.JAVA_INT, 10);
                    MemorySegment nameSegment     = arena.allocate(256);
                    MemorySegment filterConfigSeg = arena.allocate(ValueLayout.JAVA_INT);
                    int filter_type = H5Pget_filter2(dcpl_id, indx, flagsSeg, cdNeltsSeg, cdValuesSeg, 256,
                                                     nameSegment, filterConfigSeg);
                    System.out.print("Filter " + indx + ": Type is: ");
                    switch (H5Z_filter.get(filter_type)) {
                    case H5Z_FILTER_DEFLATE:
                        System.out.println("H5Z_FILTER_DEFLATE");
                        break;
                    case H5Z_FILTER_SHUFFLE:
                        System.out.println("H5Z_FILTER_SHUFFLE");
                        break;
                    case H5Z_FILTER_FLETCHER32:
                        System.out.println("H5Z_FILTER_FLETCHER32");
                        break;
                    case H5Z_FILTER_SZIP:
                        System.out.println("H5Z_FILTER_SZIP");
                        break;
                    case H5Z_FILTER_NBIT:
                        System.out.println("H5Z_FILTER_NBIT");
                        break;
                    case H5Z_FILTER_SCALEOFFSET:
                        System.out.println("H5Z_FILTER_SCALEOFFSET");
                        break;
                    default:
                        System.out.println("H5Z_FILTER_ERROR");
                    }
                    System.out.println();
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten the 1D MemorySegment to 2D array
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Find the maximum value in the dataset, to verify that it was read
        // correctly.
        int max = dset_data[0][0];
        for (int indx = 0; indx < DIM_X; indx++) {
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                if (max < dset_data[indx][jndx])
                    max = dset_data[indx][jndx];
        }
        // Print the maximum value.
        System.out.println("Maximum value in " + DATASETNAME + " is: " + max);

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            if (H5Ex_D_Shuffle.checkGzipFilter() && H5Ex_D_Shuffle.checkShuffleFilter()) {
                H5Ex_D_Shuffle.writeShuffle(arena);
                H5Ex_D_Shuffle.readShuffle(arena);
            }
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_Sofloat.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write data to a dataset
  using the Scale-Offset filter.  The program first checks
  if the Scale-Offset filter is available, then if it is it
  writes floating point numbers to a dataset using
  Scale-Offset, then closes the file Next, it reopens the
  file, reads back the data, and outputs the type of filter
  and the maximum value in the dataset to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.text.DecimalFormat;
import java.text.DecimalFormatSymbols;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Locale;
import java.util.Map;

public class H5Ex_D_Sofloat {

    private static String FILENAME    = "H5Ex_D_Sofloat.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 32;
    private static final int DIM_Y    = 64;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 8;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;

    // Values for the status of space allocation
    enum H5Z_filter {
        H5Z_FILTER_ERROR(H5Z_FILTER_ERROR()),
        H5Z_FILTER_NONE(H5Z_FILTER_NONE()),
        H5Z_FILTER_DEFLATE(H5Z_FILTER_DEFLATE()),
        H5Z_FILTER_SHUFFLE(H5Z_FILTER_SHUFFLE()),
        H5Z_FILTER_FLETCHER32(H5Z_FILTER_FLETCHER32()),
        H5Z_FILTER_SZIP(H5Z_FILTER_SZIP()),
        H5Z_FILTER_NBIT(H5Z_FILTER_NBIT()),
        H5Z_FILTER_SCALEOFFSET(H5Z_FILTER_SCALEOFFSET()),
        H5Z_FILTER_RESERVED(H5Z_FILTER_RESERVED()),
        H5Z_FILTER_MAX(H5Z_FILTER_MAX());
        private static final Map<Integer, H5Z_filter> lookup = new HashMap<Integer, H5Z_filter>();

        static
        {
            for (H5Z_filter s : EnumSet.allOf(H5Z_filter.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5Z_filter(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5Z_filter get(int code) { return lookup.get(code); }
    }

    private static boolean checkScaleoffsetFilter()
    {
        try {
            int available = H5Zfilter_avail(H5Z_FILTER_SCALEOFFSET());
            if (available == 0) {
                System.out.println("Scale-Offset filter not available.");
                return false;
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            try (Arena arena = Arena.ofConfined()) {
                MemorySegment filterInfoSeg = arena.allocate(ValueLayout.JAVA_INT);
                H5Zget_filter_info(H5Z_FILTER_SCALEOFFSET(), filterInfoSeg);
                int filter_info = filterInfoSeg.get(ValueLayout.JAVA_INT, 0);
                if (((filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED()) == 0) ||
                    ((filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED()) == 0)) {
                    System.out.println("Scale-Offset filter not available for encoding and decoding.");
                    return false;
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        return true;
    }

    private static void writeData(Arena arena)
    {
        long file_id         = H5I_INVALID_HID();
        long filespace_id    = H5I_INVALID_HID();
        long dataset_id      = H5I_INVALID_HID();
        long dcpl_id         = H5I_INVALID_HID();
        long[] dims          = {DIM_X, DIM_Y};
        long[] chunk_dims    = {CHUNK_X, CHUNK_Y};
        double[][] dset_data = new double[DIM_X][DIM_Y];

        // Initialize data.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++) {
                double x              = indx;
                double y              = jndx;
                dset_data[indx][jndx] = (x + 1) / (y + 0.3) + y;
            }

        // Find the maximum value in the dataset, to verify that it was read correctly.
        double max = dset_data[0][0];
        double min = dset_data[0][0];
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++) {
                if (max < dset_data[indx][jndx])
                    max = dset_data[indx][jndx];
                if (min > dset_data[indx][jndx])
                    min = dset_data[indx][jndx];
            }

        // Print the maximum value.
        DecimalFormat df = new DecimalFormat("#,##0.000000", new DecimalFormatSymbols(Locale.US));
        System.out.println("Maximum value in write buffer is: " + df.format(max));
        System.out.println("Minimum value in write buffer is: " + df.format(min));

        // Create a new file using the default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum size to be the current size.
        try {
            filespace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list, add the Scale-Offset
        // filter and set the chunk size.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
            if (dcpl_id >= 0) {
                H5Pset_scaleoffset(dcpl_id, H5Z_SO_FLOAT_DSCALE(), 2);
                H5Pset_chunk(dcpl_id, NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, chunk_dims));
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_IEEE_F64LE_g(),
                                        filespace_id, H5P_DEFAULT(), dcpl_id, H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten 2D array for FFM

                double[] flatData = new double[DIM_X * DIM_Y];

                for (int i = 0; i < DIM_X; i++) {

                    for (int j = 0; j < DIM_Y; j++) {

                        flatData[i * DIM_Y + j] = dset_data[i][j];
                    }
                }

                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_DOUBLE, flatData.length);

                for (int i = 0; i < flatData.length; i++) {

                    dataSeg.setAtIndex(ValueLayout.JAVA_DOUBLE, i, flatData[i]);
                }

                H5Dwrite(dataset_id, H5T_NATIVE_DOUBLE_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close and release resources.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close file
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readData(Arena arena)
    {
        long file_id         = H5I_INVALID_HID();
        long dataset_id      = H5I_INVALID_HID();
        long dcpl_id         = H5I_INVALID_HID();
        double[][] dset_data = new double[DIM_X][DIM_Y];

        // Open file using the default properties.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        // Open dataset using the default properties.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print the filter type. Here we only retrieve the
        // first filter because we know that we only added one filter.
        try {
            if (dcpl_id >= 0) {
                // FFM requires MemorySegment parameters
                MemorySegment flagsSeg   = arena.allocate(ValueLayout.JAVA_INT);
                MemorySegment cdNeltsSeg = arena.allocate(ValueLayout.JAVA_LONG);
                cdNeltsSeg.set(ValueLayout.JAVA_LONG, 0, 10L);
                MemorySegment cdValuesSeg     = arena.allocate(ValueLayout.JAVA_INT, 10);
                MemorySegment nameSegment     = arena.allocate(256);
                MemorySegment filterConfigSeg = arena.allocate(ValueLayout.JAVA_INT);
                int filter_type = H5Pget_filter2(dcpl_id, 0, flagsSeg, cdNeltsSeg, cdValuesSeg, 256,
                                                 nameSegment, filterConfigSeg);
                System.out.print("Filter type is: ");
                switch (H5Z_filter.get(filter_type)) {
                case H5Z_FILTER_DEFLATE:
                    System.out.println("H5Z_FILTER_DEFLATE");
                    break;
                case H5Z_FILTER_SHUFFLE:
                    System.out.println("H5Z_FILTER_SHUFFLE");
                    break;
                case H5Z_FILTER_FLETCHER32:
                    System.out.println("H5Z_FILTER_FLETCHER32");
                    break;
                case H5Z_FILTER_SZIP:
                    System.out.println("H5Z_FILTER_SZIP");
                    break;
                case H5Z_FILTER_NBIT:
                    System.out.println("H5Z_FILTER_NBIT");
                    break;
                case H5Z_FILTER_SCALEOFFSET:
                    System.out.println("H5Z_FILTER_SCALEOFFSET");
                    break;
                default:
                    System.out.println("H5Z_FILTER_ERROR");
                }
                System.out.println();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_DOUBLE, DIM_X * DIM_Y);

                H5Dread(dataset_id, H5T_NATIVE_DOUBLE_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);

                // Unflatten to 2D array

                for (int i = 0; i < DIM_X; i++) {

                    for (int j = 0; j < DIM_Y; j++) {

                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_DOUBLE, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Find the maximum value in the dataset, to verify that it was read correctly.
        double max = dset_data[0][0];
        double min = dset_data[0][0];
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++) {
                if (max < dset_data[indx][jndx])
                    max = dset_data[indx][jndx];
                if (min > dset_data[indx][jndx])
                    min = dset_data[indx][jndx];
            }

        // Print the maximum value.
        DecimalFormat df = new DecimalFormat("#,##0.000000", new DecimalFormatSymbols(Locale.US));
        System.out.println("Maximum value in " + DATASETNAME + " is: " + df.format(max));
        System.out.println("Minimum value in " + DATASETNAME + " is: " + df.format(min));

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            if (H5Ex_D_Sofloat.checkScaleoffsetFilter()) {
                H5Ex_D_Sofloat.writeData(arena);
                H5Ex_D_Sofloat.readData(arena);
            }
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_Soint.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
      This example shows how to read and write data to a dataset
      using the Scale-Offset filter.  The program first checks
      if the Scale-Offset filter is available, then if it is it
      writes integers to a dataset using Scale-Offset, then
      closes the file Next, it reopens the file, reads back the
      data, and outputs the type of filter and the maximum value
      in the dataset to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_D_Soint {

    private static String FILENAME    = "H5Ex_D_Soint.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 32;
    private static final int DIM_Y    = 64;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 8;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;

    // Values for the status of space allocation
    enum H5Z_filter {
        H5Z_FILTER_ERROR(H5Z_FILTER_ERROR()),
        H5Z_FILTER_NONE(H5Z_FILTER_NONE()),
        H5Z_FILTER_DEFLATE(H5Z_FILTER_DEFLATE()),
        H5Z_FILTER_SHUFFLE(H5Z_FILTER_SHUFFLE()),
        H5Z_FILTER_FLETCHER32(H5Z_FILTER_FLETCHER32()),
        H5Z_FILTER_SZIP(H5Z_FILTER_SZIP()),
        H5Z_FILTER_NBIT(H5Z_FILTER_NBIT()),
        H5Z_FILTER_SCALEOFFSET(H5Z_FILTER_SCALEOFFSET()),
        H5Z_FILTER_RESERVED(H5Z_FILTER_RESERVED()),
        H5Z_FILTER_MAX(H5Z_FILTER_MAX());
        private static final Map<Integer, H5Z_filter> lookup = new HashMap<Integer, H5Z_filter>();

        static
        {
            for (H5Z_filter s : EnumSet.allOf(H5Z_filter.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5Z_filter(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5Z_filter get(int code) { return lookup.get(code); }
    }

    private static boolean checkScaleoffsetFilter()
    {
        try {
            int available = H5Zfilter_avail(H5Z_FILTER_SCALEOFFSET());
            if (available == 0) {
                System.out.println("Scale-Offset filter not available.");
                return false;
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            try (Arena arena = Arena.ofConfined()) {
                MemorySegment filterInfoSeg = arena.allocate(ValueLayout.JAVA_INT);
                H5Zget_filter_info(H5Z_FILTER_SCALEOFFSET(), filterInfoSeg);
                int filter_info = filterInfoSeg.get(ValueLayout.JAVA_INT, 0);
                if (((filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED()) == 0) ||
                    ((filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED()) == 0)) {
                    System.out.println("Scale-Offset filter not available for encoding and decoding.");
                    return false;
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        return true;
    }

    private static void writeData(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        long[] chunk_dims = {CHUNK_X, CHUNK_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using the default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum size to be the current size.
        try {
            filespace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list, add the Scale-Offset
        // filter and set the chunk size.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
            if (dcpl_id >= 0) {
                H5Pset_scaleoffset(dcpl_id, H5Z_SO_INT(), H5Z_SO_INT_MINBITS_DEFAULT());
                H5Pset_chunk(dcpl_id, NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, chunk_dims));
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        filespace_id, H5P_DEFAULT(), dcpl_id, H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten 2D array for FFM

                int[] flatData = new int[DIM_X * DIM_Y];

                for (int i = 0; i < DIM_X; i++) {

                    for (int j = 0; j < DIM_Y; j++) {

                        flatData[i * DIM_Y + j] = dset_data[i][j];
                    }
                }

                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);

                for (int i = 0; i < flatData.length; i++) {

                    dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
                }

                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close and release resources.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close file
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readData(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open file using the default properties.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        // Open dataset using the default properties.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print the filter type. Here we only retrieve the
        // first filter because we know that we only added one filter.
        try {
            if (dcpl_id >= 0) {
                // FFM requires MemorySegment parameters
                MemorySegment flagsSeg   = arena.allocate(ValueLayout.JAVA_INT);
                MemorySegment cdNeltsSeg = arena.allocate(ValueLayout.JAVA_LONG);
                cdNeltsSeg.set(ValueLayout.JAVA_LONG, 0, 10L);
                MemorySegment cdValuesSeg     = arena.allocate(ValueLayout.JAVA_INT, 10);
                MemorySegment nameSegment     = arena.allocate(256);
                MemorySegment filterConfigSeg = arena.allocate(ValueLayout.JAVA_INT);
                int filter_type = H5Pget_filter2(dcpl_id, 0, flagsSeg, cdNeltsSeg, cdValuesSeg, 256,
                                                 nameSegment, filterConfigSeg);
                System.out.print("Filter type is: ");
                switch (H5Z_filter.get(filter_type)) {
                case H5Z_FILTER_DEFLATE:
                    System.out.println("H5Z_FILTER_DEFLATE");
                    break;
                case H5Z_FILTER_SHUFFLE:
                    System.out.println("H5Z_FILTER_SHUFFLE");
                    break;
                case H5Z_FILTER_FLETCHER32:
                    System.out.println("H5Z_FILTER_FLETCHER32");
                    break;
                case H5Z_FILTER_SZIP:
                    System.out.println("H5Z_FILTER_SZIP");
                    break;
                case H5Z_FILTER_NBIT:
                    System.out.println("H5Z_FILTER_NBIT");
                    break;
                case H5Z_FILTER_SCALEOFFSET:
                    System.out.println("H5Z_FILTER_SCALEOFFSET");
                    break;
                default:
                    System.out.println("H5Z_FILTER_ERROR");
                }
                System.out.println();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);

                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);

                // Unflatten to 2D array

                for (int i = 0; i < DIM_X; i++) {

                    for (int j = 0; j < DIM_Y; j++) {

                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Find the maximum value in the dataset, to verify that it was read correctly.
        int max = dset_data[0][0];
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++) {
                if (max < dset_data[indx][jndx])
                    max = dset_data[indx][jndx];
            }

        // Print the maximum value.
        System.out.println("Maximum value in " + DATASETNAME + " is: " + max);

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            if (H5Ex_D_Soint.checkScaleoffsetFilter()) {
                H5Ex_D_Soint.writeData(arena);
                H5Ex_D_Soint.readData(arena);
            }
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_Szip.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write data to a dataset
  using szip compression.    The program first checks if
  szip compression is available, then if it is it writes
  integers to a dataset using szip, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs the type of compression and the maximum value in
  the dataset to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_D_Szip {
    private static String FILENAME    = "H5Ex_D_Szip.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 32;
    private static final int DIM_Y    = 64;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 8;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;

    // Values for the status of space allocation
    enum H5Z_filter {
        H5Z_FILTER_ERROR(H5Z_FILTER_ERROR()),
        H5Z_FILTER_NONE(H5Z_FILTER_NONE()),
        H5Z_FILTER_DEFLATE(H5Z_FILTER_DEFLATE()),
        H5Z_FILTER_SHUFFLE(H5Z_FILTER_SHUFFLE()),
        H5Z_FILTER_FLETCHER32(H5Z_FILTER_FLETCHER32()),
        H5Z_FILTER_SZIP(H5Z_FILTER_SZIP()),
        H5Z_FILTER_NBIT(H5Z_FILTER_NBIT()),
        H5Z_FILTER_SCALEOFFSET(H5Z_FILTER_SCALEOFFSET()),
        H5Z_FILTER_RESERVED(H5Z_FILTER_RESERVED()),
        H5Z_FILTER_MAX(H5Z_FILTER_MAX());
        private static final Map<Integer, H5Z_filter> lookup = new HashMap<Integer, H5Z_filter>();

        static
        {
            for (H5Z_filter s : EnumSet.allOf(H5Z_filter.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5Z_filter(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5Z_filter get(int code) { return lookup.get(code); }
    }

    private static boolean checkSzipFilter()
    {
        try {
            int available = H5Zfilter_avail(H5Z_FILTER_SZIP());
            if (available == 0) {
                System.out.println("szip filter not available.");
                return false;
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            try (Arena arena = Arena.ofConfined()) {
                MemorySegment filterInfoSeg = arena.allocate(ValueLayout.JAVA_INT);
                H5Zget_filter_info(H5Z_FILTER_SZIP(), filterInfoSeg);
                int filter_info = filterInfoSeg.get(ValueLayout.JAVA_INT, 0);
                if (((filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED()) == 0) ||
                    ((filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED()) == 0)) {
                    System.out.println("szip filter not available for encoding and decoding.");
                    return false;
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        return true;
    }

    private static void writeSzip(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        long[] chunk_dims = {CHUNK_X, CHUNK_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            filespace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list, add the szip compression
        // filter.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
            if (dcpl_id >= 0) {
                H5Pset_szip(dcpl_id, H5_SZIP_NN_OPTION_MASK(), 8);
                // Set the chunk size.
                H5Pset_chunk(dcpl_id, NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, chunk_dims));
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        filespace_id, H5P_DEFAULT(), dcpl_id, H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten 2D array for FFM

                int[] flatData = new int[DIM_X * DIM_Y];

                for (int i = 0; i < DIM_X; i++) {

                    for (int j = 0; j < DIM_Y; j++) {

                        flatData[i * DIM_Y + j] = dset_data[i][j];
                    }
                }

                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);

                for (int i = 0; i < flatData.length; i++) {

                    dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
                }

                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readSzip(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print the filter type. Here we only retrieve the
        // first filter because we know that we only added one filter.
        try {
            if (dcpl_id >= 0) {
                // FFM requires MemorySegment parameters
                MemorySegment flagsSeg   = arena.allocate(ValueLayout.JAVA_INT);
                MemorySegment cdNeltsSeg = arena.allocate(ValueLayout.JAVA_LONG);
                cdNeltsSeg.set(ValueLayout.JAVA_LONG, 0, 10L);
                MemorySegment cdValuesSeg     = arena.allocate(ValueLayout.JAVA_INT, 10);
                MemorySegment nameSegment     = arena.allocate(256);
                MemorySegment filterConfigSeg = arena.allocate(ValueLayout.JAVA_INT);
                int filter_type = H5Pget_filter2(dcpl_id, 0, flagsSeg, cdNeltsSeg, cdValuesSeg, 256,
                                                 nameSegment, filterConfigSeg);
                System.out.print("Filter type is: ");
                switch (H5Z_filter.get(filter_type)) {
                case H5Z_FILTER_DEFLATE:
                    System.out.println("H5Z_FILTER_DEFLATE");
                    break;
                case H5Z_FILTER_SHUFFLE:
                    System.out.println("H5Z_FILTER_SHUFFLE");
                    break;
                case H5Z_FILTER_FLETCHER32:
                    System.out.println("H5Z_FILTER_FLETCHER32");
                    break;
                case H5Z_FILTER_SZIP:
                    System.out.println("H5Z_FILTER_SZIP");
                    break;
                case H5Z_FILTER_NBIT:
                    System.out.println("H5Z_FILTER_NBIT");
                    break;
                case H5Z_FILTER_SCALEOFFSET:
                    System.out.println("H5Z_FILTER_SCALEOFFSET");
                    break;
                default:
                    System.out.println("H5Z_FILTER_ERROR");
                }
                System.out.println();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);

                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);

                // Unflatten to 2D array

                for (int i = 0; i < DIM_X; i++) {

                    for (int j = 0; j < DIM_Y; j++) {

                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Find the maximum value in the dataset, to verify that it was read
        // correctly.
        int max = dset_data[0][0];
        for (int indx = 0; indx < DIM_X; indx++) {
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                if (max < dset_data[indx][jndx])
                    max = dset_data[indx][jndx];
        }
        // Print the maximum value.
        System.out.println("Maximum value in " + DATASETNAME + " is: " + max);

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            if (H5Ex_D_Szip.checkSzipFilter()) {
                H5Ex_D_Szip.writeSzip(arena);
                H5Ex_D_Szip.readSzip(arena);
            }
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_Transform.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
      This example shows how to read and write data to a dataset
      using a data transform expression.  The program first
      writes integers to a dataset using the transform
      expression TRANSFORM, then closes the file.  Next, it
      reopens the file, reads back the data without a transform,
      and outputs the data to the screen.  Finally it reads the
      data using the transform expression RTRANSFORM and outputs
      the results to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_D_Transform {

    private static String FILENAME   = "H5Ex_D_Transform.h5";
    private static String DATASET    = "DS1";
    private static final int DIM_X   = 4;
    private static final int DIM_Y   = 7;
    private static String TRANSFORM  = "x+1";
    private static String RTRANSFORM = "x-1";

    private static void writeData(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dxpl_id      = H5I_INVALID_HID();

        long[] dims       = {DIM_X, DIM_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data.
        for (int i = 0; i < DIM_X; i++)
            for (int j = 0; j < DIM_Y; j++)
                dset_data[i][j] = i * j - j;

        // Output the data to the screen.
        System.out.println("Original Data:");
        for (int i = 0; i < DIM_X; i++) {
            System.out.print(" [");
            for (int j = 0; j < DIM_Y; j++)
                System.out.print(" " + dset_data[i][j] + " ");
            System.out.println("]");
        }

        // Create a new file using the default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            filespace_id =
                H5Screate_simple(2, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset transfer property list and define the transform expression.
        try {
            dxpl_id = H5Pcreate(H5P_CLS_DATASET_XFER_ID_g());
            if (dxpl_id >= 0)
                H5Pset_data_transform(dxpl_id, arena.allocateFrom(TRANSFORM));
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset using the default properties. Unfortunately we must save as
        // a native type or the transform operation will fail.
        try {
            if ((file_id >= 0) && (filespace_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASET), H5T_NATIVE_INT_g(),
                                        filespace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset using the dataset transfer property list.
        try {
            if ((dataset_id >= 0) && (dxpl_id >= 0)) {
                // Flatten 2D array for FFM
                int[] flatData = new int[DIM_X * DIM_Y];
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        flatData[i * DIM_Y + j] = dset_data[i][j];
                    }
                }
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);
                for (int i = 0; i < flatData.length; i++) {
                    dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
                }
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), dxpl_id, dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dxpl_id >= 0)
                H5Pclose(dxpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readData(Arena arena)
    {

        long file_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dxpl_id      = H5I_INVALID_HID();
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open an existing file using the default properties.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset using the default properties.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASET), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten to 2D array
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Data as written with transform '" + TRANSFORM + "'");
        for (int i = 0; i < DIM_X; i++) {
            System.out.print(" [");
            for (int j = 0; j < DIM_Y; j++)
                System.out.print(" " + dset_data[i][j] + " ");
            System.out.println("]");
        }

        // Create the dataset transfer property list and define the transform expression.
        try {
            dxpl_id = H5Pcreate(H5P_CLS_DATASET_XFER_ID_g());
            if (dxpl_id >= 0)
                H5Pset_data_transform(dxpl_id, arena.allocateFrom(RTRANSFORM));
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the dataset transfer property list.
        try {
            if ((dataset_id >= 0) && (dxpl_id >= 0)) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), dxpl_id, dataSeg);
                // Unflatten to 2D array
                for (int i = 0; i < DIM_X; i++) {
                    for (int j = 0; j < DIM_Y; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.

        System.out.println("Data as written with transform  '" + TRANSFORM + "' and read with transform  '" +
                           RTRANSFORM + "'");
        for (int i = 0; i < DIM_X; i++) {
            System.out.print(" [");
            for (int j = 0; j < DIM_Y; j++)
                System.out.print(" " + dset_data[i][j] + " ");
            System.out.println("]");
        }

        // Close and release resources.
        try {
            if (dxpl_id >= 0)
                H5Pclose(dxpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_D_Transform.writeData(arena);
            H5Ex_D_Transform.readData(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_UnlimitedAdd.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to create and extend an unlimited
  dataset.  The program first writes integers to a dataset
  with dataspace dimensions of DIM_XxDIM_Y, then closes the
  file.  Next, it reopens the file, reads back the data,
  outputs it to the screen, extends the dataset, and writes
  new data to the extended portions of the dataset.  Finally
  it reopens the file again, reads back the data, and
  outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_D_UnlimitedAdd {
    private static String FILENAME    = "H5Ex_D_UnlimitedAdd.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 4;
    private static final int DIM_Y    = 7;
    private static final int EDIM_X   = 6;
    private static final int EDIM_Y   = 10;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 4;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;

    private static void writeUnlimited(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        long[] chunk_dims = {CHUNK_X, CHUNK_Y};
        long[] maxdims    = {H5S_UNLIMITED(), H5S_UNLIMITED()};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize the dataset.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace with unlimited dimensions.
        try {
            dataspace_id = H5Screate_simple(RANK, dims, maxdims);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Set the chunk size.
        try {
            if (dcpl_id >= 0)
                H5Pset_chunk(dcpl_id, NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, chunk_dims));
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the unlimited dataset.
        try {
            if ((file_id >= 0) && (dataspace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        dataspace_id, H5P_DEFAULT(), dcpl_id, H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0)
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dset_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void extendUnlimited(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        long[] extdims    = {EDIM_X, EDIM_Y};
        long[] start      = {0, 0};
        long[] count      = new long[2];
        int[][] dset_data;
        int[][] extend_dset_data = new int[EDIM_X][EDIM_Y];

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDWR(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer. This is a
        // two dimensional dataset so the dynamic allocation must be done
        // in steps.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sget_simple_extent_dims(dataspace_id, dims, null);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to rows.
        dset_data = new int[(int)dims[0]][(int)dims[1]];

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0)
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dset_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Dataset before extension:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Extend the dataset.
        try {
            if (dataset_id >= 0)
                H5Dset_extent(dataset_id, extdims);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataspace for the newly extended dataset.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Initialize data for writing to the extended dataset.
        for (int indx = 0; indx < EDIM_X; indx++)
            for (int jndx = 0; jndx < EDIM_Y; jndx++)
                extend_dset_data[indx][jndx] = jndx;

        // Select the entire dataspace.
        try {
            if (dataspace_id >= 0) {
                H5Sselect_all(dataspace_id);

                // Subtract a hyperslab reflecting the original dimensions from the
                // selection. The selection now contains only the newly extended
                // portions of the dataset.
                count[0] = dims[0];
                count[1] = dims[1];
                H5Sselect_hyperslab(dataspace_id, H5S_SELECT_NOTB(), start, null, count, null);

                // Write the data to the selected portion of the dataset.
                if (dataset_id >= 0)
                    H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), dataspace_id, H5P_DEFAULT(),
                             extend_dset_data);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readUnlimited(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        int[][] dset_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for the read buffer as before.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sget_simple_extent_dims(dataspace_id, dims, null);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        // Allocate array of pointers to rows.
        dset_data = new int[(int)dims[0]][(int)dims[1]];

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0)
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dset_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Dataset after extension:");
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < dims[1]; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_D_UnlimitedAdd.writeUnlimited(arena);
            H5Ex_D_UnlimitedAdd.extendUnlimited(arena);
            H5Ex_D_UnlimitedAdd.readUnlimited(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_UnlimitedGzip.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to create and extend an unlimited
  dataset with gzip compression.  The program first writes
  integers to a gzip compressed dataset with dataspace
  dimensions of DIM_XxDIM_Y, then closes the file.  Next, it
  reopens the file, reads back the data, outputs it to the
  screen, extends the dataset, and writes new data to the
  extended portions of the dataset.  Finally it reopens the
  file again, reads back the data, and outputs it to the
  screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_D_UnlimitedGzip {
    private static String FILENAME    = "H5Ex_D_UnlimitedGzip.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 4;
    private static final int DIM_Y    = 7;
    private static final int EDIM_X   = 6;
    private static final int EDIM_Y   = 10;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 4;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;

    // Values for the status of space allocation
    enum H5Z_filter {
        H5Z_FILTER_ERROR(H5Z_FILTER_ERROR()),
        H5Z_FILTER_NONE(H5Z_FILTER_NONE()),
        H5Z_FILTER_DEFLATE(H5Z_FILTER_DEFLATE()),
        H5Z_FILTER_SHUFFLE(H5Z_FILTER_SHUFFLE()),
        H5Z_FILTER_FLETCHER32(H5Z_FILTER_FLETCHER32()),
        H5Z_FILTER_SZIP(H5Z_FILTER_SZIP()),
        H5Z_FILTER_NBIT(H5Z_FILTER_NBIT()),
        H5Z_FILTER_SCALEOFFSET(H5Z_FILTER_SCALEOFFSET()),
        H5Z_FILTER_RESERVED(H5Z_FILTER_RESERVED()),
        H5Z_FILTER_MAX(H5Z_FILTER_MAX());
        private static final Map<Integer, H5Z_filter> lookup = new HashMap<Integer, H5Z_filter>();

        static
        {
            for (H5Z_filter s : EnumSet.allOf(H5Z_filter.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5Z_filter(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5Z_filter get(int code) { return lookup.get(code); }
    }

    private static boolean checkGzipFilter()
    {
        try {
            int available = H5Zfilter_avail(H5Z_FILTER_DEFLATE());
            if (available == 0) {
                System.out.println("gzip filter not available.");
                return false;
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            try (Arena arena = Arena.ofConfined()) {
                MemorySegment filterInfoSeg = arena.allocate(ValueLayout.JAVA_INT);
                H5Zget_filter_info(H5Z_FILTER_DEFLATE(), filterInfoSeg);
                int filter_info = filterInfoSeg.get(ValueLayout.JAVA_INT, 0);
                if (((filter_info & H5Z_FILTER_CONFIG_ENCODE_ENABLED()) == 0) ||
                    ((filter_info & H5Z_FILTER_CONFIG_DECODE_ENABLED()) == 0)) {
                    System.out.println("gzip filter not available for encoding and decoding.");
                    return false;
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        return true;
    }

    private static void writeUnlimited(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        long[] chunk_dims = {CHUNK_X, CHUNK_Y};
        long[] maxdims    = {H5S_UNLIMITED(), H5S_UNLIMITED()};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize the dataset.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace with unlimited dimensions.
        try {
            dataspace_id = H5Screate_simple(RANK, dims, maxdims);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list, add the gzip compression
        // filter.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
            if (dcpl_id >= 0) {
                H5Pset_deflate(dcpl_id, 9);
                // Set the chunk size.
                H5Pset_chunk(dcpl_id, NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, chunk_dims));
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the unlimited dataset.
        try {
            if ((file_id >= 0) && (dataspace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        dataspace_id, H5P_DEFAULT(), dcpl_id, H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten 2D array for FFM

                int[] flatData = new int[DIM_X * DIM_Y];

                for (int i = 0; i < DIM_X; i++) {

                    for (int j = 0; j < DIM_Y; j++) {

                        flatData[i * DIM_Y + j] = dset_data[i][j];
                    }
                }

                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, flatData.length);

                for (int i = 0; i < flatData.length; i++) {

                    dataSeg.setAtIndex(ValueLayout.JAVA_INT, i, flatData[i]);
                }

                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void extendUnlimited(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        long[] extdims    = {EDIM_X, EDIM_Y};
        long[] start      = {0, 0};
        long[] count      = new long[2];
        int[][] dset_data;
        int[][] extend_dset_data = new int[EDIM_X][EDIM_Y];

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDWR(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer. This is a
        // two dimensional dataset so the dynamic allocation must be done
        // in steps.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sget_simple_extent_dims(dataspace_id, dims, null);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to rows.
        dset_data = new int[(int)dims[0]][(int)dims[1]];

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);

                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);

                // Unflatten to 2D array

                for (int i = 0; i < DIM_X; i++) {

                    for (int j = 0; j < DIM_Y; j++) {

                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Dataset before extension:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Extend the dataset.
        try {
            if (dataset_id >= 0)
                H5Dset_extent(dataset_id, extdims);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataspace for the newly extended dataset.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Initialize data for writing to the extended dataset.
        for (int indx = 0; indx < EDIM_X; indx++)
            for (int jndx = 0; jndx < EDIM_Y; jndx++)
                extend_dset_data[indx][jndx] = jndx;

        // Select the entire dataspace.
        try {
            if (dataspace_id >= 0) {
                H5Sselect_all(dataspace_id);

                // Subtract a hyperslab reflecting the original dimensions from the
                // selection. The selection now contains only the newly extended
                // portions of the dataset.
                count[0] = dims[0];
                count[1] = dims[1];
                H5Sselect_hyperslab(dataspace_id, H5S_SELECT_NOTB(), start, null, count, null);

                // Write the data to the selected portion of the dataset.
                if (dataset_id >= 0)
                    H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), dataspace_id, H5P_DEFAULT(),
                             extend_dset_data);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readUnlimited(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        int[][] dset_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print the filter type. Here we only retrieve the
        // first filter because we know that we only added one filter.
        try {
            if (dcpl_id >= 0) {
                // FFM requires MemorySegment parameters
                MemorySegment flagsSeg   = arena.allocate(ValueLayout.JAVA_INT);
                MemorySegment cdNeltsSeg = arena.allocate(ValueLayout.JAVA_LONG);
                cdNeltsSeg.set(ValueLayout.JAVA_LONG, 0, 10L);
                MemorySegment cdValuesSeg     = arena.allocate(ValueLayout.JAVA_INT, 10);
                MemorySegment nameSegment     = arena.allocate(256);
                MemorySegment filterConfigSeg = arena.allocate(ValueLayout.JAVA_INT);
                int filter_type = H5Pget_filter2(dcpl_id, 0, flagsSeg, cdNeltsSeg, cdValuesSeg, 256,
                                                 nameSegment, filterConfigSeg);
                System.out.print("Filter type is: ");
                switch (H5Z_filter.get(filter_type)) {
                case H5Z_FILTER_DEFLATE:
                    System.out.println("H5Z_FILTER_DEFLATE");
                    break;
                case H5Z_FILTER_SHUFFLE:
                    System.out.println("H5Z_FILTER_SHUFFLE");
                    break;
                case H5Z_FILTER_FLETCHER32:
                    System.out.println("H5Z_FILTER_FLETCHER32");
                    break;
                case H5Z_FILTER_SZIP:
                    System.out.println("H5Z_FILTER_SZIP");
                    break;
                default:
                    System.out.println("H5Z_FILTER_ERROR");
                }
                System.out.println();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for the read buffer as before.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sget_simple_extent_dims(dataspace_id, dims, null);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        // Allocate array of pointers to rows.
        dset_data = new int[(int)dims[0]][(int)dims[1]];

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, DIM_X * DIM_Y);

                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);

                // Unflatten to 2D array

                for (int i = 0; i < DIM_X; i++) {

                    for (int j = 0; j < DIM_Y; j++) {

                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * DIM_Y + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Dataset after extension:");
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < dims[1]; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            if (H5Ex_D_UnlimitedGzip.checkGzipFilter()) {
                H5Ex_D_UnlimitedGzip.writeUnlimited(arena);
                H5Ex_D_UnlimitedGzip.extendUnlimited(arena);
                H5Ex_D_UnlimitedGzip.readUnlimited(arena);
            }
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/H5Ex_D_UnlimitedMod.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to create and extend an unlimited
  dataset.  The program first writes integers to a dataset
  with dataspace dimensions of DIM_XxDIM_Y, then closes the
  file.  Next, it reopens the file, reads back the data,
  outputs it to the screen, extends the dataset, and writes
  new data to the entire extended dataset.  Finally it
  reopens the file again, reads back the data, and outputs it
  to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_D_UnlimitedMod {
    private static String FILENAME    = "H5Ex_D_UnlimitedMod.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 4;
    private static final int DIM_Y    = 7;
    private static final int EDIM_X   = 6;
    private static final int EDIM_Y   = 10;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 4;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;

    private static void writeUnlimited(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dcpl_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        long[] chunk_dims = {CHUNK_X, CHUNK_Y};
        long[] maxdims    = {H5S_UNLIMITED(), H5S_UNLIMITED()};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize the dataset.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace with unlimited dimensions.
        try {
            dataspace_id = H5Screate_simple(RANK, dims, maxdims);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list.
        try {
            dcpl_id = H5Pcreate(H5P_CLS_DATASET_CREATE_ID_g());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Set the chunk size.
        try {
            if (dcpl_id >= 0)
                H5Pset_chunk(dcpl_id, NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, chunk_dims));
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the unlimited dataset.
        try {
            if ((file_id >= 0) && (dataspace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        dataspace_id, H5P_DEFAULT(), dcpl_id, H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0)
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dset_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dcpl_id >= 0)
                H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void extendUnlimited(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        long[] extdims    = {EDIM_X, EDIM_Y};
        int[][] dset_data;
        int[][] extend_dset_data = new int[EDIM_X][EDIM_Y];

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDWR(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer. This is a
        // two dimensional dataset so the dynamic allocation must be done
        // in steps.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sget_simple_extent_dims(dataspace_id, dims, null);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to rows.
        dset_data = new int[(int)dims[0]][(int)dims[1]];

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0)
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dset_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Dataset before extension:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Extend the dataset.
        try {
            if (dataset_id >= 0)
                H5Dset_extent(dataset_id, extdims);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataspace for the newly extended dataset.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Initialize data for writing to the extended dataset.
        for (int indx = 0; indx < EDIM_X; indx++)
            for (int jndx = 0; jndx < EDIM_Y; jndx++)
                extend_dset_data[indx][jndx] = jndx;

        // Write the data tto the extended dataset.
        try {
            if ((dataspace_id >= 0) && (dataset_id >= 0))
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), dataspace_id, H5P_DEFAULT(),
                         extend_dset_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readUnlimited(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM_X, DIM_Y};
        int[][] dset_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for the read buffer as before.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sget_simple_extent_dims(dataspace_id, dims, null);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        // Allocate array of pointers to rows.
        dset_data = new int[(int)dims[0]][(int)dims[1]];

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0)
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dset_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Dataset after extension:");
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < dims[1]; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_D_UnlimitedMod.writeUnlimited(arena);
            H5Ex_D_UnlimitedMod.extendUnlimited(arena);
            H5Ex_D_UnlimitedMod.readUnlimited(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5D/Java_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (HDF_JAVA_EXAMPLES
    H5Ex_D_Alloc.java
    H5Ex_D_Checksum.java
    H5Ex_D_Chunk.java
    H5Ex_D_Compact.java
    H5Ex_D_External.java
    H5Ex_D_FillValue.java
    H5Ex_D_Hyperslab.java
    H5Ex_D_ReadWrite.java
    H5Ex_D_Nbit.java
    H5Ex_D_Transform.java
    H5Ex_D_Sofloat.java
    H5Ex_D_Soint.java
)

# Unlimited examples have known FFM memory issues - only include with JNI
if (HDF5_PROVIDES_JNI)
    set (HDF_JAVA_EXAMPLES ${HDF_JAVA_EXAMPLES}
        H5Ex_D_UnlimitedAdd.java
        H5Ex_D_UnlimitedMod.java
    )
endif ()

set (HDF_JAVA_ZLIB_EXAMPLES
    H5Ex_D_Gzip.java
    H5Ex_D_Shuffle.java
)

# UnlimitedGzip has known FFM memory issues - only include with JNI
if (HDF5_PROVIDES_JNI)
    set (HDF_JAVA_ZLIB_EXAMPLES ${HDF_JAVA_ZLIB_EXAMPLES}
        H5Ex_D_UnlimitedGzip.java
    )
endif ()

set (HDF_JAVA_SZIP_EXAMPLES
    H5Ex_D_Szip.java
)

# detect whether the encoder is present.
  if (${HDF5_PROVIDES_ZLIB_SUPPORT})
    set (HDF_JAVA_EXAMPLES ${HDF_JAVA_EXAMPLES} ${HDF_JAVA_ZLIB_EXAMPLES})
  endif ()

  if (${HDF5_PROVIDES_SZIP_SUPPORT})
    set (HDF_JAVA_EXAMPLES ${HDF_JAVA_EXAMPLES} ${HDF_JAVA_SZIP_EXAMPLES})
  endif ()
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_Alloc.txt`

```
Creating datasets...
DS1 has allocation time H5D_ALLOC_TIME_LATE
DS2 has allocation time H5D_ALLOC_TIME_EARLY

Space for DS1 has not been allocated.
Storage size for DS1 is: 0 bytes.
Space for DS2 has been allocated.
Storage size for DS2 is: 112 bytes.

Writing data...

Space for DS1 has been allocated.
Storage size for DS1 is: 112 bytes.
Space for DS2 has been allocated.
Storage size for DS2 is: 112 bytes.
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_Checksum.txt`

```
Filter type is: H5Z_FILTER_FLETCHER32

Maximum value in DS1 is: 1890
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_Chunk.txt`

```
Original Data:
 [ 1 1 1 1 1 1 1 1 ]
 [ 1 1 1 1 1 1 1 1 ]
 [ 1 1 1 1 1 1 1 1 ]
 [ 1 1 1 1 1 1 1 1 ]
 [ 1 1 1 1 1 1 1 1 ]
 [ 1 1 1 1 1 1 1 1 ]

Storage layout for DS1 is: H5D_CHUNKED

Data as written to disk by hyberslabs:
 [ 0 1 0 0 1 0 0 1 ]
 [ 1 1 0 1 1 0 1 1 ]
 [ 0 0 0 0 0 0 0 0 ]
 [ 0 1 0 0 1 0 0 1 ]
 [ 1 1 0 1 1 0 1 1 ]
 [ 0 0 0 0 0 0 0 0 ]

Data as read from disk by hyberslab:
 [ 0 1 0 0 0 0 0 1 ]
 [ 0 1 0 1 0 0 1 1 ]
 [ 0 0 0 0 0 0 0 0 ]
 [ 0 0 0 0 0 0 0 0 ]
 [ 0 1 0 1 0 0 1 1 ]
 [ 0 0 0 0 0 0 0 0 ]
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_Compact.txt`

```
Storage layout for DS1 is: H5D_COMPACT

Data for DS1 is: 
 [ 0 -1 -2 -3 -4 -5 -6 ]
 [ 0 0 0 0 0 0 0 ]
 [ 0 1 2 3 4 5 6 ]
 [ 0 2 4 6 8 10 12 ]
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_External.txt`

```
DS1 is stored in file: H5Ex_D_External.data
DS1:
 [ 0 -1 -2 -3 -4 -5 -6 ]
 [ 0 0 0 0 0 0 0 ]
 [ 0 1 2 3 4 5 6 ]
 [ 0 2 4 6 8 10 12 ]
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_FillValue.txt`

```
Dataset before being written to:
 [ 99 99 99 99 99 99 99 ]
 [ 99 99 99 99 99 99 99 ]
 [ 99 99 99 99 99 99 99 ]
 [ 99 99 99 99 99 99 99 ]

Dataset after being written to:
 [ 0 -1 -2 -3 -4 -5 -6 ]
 [ 0 0 0 0 0 0 0 ]
 [ 0 1 2 3 4 5 6 ]
 [ 0 2 4 6 8 10 12 ]

Dataset after extension:
 [ 0 -1 -2 -3 -4 -5 -6 99 99 99 ]
 [ 0 0 0 0 0 0 0 99 99 99 ]
 [ 0 1 2 3 4 5 6 99 99 99 ]
 [ 0 2 4 6 8 10 12 99 99 99 ]
 [ 99 99 99 99 99 99 99 99 99 99 ]
 [ 99 99 99 99 99 99 99 99 99 99 ]
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_Gzip.txt`

```
Filter type is: H5Z_FILTER_DEFLATE

Maximum value in DS1 is: 1890
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_Hyperslab.txt`

```
Original Data:
 [ 1 1 1 1 1 1 1 1 ]
 [ 1 1 1 1 1 1 1 1 ]
 [ 1 1 1 1 1 1 1 1 ]
 [ 1 1 1 1 1 1 1 1 ]
 [ 1 1 1 1 1 1 1 1 ]
 [ 1 1 1 1 1 1 1 1 ]

Data as written to disk by hyberslabs:
 [ 0 1 0 0 1 0 0 1 ]
 [ 1 1 0 1 1 0 1 1 ]
 [ 0 0 0 0 0 0 0 0 ]
 [ 0 1 0 0 1 0 0 1 ]
 [ 1 1 0 1 1 0 1 1 ]
 [ 0 0 0 0 0 0 0 0 ]

Data as read from disk by hyberslab:
 [ 0 1 0 0 0 0 0 1 ]
 [ 0 1 0 1 0 0 1 1 ]
 [ 0 0 0 0 0 0 0 0 ]
 [ 0 0 0 0 0 0 0 0 ]
 [ 0 1 0 1 0 0 1 1 ]
 [ 0 0 0 0 0 0 0 0 ]
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_Nbit.txt`

```
Filter type is: H5Z_FILTER_NBIT

Maximum value in DS1 is: 1890
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_ReadWrite.txt`

```
DS1:
 [ 0 -1 -2 -3 -4 -5 -6 ]
 [ 0 0 0 0 0 0 0 ]
 [ 0 1 2 3 4 5 6 ]
 [ 0 2 4 6 8 10 12 ]
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_Shuffle.txt`

```
Filter 0: Type is: H5Z_FILTER_SHUFFLE

Filter 1: Type is: H5Z_FILTER_DEFLATE

Maximum value in DS1 is: 1890
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_Sofloat.txt`

```
Maximum value in write buffer is: 106.666667
Minimum value in write buffer is: 1.769231
Filter type is: H5Z_FILTER_SCALEOFFSET

Maximum value in DS1 is: 106.661698
Minimum value in DS1 is: 1.769231
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_Soint.txt`

```
Filter type is: H5Z_FILTER_SCALEOFFSET

Maximum value in DS1 is: 1890
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_Szip.txt`

```
Filter type is: H5Z_FILTER_SZIP

Maximum value in DS1 is: 1890
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_Transform.txt`

```
Original Data:
 [ 0  -1  -2  -3  -4  -5  -6 ]
 [ 0  0  0  0  0  0  0 ]
 [ 0  1  2  3  4  5  6 ]
 [ 0  2  4  6  8  10  12 ]
Data as written with transform 'x+1'
 [ 1  0  -1  -2  -3  -4  -5 ]
 [ 1  1  1  1  1  1  1 ]
 [ 1  2  3  4  5  6  7 ]
 [ 1  3  5  7  9  11  13 ]
Data as written with transform  'x+1' and read with transform  'x-1'
 [ 0  -1  -2  -3  -4  -5  -6 ]
 [ 0  0  0  0  0  0  0 ]
 [ 0  1  2  3  4  5  6 ]
 [ 0  2  4  6  8  10  12 ]
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_UnlimitedAdd.txt`

```
Dataset before extension:
 [ 0 -1 -2 -3 -4 -5 -6 ]
 [ 0 0 0 0 0 0 0 ]
 [ 0 1 2 3 4 5 6 ]
 [ 0 2 4 6 8 10 12 ]

Dataset after extension:
 [ 0 -1 -2 -3 -4 -5 -6 7 8 9 ]
 [ 0 0 0 0 0 0 0 7 8 9 ]
 [ 0 1 2 3 4 5 6 7 8 9 ]
 [ 0 2 4 6 8 10 12 7 8 9 ]
 [ 0 1 2 3 4 5 6 7 8 9 ]
 [ 0 1 2 3 4 5 6 7 8 9 ]
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_UnlimitedGzip.txt`

```
Dataset before extension:
 [ 0 -1 -2 -3 -4 -5 -6 ]
 [ 0 0 0 0 0 0 0 ]
 [ 0 1 2 3 4 5 6 ]
 [ 0 2 4 6 8 10 12 ]

Filter type is: H5Z_FILTER_DEFLATE

Dataset after extension:
 [ 0 -1 -2 -3 -4 -5 -6 7 8 9 ]
 [ 0 0 0 0 0 0 0 7 8 9 ]
 [ 0 1 2 3 4 5 6 7 8 9 ]
 [ 0 2 4 6 8 10 12 7 8 9 ]
 [ 0 1 2 3 4 5 6 7 8 9 ]
 [ 0 1 2 3 4 5 6 7 8 9 ]
```

### `HDF5Examples/JAVA/H5D/tfiles/110/H5Ex_D_UnlimitedMod.txt`

```
Dataset before extension:
 [ 0 -1 -2 -3 -4 -5 -6 ]
 [ 0 0 0 0 0 0 0 ]
 [ 0 1 2 3 4 5 6 ]
 [ 0 2 4 6 8 10 12 ]

Dataset after extension:
 [ 0 1 2 3 4 5 6 7 8 9 ]
 [ 0 1 2 3 4 5 6 7 8 9 ]
 [ 0 1 2 3 4 5 6 7 8 9 ]
 [ 0 1 2 3 4 5 6 7 8 9 ]
 [ 0 1 2 3 4 5 6 7 8 9 ]
 [ 0 1 2 3 4 5 6 7 8 9 ]
```

### `HDF5Examples/JAVA/H5G/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_JAVA_GROUPS Java)

set (CMAKE_VERBOSE_MAKEFILE 1)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (Java_sourcefiles.cmake)

if (WIN32)
  set (CMAKE_JAVA_INCLUDE_FLAG_SEP ";")
else ()
  set (CMAKE_JAVA_INCLUDE_FLAG_SEP ":")
endif ()

set (CMAKE_JAVA_INCLUDE_PATH ".")
foreach (CMAKE_JINCLUDE_PATH ${HDF5_JAVA_INCLUDE_DIRS})
  set (CMAKE_JAVA_INCLUDE_PATH "${CMAKE_JAVA_INCLUDE_PATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${CMAKE_JINCLUDE_PATH}")
endforeach ()

set (CMAKE_JAVA_CLASSPATH ".")
foreach (CMAKE_INCLUDE_PATH ${HDF5_JAVA_INCLUDE_DIRS})
  set (CMAKE_JAVA_CLASSPATH "${CMAKE_JAVA_CLASSPATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${CMAKE_INCLUDE_PATH}")
endforeach ()

foreach (HCP_JAR ${HDF5_JAVA_INCLUDE_DIRS})
  get_filename_component (_HCP_FILE ${HCP_JAR} NAME)
  set (HDFJAVA_CLASSJARS "${_HCP_FILE} ${HDFJAVA_CLASSJARS}")
endforeach ()

foreach (example ${HDF_JAVA_EXAMPLES})
  get_filename_component (example_name ${example} NAME_WE)
  file (WRITE ${PROJECT_BINARY_DIR}/${example_name}_Manifest.txt
  "Main-Class: ${example_name}
Class-Path: ${HDFJAVA_CLASSJARS}
"
  )
  add_jar (${EXAMPLE_VARNAME}J_${example_name}
      SOURCES ${example}
      MANIFEST ${PROJECT_BINARY_DIR}/${example_name}_Manifest.txt
  )
  get_target_property (${EXAMPLE_VARNAME}J_${example_name}_JAR_FILE ${EXAMPLE_VARNAME}J_${example_name} JAR_FILE)
  get_target_property (${EXAMPLE_VARNAME}J_${example_name}_CLASSPATH ${EXAMPLE_VARNAME}J_${example_name} CLASSDIR)
  if (H5EXAMPLE_JAVA_LIBRARIES)
    add_dependencies (${EXAMPLE_VARNAME}J_${example_name} ${H5EXAMPLE_JAVA_LIBRARIES})
  endif ()
endforeach ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST resultfile resultcode)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${resultfile}
        COMMAND "${CMAKE_COMMAND}"
            -D "TEST_JAVA=${CMAKE_Java_RUNTIME};${CMAKE_Java_RUNTIME_FLAGS}"
            -D "TEST_PROGRAM=${resultfile}"
            -D "TEST_ARGS:STRING=${ARGN};${CMD_ARGS}"
            -D "TEST_CLASSPATH:STRING=${CMAKE_JAVA_CLASSPATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${${EXAMPLE_VARNAME}J_${resultfile}_JAR_FILE}"
            -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
            -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
            -D "TEST_OUTPUT=${PROJECT_BINARY_DIR}/${resultfile}.out"
            -D "TEST_REFERENCE=${resultfile}.txt"
            -D "TEST_EXPECT=${resultcode}"
            -D "TEST_SKIP_COMPARE=TRUE"
            -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
    )
    if (NOT "${last_test}" STREQUAL "")
      set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${resultfile} PROPERTIES DEPENDS ${last_test})
    endif ()
    set (last_test "${EXAMPLE_VARNAME}_jnative-h5-${resultfile}")
  endmacro ()

  foreach (example ${HDF_JAVA_EXAMPLES})
    get_filename_component (example_name ${example} NAME_WE)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clearall-objects
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${PROJECT_BINARY_DIR}/${example_name}.h5
            ${PROJECT_BINARY_DIR}/${example_name}.out
            ${PROJECT_BINARY_DIR}/${example_name}.out.err
    )
    if (NOT "${last_test}" STREQUAL "")
      set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clearall-objects PROPERTIES DEPENDS ${last_test})
    endif ()
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-copy-objects
        COMMAND    ${CMAKE_COMMAND}
            -E copy_if_different
            ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.txt
            ${PROJECT_BINARY_DIR}/${example_name}.txt
    )
    set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${example_name}-copy-objects PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clearall-objects)
    set (last_test "${EXAMPLE_VARNAME}_jnative-h5-${example_name}-copy-objects")
    ADD_H5_TEST (${example_name} 0)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clean-objects
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${PROJECT_BINARY_DIR}/${example_name}.h5
            ${PROJECT_BINARY_DIR}/${example_name}.out
            ${PROJECT_BINARY_DIR}/${example_name}.out.err
    )
    set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clean-objects PROPERTIES DEPENDS ${last_test})
  endforeach ()

endif ()
```

### `HDF5Examples/JAVA/H5G/H5Ex_G_Compact.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
    Creating a file and print the storage layout.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

import org.hdfgroup.javahdf5.H5G_info_t;

public class H5Ex_G_Compact {

    private static final String FILE1 = "H5Ex_G_Compact1.h5";
    private static final String FILE2 = "H5Ex_G_Compact2.h5";
    private static final String GROUP = "G1";

    enum H5G_storage {
        H5G_STORAGE_TYPE_UNKNOWN(-1),
        H5G_STORAGE_TYPE_SYMBOL_TABLE(0),
        H5G_STORAGE_TYPE_COMPACT(1),
        H5G_STORAGE_TYPE_DENSE(2);

        private static final Map<Integer, H5G_storage> lookup = new HashMap<Integer, H5G_storage>();

        static
        {
            for (H5G_storage s : EnumSet.allOf(H5G_storage.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5G_storage(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5G_storage get(int code) { return lookup.get(code); }
    }

    public static void CreateGroup(Arena arena)
    {
        long file_id        = H5I_INVALID_HID();
        long group_id       = H5I_INVALID_HID();
        long fapl_id        = H5I_INVALID_HID();
        MemorySegment ginfo = H5G_info_t.allocate(arena);
        long size;

        // Create file 1. This file will use original format groups.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILE1), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        // Create a group in the file1.
        try {
            if (file_id >= 0)
                group_id = H5Gcreate2(file_id, arena.allocateFrom(GROUP), H5P_DEFAULT(), H5P_DEFAULT(),
                                      H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Obtain the group info and print the group storage type.
        try {
            if (group_id >= 0) {
                H5Gget_info(group_id, ginfo);
                System.out.print("Group storage type for " + FILE1 + " is: ");
                switch (H5G_storage.get(H5G_info_t.storage_type(ginfo))) {
                case H5G_STORAGE_TYPE_COMPACT:
                    System.out.println("H5G_STORAGE_TYPE_COMPACT"); // New compact format
                    break;
                case H5G_STORAGE_TYPE_DENSE:
                    System.out.println("H5G_STORAGE_TYPE_DENSE"); // New dense (indexed) format
                    break;
                case H5G_STORAGE_TYPE_SYMBOL_TABLE:
                    System.out.println("H5G_STORAGE_TYPE_SYMBOL_TABLE"); // Original format
                    break;
                case H5G_STORAGE_TYPE_UNKNOWN:
                    System.out.println("H5G_STORAGE_TYPE_UNKNOWN");
                    break;
                default:
                    System.out.println("Storage Type Invalid");
                    break;
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the group.
        try {
            if (group_id >= 0)
                H5Gclose(group_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // close the file 1.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Re-open file 1. Need to get the correct file size.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILE1), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Obtain and print the file size.
        try {
            if (file_id >= 0) {
                MemorySegment sizeSeg = arena.allocate(ValueLayout.JAVA_LONG);
                H5Fget_filesize(file_id, sizeSeg);
                size = sizeSeg.get(ValueLayout.JAVA_LONG, 0);
                System.out.println("File size for " + FILE1 + " is: " + size + " bytes");
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close FILE1.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Set file access property list to allow the latest file format.
        // This will allow the library to create new compact format groups.
        try {
            fapl_id = H5Pcreate(H5P_CLS_FILE_ACCESS_ID_g());
            if (fapl_id >= 0)
                H5Pset_libver_bounds(fapl_id, H5F_LIBVER_LATEST(), H5F_LIBVER_LATEST());
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        System.out.println();
        // Create file 2 using the new file access property list.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILE2), H5F_ACC_TRUNC(), H5P_DEFAULT(), fapl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        // Create group in file2.
        try {
            if (file_id >= 0)
                group_id = H5Gcreate2(file_id, arena.allocateFrom(GROUP), H5P_DEFAULT(), H5P_DEFAULT(),
                                      H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Obtain the group info and print the group storage type.
        try {
            if (group_id >= 0) {
                H5Gget_info(group_id, ginfo);
                System.out.print("Group storage type for " + FILE2 + " is: ");
                switch (H5G_storage.get(H5G_info_t.storage_type(ginfo))) {
                case H5G_STORAGE_TYPE_COMPACT:
                    System.out.println("H5G_STORAGE_TYPE_COMPACT"); // New compact format
                    break;
                case H5G_STORAGE_TYPE_DENSE:
                    System.out.println("H5G_STORAGE_TYPE_DENSE"); // New dense (indexed) format
                    break;
                case H5G_STORAGE_TYPE_SYMBOL_TABLE:
                    System.out.println("H5G_STORAGE_TYPE_SYMBOL_TABLE"); // Original format
                    break;
                case H5G_STORAGE_TYPE_UNKNOWN:
                    System.out.println("H5G_STORAGE_TYPE_UNKNOWN");
                    break;
                default:
                    System.out.println("Storage Type Invalid");
                    break;
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the group.
        try {
            if (group_id >= 0)
                H5Gclose(group_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // close the file 2.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Re-open file 2. Needed to get the correct file size.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILE2), H5F_ACC_RDONLY(), fapl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Obtain and print the file size.
        try {
            if (file_id >= 0) {
                MemorySegment sizeSeg = arena.allocate(ValueLayout.JAVA_LONG);
                H5Fget_filesize(file_id, sizeSeg);
                size = sizeSeg.get(ValueLayout.JAVA_LONG, 0);
                System.out.println("File size for " + FILE2 + " is: " + size + " bytes");
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close FILE2.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            H5Ex_G_Compact.CreateGroup(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5G/H5Ex_G_Corder.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
/************************************************************
    Creating a file with creation properties and traverse the
    groups in alphabetical and creation order.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

import org.hdfgroup.javahdf5.H5G_info_t;

public class H5Ex_G_Corder {
    private static String FILENAME = "H5Ex_G_Corder.h5";

    private static void CreateGroup(Arena arena) throws Exception
    {
        long file_id     = H5I_INVALID_HID();
        long group_id    = H5I_INVALID_HID();
        long subgroup_id = H5I_INVALID_HID();
        long gcpl_id     = H5I_INVALID_HID();
        int status;
        MemorySegment ginfo = H5G_info_t.allocate(arena);
        int i;
        String name;

        try {
            // Create a new file using default properties.
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());

            // Create group creation property list and enable link creation order tracking.
            gcpl_id = H5Pcreate(H5P_CLS_GROUP_CREATE_ID_g());
            status  = H5Pset_link_creation_order(gcpl_id, H5P_CRT_ORDER_TRACKED() + H5P_CRT_ORDER_INDEXED());

            // Create primary group using the property list.
            if (status >= 0)
                group_id = H5Gcreate2(file_id, arena.allocateFrom("index_group"), H5P_DEFAULT(), gcpl_id,
                                      H5P_DEFAULT());

            try {
                /*
                 * Create subgroups in the primary group. These will be tracked by creation order. Note that
                 * these groups do not have to have the creation order tracking property set.
                 */
                subgroup_id = H5Gcreate2(group_id, arena.allocateFrom("H"), H5P_DEFAULT(), H5P_DEFAULT(),
                                         H5P_DEFAULT());
                status      = H5Gclose(subgroup_id);
                subgroup_id = H5Gcreate2(group_id, arena.allocateFrom("D"), H5P_DEFAULT(), H5P_DEFAULT(),
                                         H5P_DEFAULT());
                status      = H5Gclose(subgroup_id);
                subgroup_id = H5Gcreate2(group_id, arena.allocateFrom("F"), H5P_DEFAULT(), H5P_DEFAULT(),
                                         H5P_DEFAULT());
                status      = H5Gclose(subgroup_id);
                subgroup_id = H5Gcreate2(group_id, arena.allocateFrom("5"), H5P_DEFAULT(), H5P_DEFAULT(),
                                         H5P_DEFAULT());
                status      = H5Gclose(subgroup_id);

                // Get group info.
                H5Gget_info(group_id, ginfo);
                long nlinks = H5G_info_t.nlinks(ginfo);

                // Traverse links in the primary group using alphabetical indices (H5_INDEX_NAME).
                System.out.println("Traversing group using alphabetical indices:");
                for (i = 0; i < nlinks; i++) {
                    // Retrieve the name of the ith link in a group - first query size
                    long name_size =
                        H5Lget_name_by_idx(group_id, arena.allocateFrom("."), H5_INDEX_NAME(), H5_ITER_INC(),
                                           i, MemorySegment.NULL, 0, H5P_DEFAULT());
                    MemorySegment nameBuffer = arena.allocate(name_size + 1);
                    H5Lget_name_by_idx(group_id, arena.allocateFrom("."), H5_INDEX_NAME(), H5_ITER_INC(), i,
                                       nameBuffer, name_size + 1, H5P_DEFAULT());
                    name = nameBuffer.getString(0);
                    System.out.println("Index " + i + ": " + name);
                }

                // Traverse links in the primary group by creation order (H5_INDEX_CRT_ORDER).
                System.out.println("Traversing group using creation order indices:");
                for (i = 0; i < nlinks; i++) {
                    // Retrieve the name of the ith link in a group - first query size
                    long name_size =
                        H5Lget_name_by_idx(group_id, arena.allocateFrom("."), H5_INDEX_CRT_ORDER(),
                                           H5_ITER_INC(), i, MemorySegment.NULL, 0, H5P_DEFAULT());
                    MemorySegment nameBuffer = arena.allocate(name_size + 1);
                    H5Lget_name_by_idx(group_id, arena.allocateFrom("."), H5_INDEX_CRT_ORDER(), H5_ITER_INC(),
                                       i, nameBuffer, name_size + 1, H5P_DEFAULT());
                    name = nameBuffer.getString(0);
                    System.out.println("Index " + i + ": " + name);
                }
            }
            catch (Exception e) {
                e.printStackTrace();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        finally {
            // Close and release resources.
            if (gcpl_id >= 0)
                H5Pclose(gcpl_id);
            if (group_id >= 0)
                H5Gclose(group_id);
            if (file_id >= 0)
                H5Fclose(file_id);
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            try {
                H5Ex_G_Corder.CreateGroup(arena);
            }
            catch (Exception ex) {
                ex.printStackTrace();
            }
        }
    }
}
```

### `HDF5Examples/JAVA/H5G/H5Ex_G_Create.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to create, open, and close a group.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_G_Create {
    private static String FILENAME  = "H5Ex_G_Create.h5";
    private static String GROUPNAME = "G1";

    private static void CreateGroup(Arena arena)
    {
        long file_id  = H5I_INVALID_HID();
        long group_id = H5I_INVALID_HID();

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create a group in the file.
        try {
            if (file_id >= 0)
                group_id = H5Gcreate2(file_id, arena.allocateFrom("/" + GROUPNAME), H5P_DEFAULT(),
                                      H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the group. The handle "group" can no longer be used.
        try {
            if (group_id >= 0)
                H5Gclose(group_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Re-open the group, obtaining a new handle.
        try {
            if (file_id >= 0)
                group_id = H5Gopen2(file_id, arena.allocateFrom("/" + GROUPNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the group.
        try {
            if (group_id >= 0)
                H5Gclose(group_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            H5Ex_G_Create.CreateGroup(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5G/H5Ex_G_Intermediate.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
 This example shows how to create intermediate groups with
 a single call to H5Gcreate.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

import org.hdfgroup.javahdf5.*;

public class H5Ex_G_Intermediate {

    private static String FILENAME = "H5Ex_G_Intermediate.h5";

    private void CreateGroup(Arena arena) throws Exception
    {
        long file_id  = H5I_INVALID_HID();
        long group_id = H5I_INVALID_HID();
        long gcpl_id  = H5I_INVALID_HID();

        try {
            // Create a new file using the default properties
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());

            // Create group creation property list and set it to allow creation of intermediate groups
            gcpl_id = H5Pcreate(H5P_CLS_LINK_CREATE_ID_g());
            H5Pset_create_intermediate_group(gcpl_id, 1); // 1 = true

            /*
             * Create the group /G1/G2/G3. Note that /G1 and /G1/G2 do not exist yet.
             * This call would cause an error if we did not use the previously created property list.
             */
            group_id =
                H5Gcreate2(file_id, arena.allocateFrom("/G1/G2/G3"), gcpl_id, H5P_DEFAULT(), H5P_DEFAULT());

            // Print all the objects in the file to show that intermediate groups have been created
            System.out.println("Objects in the file:");

            // Create callback for H5Ovisit
            H5O_iterate2_t.Function obj_callback =
                (long obj, MemorySegment name, MemorySegment info, MemorySegment op_data) ->
            {
                String obj_name = name.getString(0);
                int obj_type    = H5O_info2_t.type(info);

                System.out.print("/"); // Print root group in object path

                // Check if the current object is the root group, and if not print the full path name and type
                if (obj_name.charAt(0) == '.') {
                    // Root group, do not print '.'
                    System.out.println("  (Group)");
                }
                else if (obj_type == H5O_TYPE_GROUP()) {
                    System.out.println(obj_name + "  (Group)");
                }
                else if (obj_type == H5O_TYPE_DATASET()) {
                    System.out.println(obj_name + "  (Dataset)");
                }
                else if (obj_type == H5O_TYPE_NAMED_DATATYPE()) {
                    System.out.println(obj_name + "  (Datatype)");
                }
                else {
                    System.out.println(obj_name + "  (Unknown)");
                }

                return 0; // Continue iteration
            };

            // Allocate upcall stub for callback
            MemorySegment obj_callback_stub = H5O_iterate2_t.allocate(obj_callback, arena);

            // Call H5Ovisit
            H5Ovisit3(file_id, H5_INDEX_NAME(), H5_ITER_NATIVE(), obj_callback_stub, MemorySegment.NULL,
                      H5O_INFO_ALL());
        }
        finally {
            // Close and release resources
            if (gcpl_id >= 0)
                H5Pclose(gcpl_id);
            if (group_id >= 0)
                H5Gclose(group_id);
            if (file_id >= 0)
                H5Fclose(file_id);
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            (new H5Ex_G_Intermediate()).CreateGroup(arena);
        }
        catch (Exception ex) {
            ex.printStackTrace();
        }
    }
}
```

### `HDF5Examples/JAVA/H5G/H5Ex_G_Iterate.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to iterate over group members using
  H5Gget_obj_info_all.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

import hdf.hdf5lib.H5;
import hdf.hdf5lib.structs.H5O_token_t;

public class H5Ex_G_Iterate {
    private static String FILENAME    = "groups/h5ex_g_iterate.h5";
    private static String DATASETNAME = "/";

    enum H5O_type {
        H5O_TYPE_UNKNOWN(-1),       // Unknown object type
        H5O_TYPE_GROUP(0),          // Object is a group
        H5O_TYPE_DATASET(1),        // Object is a dataset
        H5O_TYPE_NAMED_DATATYPE(2), // Object is a named data type
        H5O_TYPE_NTYPES(3);         // Number of different object types
        private static final Map<Integer, H5O_type> lookup = new HashMap<Integer, H5O_type>();

        static
        {
            for (H5O_type s : EnumSet.allOf(H5O_type.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5O_type(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5O_type get(int code) { return lookup.get(code); }
    }

    private static void do_iterate(Arena arena)
    {
        long file_id = H5I_INVALID_HID();

        // Open a file using default properties.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Begin iteration.
        System.out.println("Objects in root group:");
        try {
            if (file_id >= 0) {
                int count             = (int)H5.H5Gn_members(file_id, DATASETNAME);
                String[] oname        = new String[count];
                int[] otype           = new int[count];
                int[] ltype           = new int[count];
                H5O_token_t[] otokens = new H5O_token_t[count];
                H5.H5Gget_obj_info_all(file_id, DATASETNAME, oname, otype, ltype, otokens, H5_INDEX_NAME());

                // Get type of the object and display its name and type.
                for (int indx = 0; indx < otype.length; indx++) {
                    switch (H5O_type.get(otype[indx])) {
                    case H5O_TYPE_GROUP:
                        System.out.println("  Group: " + oname[indx]);
                        break;
                    case H5O_TYPE_DATASET:
                        System.out.println("  Dataset: " + oname[indx]);
                        break;
                    case H5O_TYPE_NAMED_DATATYPE:
                        System.out.println("  Datatype: " + oname[indx]);
                        break;
                    default:
                        System.out.println("  Unknown: " + oname[indx]);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            H5Ex_G_Iterate.do_iterate(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5G/H5Ex_G_Phase.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to set the conditions for
  conversion between compact and dense (indexed) groups.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

import org.hdfgroup.javahdf5.H5G_info_t;

public class H5Ex_G_Phase {
    private static String FILENAME = "H5Ex_G_Phase.h5";
    private static int MAX_GROUPS  = 7;
    private static int MAX_COMPACT = 5;
    private static int MIN_DENSE   = 3;

    enum H5G_storage {
        H5G_STORAGE_TYPE_UNKNOWN(-1),
        H5G_STORAGE_TYPE_SYMBOL_TABLE(0),
        H5G_STORAGE_TYPE_COMPACT(1),
        H5G_STORAGE_TYPE_DENSE(2);

        private static final Map<Integer, H5G_storage> lookup = new HashMap<Integer, H5G_storage>();

        static
        {
            for (H5G_storage s : EnumSet.allOf(H5G_storage.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5G_storage(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5G_storage get(int code) { return lookup.get(code); }
    }

    private static void CreateGroup(Arena arena)
    {
        long file_id        = H5I_INVALID_HID();
        long group_id       = H5I_INVALID_HID();
        long subgroup_id    = H5I_INVALID_HID();
        long fapl_id        = H5I_INVALID_HID();
        long gcpl_id        = H5I_INVALID_HID();
        MemorySegment ginfo = H5G_info_t.allocate(arena);
        String name         = "G0"; // Name of subgroup_id
        int i;

        // Set file access property list to allow the latest file format.This will allow the library to create
        // new format groups.
        try {
            fapl_id = H5Pcreate(H5P_CLS_FILE_ACCESS_ID_g());
            if (fapl_id >= 0)
                H5Pset_libver_bounds(fapl_id, H5F_LIBVER_LATEST(), H5F_LIBVER_LATEST());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create group access property list and set the phase change conditions.
        try {
            gcpl_id = H5Pcreate(H5P_CLS_GROUP_CREATE_ID_g());
            if (gcpl_id >= 0)
                H5Pset_link_phase_change(gcpl_id, MAX_COMPACT, MIN_DENSE);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create a new file using the default properties.
        try {
            if (fapl_id >= 0)
                file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), fapl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create primary group.
        try {
            if ((file_id >= 0) && (gcpl_id >= 0))
                group_id =
                    H5Gcreate2(file_id, arena.allocateFrom(name), H5P_DEFAULT(), gcpl_id, H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Add subgroups to "group" one at a time, print the storage type for "group" after each subgroup is
        // created.
        for (i = 1; i <= MAX_GROUPS; i++) {
            // Define the subgroup name and create the subgroup.
            char append = (char)(((char)i) + '0');
            name        = name + append; /* G1, G2, G3 etc. */
            try {
                if (group_id >= 0) {
                    subgroup_id = H5Gcreate2(group_id, arena.allocateFrom(name), H5P_DEFAULT(), H5P_DEFAULT(),
                                             H5P_DEFAULT());
                    H5Gclose(subgroup_id);
                }
            }
            catch (Exception e) {
                e.printStackTrace();
            }

            // Obtain the group info and print the group storage type
            try {
                if (group_id >= 0) {
                    H5Gget_info(group_id, ginfo);
                    System.out.print(H5G_info_t.nlinks(ginfo) + " Group" +
                                     (H5G_info_t.nlinks(ginfo) == 1 ? " " : "s") + ": Storage type is ");
                    switch (H5G_storage.get(H5G_info_t.storage_type(ginfo))) {
                    case H5G_STORAGE_TYPE_COMPACT:
                        System.out.println("H5G_STORAGE_TYPE_COMPACT"); // New compact format
                        break;
                    case H5G_STORAGE_TYPE_DENSE:
                        System.out.println("H5G_STORAGE_TYPE_DENSE"); // New dense (indexed) format
                        break;
                    case H5G_STORAGE_TYPE_SYMBOL_TABLE:
                        System.out.println("H5G_STORAGE_TYPE_SYMBOL_TABLE"); // Original format
                        break;
                    case H5G_STORAGE_TYPE_UNKNOWN:
                        System.out.println("H5G_STORAGE_TYPE_UNKNOWN");
                        break;
                    default:
                        System.out.println("Storage Type Invalid");
                        break;
                    }
                }
            }
            catch (Exception e) {
                e.printStackTrace();
            }
        }

        System.out.println();

        // Delete subgroups one at a time, print the storage type for "group" after each subgroup is deleted.
        for (i = MAX_GROUPS; i >= 1; i--) {
            // Define the subgroup name and delete the subgroup.
            try {
                H5Ldelete(group_id, arena.allocateFrom(name), H5P_DEFAULT());
            }
            catch (Exception e) {
                e.printStackTrace();
            }
            name = name.substring(0, i + 1);

            // Obtain the group info and print the group storage type
            try {
                if (group_id >= 0) {
                    H5Gget_info(group_id, ginfo);
                    System.out.print(H5G_info_t.nlinks(ginfo) + " Group" +
                                     (H5G_info_t.nlinks(ginfo) == 1 ? " " : "s") + ": Storage type is ");
                    switch (H5G_storage.get(H5G_info_t.storage_type(ginfo))) {
                    case H5G_STORAGE_TYPE_COMPACT:
                        System.out.println("H5G_STORAGE_TYPE_COMPACT"); // New compact format
                        break;
                    case H5G_STORAGE_TYPE_DENSE:
                        System.out.println("H5G_STORAGE_TYPE_DENSE"); // New dense (indexed) format
                        break;
                    case H5G_STORAGE_TYPE_SYMBOL_TABLE:
                        System.out.println("H5G_STORAGE_TYPE_SYMBOL_TABLE"); // Original format
                        break;
                    case H5G_STORAGE_TYPE_UNKNOWN:
                        System.out.println("H5G_STORAGE_TYPE_UNKNOWN");
                        break;
                    default:
                        System.out.println("Storage Type Invalid");
                        break;
                    }
                }
            }
            catch (Exception e) {
                e.printStackTrace();
            }
        }

        // Close and release resources
        try {
            if (fapl_id >= 0)
                H5Pclose(fapl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (gcpl_id >= 0)
                H5Pclose(gcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the group
        try {
            if (group_id >= 0)
                H5Gclose(group_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            H5Ex_G_Phase.CreateGroup(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5G/H5Ex_G_Traverse.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
This example shows a way to recursively traverse the file
using H5Literate.  The method shown here guarantees that
the recursion will not enter an infinite loop, but does
not prevent objects from being visited more than once.
The program prints the directory structure of the file
specified in FILE.  The default file used by this example
implements the structure described in the User's Guide,
chapter 4, figure 26.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.ArrayList;
import java.util.List;

import hdf.hdf5lib.structs.H5O_token_t;

import org.hdfgroup.javahdf5.*;

public class H5Ex_G_Traverse {

    private static String FILENAME = "groups/h5ex_g_traverse.h5";

    // Operator data structure for iteration
    static class OpData {
        int recurs;            // Recursion level
        OpData prev;           // Previous operator data
        H5O_token_t obj_token; // Object token
    }

    private static void OpenGroup(Arena arena)
    {
        long file_id = H5I_INVALID_HID();

        try {
            // Open file
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());

            if (file_id >= 0) {
                // Get info for root group
                MemorySegment root_info = arena.allocate(H5O_info2_t.sizeof());
                H5Oget_info3(file_id, root_info, H5O_INFO_ALL());

                // Initialize operator data
                OpData od    = new OpData();
                od.recurs    = 0;
                od.prev      = null;
                od.obj_token = new H5O_token_t(H5O_info2_t.token(root_info));

                // Print root group and begin iteration
                System.out.println("/ {");

                // Create and iterate
                iterateGroup(arena, file_id, od);

                System.out.println("}");
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        finally {
            // Close and release resources
            try {
                if (file_id >= 0)
                    H5Fclose(file_id);
            }
            catch (Exception e) {
                e.printStackTrace();
            }
        }
    }

    private static void iterateGroup(Arena arena, long group_id, OpData od)
    {
        // Create callback for H5Literate
        H5L_iterate2_t.Function link_callback =
            (long group, MemorySegment name, MemorySegment info, MemorySegment op_data) ->
        {
            String link_name = name.getString(0);
            int spaces       = 2 * (od.recurs + 1); // Indentation

            try {
                // Get object info
                MemorySegment obj_info = arena.allocate(H5O_info2_t.sizeof());
                int ret = H5Oget_info_by_name3(group, name, obj_info, H5O_INFO_ALL(), H5P_DEFAULT());

                if (ret >= 0) {
                    int obj_type          = H5O_info2_t.type(obj_info);
                    H5O_token_t obj_token = new H5O_token_t(H5O_info2_t.token(obj_info));

                    // Print indentation
                    for (int i = 0; i < spaces; i++)
                        System.out.print(" ");

                    if (obj_type == H5O_TYPE_GROUP()) {
                        System.out.println("Group: " + link_name + " {");

                        // Check for loops
                        if (groupCheck(od, obj_token)) {
                            for (int i = 0; i < spaces; i++)
                                System.out.print(" ");
                            System.out.println("  Warning: Loop detected!");
                        }
                        else {
                            // Create new operator data for recursion
                            OpData nextod    = new OpData();
                            nextod.recurs    = od.recurs + 1;
                            nextod.prev      = od;
                            nextod.obj_token = obj_token;

                            // Recurse into group
                            long subgroup_id = H5Gopen2(group, name, H5P_DEFAULT());
                            if (subgroup_id >= 0) {
                                try {
                                    iterateGroup(arena, subgroup_id, nextod);
                                }
                                finally {
                                    H5Gclose(subgroup_id);
                                }
                            }
                        }

                        for (int i = 0; i < spaces; i++)
                            System.out.print(" ");
                        System.out.println("}");
                    }
                    else if (obj_type == H5O_TYPE_DATASET()) {
                        System.out.println("Dataset: " + link_name);
                    }
                    else if (obj_type == H5O_TYPE_NAMED_DATATYPE()) {
                        System.out.println("Datatype: " + link_name);
                    }
                    else {
                        System.out.println("Unknown: " + link_name);
                    }
                }
            }
            catch (Exception e) {
                e.printStackTrace();
            }

            return 0; // Continue iteration
        };

        // Allocate upcall stub
        MemorySegment link_callback_stub = H5L_iterate2_t.allocate(link_callback, arena);

        // Iterate over links in group
        H5Literate2(group_id, H5_INDEX_NAME(), H5_ITER_NATIVE(), MemorySegment.NULL, link_callback_stub,
                    MemorySegment.NULL);
    }

    // Check if we've already visited this object token (loop detection)
    private static boolean groupCheck(OpData od, H5O_token_t target_token)
    {
        if (od.obj_token.equals(target_token))
            return true; // Object tokens match
        else if (od.recurs == 0)
            return false; // Root group reached with no matches
        else
            return groupCheck(od.prev, target_token); // Recursively examine the next node
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            H5Ex_G_Traverse.OpenGroup(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5G/H5Ex_G_Visit.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
 This example shows how to recursively traverse a file
 using H5Ovisit and H5Lvisit.  The program prints all of
 the objects in the file specified in FILE, then prints all
 of the links in that file.  The default file used by this
 example implements the structure described in the User
 Guide, chapter 4, figure 26.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

import org.hdfgroup.javahdf5.*;

public class H5Ex_G_Visit {

    private static String FILENAME = "groups/h5ex_g_visit.h5";

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            (new H5Ex_G_Visit()).VisitGroup(arena);
        }
        catch (Exception ex) {
            ex.printStackTrace();
        }
    }

    private void VisitGroup(Arena arena) throws Exception
    {
        long file_id = H5I_INVALID_HID();

        try {
            // Open file
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());

            // Begin iteration using H5Ovisit
            System.out.println("Objects in the file:");

            // Create callback for H5Ovisit
            H5O_iterate2_t.Function obj_callback =
                (long group, MemorySegment name, MemorySegment info, MemorySegment op_data) ->
            {
                String obj_name = name.getString(0);
                int obj_type    = H5O_info2_t.type(info);

                System.out.print("/"); // Print root group in object path

                // Check if the current object is the root group
                if (obj_name.charAt(0) == '.') {
                    // Root group, do not print '.'
                    System.out.println("  (Group)");
                }
                else if (obj_type == H5O_TYPE_GROUP()) {
                    System.out.println(obj_name + "  (Group)");
                }
                else if (obj_type == H5O_TYPE_DATASET()) {
                    System.out.println(obj_name + "  (Dataset)");
                }
                else if (obj_type == H5O_TYPE_NAMED_DATATYPE()) {
                    System.out.println(obj_name + "  (Datatype)");
                }
                else {
                    System.out.println(obj_name + "  (Unknown)");
                }

                return 0; // Continue iteration
            };

            // Allocate upcall stub for object callback
            MemorySegment obj_callback_stub = H5O_iterate2_t.allocate(obj_callback, arena);

            // Call H5Ovisit
            H5Ovisit3(file_id, H5_INDEX_NAME(), H5_ITER_NATIVE(), obj_callback_stub, MemorySegment.NULL,
                      H5O_INFO_ALL());

            System.out.println();

            // Repeat the same process using H5Lvisit
            System.out.println("Links in the file:");

            // Create callback for H5Lvisit
            H5L_iterate2_t.Function link_callback =
                (long group, MemorySegment name, MemorySegment info, MemorySegment op_data) ->
            {
                String link_name = name.getString(0);
                int link_type    = H5L_info2_t.type(info);

                // Get type of the object the link points to
                try {
                    MemorySegment obj_info = arena.allocate(H5O_info2_t.sizeof());
                    int ret = H5Oget_info_by_name3(group, name, obj_info, H5O_INFO_ALL(), H5P_DEFAULT());

                    if (ret >= 0) {
                        int obj_type = H5O_info2_t.type(obj_info);

                        System.out.print("/"); // Print root group in object path

                        // Check if current object is root group
                        if (link_name.charAt(0) == '.') {
                            System.out.println("  (Group)");
                        }
                        else if (obj_type == H5O_TYPE_GROUP()) {
                            System.out.println(link_name + "  (Group)");
                        }
                        else if (obj_type == H5O_TYPE_DATASET()) {
                            System.out.println(link_name + "  (Dataset)");
                        }
                        else if (obj_type == H5O_TYPE_NAMED_DATATYPE()) {
                            System.out.println(link_name + "  (Datatype)");
                        }
                        else {
                            System.out.println(link_name + "  (Unknown)");
                        }
                    }
                }
                catch (Exception e) {
                    e.printStackTrace();
                }

                return 0; // Continue iteration
            };

            // Allocate upcall stub for link callback
            MemorySegment link_callback_stub = H5L_iterate2_t.allocate(link_callback, arena);

            // Call H5Lvisit
            H5Lvisit2(file_id, H5_INDEX_NAME(), H5_ITER_NATIVE(), link_callback_stub, MemorySegment.NULL);
        }
        finally {
            // Close and release resources
            if (file_id >= 0)
                H5Fclose(file_id);
        }
    }
}
```

### `HDF5Examples/JAVA/H5G/Java_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (HDF_JAVA_EXAMPLES
    H5Ex_G_Create.java
    H5Ex_G_Compact.java
    H5Ex_G_Corder.java
    H5Ex_G_Phase.java
    H5Ex_G_Iterate.java
    H5Ex_G_Intermediate.java
    H5Ex_G_Visit.java
)
set (HDF_JAVA_EXTRA_EXAMPLES
    H5Ex_G_Traverse.java
)
```

### `HDF5Examples/JAVA/H5G/tfiles/110/H5Ex_G_Compact.txt`

```
Group storage type for H5Ex_G_Compact1.h5 is: H5G_STORAGE_TYPE_SYMBOL_TABLE
File size for H5Ex_G_Compact1.h5 is: 1832 bytes

Group storage type for H5Ex_G_Compact2.h5 is: H5G_STORAGE_TYPE_COMPACT
File size for H5Ex_G_Compact2.h5 is: 342 bytes
```

### `HDF5Examples/JAVA/H5G/tfiles/110/H5Ex_G_Corder.txt`

```
Traversing group using alphabetical indices:
Index 0: 5
Index 1: D
Index 2: F
Index 3: H
Traversing group using creation order indices:
Index 0: H
Index 1: D
Index 2: F
Index 3: 5
```

### `HDF5Examples/JAVA/H5G/tfiles/110/H5Ex_G_Create.txt`

```

```

### `HDF5Examples/JAVA/H5G/tfiles/110/H5Ex_G_Intermediate.txt`

```
Objects in the file_id:
/  (Group)
/G1  (Group)
/G1/G2  (Group)
/G1/G2/G3  (Group)
```

### `HDF5Examples/JAVA/H5G/tfiles/110/H5Ex_G_Iterate.txt`

```
Objects in root group:
  Dataset: DS1
  Datatype: DT1
  Group: G1
  Dataset: L1
```

### `HDF5Examples/JAVA/H5G/tfiles/110/H5Ex_G_Phase.txt`

```
1 Group : Storage type is H5G_STORAGE_TYPE_COMPACT
2 Groups: Storage type is H5G_STORAGE_TYPE_COMPACT
3 Groups: Storage type is H5G_STORAGE_TYPE_COMPACT
4 Groups: Storage type is H5G_STORAGE_TYPE_COMPACT
5 Groups: Storage type is H5G_STORAGE_TYPE_COMPACT
6 Groups: Storage type is H5G_STORAGE_TYPE_DENSE
7 Groups: Storage type is H5G_STORAGE_TYPE_DENSE

6 Groups: Storage type is H5G_STORAGE_TYPE_DENSE
5 Groups: Storage type is H5G_STORAGE_TYPE_DENSE
4 Groups: Storage type is H5G_STORAGE_TYPE_DENSE
3 Groups: Storage type is H5G_STORAGE_TYPE_DENSE
2 Groups: Storage type is H5G_STORAGE_TYPE_COMPACT
1 Group : Storage type is H5G_STORAGE_TYPE_COMPACT
0 Groups: Storage type is H5G_STORAGE_TYPE_COMPACT
```

### `HDF5Examples/JAVA/H5G/tfiles/110/H5Ex_G_Visit.txt`

```
Objects in the file:
/  (Group)
/group1  (Group)
/group1/dset1  (Dataset)
/group1/group3  (Group)
/group1/group3/group4  (Group)
/group1/group3/group4/group1  (Group)
/group1/group3/group4/group2  (Group)

Links in the file:
/group1  (Group)
/group1/dset1  (Dataset)
/group1/group3  (Group)
/group1/group3/dset2  (Dataset)
/group1/group3/group4  (Group)
/group1/group3/group4/group1  (Group)
/group1/group3/group4/group1/group5  (Group)
/group1/group3/group4/group2  (Group)
/group2  (Group)
```

### `HDF5Examples/JAVA/H5T/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_JAVA_H5T Java)

set (CMAKE_VERBOSE_MAKEFILE 1)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (Java_sourcefiles.cmake)

if (WIN32)
  set (CMAKE_JAVA_INCLUDE_FLAG_SEP ";")
else ()
  set (CMAKE_JAVA_INCLUDE_FLAG_SEP ":")
endif ()

set (CMAKE_JAVA_INCLUDE_PATH ".")
foreach (CMAKE_JINCLUDE_PATH ${HDF5_JAVA_INCLUDE_DIRS})
  set (CMAKE_JAVA_INCLUDE_PATH "${CMAKE_JAVA_INCLUDE_PATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${CMAKE_JINCLUDE_PATH}")
endforeach ()
if (Java_VERSION_STRING VERSION_LESS "25.0.0" OR HDF5_PROVIDES_JNI)
  set (CMD_ARGS "-Dhdf.hdf5lib.H5.loadLibraryName=${H5EXAMPLE_JAVA_LIBRARY}$<$<OR:$<CONFIG:Debug>,$<CONFIG:Developer>>:${CMAKE_DEBUG_POSTFIX}>;")
endif ()

set (CMAKE_JAVA_CLASSPATH ".")
foreach (CMAKE_INCLUDE_PATH ${HDF5_JAVA_INCLUDE_DIRS})
  set (CMAKE_JAVA_CLASSPATH "${CMAKE_JAVA_CLASSPATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${CMAKE_INCLUDE_PATH}")
endforeach ()

foreach (HCP_JAR ${HDF5_JAVA_INCLUDE_DIRS})
  get_filename_component (_HCP_FILE ${HCP_JAR} NAME)
  set (HDFJAVA_CLASSJARS "${_HCP_FILE} ${HDFJAVA_CLASSJARS}")
endforeach ()

foreach (example ${HDF_JAVA_EXAMPLES})
  get_filename_component (example_name ${example} NAME_WE)
  file (WRITE ${PROJECT_BINARY_DIR}/${example_name}_Manifest.txt
  "Main-Class: ${example_name}
Class-Path: ${HDFJAVA_CLASSJARS}
"
  )
  add_jar (${EXAMPLE_VARNAME}J_${example_name}
      SOURCES ${example}
      MANIFEST ${PROJECT_BINARY_DIR}/${example_name}_Manifest.txt
  )
  get_target_property (${EXAMPLE_VARNAME}J_${example_name}_JAR_FILE ${EXAMPLE_VARNAME}J_${example_name} JAR_FILE)
  get_target_property (${EXAMPLE_VARNAME}J_${example_name}_CLASSPATH ${EXAMPLE_VARNAME}J_${example_name} CLASSDIR)
  if (H5EXAMPLE_JAVA_LIBRARIES)
    add_dependencies (${EXAMPLE_VARNAME}J_${example_name} ${H5EXAMPLE_JAVA_LIBRARIES})
  endif ()
endforeach ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST resultfile resultcode)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${resultfile}
        COMMAND "${CMAKE_COMMAND}"
            -D "TEST_JAVA=${CMAKE_Java_RUNTIME};${CMAKE_Java_RUNTIME_FLAGS}"
            -D "TEST_PROGRAM=${resultfile}"
            -D "TEST_ARGS:STRING=${ARGN};${CMD_ARGS}"
            -D "TEST_CLASSPATH:STRING=${CMAKE_JAVA_CLASSPATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${${EXAMPLE_VARNAME}J_${resultfile}_JAR_FILE}"
            -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
            -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
            -D "TEST_OUTPUT=${PROJECT_BINARY_DIR}/${resultfile}.out"
            -D "TEST_REFERENCE=${resultfile}.txt"
            -D "TEST_EXPECT=${resultcode}"
            -D "TEST_SKIP_COMPARE=TRUE"
            -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
    )
    if (NOT "${last_test}" STREQUAL "")
      set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${resultfile} PROPERTIES DEPENDS ${last_test})
    endif ()
    set (last_test "${EXAMPLE_VARNAME}_jnative-h5-${resultfile}")
  endmacro ()

  foreach (example ${HDF_JAVA_EXAMPLES})
    get_filename_component (example_name ${example} NAME_WE)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clearall-objects
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${PROJECT_BINARY_DIR}/${example_name}.h5
            ${PROJECT_BINARY_DIR}/${example_name}.out
            ${PROJECT_BINARY_DIR}/${example_name}.out.err
    )
    if (NOT "${last_test}" STREQUAL "")
      set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clearall-objects PROPERTIES DEPENDS ${last_test})
    endif ()
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-copy-objects
        COMMAND    ${CMAKE_COMMAND}
            -E copy_if_different
            ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.txt
            ${PROJECT_BINARY_DIR}/${example_name}.txt
    )
    set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${example_name}-copy-objects PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clearall-objects)
    set (last_test "${EXAMPLE_VARNAME}_jnative-h5-${example_name}-copy-objects")
    ADD_H5_TEST (${example_name} 0)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clean-objects
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${PROJECT_BINARY_DIR}/${example_name}.h5
            ${PROJECT_BINARY_DIR}/${example_name}.out
            ${PROJECT_BINARY_DIR}/${example_name}.out.err
    )
    set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clean-objects PROPERTIES DEPENDS ${last_test})
  endforeach ()

endif ()
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_Array.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write array datatypes
  to a dataset.  The program first writes integers arrays of
  dimension ADIM0xADIM1 to a dataset with a dataspace of
  DIM0, then closes the  file.  Next, it reopens the file,
  reads back the data, and outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_T_Array {
    private static String FILENAME    = "H5Ex_T_Array.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM0     = 4;
    private static final int ADIM0    = 3;
    private static final int ADIM1    = 5;
    private static final int RANK     = 1;
    private static final int NDIMS    = 2;

    private static void CreateDataset(Arena arena)
    {
        long file_id        = H5I_INVALID_HID();
        long filetype_id    = H5I_INVALID_HID();
        long memtype_id     = H5I_INVALID_HID();
        long dataspace_id   = H5I_INVALID_HID();
        long dataset_id     = H5I_INVALID_HID();
        long[] dims         = {DIM0};
        long[] adims        = {ADIM0, ADIM1};
        int[][][] dset_data = new int[DIM0][ADIM0][ADIM1];

        // Initialize data. indx is the element in the dataspace, jndx and kndx the
        // elements within the array datatype.
        for (int indx = 0; indx < DIM0; indx++)
            for (int jndx = 0; jndx < ADIM0; jndx++)
                for (int kndx = 0; kndx < ADIM1; kndx++)
                    dset_data[indx][jndx][kndx] = indx * jndx - jndx * kndx + indx * kndx;

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create array datatypes for file.
        try {
            filetype_id =
                H5Tarray_create2(H5T_STD_I64LE_g(), NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, adims));
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create array datatypes for memory.
        try {
            memtype_id =
                H5Tarray_create2(H5T_NATIVE_INT_g(), NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, adims));
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset.
        try {
            if ((file_id >= 0) && (dataspace_id >= 0) && (filetype_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), filetype_id, dataspace_id,
                                        H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the dataset.
        try {
            if ((dataset_id >= 0) && (memtype_id >= 0)) {
                // Flatten the 3D array to 1D for MemorySegment
                int totalSize  = DIM0 * ADIM0 * ADIM1;
                int[] flatData = new int[totalSize];
                for (int i = 0; i < DIM0; i++) {
                    for (int j = 0; j < ADIM0; j++) {
                        for (int k = 0; k < ADIM1; k++) {
                            flatData[i * ADIM0 * ADIM1 + j * ADIM1 + k] = dset_data[i][j][k];
                        }
                    }
                }
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_INT, flatData);
                H5Dwrite(dataset_id, memtype_id, H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the file type.
        try {
            if (filetype_id >= 0)
                H5Tclose(filetype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the mem type.
        try {
            if (memtype_id >= 0)
                H5Tclose(memtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id     = H5I_INVALID_HID();
        long filetype_id = H5I_INVALID_HID();
        long memtype_id  = H5I_INVALID_HID();
        long dataset_id  = H5I_INVALID_HID();
        long[] dims      = {DIM0};
        long[] adims     = {ADIM0, ADIM1};
        int[][][] dset_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get the datatype.
        try {
            if (dataset_id >= 0)
                filetype_id = H5Dget_type(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get the datatype's dimensions.
        try {
            if (filetype_id >= 0) {
                MemorySegment adimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, adims);
                H5Tget_array_dims2(filetype_id, adimsSeg);
                // Read back the dimensions
                for (int i = 0; i < adims.length; i++) {
                    adims[i] = adimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to two-dimensional arrays (the
        // elements of the dataset.
        dset_data = new int[(int)dims[0]][(int)(adims[0])][(int)(adims[1])];

        // Create array datatypes for memory.
        try {
            memtype_id =
                H5Tarray_create2(H5T_NATIVE_INT_g(), 2, arena.allocateFrom(ValueLayout.JAVA_LONG, adims));
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read data.
        try {
            if ((dataset_id >= 0) && (memtype_id >= 0)) {
                int totalSize         = (int)(dims[0] * adims[0] * adims[1]);
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, totalSize);
                H5Dread(dataset_id, memtype_id, H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten the 1D MemorySegment to 3D array
                for (int i = 0; i < dims[0]; i++) {
                    for (int j = 0; j < adims[0]; j++) {
                        for (int k = 0; k < adims[1]; k++) {
                            dset_data[i][j][k] = dataSeg.getAtIndex(
                                ValueLayout.JAVA_INT, (int)(i * adims[0] * adims[1] + j * adims[1] + k));
                        }
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.println(DATASETNAME + " [" + indx + "]:");
            for (int jndx = 0; jndx < adims[0]; jndx++) {
                System.out.print(" [");
                for (int kndx = 0; kndx < adims[1]; kndx++)
                    System.out.print(dset_data[indx][jndx][kndx] + " ");
                System.out.println("]");
            }
            System.out.println();
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the file type.
        try {
            if (filetype_id >= 0)
                H5Tclose(filetype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the mem type.
        try {
            if (memtype_id >= 0)
                H5Tclose(memtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_Array.CreateDataset(arena);
            H5Ex_T_Array.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_ArrayAttribute.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write array datatypes
  to an attribute.  The program first writes integers arrays
  of dimension ADIM0xADIM1 to an attribute with a dataspace
  of DIM0, then closes the  file.  Next, it reopens the
  file, reads back the data, and outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_T_ArrayAttribute {
    private static String FILENAME      = "H5Ex_T_ArrayAttribute.h5";
    private static String DATASETNAME   = "DS1";
    private static String ATTRIBUTENAME = "A1";
    private static final int DIM0       = 4;
    private static final int ADIM0      = 3;
    private static final int ADIM1      = 5;
    private static final int RANK       = 1;
    private static final int NDIMS      = 2;

    private static void CreateDataset(Arena arena)
    {
        long file_id        = H5I_INVALID_HID();
        long filetype_id    = H5I_INVALID_HID();
        long memtype_id     = H5I_INVALID_HID();
        long dataspace_id   = H5I_INVALID_HID();
        long dataset_id     = H5I_INVALID_HID();
        long attribute_id   = H5I_INVALID_HID();
        long[] dims         = {DIM0};
        long[] adims        = {ADIM0, ADIM1};
        int[][][] dset_data = new int[DIM0][ADIM0][ADIM1];

        // Initialize data. indx is the element in the dataspace, jndx and kndx the
        // elements within the array datatype.
        for (int indx = 0; indx < DIM0; indx++)
            for (int jndx = 0; jndx < ADIM0; jndx++)
                for (int kndx = 0; kndx < ADIM1; kndx++)
                    dset_data[indx][jndx][kndx] = indx * jndx - jndx * kndx + indx * kndx;

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create array datatypes for file.
        try {
            filetype_id =
                H5Tarray_create2(H5T_STD_I64LE_g(), NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, adims));
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create array datatypes for memory.
        try {
            memtype_id =
                H5Tarray_create2(H5T_NATIVE_INT_g(), NDIMS, arena.allocateFrom(ValueLayout.JAVA_LONG, adims));
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataset with a scalar dataspace.
        try {
            dataspace_id = H5Screate(H5S_SCALAR());
            if (dataspace_id >= 0) {
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                H5Sclose(dataspace_id);
                dataspace_id = H5I_INVALID_HID();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the attribute and write the array data to it.
        try {
            if ((dataset_id >= 0) && (dataspace_id >= 0) && (filetype_id >= 0))
                attribute_id = H5Acreate2(dataset_id, arena.allocateFrom(ATTRIBUTENAME), filetype_id,
                                          dataspace_id, H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the dataset.
        try {
            if ((attribute_id >= 0) && (memtype_id >= 0)) {
                // Flatten the 3D array to 1D for MemorySegment
                int totalSize  = DIM0 * ADIM0 * ADIM1;
                int[] flatData = new int[totalSize];
                for (int i = 0; i < DIM0; i++) {
                    for (int j = 0; j < ADIM0; j++) {
                        for (int k = 0; k < ADIM1; k++) {
                            flatData[i * ADIM0 * ADIM1 + j * ADIM1 + k] = dset_data[i][j][k];
                        }
                    }
                }
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_INT, flatData);
                H5Awrite(attribute_id, memtype_id, dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the file type.
        try {
            if (filetype_id >= 0)
                H5Tclose(filetype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the mem type.
        try {
            if (memtype_id >= 0)
                H5Tclose(memtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filetype_id  = H5I_INVALID_HID();
        long memtype_id   = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();
        long[] dims       = {DIM0};
        long[] adims      = {ADIM0, ADIM1};
        int[][][] dset_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                attribute_id =
                    H5Aopen_by_name(dataset_id, arena.allocateFrom("."), arena.allocateFrom(ATTRIBUTENAME),
                                    H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get the datatype.
        try {
            if (attribute_id >= 0)
                filetype_id = H5Aget_type(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get the datatype's dimensions.
        try {
            if (filetype_id >= 0) {
                MemorySegment adimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, adims);
                H5Tget_array_dims2(filetype_id, adimsSeg);
                // Read back the dimensions
                for (int i = 0; i < adims.length; i++) {
                    adims[i] = adimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to two-dimensional arrays (the
        // elements of the dataset.
        dset_data = new int[(int)dims[0]][(int)(adims[0])][(int)(adims[1])];

        // Create array datatypes for memory.
        try {
            memtype_id =
                H5Tarray_create2(H5T_NATIVE_INT_g(), 2, arena.allocateFrom(ValueLayout.JAVA_LONG, adims));
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read data.
        try {
            if ((attribute_id >= 0) && (memtype_id >= 0)) {
                int totalSize         = (int)(dims[0] * adims[0] * adims[1]);
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, totalSize);
                H5Aread(attribute_id, memtype_id, dataSeg);
                // Unflatten the 1D MemorySegment to 3D array
                for (int i = 0; i < dims[0]; i++) {
                    for (int j = 0; j < adims[0]; j++) {
                        for (int k = 0; k < adims[1]; k++) {
                            dset_data[i][j][k] = dataSeg.getAtIndex(
                                ValueLayout.JAVA_INT, (int)(i * adims[0] * adims[1] + j * adims[1] + k));
                        }
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.println(ATTRIBUTENAME + " [" + indx + "]:");
            for (int jndx = 0; jndx < adims[0]; jndx++) {
                System.out.print(" [");
                for (int kndx = 0; kndx < adims[1]; kndx++)
                    System.out.print(dset_data[indx][jndx][kndx] + " ");
                System.out.println("]");
            }
            System.out.println();
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the file type.
        try {
            if (filetype_id >= 0)
                H5Tclose(filetype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the mem type.
        try {
            if (memtype_id >= 0)
                H5Tclose(memtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_ArrayAttribute.CreateDataset(arena);
            H5Ex_T_ArrayAttribute.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_Bit.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write bitfield
  datatypes to a dataset.  The program first writes bit
  fields to a dataset with a dataspace of DIM0xDIM1, then
  closes the file.  Next, it reopens the file, reads back
  the data, and outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_T_Bit {
    private static String FILENAME    = "H5Ex_T_Bit.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM0     = 4;
    private static final int DIM1     = 7;
    private static final int RANK     = 2;

    private static void CreateDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM0, DIM1};
        int[][] dset_data = new int[DIM0][DIM1];

        // Initialize data.
        for (int indx = 0; indx < DIM0; indx++)
            for (int jndx = 0; jndx < DIM1; jndx++) {
                dset_data[indx][jndx] = 0;
                dset_data[indx][jndx] |= (indx * jndx - jndx) & 0x03; /* Field "A" */
                dset_data[indx][jndx] |= (indx & 0x03) << 2;          /* Field "B" */
                dset_data[indx][jndx] |= (jndx & 0x03) << 4;          /* Field "C" */
                dset_data[indx][jndx] |= ((indx + jndx) & 0x03) << 6; /* Field "D" */
            }

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset.
        try {
            if ((file_id >= 0) && (dataspace_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_B8BE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the bitfield data to the dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten the 2D array to 1D for MemorySegment
                int[] flatData = new int[DIM0 * DIM1];
                for (int i = 0; i < DIM0; i++) {
                    for (int j = 0; j < DIM1; j++) {
                        flatData[i * DIM1 + j] = dset_data[i][j];
                    }
                }
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_INT, flatData);
                H5Dwrite(dataset_id, H5T_NATIVE_B8_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM0, DIM1};
        int[][] dset_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0) {
                MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                // Read back the dimensions
                for (int i = 0; i < dims.length; i++) {
                    dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to two-dimensional arrays (the
        // elements of the dataset.
        dset_data = new int[(int)dims[0]][(int)(dims[1])];

        // Read data.
        try {
            if (dataset_id >= 0) {
                int totalSize         = (int)(dims[0] * dims[1]);
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, totalSize);
                H5Dread(dataset_id, H5T_NATIVE_B8_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten the 1D MemorySegment to 2D array
                for (int i = 0; i < dims[0]; i++) {
                    for (int j = 0; j < dims[1]; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * (int)dims[1] + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println(DATASETNAME + ":");
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.print(" [");
            for (int jndx = 0; jndx < dims[1]; jndx++) {
                System.out.print("{" + (dset_data[indx][jndx] & 0x03) + ", ");
                System.out.print(((dset_data[indx][jndx] >> 2) & 0x03) + ", ");
                System.out.print(((dset_data[indx][jndx] >> 4) & 0x03) + ", ");
                System.out.print(((dset_data[indx][jndx] >> 6) & 0x03) + "}");
            }
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_Bit.CreateDataset(arena);
            H5Ex_T_Bit.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_BitAttribute.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write bitfield
  datatypes to an attribute.  The program first writes bit
  fields to an attribute with a dataspace of DIM0xDIM1, then
  closes the file.  Next, it reopens the file, reads back
  the data, and outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_T_BitAttribute {
    private static String FILENAME      = "H5Ex_T_BitAttribute.h5";
    private static String DATASETNAME   = "DS1";
    private static String ATTRIBUTENAME = "A1";
    private static final int DIM0       = 4;
    private static final int DIM1       = 7;
    private static final int RANK       = 2;

    private static void CreateDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();
        long[] dims       = {DIM0, DIM1};
        int[][] dset_data = new int[DIM0][DIM1];

        // Initialize data.
        for (int indx = 0; indx < DIM0; indx++)
            for (int jndx = 0; jndx < DIM1; jndx++) {
                dset_data[indx][jndx] = 0;
                dset_data[indx][jndx] |= (indx * jndx - jndx) & 0x03; /* Field "A" */
                dset_data[indx][jndx] |= (indx & 0x03) << 2;          /* Field "B" */
                dset_data[indx][jndx] |= (jndx & 0x03) << 4;          /* Field "C" */
                dset_data[indx][jndx] |= ((indx + jndx) & 0x03) << 6; /* Field "D" */
            }

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataset with a scalar dataspace.
        try {
            dataspace_id = H5Screate(H5S_SCALAR());
            if (dataspace_id >= 0) {
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                H5Sclose(dataspace_id);
                dataspace_id = H5I_INVALID_HID();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the attribute and write the array data to it.
        try {
            if ((dataset_id >= 0) && (dataspace_id >= 0))
                attribute_id = H5Acreate2(dataset_id, arena.allocateFrom(ATTRIBUTENAME), H5T_STD_B8BE_g(),
                                          dataspace_id, H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the dataset.
        try {
            if (attribute_id >= 0) {
                // Flatten the 2D array to 1D for MemorySegment
                int[] flatData = new int[4 * 7];
                for (int i = 0; i < 4; i++) {
                    for (int j = 0; j < 7; j++) {
                        flatData[i * 7 + j] = dset_data[i][j];
                    }
                }
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_INT, flatData);
                H5Awrite(attribute_id, H5T_NATIVE_B8_g(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();
        long[] dims       = {DIM0, DIM1};
        int[][] dset_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                attribute_id =
                    H5Aopen_by_name(dataset_id, arena.allocateFrom("."), arena.allocateFrom(ATTRIBUTENAME),
                                    H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer.
        try {
            if (attribute_id >= 0)
                dataspace_id = H5Aget_space(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0) {
                MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                // Read back the dimensions
                for (int i = 0; i < dims.length; i++) {
                    dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to two-dimensional arrays (the
        // elements of the dataset.
        dset_data = new int[(int)dims[0]][(int)(dims[1])];

        // Read data.
        try {
            if (attribute_id >= 0) {
                int totalSize         = 4 * 7;
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, totalSize);
                H5Aread(attribute_id, H5T_NATIVE_B8_g(), dataSeg);
                // Unflatten the 1D MemorySegment to 2D array
                for (int i = 0; i < 4; i++) {
                    for (int j = 0; j < 7; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * 7 + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println(ATTRIBUTENAME + ":");
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.print(" [");
            for (int jndx = 0; jndx < dims[1]; jndx++) {
                System.out.print("{" + (dset_data[indx][jndx] & 0x03) + ", ");
                System.out.print(((dset_data[indx][jndx] >> 2) & 0x03) + ", ");
                System.out.print(((dset_data[indx][jndx] >> 4) & 0x03) + ", ");
                System.out.print(((dset_data[indx][jndx] >> 6) & 0x03) + "}");
            }
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_BitAttribute.CreateDataset(arena);
            H5Ex_T_BitAttribute.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_Commit.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to commit a named datatype to a
  file, and read back that datatype.  The program first
  defines a compound datatype, commits it to a file, then
  closes the file.  Next, it reopens the file, opens the
  datatype, and outputs the names of its fields to the
  screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_T_Commit {
    private static String FILENAME           = "H5Ex_T_Commit.h5";
    private static String DATATYPENAME       = "Sensor_Type";
    protected static final int INTEGERSIZE   = 4;
    protected static final int DOUBLESIZE    = 8;
    protected final static int MAXSTRINGSIZE = 80;

    // Values for the various classes of datatypes
    enum H5T_class {
        H5T_NO_CLASS(H5T_NO_CLASS()),   // error
        H5T_INTEGER(H5T_INTEGER()),     // integer types
        H5T_FLOAT(H5T_FLOAT()),         // floating-point types
        H5T_TIME(H5T_TIME()),           // date and time types
        H5T_STRING(H5T_STRING()),       // character string types
        H5T_BITFIELD(H5T_BITFIELD()),   // bit field types
        H5T_OPAQUE(H5T_OPAQUE()),       // opaque types
        H5T_COMPOUND(H5T_COMPOUND()),   // compound types
        H5T_REFERENCE(H5T_REFERENCE()), // reference types
        H5T_ENUM(H5T_ENUM()),           // enumeration types
        H5T_VLEN(H5T_VLEN()),           // Variable-Length types
        H5T_ARRAY(H5T_ARRAY()),         // Array types
        H5T_COMPLEX(H5T_COMPLEX()),     // Complex number types
        H5T_NCLASSES(12);               // this must be last

        private static final Map<Long, H5T_class> lookup = new HashMap<Long, H5T_class>();

        static
        {
            for (H5T_class s : EnumSet.allOf(H5T_class.class))
                lookup.put(s.getCode(), s);
        }

        private long code;

        H5T_class(long layout_type) { this.code = layout_type; }

        public long getCode() { return this.code; }

        public static H5T_class get(long typeclass_id) { return lookup.get(typeclass_id); }
    }

    // The supporting Sensor_Datatype class.
    private static class Sensor_Datatype {
        static int numberMembers = 4;
        static int[] memberDims  = {1, 1, 1, 1};

        String[] memberNames   = {"Serial number", "Location", "Temperature (F)", "Pressure (inHg)"};
        long[] memberFileTypes = {H5T_STD_I32BE_g(), H5T_C_S1_g(), H5T_IEEE_F64BE_g(), H5T_IEEE_F64BE_g()};
        static int[] memberStorage = {INTEGERSIZE, MAXSTRINGSIZE, DOUBLESIZE, DOUBLESIZE};

        // Data size is the storage size for the members not the object.
        static long getDataSize()
        {
            long data_size = 0;
            for (int indx = 0; indx < numberMembers; indx++)
                data_size += memberStorage[indx] * memberDims[indx];
            return data_size;
        }

        static int getOffset(int memberItem)
        {
            int data_offset = 0;
            for (int indx = 0; indx < memberItem; indx++)
                data_offset += memberStorage[indx];
            return data_offset;
        }
    }

    private static void CreateDataType(Arena arena)
    {
        long file_id              = H5I_INVALID_HID();
        long strtype_id           = H5I_INVALID_HID();
        long filetype_id          = H5I_INVALID_HID();
        Sensor_Datatype datatypes = new Sensor_Datatype();
        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create string datatype.
        try {
            strtype_id = H5Tcopy(H5T_C_S1_g());
            if (strtype_id >= 0)
                H5Tset_size(strtype_id, MAXSTRINGSIZE);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the compound datatype for the file. Because the standard
        // types we are using for the file may have different sizes than
        // the corresponding native types, we must manually calculate the
        // offset of each member.
        try {
            filetype_id = H5Tcreate(H5T_COMPOUND(), Sensor_Datatype.getDataSize());
            if (filetype_id >= 0) {
                for (int indx = 0; indx < Sensor_Datatype.numberMembers; indx++) {
                    long type_id = datatypes.memberFileTypes[indx];
                    if (type_id == H5T_C_S1_g())
                        type_id = strtype_id;
                    H5Tinsert(filetype_id, arena.allocateFrom(datatypes.memberNames[indx]),
                              Sensor_Datatype.getOffset(indx), type_id);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Commit the compound datatype to the file, creating a named datatype.
        try {
            if ((file_id >= 0) && (filetype_id >= 0))
                H5Tcommit2(file_id, arena.allocateFrom(DATATYPENAME), filetype_id, H5P_DEFAULT(),
                           H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the file type.
        try {
            if (filetype_id >= 0)
                H5Tclose(filetype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the str type.
        try {
            if (strtype_id >= 0)
                H5Tclose(strtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataType(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long typeclass_id = H5I_INVALID_HID();
        long filetype_id  = H5I_INVALID_HID();

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open named datatype.
        try {
            if (file_id >= 0)
                filetype_id = H5Topen2(file_id, arena.allocateFrom(DATATYPENAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Named datatype:  " + DATATYPENAME + ":");

        // Get datatype class. If it isn't compound, we won't print anything.
        try {
            if (filetype_id >= 0)
                typeclass_id = H5Tget_class(filetype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        // Read data.
        try {
            if (H5T_class.get(typeclass_id) == H5T_class.H5T_COMPOUND) {
                System.out.println("   Class: H5T_COMPOUND");
                int nmembs = H5Tget_nmembers(filetype_id);
                // Iterate over compound datatype members.
                for (int indx = 0; indx < nmembs; indx++) {
                    MemorySegment nameSeg = H5Tget_member_name(filetype_id, indx);
                    String member_name    = nameSeg.getString(0);
                    System.out.println("    " + member_name);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the mem type.
        try {
            if (filetype_id >= 0)
                H5Tclose(filetype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_Commit.CreateDataType(arena);
            H5Ex_T_Commit.ReadDataType(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_Compound.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write compound
  datatypes to a dataset.  The program first writes
  compound structures to a dataset with a dataspace of DIM0,
  then closes the file.  Next, it reopens the file, reads
  back the data, and outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import hdf.hdf5lib.H5;

public class H5Ex_T_Compound {
    private static String FILENAME           = "H5Ex_T_Compound.h5";
    private static String DATASETNAME        = "DS1";
    private static final int DIM0            = 4;
    private static final int RANK            = 1;
    protected static final int INTEGERSIZE   = 4;
    protected static final int DOUBLESIZE    = 8;
    protected final static int MAXSTRINGSIZE = 80;

    static class Sensor_Datatype {
        static int numberMembers = 4;
        static int[] memberDims  = {1, 1, 1, 1};

        static String[] memberNames   = {"Serial number", "Location", "Temperature (F)", "Pressure (inHg)"};
        static long[] memberMemTypes  = {H5T_NATIVE_INT_g(), H5T_C_S1_g(), H5T_NATIVE_DOUBLE_g(),
                                         H5T_NATIVE_DOUBLE_g()};
        static long[] memberFileTypes = {H5T_STD_I32BE_g(), H5T_C_S1_g(), H5T_IEEE_F64BE_g(),
                                         H5T_IEEE_F64BE_g()};
        static int[] memberStorage    = {INTEGERSIZE, MAXSTRINGSIZE, DOUBLESIZE, DOUBLESIZE};

        // Data size is the storage size for the members.
        static long getTotalDataSize()
        {
            long data_size = 0;
            for (int indx = 0; indx < numberMembers; indx++)
                data_size += memberStorage[indx] * memberDims[indx];
            return DIM0 * data_size;
        }

        static long getDataSize()
        {
            long data_size = 0;
            for (int indx = 0; indx < numberMembers; indx++)
                data_size += memberStorage[indx] * memberDims[indx];
            return data_size;
        }

        static int getOffset(int memberItem)
        {
            int data_offset = 0;
            for (int indx = 0; indx < memberItem; indx++)
                data_offset += memberStorage[indx];
            return data_offset;
        }
    }

    static class Sensor {
        public int serial_no;
        public String location;
        public double temperature;
        public double pressure;

        Sensor(int serial_no, String location, double temperature, double pressure)
        {
            this.serial_no   = serial_no;
            this.location    = location;
            this.temperature = temperature;
            this.pressure    = pressure;
        }

        Sensor(List data)
        {
            this.serial_no   = (int)data.get(0);
            this.location    = (String)data.get(1);
            this.temperature = (double)data.get(2);
            this.pressure    = (double)data.get(3);
        }

        Sensor(ByteBuffer databuf, int dbposition) { readBuffer(databuf, dbposition); }

        void writeBuffer(ByteBuffer databuf, int dbposition)
        {
            databuf.putInt(dbposition + Sensor_Datatype.getOffset(0), serial_no);
            byte[] temp_str = location.getBytes(Charset.forName("UTF-8"));
            int arraylen    = (temp_str.length > MAXSTRINGSIZE) ? MAXSTRINGSIZE : temp_str.length;
            for (int ndx = 0; ndx < arraylen; ndx++)
                databuf.put(dbposition + Sensor_Datatype.getOffset(1) + ndx, temp_str[ndx]);
            for (int ndx = arraylen; ndx < MAXSTRINGSIZE; ndx++)
                databuf.put(dbposition + Sensor_Datatype.getOffset(1) + arraylen, (byte)0);
            databuf.putDouble(dbposition + Sensor_Datatype.getOffset(2), temperature);
            databuf.putDouble(dbposition + Sensor_Datatype.getOffset(3), pressure);
        }

        void readBuffer(ByteBuffer databuf, int dbposition)
        {
            this.serial_no       = databuf.getInt(dbposition + Sensor_Datatype.getOffset(0));
            ByteBuffer stringbuf = databuf.duplicate();
            stringbuf.position(dbposition + Sensor_Datatype.getOffset(1));
            stringbuf.limit(dbposition + Sensor_Datatype.getOffset(1) + MAXSTRINGSIZE);
            byte[] bytearr = new byte[stringbuf.remaining()];
            stringbuf.get(bytearr);
            this.location    = new String(bytearr, Charset.forName("UTF-8")).trim();
            this.temperature = databuf.getDouble(dbposition + Sensor_Datatype.getOffset(2));
            this.pressure    = databuf.getDouble(dbposition + Sensor_Datatype.getOffset(3));
        }

        List get()
        {
            List data = new ArrayList<>();
            data.add(this.serial_no);
            data.add(this.location);
            data.add(this.temperature);
            data.add(this.pressure);
            return data;
        }

        void put(List data)
        {
            this.serial_no   = (int)data.get(0);
            this.location    = (String)data.get(1);
            this.temperature = (double)data.get(2);
            this.pressure    = (double)data.get(3);
        }

        @Override
        public String toString()
        {
            return String.format("Serial number   : " + serial_no + "%n"
                                 + "Location        : " + location + "%n"
                                 + "Temperature (F) : " + temperature + "%n"
                                 + "Pressure (inHg) : " + pressure + "%n");
        }
    }

    private static void CreateDataset(Arena arena)
    {
        long file_id            = H5I_INVALID_HID();
        long strtype_id         = H5I_INVALID_HID();
        long memtype_id         = H5I_INVALID_HID();
        long filetype_id        = H5I_INVALID_HID();
        long dataspace_id       = H5I_INVALID_HID();
        long dataset_id         = H5I_INVALID_HID();
        long[] dims             = {DIM0};
        ArrayList[] object_data = new ArrayList[DIM0];
        byte[] dset_data        = null;

        // Initialize data.
        object_data[0] = (ArrayList) new Sensor(1153, new String("Exterior (static)"), 53.23, 24.57).get();
        object_data[1] = (ArrayList) new Sensor(1184, new String("Intake"), 55.12, 22.95).get();
        object_data[2] = (ArrayList) new Sensor(1027, new String("Intake manifold"), 103.55, 31.23).get();
        object_data[3] = (ArrayList) new Sensor(1313, new String("Exhaust manifold"), 1252.89, 84.11).get();

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create string datatype.
        try {
            strtype_id = H5Tcopy(H5T_C_S1_g());
            if (strtype_id >= 0)
                H5Tset_size(strtype_id, MAXSTRINGSIZE);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the compound datatype for memory.
        try {
            memtype_id = H5Tcreate(H5T_COMPOUND(), Sensor_Datatype.getDataSize());
            if (memtype_id >= 0) {
                for (int indx = 0; indx < Sensor_Datatype.numberMembers; indx++) {
                    long type_id = Sensor_Datatype.memberMemTypes[indx];
                    if (type_id == H5T_C_S1_g())
                        type_id = strtype_id;
                    H5Tinsert(memtype_id, arena.allocateFrom(Sensor_Datatype.memberNames[indx]),
                              Sensor_Datatype.getOffset(indx), type_id);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the compound datatype for the file. Because the standard
        // types we are using for the file may have different sizes than
        // the corresponding native types, we must manually calculate the
        // offset of each member.
        try {
            filetype_id = H5Tcreate(H5T_COMPOUND(), Sensor_Datatype.getDataSize());
            if (filetype_id >= 0) {
                for (int indx = 0; indx < Sensor_Datatype.numberMembers; indx++) {
                    long type_id = Sensor_Datatype.memberFileTypes[indx];
                    if (type_id == H5T_C_S1_g())
                        type_id = strtype_id;
                    H5Tinsert(filetype_id, arena.allocateFrom(Sensor_Datatype.memberNames[indx]),
                              Sensor_Datatype.getOffset(indx), type_id);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset.
        try {
            if ((file_id >= 0) && (dataspace_id >= 0) && (filetype_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), filetype_id, dataspace_id,
                                        H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the compound data to the dataset.
        try {
            if ((dataset_id >= 0) && (memtype_id >= 0))
                H5.H5DwriteVL(dataset_id, memtype_id, H5S_ALL(), H5S_ALL(), H5P_DEFAULT(),
                              (Object[])object_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the file type.
        try {
            if (filetype_id >= 0)
                H5Tclose(filetype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the mem type.
        try {
            if (memtype_id >= 0)
                H5Tclose(memtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (strtype_id >= 0)
                H5Tclose(strtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id          = H5I_INVALID_HID();
        long strtype_id       = H5I_INVALID_HID();
        long memtype_id       = H5I_INVALID_HID();
        long dataspace_id     = H5I_INVALID_HID();
        long dataset_id       = H5I_INVALID_HID();
        long[] dims           = {DIM0};
        Sensor[] object_data2 = new Sensor[(int)dims[0]];

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0) {
                MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                // Read back the dimensions
                for (int i = 0; i < dims.length; i++) {
                    dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create string datatype.
        try {
            strtype_id = H5Tcopy(H5T_C_S1_g());
            if (strtype_id >= 0)
                H5Tset_size(strtype_id, MAXSTRINGSIZE);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the compound datatype for memory.
        try {
            memtype_id = H5Tcreate(H5T_COMPOUND(), Sensor_Datatype.getDataSize());
            if (memtype_id >= 0) {
                for (int indx = 0; indx < Sensor_Datatype.numberMembers; indx++) {
                    long type_id = Sensor_Datatype.memberMemTypes[indx];
                    if (type_id == H5T_C_S1_g())
                        type_id = strtype_id;
                    H5Tinsert(memtype_id, arena.allocateFrom(Sensor_Datatype.memberNames[indx]),
                              Sensor_Datatype.getOffset(indx), type_id);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        ArrayList[] object_data = new ArrayList[(int)dims[0]];

        // Read data.
        try {
            if ((dataset_id >= 0) && (memtype_id >= 0))
                H5.H5DreadVL(dataset_id, memtype_id, H5S_ALL(), H5S_ALL(), H5P_DEFAULT(),
                             (Object[])object_data);

            for (int indx = 0; indx < (int)dims[0]; indx++) {
                object_data2[indx] = new Sensor(object_data[indx]);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.println(DATASETNAME + " [" + indx + "]:");
            System.out.println(object_data2[indx].toString());
        }
        System.out.println();

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the mem type.
        try {
            if (memtype_id >= 0)
                H5Tclose(memtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (strtype_id >= 0)
                H5Tclose(strtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_Compound.CreateDataset(arena);
            H5Ex_T_Compound.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_CompoundAttribute.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write compound
  datatypes to an attribute.  The program first writes
  compound structures to an attribute with a dataspace of
  DIM0, then closes the file.  Next, it reopens the file,
  reads back the data, and outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import hdf.hdf5lib.H5;

public class H5Ex_T_CompoundAttribute {
    private static String FILENAME           = "H5Ex_T_CompoundAttribute.h5";
    private static String DATASETNAME        = "DS1";
    private static String ATTRIBUTENAME      = "A1";
    private static final int DIM0            = 4;
    private static final int RANK            = 1;
    protected static final int INTEGERSIZE   = 4;
    protected static final int DOUBLESIZE    = 8;
    protected final static int MAXSTRINGSIZE = 80;

    static class Sensor_Datatype {
        static int numberMembers = 4;
        static int[] memberDims  = {1, 1, 1, 1};

        static String[] memberNames   = {"Serial number", "Location", "Temperature (F)", "Pressure (inHg)"};
        static long[] memberMemTypes  = {H5T_NATIVE_INT_g(), H5T_C_S1_g(), H5T_NATIVE_DOUBLE_g(),
                                         H5T_NATIVE_DOUBLE_g()};
        static long[] memberFileTypes = {H5T_STD_I32BE_g(), H5T_C_S1_g(), H5T_IEEE_F64BE_g(),
                                         H5T_IEEE_F64BE_g()};
        static int[] memberStorage    = {INTEGERSIZE, MAXSTRINGSIZE, DOUBLESIZE, DOUBLESIZE};

        // Data size is the storage size for the members not the object.
        static long getTotalDataSize()
        {
            long data_size = 0;
            for (int indx = 0; indx < numberMembers; indx++)
                data_size += memberStorage[indx] * memberDims[indx];
            return DIM0 * data_size;
        }

        static long getDataSize()
        {
            long data_size = 0;
            for (int indx = 0; indx < numberMembers; indx++)
                data_size += memberStorage[indx] * memberDims[indx];
            return data_size;
        }

        static int getOffset(int memberItem)
        {
            int data_offset = 0;
            for (int indx = 0; indx < memberItem; indx++)
                data_offset += memberStorage[indx];
            return data_offset;
        }
    }

    static class Sensor {
        public int serial_no;
        public String location;
        public double temperature;
        public double pressure;

        Sensor(int serial_no, String location, double temperature, double pressure)
        {
            this.serial_no   = serial_no;
            this.location    = location;
            this.temperature = temperature;
            this.pressure    = pressure;
        }

        Sensor(List data)
        {
            this.serial_no   = (int)data.get(0);
            this.location    = (String)data.get(1);
            this.temperature = (double)data.get(2);
            this.pressure    = (double)data.get(3);
        }

        Sensor(ByteBuffer databuf, int dbposition) { readBuffer(databuf, dbposition); }

        void writeBuffer(ByteBuffer databuf, int dbposition)
        {
            databuf.putInt(dbposition + Sensor_Datatype.getOffset(0), serial_no);
            byte[] temp_str = location.getBytes(Charset.forName("UTF-8"));
            int arraylen    = (temp_str.length > MAXSTRINGSIZE) ? MAXSTRINGSIZE : temp_str.length;
            for (int ndx = 0; ndx < arraylen; ndx++)
                databuf.put(dbposition + Sensor_Datatype.getOffset(1) + ndx, temp_str[ndx]);
            for (int ndx = arraylen; ndx < MAXSTRINGSIZE; ndx++)
                databuf.put(dbposition + Sensor_Datatype.getOffset(1) + arraylen, (byte)0);
            databuf.putDouble(dbposition + Sensor_Datatype.getOffset(2), temperature);
            databuf.putDouble(dbposition + Sensor_Datatype.getOffset(3), pressure);
        }

        void readBuffer(ByteBuffer databuf, int dbposition)
        {
            this.serial_no       = databuf.getInt(dbposition + Sensor_Datatype.getOffset(0));
            ByteBuffer stringbuf = databuf.duplicate();
            stringbuf.position(dbposition + Sensor_Datatype.getOffset(1));
            stringbuf.limit(dbposition + Sensor_Datatype.getOffset(1) + MAXSTRINGSIZE);
            byte[] bytearr = new byte[stringbuf.remaining()];
            stringbuf.get(bytearr);
            this.location    = new String(bytearr, Charset.forName("UTF-8")).trim();
            this.temperature = databuf.getDouble(dbposition + Sensor_Datatype.getOffset(2));
            this.pressure    = databuf.getDouble(dbposition + Sensor_Datatype.getOffset(3));
        }

        List get()
        {
            List data = new ArrayList<>();
            data.add(this.serial_no);
            data.add(this.location);
            data.add(this.temperature);
            data.add(this.pressure);
            return data;
        }

        void put(List data)
        {
            this.serial_no   = (int)data.get(0);
            this.location    = (String)data.get(1);
            this.temperature = (double)data.get(2);
            this.pressure    = (double)data.get(3);
        }

        @Override
        public String toString()
        {
            return String.format("Serial number   : " + serial_no + "%n"
                                 + "Location        : " + location + "%n"
                                 + "Temperature (F) : " + temperature + "%n"
                                 + "Pressure (inHg) : " + pressure + "%n");
        }
    }

    private static void CreateDataset(Arena arena)
    {
        long file_id            = H5I_INVALID_HID();
        long strtype_id         = H5I_INVALID_HID();
        long memtype_id         = H5I_INVALID_HID();
        long filetype_id        = H5I_INVALID_HID();
        long dataspace_id       = H5I_INVALID_HID();
        long dataset_id         = H5I_INVALID_HID();
        long attribute_id       = H5I_INVALID_HID();
        long[] dims             = {DIM0};
        ArrayList[] object_data = new ArrayList[DIM0];
        byte[] dset_data        = null;

        // Initialize data.
        object_data[0] = (ArrayList) new Sensor(1153, new String("Exterior (static)"), 53.23, 24.57).get();
        object_data[1] = (ArrayList) new Sensor(1184, new String("Intake"), 55.12, 22.95).get();
        object_data[2] = (ArrayList) new Sensor(1027, new String("Intake manifold"), 103.55, 31.23).get();
        object_data[3] = (ArrayList) new Sensor(1313, new String("Exhaust manifold"), 1252.89, 84.11).get();

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create string datatype.
        try {
            strtype_id = H5Tcopy(H5T_C_S1_g());
            if (strtype_id >= 0)
                H5Tset_size(strtype_id, MAXSTRINGSIZE);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the compound datatype for memory.
        try {
            memtype_id = H5Tcreate(H5T_COMPOUND(), Sensor_Datatype.getDataSize());
            if (memtype_id >= 0) {
                for (int indx = 0; indx < Sensor_Datatype.numberMembers; indx++) {
                    long type_id = Sensor_Datatype.memberMemTypes[indx];
                    if (type_id == H5T_C_S1_g())
                        type_id = strtype_id;
                    H5Tinsert(memtype_id, arena.allocateFrom(Sensor_Datatype.memberNames[indx]),
                              Sensor_Datatype.getOffset(indx), type_id);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the compound datatype for the file. Because the standard
        // types we are using for the file may have different sizes than
        // the corresponding native types, we must manually calculate the
        // offset of each member.
        try {
            filetype_id = H5Tcreate(H5T_COMPOUND(), Sensor_Datatype.getDataSize());
            if (filetype_id >= 0) {
                for (int indx = 0; indx < Sensor_Datatype.numberMembers; indx++) {
                    long type_id = Sensor_Datatype.memberFileTypes[indx];
                    if (type_id == H5T_C_S1_g())
                        type_id = strtype_id;
                    H5Tinsert(filetype_id, arena.allocateFrom(Sensor_Datatype.memberNames[indx]),
                              Sensor_Datatype.getOffset(indx), type_id);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataset with a scalar dataspace.
        try {
            dataspace_id = H5Screate(H5S_SCALAR());
            if (dataspace_id >= 0) {
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                H5Sclose(dataspace_id);
                dataspace_id = H5I_INVALID_HID();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the attribute.
        try {
            if ((dataset_id >= 0) && (dataspace_id >= 0) && (filetype_id >= 0))
                attribute_id = H5Acreate2(dataset_id, arena.allocateFrom(ATTRIBUTENAME), filetype_id,
                                          dataspace_id, H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the compound data.
        try {
            if ((attribute_id >= 0) && (memtype_id >= 0))
                H5.H5AwriteVL(attribute_id, memtype_id, (Object[])object_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the file type.
        try {
            if (filetype_id >= 0)
                H5Tclose(filetype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the mem type.
        try {
            if (memtype_id >= 0)
                H5Tclose(memtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (strtype_id >= 0)
                H5Tclose(strtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id          = H5I_INVALID_HID();
        long strtype_id       = H5I_INVALID_HID();
        long memtype_id       = H5I_INVALID_HID();
        long dataspace_id     = H5I_INVALID_HID();
        long dataset_id       = H5I_INVALID_HID();
        long attribute_id     = H5I_INVALID_HID();
        long[] dims           = {DIM0};
        Sensor[] object_data2 = new Sensor[(int)dims[0]];

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                attribute_id =
                    H5Aopen_by_name(dataset_id, arena.allocateFrom("."), arena.allocateFrom(ATTRIBUTENAME),
                                    H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer. This is a
        // three dimensional dataset when the array datatype is included so
        // the dynamic allocation must be done in steps.
        try {
            if (attribute_id >= 0)
                dataspace_id = H5Aget_space(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0) {
                MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                // Read back the dimensions
                for (int i = 0; i < dims.length; i++) {
                    dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create string datatype.
        try {
            strtype_id = H5Tcopy(H5T_C_S1_g());
            if (strtype_id >= 0)
                H5Tset_size(strtype_id, MAXSTRINGSIZE);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the compound datatype for memory.
        try {
            memtype_id = H5Tcreate(H5T_COMPOUND(), Sensor_Datatype.getDataSize());
            if (memtype_id >= 0) {
                for (int indx = 0; indx < Sensor_Datatype.numberMembers; indx++) {
                    long type_id = Sensor_Datatype.memberMemTypes[indx];
                    if (type_id == H5T_C_S1_g())
                        type_id = strtype_id;
                    H5Tinsert(memtype_id, arena.allocateFrom(Sensor_Datatype.memberNames[indx]),
                              Sensor_Datatype.getOffset(indx), type_id);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        ArrayList[] object_data = new ArrayList[(int)dims[0]];

        // Read data.
        try {
            if ((attribute_id >= 0) && (memtype_id >= 0))
                H5.H5AreadVL(attribute_id, memtype_id, (Object[])object_data);

            for (int indx = 0; indx < (int)dims[0]; indx++) {
                object_data2[indx] = new Sensor(object_data[indx]);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.println(ATTRIBUTENAME + " [" + indx + "]:");
            System.out.println(object_data2[indx].toString());
        }
        System.out.println();

        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the mem type.
        try {
            if (memtype_id >= 0)
                H5Tclose(memtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (strtype_id >= 0)
                H5Tclose(strtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_CompoundAttribute.CreateDataset(arena);
            H5Ex_T_CompoundAttribute.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_Float.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write integer datatypes
  to a dataset.  The program first writes integers to a
  dataset with a dataspace of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data, and
  outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.text.DecimalFormat;
import java.text.DecimalFormatSymbols;
import java.util.Locale;

public class H5Ex_T_Float {
    private static String FILENAME    = "H5Ex_T_Float.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM0     = 4;
    private static final int DIM1     = 7;
    private static final int RANK     = 2;

    private static void CreateDataset(Arena arena)
    {
        long file_id         = H5I_INVALID_HID();
        long dataspace_id    = H5I_INVALID_HID();
        long dataset_id      = H5I_INVALID_HID();
        long[] dims          = {DIM0, DIM1};
        double[][] dset_data = new double[DIM0][DIM1];

        // Initialize data.
        for (int indx = 0; indx < DIM0; indx++)
            for (int jndx = 0; jndx < DIM1; jndx++) {
                dset_data[indx][jndx] = indx / (jndx + 0.5) + jndx;
            }

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset and write the floating point data to it. In
        // this example we will save the data as 64 bit little endian IEEE
        // floating point numbers, regardless of the native type. The HDF5
        // library automatically converts between different floating point
        // types.
        try {
            if ((file_id >= 0) && (dataspace_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_IEEE_F64LE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten the 2D array to 1D for MemorySegment
                double[] flatData = new double[4 * 7];
                for (int i = 0; i < 4; i++) {
                    for (int j = 0; j < 7; j++) {
                        flatData[i * 7 + j] = dset_data[i][j];
                    }
                }
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_DOUBLE, flatData);
                H5Dwrite(dataset_id, H5T_NATIVE_DOUBLE_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM0, DIM1};
        double[][] dset_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0) {
                MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                // Read back the dimensions
                for (int i = 0; i < dims.length; i++) {
                    dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to two-dimensional arrays (the
        // elements of the dataset.
        dset_data = new double[(int)dims[0]][(int)(dims[1])];

        // Read data.
        try {
            if (dataset_id >= 0) {
                int totalSize         = 4 * 7;
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_DOUBLE, totalSize);
                H5Dread(dataset_id, H5T_NATIVE_DOUBLE_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten the 1D MemorySegment to 2D array
                for (int i = 0; i < 4; i++) {
                    for (int j = 0; j < 7; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_DOUBLE, i * 7 + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        DecimalFormat df = new DecimalFormat("#,##0.0000", new DecimalFormatSymbols(Locale.US));
        System.out.println(DATASETNAME + ":");
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.print(" [");
            for (int jndx = 0; jndx < dims[1]; jndx++) {
                System.out.print(" " + df.format(dset_data[indx][jndx]));
            }
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_Float.CreateDataset(arena);
            H5Ex_T_Float.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_FloatAttribute.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write floating point
  datatypes to an attribute.  The program first writes
  floating point numbers to an attribute with a dataspace of
  DIM0xDIM1, then closes the file.  Next, it reopens the
  file, reads back the data, and outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.text.DecimalFormat;
import java.text.DecimalFormatSymbols;
import java.util.Locale;

public class H5Ex_T_FloatAttribute {
    private static String FILENAME      = "H5Ex_T_FloatAttribute.h5";
    private static String DATASETNAME   = "DS1";
    private static String ATTRIBUTENAME = "A1";
    private static final int DIM0       = 4;
    private static final int DIM1       = 7;
    private static final int RANK       = 2;

    private static void CreateDataset(Arena arena)
    {
        long file_id         = H5I_INVALID_HID();
        long dataspace_id    = H5I_INVALID_HID();
        long dataset_id      = H5I_INVALID_HID();
        long attribute_id    = H5I_INVALID_HID();
        long[] dims          = {DIM0, DIM1};
        double[][] dset_data = new double[DIM0][DIM1];

        // Initialize data.
        for (int indx = 0; indx < DIM0; indx++)
            for (int jndx = 0; jndx < DIM1; jndx++) {
                dset_data[indx][jndx] = indx / (jndx + 0.5) + jndx;
            }

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataset with a scalar dataspace.
        try {
            dataspace_id = H5Screate(H5S_SCALAR());
            if (dataspace_id >= 0) {
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                H5Sclose(dataspace_id);
                dataspace_id = H5I_INVALID_HID();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the attribute and write the array data to it.
        try {
            if ((dataset_id >= 0) && (dataspace_id >= 0))
                attribute_id = H5Acreate2(dataset_id, arena.allocateFrom(ATTRIBUTENAME), H5T_IEEE_F64LE_g(),
                                          dataspace_id, H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the dataset.
        try {
            if (attribute_id >= 0) {
                // Flatten the 2D array to 1D for MemorySegment
                double[] flatData = new double[4 * 7];
                for (int i = 0; i < 4; i++) {
                    for (int j = 0; j < 7; j++) {
                        flatData[i * 7 + j] = dset_data[i][j];
                    }
                }
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_DOUBLE, flatData);
                H5Awrite(attribute_id, H5T_NATIVE_DOUBLE_g(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();
        long[] dims       = {DIM0, DIM1};
        double[][] dset_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                attribute_id =
                    H5Aopen_by_name(dataset_id, arena.allocateFrom("."), arena.allocateFrom(ATTRIBUTENAME),
                                    H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer.
        try {
            if (attribute_id >= 0)
                dataspace_id = H5Aget_space(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0) {
                MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                // Read back the dimensions
                for (int i = 0; i < dims.length; i++) {
                    dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to two-dimensional arrays (the
        // elements of the dataset.
        dset_data = new double[(int)dims[0]][(int)(dims[1])];

        // Read data.
        try {
            if (attribute_id >= 0) {
                int totalSize         = 4 * 7;
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_DOUBLE, totalSize);
                H5Aread(attribute_id, H5T_NATIVE_DOUBLE_g(), dataSeg);
                // Unflatten the 1D MemorySegment to 2D array
                for (int i = 0; i < 4; i++) {
                    for (int j = 0; j < 7; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_DOUBLE, i * 7 + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        DecimalFormat df = new DecimalFormat("#,##0.0000", new DecimalFormatSymbols(Locale.US));
        System.out.println(ATTRIBUTENAME + ":");
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.print(" [");
            for (int jndx = 0; jndx < dims[1]; jndx++) {
                System.out.print(" " + df.format(dset_data[indx][jndx]));
            }
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_FloatAttribute.CreateDataset(arena);
            H5Ex_T_FloatAttribute.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_Integer.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write integer datatypes
  to a dataset.  The program first writes integers to a
  dataset with a dataspace of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data, and
  outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.text.DecimalFormat;

public class H5Ex_T_Integer {
    private static String FILENAME    = "H5Ex_T_Integer.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM0     = 4;
    private static final int DIM1     = 7;
    private static final int RANK     = 2;

    private static void CreateDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM0, DIM1};
        int[][] dset_data = new int[DIM0][DIM1];

        // Initialize data.
        for (int indx = 0; indx < DIM0; indx++)
            for (int jndx = 0; jndx < DIM1; jndx++) {
                dset_data[indx][jndx] = indx * jndx - jndx;
            }

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset and write the integer data to it. In this
        // example we will save the data as 64 bit big endian integers,
        // regardless of the native integer type. The HDF5 library
        // automatically converts between different integer types.
        try {
            if ((file_id >= 0) && (dataspace_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I64BE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten the 2D array to 1D for MemorySegment
                int[] flatData = new int[4 * 7];
                for (int i = 0; i < 4; i++) {
                    for (int j = 0; j < 7; j++) {
                        flatData[i * 7 + j] = dset_data[i][j];
                    }
                }
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_INT, flatData);
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM0, DIM1};
        int[][] dset_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0) {
                MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                // Read back the dimensions
                for (int i = 0; i < dims.length; i++) {
                    dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to two-dimensional arrays (the
        // elements of the dataset.
        dset_data = new int[(int)dims[0]][(int)(dims[1])];

        // Read data.
        try {
            if (dataset_id >= 0) {
                int totalSize         = 4 * 7;
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, totalSize);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten the 1D MemorySegment to 2D array
                for (int i = 0; i < 4; i++) {
                    for (int j = 0; j < 7; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * 7 + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        DecimalFormat df = new DecimalFormat("#,##0");
        System.out.println(DATASETNAME + ":");
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.print(" [");
            for (int jndx = 0; jndx < dims[1]; jndx++) {
                System.out.print(" " + df.format(dset_data[indx][jndx]));
            }
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_Integer.CreateDataset(arena);
            H5Ex_T_Integer.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_IntegerAttribute.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write integer datatypes
  to an attribute.  The program first writes integers to an
  attribute with a dataspace of DIM0xDIM1, then closes the
  file.  Next, it reopens the file, reads back the data, and
  outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.text.DecimalFormat;

public class H5Ex_T_IntegerAttribute {
    private static String FILENAME      = "H5Ex_T_IntegerAttribute.h5";
    private static String DATASETNAME   = "DS1";
    private static String ATTRIBUTENAME = "A1";
    private static final int DIM0       = 4;
    private static final int DIM1       = 7;
    private static final int RANK       = 2;

    private static void CreateDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();
        long[] dims       = {DIM0, DIM1};
        int[][] dset_data = new int[DIM0][DIM1];

        // Initialize data.
        for (int indx = 0; indx < DIM0; indx++)
            for (int jndx = 0; jndx < DIM1; jndx++) {
                dset_data[indx][jndx] = indx * jndx - jndx;
            }

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataset with a scalar dataspace.
        try {
            dataspace_id = H5Screate(H5S_SCALAR());
            if (dataspace_id >= 0) {
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                H5Sclose(dataspace_id);
                dataspace_id = H5I_INVALID_HID();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the attribute and write the array data to it.
        try {
            if ((dataset_id >= 0) && (dataspace_id >= 0))
                attribute_id = H5Acreate2(dataset_id, arena.allocateFrom(ATTRIBUTENAME), H5T_STD_I64BE_g(),
                                          dataspace_id, H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the dataset.
        try {
            if (attribute_id >= 0) {
                // Flatten the 2D array to 1D for MemorySegment
                int[] flatData = new int[4 * 7];
                for (int i = 0; i < 4; i++) {
                    for (int j = 0; j < 7; j++) {
                        flatData[i * 7 + j] = dset_data[i][j];
                    }
                }
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_INT, flatData);
                H5Awrite(attribute_id, H5T_NATIVE_INT_g(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();
        long[] dims       = {DIM0, DIM1};
        int[][] dset_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                attribute_id =
                    H5Aopen_by_name(dataset_id, arena.allocateFrom("."), arena.allocateFrom(ATTRIBUTENAME),
                                    H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer.
        try {
            if (attribute_id >= 0)
                dataspace_id = H5Aget_space(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0) {
                MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                // Read back the dimensions
                for (int i = 0; i < dims.length; i++) {
                    dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to two-dimensional arrays (the
        // elements of the dataset.
        dset_data = new int[(int)dims[0]][(int)(dims[1])];

        // Read data.
        try {
            if (attribute_id >= 0) {
                int totalSize         = 4 * 7;
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, totalSize);
                H5Aread(attribute_id, H5T_NATIVE_INT_g(), dataSeg);
                // Unflatten the 1D MemorySegment to 2D array
                for (int i = 0; i < 4; i++) {
                    for (int j = 0; j < 7; j++) {
                        dset_data[i][j] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i * 7 + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        DecimalFormat df = new DecimalFormat("#,##0");
        System.out.println(ATTRIBUTENAME + ":");
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.print(" [");
            for (int jndx = 0; jndx < dims[1]; jndx++) {
                System.out.print(" " + df.format(dset_data[indx][jndx]));
            }
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_IntegerAttribute.CreateDataset(arena);
            H5Ex_T_IntegerAttribute.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_ObjectReference.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write object references
  to a dataset.  The program first creates objects in the
  file and writes references to those objects to a dataset
  with a dataspace of DIM0, then closes the file.  Next, it
  reopens the file, dereferences the references, and outputs
  the names of their targets to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_T_ObjectReference {
    private static String FILENAME     = "H5Ex_T_ObjectReference.h5";
    private static String DATASETNAME  = "DS1";
    private static String DATASETNAME2 = "DS2";
    private static String GROUPNAME    = "G1";
    private static final int DIM0      = 2;
    private static final int RANK      = 1;

    // Values for the status of space allocation
    enum H5G_obj {
        H5G_UNKNOWN(H5O_TYPE_UNKNOWN()),     /* Unknown object type */
        H5G_GROUP(H5O_TYPE_GROUP()),         /* Object is a group */
        H5G_DATASET(H5O_TYPE_DATASET()),     /* Object is a dataset */
        H5G_TYPE(H5O_TYPE_NAMED_DATATYPE()); /* Object is a named data type */
        private static final Map<Integer, H5G_obj> lookup = new HashMap<Integer, H5G_obj>();

        static
        {
            for (H5G_obj s : EnumSet.allOf(H5G_obj.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5G_obj(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5G_obj get(int code) { return lookup.get(code); }
    }

    private static void writeObjRef(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long group_id     = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM0};

        // Allocate MemorySegments for references
        MemorySegment[] refs = new MemorySegment[DIM0];
        for (int i = 0; i < DIM0; i++) {
            refs[i] = arena.allocate(H5R_REF_BUF_SIZE());
        }

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataset with a scalar dataspace.
        try {
            dataspace_id = H5Screate(H5S_SCALAR());
            if ((file_id >= 0) && (dataspace_id >= 0)) {
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME2), H5T_STD_I32LE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                if (dataset_id >= 0)
                    H5Dclose(dataset_id);
                dataset_id = H5I_INVALID_HID();
                H5Sclose(dataspace_id);
                dataspace_id = H5I_INVALID_HID();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create a group in the file.
        try {
            if (file_id >= 0)
                group_id = H5Gcreate2(file_id, arena.allocateFrom(GROUPNAME), H5P_DEFAULT(), H5P_DEFAULT(),
                                      H5P_DEFAULT());
            if (group_id >= 0)
                H5Gclose(group_id);
            group_id = H5I_INVALID_HID();
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (file_id >= 0) {
                // Create object reference to group
                try {
                    H5Rcreate_object(file_id, arena.allocateFrom(GROUPNAME), H5P_DEFAULT(), refs[0]);
                }
                catch (Throwable err) {
                    err.printStackTrace();
                }

                // Create object reference to dataset
                try {
                    H5Rcreate_object(file_id, arena.allocateFrom(DATASETNAME2), H5P_DEFAULT(), refs[1]);
                }
                catch (Throwable err) {
                    err.printStackTrace();
                }
            }

            // Create dataspace. Setting maximum size to NULL sets the maximum
            // size to be the current size.
            try {
                filespace_id = H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims),
                                                MemorySegment.NULL);
            }
            catch (Exception e) {
                e.printStackTrace();
            }

            // Create the dataset.
            try {
                if ((file_id >= 0) && (filespace_id >= 0))
                    dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_REF_g(),
                                            filespace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
            }
            catch (Exception e) {
                e.printStackTrace();
            }

            // Write the object references to it.
            try {
                if (dataset_id >= 0) {
                    // Pack references into contiguous MemorySegment
                    int refSize           = H5R_REF_BUF_SIZE();
                    MemorySegment refData = arena.allocate(refSize * DIM0);
                    for (int i = 0; i < DIM0; i++) {
                        MemorySegment.copy(refs[i], 0, refData, i * refSize, refSize);
                    }
                    H5Dwrite(dataset_id, H5T_STD_REF_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), refData);
                }
            }
            catch (Exception e) {
                e.printStackTrace();
            }
        }
        catch (Exception ex) {
            ex.printStackTrace();
        }
        finally {
            try {
                H5Rdestroy(refs[1]);
            }
            catch (Exception ex) {
            }
            try {
                H5Rdestroy(refs[0]);
            }
            catch (Exception ex) {
            }
        }

        // End access to the dataset and release resources used by it.

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readObjRef(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        int object_type   = -1;
        long object_id    = H5I_INVALID_HID();
        long[] dims       = {DIM0};

        // Allocate MemorySegments for references
        MemorySegment[] refs = new MemorySegment[DIM0];
        for (int i = 0; i < DIM0; i++) {
            refs[i] = arena.allocate(H5R_REF_BUF_SIZE());
        }

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());

            // Open an existing dataset.
            try {
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());

                try {
                    // Get dataspace and allocate memory for read buffer.
                    dataspace_id          = H5Dget_space(dataset_id);
                    MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                    H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                    // Read back the dimensions
                    for (int i = 0; i < dims.length; i++) {
                        dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                    }

                    // Read data into contiguous MemorySegment
                    int refSize           = H5R_REF_BUF_SIZE();
                    MemorySegment refData = arena.allocate(refSize * dims[0]);
                    H5Dread(dataset_id, H5T_STD_REF_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), refData);

                    // Unpack references from contiguous MemorySegment
                    for (int i = 0; i < dims[0]; i++) {
                        MemorySegment.copy(refData, i * refSize, refs[i], 0, refSize);
                    }

                    // Output the data to the screen.
                    for (int indx = 0; indx < dims[0]; indx++) {
                        System.out.println(DATASETNAME + "[" + indx + "]:");
                        System.out.print("  ->");
                        // Open the referenced object, get its name and type.
                        try {
                            object_id = H5Ropen_object(refs[indx], H5P_DEFAULT(), H5P_DEFAULT());
                            try {
                                // Get object type
                                MemorySegment objTypeSeg = arena.allocate(ValueLayout.JAVA_INT);
                                H5Rget_obj_type3(refs[indx], H5P_DEFAULT(), objTypeSeg);
                                object_type = objTypeSeg.get(ValueLayout.JAVA_INT, 0);

                                String obj_name = null;
                                if (object_type >= 0) {
                                    // Get the name - first query size
                                    long name_size = H5Iget_name(object_id, MemorySegment.NULL, 0);
                                    if (name_size > 0) {
                                        MemorySegment nameBuffer = arena.allocate(name_size + 1);
                                        H5Iget_name(object_id, nameBuffer, name_size + 1);
                                        obj_name = nameBuffer.getString(0);
                                    }
                                }
                                if ((object_id >= 0) && (object_type >= -1)) {
                                    switch (H5G_obj.get(object_type)) {
                                    case H5G_GROUP:
                                        System.out.print("H5G_GROUP");
                                        break;
                                    case H5G_DATASET:
                                        System.out.print("H5G_DATASET");
                                        break;
                                    case H5G_TYPE:
                                        System.out.print("H5G_TYPE");
                                        break;
                                    default:
                                        System.out.print("UNHANDLED");
                                    }
                                }
                                // Print the name.
                                System.out.println(": " + obj_name);
                            }
                            catch (Exception e) {
                                e.printStackTrace();
                            }
                            finally {
                                try {
                                    H5Oclose(object_id);
                                }
                                catch (Exception e) {
                                }
                            }
                        }
                        catch (Exception e4) {
                            e4.printStackTrace();
                        }
                        finally {
                            try {
                                H5Rdestroy(refs[indx]);
                            }
                            catch (Exception e4) {
                            }
                        }
                    } // end for
                }
                catch (Exception e3) {
                    e3.printStackTrace();
                }
                finally {
                    try {
                        H5Sclose(dataspace_id);
                    }
                    catch (Exception e3) {
                    }
                }
            }
            catch (Exception e2) {
                e2.printStackTrace();
            }
            finally {
                try {
                    H5Dclose(dataset_id);
                }
                catch (Exception e2) {
                }
            }
        }
        catch (Exception e1) {
            e1.printStackTrace();
        }
        finally {
            try {
                H5Fclose(file_id);
            }
            catch (Exception e1) {
            }
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_ObjectReference.writeObjRef(arena);
            H5Ex_T_ObjectReference.readObjRef(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_ObjectReferenceAttribute.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write object references
  to an attribute.  The program first creates objects in the
  file and writes references to those objects to an
  attribute with a dataspace of DIM0, then closes the file.
  Next, it reopens the file, dereferences the references,
  and outputs the names of their targets to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_T_ObjectReferenceAttribute {
    private static String FILENAME      = "H5Ex_T_ObjectReferenceAttribute.h5";
    private static String DATASETNAME   = "DS1";
    private static String ATTRIBUTENAME = "A1";
    private static String DATASETNAME2  = "DS2";
    private static String GROUPNAME     = "G1";
    private static final int DIM0       = 2;
    private static final int RANK       = 1;

    // Values for the status of space allocation
    enum H5G_obj {
        H5G_UNKNOWN(H5O_TYPE_UNKNOWN()),     /* Unknown object type */
        H5G_GROUP(H5O_TYPE_GROUP()),         /* Object is a group */
        H5G_DATASET(H5O_TYPE_DATASET()),     /* Object is a dataset */
        H5G_TYPE(H5O_TYPE_NAMED_DATATYPE()); /* Object is a named data type */
        private static final Map<Integer, H5G_obj> lookup = new HashMap<Integer, H5G_obj>();

        static
        {
            for (H5G_obj s : EnumSet.allOf(H5G_obj.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5G_obj(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5G_obj get(int code) { return lookup.get(code); }
    }

    private static void CreateDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long group_id     = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();
        long[] dims       = {DIM0};

        // Allocate MemorySegments for references
        MemorySegment[] refs = new MemorySegment[DIM0];
        for (int i = 0; i < DIM0; i++) {
            refs[i] = arena.allocate(H5R_REF_BUF_SIZE());
        }

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataset with a scalar dataspace.
        try {
            dataspace_id = H5Screate(H5S_SCALAR());
            if ((file_id >= 0) && (dataspace_id >= 0)) {
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME2), H5T_STD_I32LE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                if (dataset_id >= 0)
                    H5Dclose(dataset_id);
                dataset_id = H5I_INVALID_HID();
                H5Sclose(dataspace_id);
                dataspace_id = H5I_INVALID_HID();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create a group in the file.
        try {
            if (file_id >= 0)
                group_id = H5Gcreate2(file_id, arena.allocateFrom(GROUPNAME), H5P_DEFAULT(), H5P_DEFAULT(),
                                      H5P_DEFAULT());
            if (group_id >= 0)
                H5Gclose(group_id);
            group_id = H5I_INVALID_HID();
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (file_id >= 0) {
                try {
                    H5Rcreate_object(file_id, arena.allocateFrom(GROUPNAME), H5P_DEFAULT(), refs[0]);
                }
                catch (Throwable err) {
                    err.printStackTrace();
                }

                try {
                    H5Rcreate_object(file_id, arena.allocateFrom(DATASETNAME2), H5P_DEFAULT(), refs[1]);
                }
                catch (Throwable err) {
                    err.printStackTrace();
                }
            }

            // Create dataset with a scalar dataspace to serve as the parent
            // for the attribute.
            try {
                dataspace_id = H5Screate(H5S_SCALAR());
                if (dataspace_id >= 0) {
                    dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                            dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                    H5Sclose(dataspace_id);
                    dataspace_id = H5I_INVALID_HID();
                }
            }
            catch (Exception e) {
                e.printStackTrace();
            }

            // Create dataspace. Setting maximum size to NULL sets the maximum
            // size to be the current size.
            try {
                dataspace_id = H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims),
                                                MemorySegment.NULL);
            }
            catch (Exception e) {
                e.printStackTrace();
            }

            // Create the attribute and write the array data to it.
            try {
                if ((dataset_id >= 0) && (dataspace_id >= 0))
                    attribute_id = H5Acreate2(dataset_id, arena.allocateFrom(ATTRIBUTENAME), H5T_STD_REF_g(),
                                              dataspace_id, H5P_DEFAULT(), H5P_DEFAULT());
            }
            catch (Exception e) {
                e.printStackTrace();
            }

            // Write the attribute.
            try {
                if (attribute_id >= 0) {
                    // Pack references into contiguous MemorySegment
                    int refSize           = H5R_REF_BUF_SIZE();
                    MemorySegment refData = arena.allocate(refSize * DIM0);
                    for (int i = 0; i < DIM0; i++) {
                        MemorySegment.copy(refs[i], 0, refData, i * refSize, refSize);
                    }
                    H5Awrite(attribute_id, H5T_STD_REF_g(), refData);
                }
            }
            catch (Exception e) {
                e.printStackTrace();
            }
        }
        catch (Exception ex) {
            ex.printStackTrace();
        }
        finally {
            try {
                H5Rdestroy(refs[1]);
            }
            catch (Exception ex) {
            }
            try {
                H5Rdestroy(refs[0]);
            }
            catch (Exception ex) {
            }
        }

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();
        int object_type   = -1;
        long object_id    = H5I_INVALID_HID();
        long[] dims       = {DIM0};

        // Allocate MemorySegments for references
        MemorySegment[] refs = new MemorySegment[DIM0];
        for (int i = 0; i < DIM0; i++) {
            refs[i] = arena.allocate(H5R_REF_BUF_SIZE());
        }

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());

            // Open an existing dataset.
            try {
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());

                try {
                    attribute_id =
                        H5Aopen_by_name(dataset_id, arena.allocateFrom("."),
                                        arena.allocateFrom(ATTRIBUTENAME), H5P_DEFAULT(), H5P_DEFAULT());

                    // Get dataspace and allocate memory for read buffer.
                    try {
                        dataspace_id          = H5Aget_space(attribute_id);
                        MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                        H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                        // Read back the dimensions
                        for (int i = 0; i < dims.length; i++) {
                            dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                        }

                        // Read data.
                        // Read data into contiguous MemorySegment

                        int refSize = H5R_REF_BUF_SIZE();

                        MemorySegment refData = arena.allocate(refSize * dims[0]);

                        H5Aread(attribute_id, H5T_STD_REF_g(), refData);

                        // Unpack references from contiguous MemorySegment

                        for (int i = 0; i < dims[0]; i++) {

                            MemorySegment.copy(refData, i * refSize, refs[i], 0, refSize);
                        }

                        // Output the data to the screen.
                        for (int indx = 0; indx < dims[0]; indx++) {
                            System.out.println(ATTRIBUTENAME + "[" + indx + "]:");
                            System.out.print("  ->");
                            // Open the referenced object, get its name and type.
                            try {
                                object_id = H5Ropen_object(refs[indx], H5P_DEFAULT(), H5P_DEFAULT());
                                try {
                                    // Get object type
                                    MemorySegment objTypeSeg = arena.allocate(ValueLayout.JAVA_INT);
                                    H5Rget_obj_type3(refs[indx], H5P_DEFAULT(), objTypeSeg);
                                    object_type     = objTypeSeg.get(ValueLayout.JAVA_INT, 0);
                                    String obj_name = null;
                                    if (object_type >= 0) {
                                        // Get the name.
                                        // Get the name - first query size
                                        long name_size = H5Iget_name(object_id, MemorySegment.NULL, 0);
                                        if (name_size > 0) {
                                            MemorySegment nameBuffer = arena.allocate(name_size + 1);
                                            H5Iget_name(object_id, nameBuffer, name_size + 1);
                                            obj_name = nameBuffer.getString(0);
                                        }
                                    }
                                    if ((object_id >= 0) && (object_type >= -1)) {
                                        switch (H5G_obj.get(object_type)) {
                                        case H5G_GROUP:
                                            System.out.print("H5G_GROUP");
                                            break;
                                        case H5G_DATASET:
                                            System.out.print("H5G_DATASET");
                                            break;
                                        case H5G_TYPE:
                                            System.out.print("H5G_TYPE");
                                            break;
                                        default:
                                            System.out.print("UNHANDLED");
                                        }
                                    }
                                    // Print the name.
                                    System.out.println(": " + obj_name);
                                }
                                catch (Exception e) {
                                    e.printStackTrace();
                                }
                                finally {
                                    try {
                                        H5Oclose(object_id);
                                    }
                                    catch (Exception e) {
                                    }
                                }
                            }
                            catch (Exception e5) {
                                e5.printStackTrace();
                            }
                            finally {
                                try {
                                    H5Rdestroy(refs[indx]);
                                }
                                catch (Exception e5) {
                                }
                            }
                        } // end for
                    }
                    catch (Exception e4) {
                        e4.printStackTrace();
                    }
                    finally {
                        try {
                            H5Sclose(dataspace_id);
                        }
                        catch (Exception e3) {
                        }
                    }
                }
                catch (Exception e3) {
                    e3.printStackTrace();
                }
                finally {
                    try {
                        H5Aclose(attribute_id);
                    }
                    catch (Exception e4) {
                    }
                }
            }
            catch (Exception e2) {
                e2.printStackTrace();
            }
            finally {
                try {
                    H5Dclose(dataset_id);
                }
                catch (Exception e2) {
                }
            }
        }
        catch (Exception e1) {
            e1.printStackTrace();
        }
        finally {
            try {
                H5Fclose(file_id);
            }
            catch (Exception e1) {
            }
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_ObjectReferenceAttribute.CreateDataset(arena);
            H5Ex_T_ObjectReferenceAttribute.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_Opaque.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write opaque datatypes
  to a dataset.  The program first writes opaque data to a
  dataset with a dataspace of DIM0, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_T_Opaque {
    private static String FILENAME    = "H5Ex_T_Opaque.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM0     = 4;
    private static final int LEN      = 7;
    private static final int RANK     = 1;

    private static void CreateDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long datatype_id  = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM0};
        byte[] dset_data  = new byte[DIM0 * LEN];
        byte[] str_data   = {'O', 'P', 'A', 'Q', 'U', 'E'};

        // Initialize data.
        for (int indx = 0; indx < DIM0; indx++) {
            for (int jndx = 0; jndx < LEN - 1; jndx++)
                dset_data[jndx + indx * LEN] = str_data[jndx];
            dset_data[LEN - 1 + indx * LEN] = (byte)(indx + '0');
        }

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create opaque datatype and set the tag to something appropriate.
        // For this example we will write and view the data as a character
        // array.
        try {
            datatype_id = H5Tcreate(H5T_OPAQUE(), (long)LEN);
            if (datatype_id >= 0)
                H5Tset_tag(datatype_id, arena.allocateFrom("Character array"));
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset and write the integer data to it. In this
        // example we will save the data as 64 bit big endian integers,
        // regardless of the native integer type. The HDF5 library
        // automatically converts between different integer types.
        try {
            if ((file_id >= 0) && (datatype_id >= 0) && (dataspace_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), datatype_id, dataspace_id,
                                        H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the opaque data to the dataset.
        try {
            if ((dataset_id >= 0) && (datatype_id >= 0)) {
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_BYTE, dset_data);
                H5Dwrite(dataset_id, datatype_id, H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (datatype_id >= 0)
                H5Tclose(datatype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long datatype_id  = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long type_len     = H5I_INVALID_HID();
        long[] dims       = {DIM0};
        byte[] dset_data;
        String tag_name = null;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get datatype and properties for the datatype.
        try {
            if (dataset_id >= 0)
                datatype_id = H5Dget_type(dataset_id);
            if (datatype_id >= 0) {
                type_len             = H5Tget_size(datatype_id);
                MemorySegment tagSeg = H5Tget_tag(datatype_id);
                tag_name             = tagSeg.getString(0);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0) {
                MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                // Read back the dimensions
                for (int i = 0; i < dims.length; i++) {
                    dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate buffer.
        dset_data = new byte[(int)(dims[0] * type_len)];

        // Read data.
        try {
            if ((dataset_id >= 0) && (datatype_id >= 0)) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_BYTE, dset_data.length);
                H5Dread(dataset_id, datatype_id, H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Copy from MemorySegment back to byte array
                for (int i = 0; i < dset_data.length; i++) {
                    dset_data[i] = dataSeg.get(ValueLayout.JAVA_BYTE, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Datatype tag for " + DATASETNAME + " is: \"" + tag_name + "\"");
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.print(DATASETNAME + "[" + indx + "]: ");
            for (int jndx = 0; jndx < type_len; jndx++) {
                char temp = (char)dset_data[jndx + indx * (int)type_len];
                System.out.print(temp);
            }
            System.out.println("");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (datatype_id >= 0)
                H5Tclose(datatype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_Opaque.CreateDataset(arena);
            H5Ex_T_Opaque.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_OpaqueAttribute.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write opaque datatypes
  to an attribute.  The program first writes opaque data to
  an attribute with a dataspace of DIM0, then closes the
  file. Next, it reopens the file, reads back the data, and
  outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_T_OpaqueAttribute {
    private static String FILENAME      = "H5Ex_T_OpaqueAttribute.h5";
    private static String DATASETNAME   = "DS1";
    private static String ATTRIBUTENAME = "A1";
    private static final int DIM0       = 4;
    private static final int LEN        = 7;
    private static final int RANK       = 1;

    private static void CreateDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long datatype_id  = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();
        long[] dims       = {DIM0};
        byte[] dset_data  = new byte[DIM0 * LEN];
        byte[] str_data   = {'O', 'P', 'A', 'Q', 'U', 'E'};

        // Initialize data.
        for (int indx = 0; indx < DIM0; indx++) {
            for (int jndx = 0; jndx < LEN - 1; jndx++)
                dset_data[jndx + indx * LEN] = str_data[jndx];
            dset_data[LEN - 1 + indx * LEN] = (byte)(indx + '0');
        }

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataset with a scalar dataspace.
        try {
            dataspace_id = H5Screate(H5S_SCALAR());
            if (dataspace_id >= 0) {
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                H5Sclose(dataspace_id);
                dataspace_id = H5I_INVALID_HID();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create opaque datatype and set the tag to something appropriate.
        // For this example we will write and view the data as a character
        // array.
        try {
            datatype_id = H5Tcreate(H5T_OPAQUE(), (long)LEN);
            if (datatype_id >= 0)
                H5Tset_tag(datatype_id, arena.allocateFrom("Character array"));
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the attribute and write the array data to it.
        try {
            if ((dataset_id >= 0) && (datatype_id >= 0) && (dataspace_id >= 0))
                attribute_id = H5Acreate2(dataset_id, arena.allocateFrom(ATTRIBUTENAME), datatype_id,
                                          dataspace_id, H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the dataset.
        try {
            if ((attribute_id >= 0) && (datatype_id >= 0)) {
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_BYTE, dset_data);
                H5Awrite(attribute_id, datatype_id, dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (datatype_id >= 0)
                H5Tclose(datatype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long datatype_id  = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();
        long type_len     = -1;
        long[] dims       = {DIM0};
        byte[] dset_data;
        String tag_name = null;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                attribute_id =
                    H5Aopen_by_name(dataset_id, arena.allocateFrom("."), arena.allocateFrom(ATTRIBUTENAME),
                                    H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get datatype and properties for the datatype.
        try {
            if (attribute_id >= 0)
                datatype_id = H5Aget_type(attribute_id);
            if (datatype_id >= 0) {
                type_len             = H5Tget_size(datatype_id);
                MemorySegment tagSeg = H5Tget_tag(datatype_id);
                tag_name             = tagSeg.getString(0);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer.
        try {
            if (attribute_id >= 0)
                dataspace_id = H5Aget_space(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0) {
                MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                // Read back the dimensions
                for (int i = 0; i < dims.length; i++) {
                    dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate buffer.
        dset_data = new byte[(int)(dims[0] * type_len)];

        // Read data.
        try {
            if ((attribute_id >= 0) && (datatype_id >= 0)) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_BYTE, dset_data.length);
                H5Aread(attribute_id, datatype_id, dataSeg);
                // Copy from MemorySegment back to byte array
                for (int i = 0; i < dset_data.length; i++) {
                    dset_data[i] = dataSeg.get(ValueLayout.JAVA_BYTE, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Datatype tag for " + ATTRIBUTENAME + " is: \"" + tag_name + "\"");
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.print(ATTRIBUTENAME + "[" + indx + "]: ");
            for (int jndx = 0; jndx < type_len; jndx++) {
                char temp = (char)dset_data[jndx + indx * (int)type_len];
                System.out.print(temp);
            }
            System.out.println("");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (datatype_id >= 0)
                H5Tclose(datatype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_OpaqueAttribute.CreateDataset(arena);
            H5Ex_T_OpaqueAttribute.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_RegionReference.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write object references
  to a dataset.  The program first creates objects in the
  file and writes references to those objects to a dataset
  with a dataspace of DIM0, then closes the file.  Next, it
  reopens the file, dereferences the references, and outputs
  the names of their targets to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_T_RegionReference {
    private static String FILENAME     = "H5Ex_T_RegionReference.h5";
    private static String DATASETNAME  = "DS1";
    private static String DATASETNAME2 = "DS2";
    private static String GROUPNAME    = "G1";
    private static final int DIM0      = 2;
    private static final int DS2DIM0   = 3;
    private static final int DS2DIM1   = 16;
    private static final int RANK      = 1;

    private static void writeRegRef(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long group_id     = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long[] dims       = {DIM0};
        long[] dims2      = {DS2DIM0, DS2DIM1};
        // data buffer for writing region reference
        // Allocate MemorySegments for references
        MemorySegment[] refs = new MemorySegment[DIM0];
        for (int i = 0; i < DIM0; i++) {
            refs[i] = arena.allocate(H5R_REF_BUF_SIZE());
        }
        // data buffer for writing dataset
        byte[][] write_data     = new byte[DS2DIM0][DS2DIM1];
        StringBuffer[] str_data = {new StringBuffer("The quick brown"), new StringBuffer("fox jumps over "),
                                   new StringBuffer("the 5 lazy dogs")};

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataset with character data.
        try {
            dataspace_id =
                H5Screate_simple(2, arena.allocateFrom(ValueLayout.JAVA_LONG, dims2), MemorySegment.NULL);
            if ((file_id >= 0) && (dataspace_id >= 0)) {
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME2), H5T_STD_I8LE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                for (int indx = 0; indx < DS2DIM0; indx++) {
                    for (int jndx = 0; jndx < DS2DIM1; jndx++) {
                        if (jndx < str_data[indx].length())
                            write_data[indx][jndx] = (byte)str_data[indx].charAt(jndx);
                        else
                            write_data[indx][jndx] = 0;
                    }
                }
                // Flatten 2D byte array to 1D for MemorySegment
                byte[] flatData = new byte[DS2DIM0 * DS2DIM1];
                for (int i = 0; i < DS2DIM0; i++) {
                    for (int j = 0; j < DS2DIM1; j++) {
                        flatData[i * DS2DIM1 + j] = write_data[i][j];
                    }
                }
                MemorySegment writeSeg = arena.allocateFrom(ValueLayout.JAVA_BYTE, flatData);
                H5Dwrite(dataset_id, H5T_STD_I8LE_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), writeSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create reference to a list of elements in dset2.
        try {
            long[][] coords = {{0, 1}, {2, 11}, {1, 0}, {2, 4}};
            // Flatten coords for MemorySegment
            long[] flatCoords = new long[4 * 2];
            for (int i = 0; i < 4; i++) {
                flatCoords[i * 2]     = coords[i][0];
                flatCoords[i * 2 + 1] = coords[i][1];
            }
            MemorySegment coordsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, flatCoords);
            H5Sselect_elements(dataspace_id, H5S_SELECT_SET(), 4, coordsSeg);
            if (file_id >= 0)
                H5Rcreate_region(file_id, arena.allocateFrom(DATASETNAME2), dataspace_id, H5P_DEFAULT(),
                                 refs[0]);
        }
        catch (Exception ex) {
            ex.printStackTrace();
        }

        // Create reference to a hyperslab in dset2.
        try {
            long[] start  = {0, 0};  // Starting location of hyperslab
            long[] stride = {2, 11}; // Stride of hyperslab
            long[] count  = {2, 2};  // Element count of hyperslab
            long[] block  = {1, 3};  // Block size of hyperslab
            // Convert arrays to MemorySegments
            MemorySegment startSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, start);
            MemorySegment strideSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, stride);
            MemorySegment countSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, count);
            MemorySegment blockSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, block);
            H5Sselect_hyperslab(dataspace_id, H5S_SELECT_SET(), startSeg, strideSeg, countSeg, blockSeg);
            if (file_id >= 0)
                H5Rcreate_region(file_id, arena.allocateFrom(DATASETNAME2), dataspace_id, H5P_DEFAULT(),
                                 refs[1]);
            ;
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            H5Sclose(dataspace_id);
        }
        catch (Exception e) {
        }

        // Create the dataset and write the region references to it.
        try {
            dataspace_id =
                H5Screate_simple(1, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
            if ((file_id >= 0) && (dataspace_id >= 0)) {
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_REF_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                // Pack references into contiguous MemorySegment
                int refSize           = H5R_REF_BUF_SIZE();
                MemorySegment refData = arena.allocate(refSize * DIM0);
                for (int i = 0; i < DIM0; i++) {
                    MemorySegment.copy(refs[i], 0, refData, i * refSize, refSize);
                }
                H5Dwrite(dataset_id, H5T_STD_REF_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), refData);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            H5Rdestroy(refs[0]);
        }
        catch (Exception ex) {
        }

        try {
            H5Rdestroy(refs[1]);
        }
        catch (Exception ex) {
        }

        // End access to the dataset and release resources used by it.
        try {
            H5Sclose(dataspace_id);
        }
        catch (Exception e) {
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readRegRef(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        int object_type   = -1;
        long object_id    = H5I_INVALID_HID();
        long region_id    = H5I_INVALID_HID();
        long[] dims       = {DIM0};
        // Allocate MemorySegments for references
        MemorySegment[] refs = new MemorySegment[DIM0];
        for (int i = 0; i < DIM0; i++) {
            refs[i] = arena.allocate(H5R_REF_BUF_SIZE());
        }
        StringBuffer str_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());

            // Open an existing dataset.
            try {
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());

                try {
                    // Get dataspace and allocate memory for read buffer.
                    dataspace_id          = H5Dget_space(dataset_id);
                    MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                    H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                    // Read back the dimensions
                    for (int i = 0; i < dims.length; i++) {
                        dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                    }

                    // Read data.
                    // Read data into contiguous MemorySegment
                    int refSize           = H5R_REF_BUF_SIZE();
                    MemorySegment refData = arena.allocate(refSize * dims[0]);
                    H5Dread(dataset_id, H5T_STD_REF_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), refData);

                    // Unpack references from contiguous MemorySegment
                    for (int i = 0; i < dims[0]; i++) {
                        MemorySegment.copy(refData, i * refSize, refs[i], 0, refSize);
                    }

                    // Output the data to the screen.
                    for (int indx = 0; indx < dims[0]; indx++) {
                        System.out.println(DATASETNAME + "[" + indx + "]:");
                        System.out.print("  ->");
                        // Open the referenced object.
                        try {
                            object_id = H5Ropen_object(refs[indx], H5P_DEFAULT(), H5P_DEFAULT());
                            try {
                                // Get the name - first query size
                                long name_size  = H5Iget_name(object_id, MemorySegment.NULL, 0);
                                String obj_name = null;
                                if (name_size > 0) {
                                    MemorySegment nameBuffer = arena.allocate(name_size + 1);
                                    H5Iget_name(object_id, nameBuffer, name_size + 1);
                                    obj_name = nameBuffer.getString(0);
                                }

                                region_id = H5Ropen_region(refs[indx], H5P_DEFAULT(), H5P_DEFAULT());
                                if ((object_id >= 0) && (region_id >= 0)) {
                                    try {
                                        long reg_npoints = H5Sget_select_npoints(region_id);
                                        long[] dims2     = new long[1];
                                        dims2[0]         = (int)reg_npoints;
                                        dataspace_id     = H5Screate_simple(
                                            1, arena.allocateFrom(ValueLayout.JAVA_LONG, dims2),
                                            MemorySegment.NULL);

                                        // Read data.
                                        MemorySegment refbuf = arena.allocate((int)reg_npoints + 1);
                                        H5Dread(object_id, H5T_STD_I8LE_g(), dataspace_id, region_id,
                                                H5P_DEFAULT(), refbuf);
                                        refbuf.set(ValueLayout.JAVA_BYTE, (int)reg_npoints, (byte)0);
                                        str_data = new StringBuffer(refbuf.getString(0).trim());

                                        System.out.println(" " + obj_name + ": " + str_data);
                                    }
                                    catch (Throwable err2) {
                                        err2.printStackTrace();
                                    }
                                }
                            }
                            catch (Throwable err1) {
                                err1.printStackTrace();
                            }
                            finally {
                                try {
                                    H5Sclose(region_id);
                                }
                                catch (Exception ex) {
                                }
                            }
                        }
                        catch (Throwable err0) {
                            err0.printStackTrace();
                        }
                        finally {
                            try {
                                H5Dclose(object_id);
                            }
                            catch (Exception ex) {
                            }
                        }
                    } // end for
                }
                catch (Exception e4) {
                    e4.printStackTrace();
                }
                finally {
                    try {
                        H5Sclose(dataspace_id);
                        for (int indx = 0; indx < dims[0]; indx++)
                            H5Rdestroy(refs[indx]);
                    }
                    catch (Exception e4) {
                    }
                }
            }
            catch (Exception e3) {
                e3.printStackTrace();
            }
            finally {
                try {
                    H5Dclose(dataset_id);
                }
                catch (Exception e3) {
                }
            }
        }
        catch (Exception e2) {
            e2.printStackTrace();
        }
        finally {
            try {
                H5Fclose(file_id);
            }
            catch (Exception e2) {
            }
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_RegionReference.writeRegRef(arena);
            H5Ex_T_RegionReference.readRegRef(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_RegionReferenceAttribute.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write object references
  to a dataset.  The program first creates objects in the
  file and writes references to those objects to a dataset
  with a dataspace of DIM0, then closes the file.  Next, it
  reopens the file, dereferences the references, and outputs
  the names of their targets to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

public class H5Ex_T_RegionReferenceAttribute {
    private static String FILENAME      = "H5Ex_T_RegionReferenceAttribute.h5";
    private static String DATASETNAME   = "DS1";
    private static String ATTRIBUTENAME = "A1";
    private static String DATASETNAME2  = "DS2";
    private static String GROUPNAME     = "G1";
    private static final int DIM0       = 2;
    private static final int DS2DIM0    = 3;
    private static final int DS2DIM1    = 16;
    private static final int RANK       = 1;

    private static void writeRegRef(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long group_id     = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();
        long[] dims       = {DIM0};
        long[] dims2      = {DS2DIM0, DS2DIM1};
        // data buffer for writing region reference
        // Allocate MemorySegments for references
        MemorySegment[] refs = new MemorySegment[DIM0];
        for (int i = 0; i < DIM0; i++) {
            refs[i] = arena.allocate(H5R_REF_BUF_SIZE());
        }
        // data buffer for writing dataset
        byte[][] write_data     = new byte[DS2DIM0][DS2DIM1];
        StringBuffer[] str_data = {new StringBuffer("The quick brown"), new StringBuffer("fox jumps over "),
                                   new StringBuffer("the 5 lazy dogs")};

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataset with character data.
        try {
            dataspace_id =
                H5Screate_simple(2, arena.allocateFrom(ValueLayout.JAVA_LONG, dims2), MemorySegment.NULL);
            if ((file_id >= 0) && (dataspace_id >= 0)) {
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME2), H5T_STD_I8LE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                for (int indx = 0; indx < DS2DIM0; indx++) {
                    for (int jndx = 0; jndx < DS2DIM1; jndx++) {
                        if (jndx < str_data[indx].length())
                            write_data[indx][jndx] = (byte)str_data[indx].charAt(jndx);
                        else
                            write_data[indx][jndx] = 0;
                    }
                }
                // Flatten 2D byte array to 1D for MemorySegment
                byte[] flatData = new byte[DS2DIM0 * DS2DIM1];
                for (int i = 0; i < DS2DIM0; i++) {
                    for (int j = 0; j < DS2DIM1; j++) {
                        flatData[i * DS2DIM1 + j] = write_data[i][j];
                    }
                }
                MemorySegment writeSeg = arena.allocateFrom(ValueLayout.JAVA_BYTE, flatData);
                H5Dwrite(dataset_id, H5T_STD_I8LE_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), writeSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create reference to a list of elements in dset2.
        try {
            long[][] coords = {{0, 1}, {2, 11}, {1, 0}, {2, 4}};
            // Flatten coords for MemorySegment
            long[] flatCoords = new long[4 * 2];
            for (int i = 0; i < 4; i++) {
                flatCoords[i * 2]     = coords[i][0];
                flatCoords[i * 2 + 1] = coords[i][1];
            }
            MemorySegment coordsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, flatCoords);
            H5Sselect_elements(dataspace_id, H5S_SELECT_SET(), 4, coordsSeg);
            if (file_id >= 0)
                H5Rcreate_region(file_id, arena.allocateFrom(DATASETNAME2), dataspace_id, H5P_DEFAULT(),
                                 refs[0]);
        }
        catch (Exception ex) {
            ex.printStackTrace();
        }

        // Create reference to a hyperslab in dset2.
        try {
            long[] start  = {0, 0};  // Starting location of hyperslab
            long[] stride = {2, 11}; // Stride of hyperslab
            long[] count  = {2, 2};  // Element count of hyperslab
            long[] block  = {1, 3};  // Block size of hyperslab
            // Convert arrays to MemorySegments
            MemorySegment startSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, start);
            MemorySegment strideSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, stride);
            MemorySegment countSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, count);
            MemorySegment blockSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, block);
            H5Sselect_hyperslab(dataspace_id, H5S_SELECT_SET(), startSeg, strideSeg, countSeg, blockSeg);
            if (file_id >= 0)
                H5Rcreate_region(file_id, arena.allocateFrom(DATASETNAME2), dataspace_id, H5P_DEFAULT(),
                                 refs[1]);
            ;
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            H5Sclose(dataspace_id);
        }
        catch (Exception e) {
        }

        // Create dataset with a null dataspace to serve as the parent for the attribute.
        try {
            dataspace_id = H5Screate(H5S_NULL());
            dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(), dataspace_id,
                                    H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            H5Sclose(dataspace_id);
        }
        catch (Exception e) {
        }

        // Create the attribute and write the region references to it.
        try {
            dataspace_id =
                H5Screate_simple(1, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
            if ((file_id >= 0) && (attribute_id >= 0)) {
                attribute_id = H5Acreate2(file_id, arena.allocateFrom(ATTRIBUTENAME), H5T_STD_REF_g(),
                                          dataspace_id, H5P_DEFAULT(), H5P_DEFAULT());
                // Pack references into contiguous MemorySegment
                int refSize           = H5R_REF_BUF_SIZE();
                MemorySegment refData = arena.allocate(refSize * DIM0);
                for (int i = 0; i < DIM0; i++) {
                    MemorySegment.copy(refs[i], 0, refData, i * refSize, refSize);
                }
                H5Awrite(attribute_id, H5T_STD_REF_g(), refData);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            H5Rdestroy(refs[0]);
        }
        catch (Exception ex) {
        }

        try {
            H5Rdestroy(refs[1]);
        }
        catch (Exception ex) {
        }

        // End access to theattribute, dataset and release resources used by it.
        try {
            H5Sclose(dataspace_id);
        }
        catch (Exception e) {
        }

        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readRegRef(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();
        int object_type   = -1;
        long object_id    = H5I_INVALID_HID();
        long region_id    = H5I_INVALID_HID();
        long[] dims       = {DIM0};
        // Allocate MemorySegments for references
        MemorySegment[] refs = new MemorySegment[DIM0];
        for (int i = 0; i < DIM0; i++) {
            refs[i] = arena.allocate(H5R_REF_BUF_SIZE());
        }
        StringBuffer str_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());

            // Open an existing attribute.
            try {
                attribute_id = H5Aopen(file_id, arena.allocateFrom(ATTRIBUTENAME), H5P_DEFAULT());

                try {
                    // Get dataspace and allocate memory for read buffer.
                    dataspace_id          = H5Aget_space(attribute_id);
                    MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                    H5Sget_simple_extent_dims(attribute_id, dimsSeg, MemorySegment.NULL);
                    // Read back the dimensions
                    for (int i = 0; i < dims.length; i++) {
                        dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                    }

                    // Read data.
                    // Read data into contiguous MemorySegment
                    int refSize           = H5R_REF_BUF_SIZE();
                    MemorySegment refData = arena.allocate(refSize * dims[0]);
                    H5Aread(attribute_id, H5T_STD_REF_g(), refData);

                    // Unpack references from contiguous MemorySegment
                    for (int i = 0; i < dims[0]; i++) {
                        MemorySegment.copy(refData, i * refSize, refs[i], 0, refSize);
                    }

                    // Output the data to the screen.
                    for (int indx = 0; indx < dims[0]; indx++) {
                        System.out.println(DATASETNAME + "[" + indx + "]:");
                        System.out.print("  ->");
                        // Open the referenced object.
                        try {
                            object_id = H5Ropen_object(refs[indx], H5P_DEFAULT(), H5P_DEFAULT());
                            try {
                                // Get the name - first query size
                                long name_size  = H5Iget_name(object_id, MemorySegment.NULL, 0);
                                String obj_name = null;
                                if (name_size > 0) {
                                    MemorySegment nameBuffer = arena.allocate(name_size + 1);
                                    H5Iget_name(object_id, nameBuffer, name_size + 1);
                                    obj_name = nameBuffer.getString(0);
                                }

                                region_id = H5Ropen_region(refs[indx], H5P_DEFAULT(), H5P_DEFAULT());
                                if ((object_id >= 0) && (region_id >= 0)) {
                                    try {
                                        long reg_npoints = H5Sget_select_npoints(region_id);
                                        long[] dims2     = new long[1];
                                        dims2[0]         = (int)reg_npoints;
                                        dataspace_id     = H5Screate_simple(
                                            1, arena.allocateFrom(ValueLayout.JAVA_LONG, dims2),
                                            MemorySegment.NULL);

                                        // Read data.
                                        MemorySegment refbuf = arena.allocate((int)reg_npoints + 1);
                                        H5Dread(object_id, H5T_STD_I8LE_g(), dataspace_id, region_id,
                                                H5P_DEFAULT(), refbuf);
                                        refbuf.set(ValueLayout.JAVA_BYTE, (int)reg_npoints, (byte)0);
                                        str_data = new StringBuffer(refbuf.getString(0).trim());

                                        System.out.println(" " + obj_name + ": " + str_data);
                                    }
                                    catch (Throwable err2) {
                                        err2.printStackTrace();
                                    }
                                }
                            }
                            catch (Throwable err1) {
                                err1.printStackTrace();
                            }
                            finally {
                                try {
                                    H5Sclose(region_id);
                                }
                                catch (Exception ex) {
                                }
                            }
                        }
                        catch (Throwable err0) {
                            err0.printStackTrace();
                        }
                        finally {
                            try {
                                H5Dclose(object_id);
                            }
                            catch (Exception ex) {
                            }
                        }
                    } // end for
                }
                catch (Exception e4) {
                    e4.printStackTrace();
                }
                finally {
                    try {
                        H5Sclose(dataspace_id);
                        for (int indx = 0; indx < dims[0]; indx++)
                            H5Rdestroy(refs[indx]);
                    }
                    catch (Exception e4) {
                    }
                }
            }
            catch (Exception e3) {
                e3.printStackTrace();
            }
            finally {
                try {
                    H5Aclose(attribute_id);
                }
                catch (Exception e3) {
                }
            }
        }
        catch (Exception e2) {
            e2.printStackTrace();
        }
        finally {
            try {
                H5Fclose(file_id);
            }
            catch (Exception e2) {
            }
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_RegionReferenceAttribute.writeRegRef(arena);
            H5Ex_T_RegionReferenceAttribute.readRegRef(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_String.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write string datatypes
  to a dataset.  The program first writes strings to a
  dataset with a dataspace of DIM0, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_T_String {
    private static String FILENAME    = "H5Ex_T_String.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM0     = 4;
    private static final int SDIM     = 8;
    private static final int RANK     = 1;

    private static void CreateDataset(Arena arena)
    {
        long file_id            = H5I_INVALID_HID();
        long memtype_id         = H5I_INVALID_HID();
        long filetype_id        = H5I_INVALID_HID();
        long dataspace_id       = H5I_INVALID_HID();
        long dataset_id         = H5I_INVALID_HID();
        long[] dims             = {DIM0};
        byte[][] dset_data      = new byte[DIM0][SDIM];
        StringBuffer[] str_data = {new StringBuffer("Parting"), new StringBuffer("is such"),
                                   new StringBuffer("sweet"), new StringBuffer("sorrow.")};

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create file and memory datatypes. For this example we will save
        // the strings as FORTRAN strings, therefore they do not need space
        // for the null terminator in the file.
        try {
            filetype_id = H5Tcopy(H5T_FORTRAN_S1_g());
            if (filetype_id >= 0)
                H5Tset_size(filetype_id, SDIM - 1);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            memtype_id = H5Tcopy(H5T_C_S1_g());
            if (memtype_id >= 0)
                H5Tset_size(memtype_id, SDIM);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset and write the string data to it.
        try {
            if ((file_id >= 0) && (filetype_id >= 0) && (dataspace_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), filetype_id, dataspace_id,
                                        H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            for (int indx = 0; indx < DIM0; indx++) {
                for (int jndx = 0; jndx < SDIM; jndx++) {
                    if (jndx < str_data[indx].length())
                        dset_data[indx][jndx] = (byte)str_data[indx].charAt(jndx);
                    else
                        dset_data[indx][jndx] = 0;
                }
            }
            if ((dataset_id >= 0) && (memtype_id >= 0)) {
                // Flatten 2D byte array to 1D for MemorySegment
                byte[] flatData = new byte[DIM0 * SDIM];
                for (int i = 0; i < DIM0; i++) {
                    for (int j = 0; j < SDIM; j++) {
                        flatData[i * SDIM + j] = dset_data[i][j];
                    }
                }
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_BYTE, flatData);
                H5Dwrite(dataset_id, memtype_id, H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the file type.
        try {
            if (filetype_id >= 0)
                H5Tclose(filetype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the mem type.
        try {
            if (memtype_id >= 0)
                H5Tclose(memtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filetype_id  = H5I_INVALID_HID();
        long memtype_id   = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long sdim         = 0;
        long[] dims       = {DIM0};
        byte[][] dset_data;
        StringBuffer[] str_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get the datatype and its size.
        try {
            if (dataset_id >= 0)
                filetype_id = H5Dget_type(dataset_id);
            if (filetype_id >= 0) {
                sdim = H5Tget_size(filetype_id);
                sdim++; // Make room for null terminator
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer.
        try {
            if (dataset_id >= 0)
                dataspace_id = H5Dget_space(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0) {
                MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                // Read back the dimensions
                for (int i = 0; i < dims.length; i++) {
                    dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate space for data.
        dset_data = new byte[(int)dims[0]][(int)sdim];
        str_data  = new StringBuffer[(int)dims[0]];

        // Create the memory datatype.
        try {
            memtype_id = H5Tcopy(H5T_C_S1_g());
            if (memtype_id >= 0)
                H5Tset_size(memtype_id, sdim);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read data.
        try {
            if ((dataset_id >= 0) && (memtype_id >= 0)) {
                int totalSize         = (int)dims[0] * (int)sdim;
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_BYTE, totalSize);
                H5Dread(dataset_id, memtype_id, H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
                // Unflatten 1D MemorySegment to 2D byte array
                for (int i = 0; i < (int)dims[0]; i++) {
                    for (int j = 0; j < sdim; j++) {
                        dset_data[i][j] = dataSeg.get(ValueLayout.JAVA_BYTE, i * sdim + j);
                    }
                }
            }
            byte[] tempbuf = new byte[(int)sdim];
            for (int indx = 0; indx < (int)dims[0]; indx++) {
                for (int jndx = 0; jndx < sdim; jndx++) {
                    tempbuf[jndx] = dset_data[indx][jndx];
                }
                str_data[indx] = new StringBuffer(new String(tempbuf).trim());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.println(DATASETNAME + " [" + indx + "]: " + str_data[indx]);
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the file type.
        try {
            if (filetype_id >= 0)
                H5Tclose(filetype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the mem type.
        try {
            if (memtype_id >= 0)
                H5Tclose(memtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_String.CreateDataset(arena);
            H5Ex_T_String.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_StringAttribute.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write string datatypes
  to an attribute.  The program first writes strings to an
  attribute with a dataspace of DIM0, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs it to the screen.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class H5Ex_T_StringAttribute {
    private static String FILENAME      = "H5Ex_T_StringAttribute.h5";
    private static String DATASETNAME   = "DS1";
    private static String ATTRIBUTENAME = "A1";
    private static final int DIM0       = 4;
    private static final int SDIM       = 8;
    private static final int RANK       = 1;

    private static void CreateDataset(Arena arena)
    {
        long file_id            = H5I_INVALID_HID();
        long memtype_id         = H5I_INVALID_HID();
        long filetype_id        = H5I_INVALID_HID();
        long dataspace_id       = H5I_INVALID_HID();
        long dataset_id         = H5I_INVALID_HID();
        long attribute_id       = H5I_INVALID_HID();
        long[] dims             = {DIM0};
        byte[][] dset_data      = new byte[DIM0][SDIM];
        StringBuffer[] str_data = {new StringBuffer("Parting"), new StringBuffer("is such"),
                                   new StringBuffer("sweet"), new StringBuffer("sorrow.")};

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create file and memory datatypes. For this example we will save
        // the strings as FORTRAN strings, therefore they do not need space
        // for the null terminator in the file.
        try {
            filetype_id = H5Tcopy(H5T_FORTRAN_S1_g());
            if (filetype_id >= 0)
                H5Tset_size(filetype_id, SDIM - 1);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            memtype_id = H5Tcopy(H5T_C_S1_g());
            if (memtype_id >= 0)
                H5Tset_size(memtype_id, SDIM);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataset with a scalar dataspace.
        try {
            dataspace_id = H5Screate(H5S_SCALAR());
            if (dataspace_id >= 0) {
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), H5T_STD_I32LE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                H5Sclose(dataspace_id);
                dataspace_id = H5I_INVALID_HID();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(RANK, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the attribute.
        try {
            if ((dataset_id >= 0) && (dataspace_id >= 0) && (filetype_id >= 0))
                attribute_id = H5Acreate2(dataset_id, arena.allocateFrom(ATTRIBUTENAME), filetype_id,
                                          dataspace_id, H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            for (int indx = 0; indx < DIM0; indx++) {
                for (int jndx = 0; jndx < SDIM; jndx++) {
                    if (jndx < str_data[indx].length())
                        dset_data[indx][jndx] = (byte)str_data[indx].charAt(jndx);
                    else
                        dset_data[indx][jndx] = 0;
                }
            }
            if ((attribute_id >= 0) && (memtype_id >= 0)) {
                // Flatten 2D byte array to 1D for MemorySegment
                byte[] flatData = new byte[DIM0 * SDIM];
                for (int i = 0; i < DIM0; i++) {
                    for (int j = 0; j < SDIM; j++) {
                        flatData[i * SDIM + j] = dset_data[i][j];
                    }
                }
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_BYTE, flatData);
                H5Awrite(attribute_id, memtype_id, dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the file type.
        try {
            if (filetype_id >= 0)
                H5Tclose(filetype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the mem type.
        try {
            if (memtype_id >= 0)
                H5Tclose(memtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void ReadDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long filetype_id  = H5I_INVALID_HID();
        long memtype_id   = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();
        long sdim         = 0;
        long[] dims       = {DIM0};
        byte[][] dset_data;
        StringBuffer[] str_data;

        // Open an existing file.
        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                attribute_id =
                    H5Aopen_by_name(dataset_id, arena.allocateFrom("."), arena.allocateFrom(ATTRIBUTENAME),
                                    H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get the datatype and its size.
        try {
            if (attribute_id >= 0)
                filetype_id = H5Aget_type(attribute_id);
            if (filetype_id >= 0) {
                sdim = H5Tget_size(filetype_id);
                sdim++; // Make room for null terminator
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer.
        try {
            if (attribute_id >= 0)
                dataspace_id = H5Aget_space(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0) {
                MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, dims);
                H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                // Read back the dimensions
                for (int i = 0; i < dims.length; i++) {
                    dims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate space for data.
        dset_data = new byte[(int)dims[0]][(int)sdim];
        str_data  = new StringBuffer[(int)dims[0]];

        // Create the memory datatype.
        try {
            memtype_id = H5Tcopy(H5T_C_S1_g());
            if (memtype_id >= 0)
                H5Tset_size(memtype_id, sdim);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read data.
        try {
            if ((attribute_id >= 0) && (memtype_id >= 0)) {
                int totalSize         = (int)dims[0] * (int)sdim;
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_BYTE, totalSize);
                H5Aread(attribute_id, memtype_id, dataSeg);
                // Unflatten 1D MemorySegment to 2D byte array
                for (int i = 0; i < (int)dims[0]; i++) {
                    for (int j = 0; j < sdim; j++) {
                        dset_data[i][j] = dataSeg.get(ValueLayout.JAVA_BYTE, i * sdim + j);
                    }
                }
            }
            byte[] tempbuf = new byte[(int)sdim];
            for (int indx = 0; indx < (int)dims[0]; indx++) {
                for (int jndx = 0; jndx < sdim; jndx++) {
                    tempbuf[jndx] = dset_data[indx][jndx];
                }
                str_data[indx] = new StringBuffer(new String(tempbuf).trim());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        for (int indx = 0; indx < dims[0]; indx++) {
            System.out.println(DATASETNAME + " [" + indx + "]: " + str_data[indx]);
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the file type.
        try {
            if (filetype_id >= 0)
                H5Tclose(filetype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the mem type.
        try {
            if (memtype_id >= 0)
                H5Tclose(memtype_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_StringAttribute.CreateDataset(arena);
            H5Ex_T_StringAttribute.ReadDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/H5Ex_T_VLString.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
    Creating and writing a VL string to a file.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

import hdf.hdf5lib.H5;

public class H5Ex_T_VLString {
    private static String FILENAME    = "H5Ex_T_VLString.h5";
    private static String DATASETNAME = "DS1";

    private static void createDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long type_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        int rank          = 1;
        String[] str_data = {"Parting", "is such", "sweet", "sorrow."};
        long[] dims       = {str_data.length};

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            type_id = H5Tcopy(H5T_C_S1_g());
            H5Tset_size(type_id, H5T_VARIABLE());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            dataspace_id =
                H5Screate_simple(rank, arena.allocateFrom(ValueLayout.JAVA_LONG, dims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset and write the string data to it.
        try {
            if ((file_id >= 0) && (type_id >= 0) && (dataspace_id >= 0)) {
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(DATASETNAME), type_id, dataspace_id,
                                        H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0)
                H5.H5DwriteVL(dataset_id, type_id, H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), str_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            H5Sclose(dataspace_id);
            H5Tclose(type_id);
            H5Dclose(dataset_id);
            H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readDataset(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long type_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        String[] str_data = {"", "", "", ""};

        try {
            file_id = H5Fopen(arena.allocateFrom(FILENAME), H5F_ACC_RDONLY(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            dataset_id = H5Dopen2(file_id, arena.allocateFrom(DATASETNAME), H5P_DEFAULT());
            type_id    = H5Dget_type(dataset_id);
            H5.H5DreadVL(dataset_id, type_id, H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), str_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        for (int indx = 0; indx < str_data.length; indx++)
            System.out.println(DATASETNAME + " [" + indx + "]: " + str_data[indx]);

        try {
            H5Tclose(type_id);
            H5Dclose(dataset_id);
            H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            H5Ex_T_VLString.createDataset(arena);
            H5Ex_T_VLString.readDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/H5T/Java_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (HDF_JAVA_EXAMPLES
    H5Ex_T_Array.java
    H5Ex_T_ArrayAttribute.java
    H5Ex_T_Bit.java
    H5Ex_T_BitAttribute.java
    H5Ex_T_Commit.java
    H5Ex_T_Compound.java
    H5Ex_T_CompoundAttribute.java
    H5Ex_T_Float.java
    H5Ex_T_FloatAttribute.java
    H5Ex_T_Integer.java
    H5Ex_T_IntegerAttribute.java
    H5Ex_T_Opaque.java
    H5Ex_T_OpaqueAttribute.java
    H5Ex_T_String.java
    H5Ex_T_StringAttribute.java
    H5Ex_T_VLString.java
)
if (${H5_LIBVER_DIR} GREATER 18)
  if (${H5_LIBVER_DIR} EQUAL 110)
    set (HDF_JAVA_EXAMPLES ${HDF_JAVA_EXAMPLES}
        110/H5Ex_T_ObjectReference.java
        110/H5Ex_T_ObjectReferenceAttribute.java
    )
  else ()
    set (HDF_JAVA_EXAMPLES ${HDF_JAVA_EXAMPLES}
        H5Ex_T_ObjectReference.java
        H5Ex_T_ObjectReferenceAttribute.java
        H5Ex_T_RegionReference.java
        H5Ex_T_RegionReferenceAttribute.java
    )
  endif ()
endif ()
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_Array.txt`

```
DS1 [0]:
 [0 0 0 0 0 ]
 [0 -1 -2 -3 -4 ]
 [0 -2 -4 -6 -8 ]

DS1 [1]:
 [0 1 2 3 4 ]
 [1 1 1 1 1 ]
 [2 1 0 -1 -2 ]

DS1 [2]:
 [0 2 4 6 8 ]
 [2 3 4 5 6 ]
 [4 4 4 4 4 ]

DS1 [3]:
 [0 3 6 9 12 ]
 [3 5 7 9 11 ]
 [6 7 8 9 10 ]
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_ArrayAttribute.txt`

```
A1 [0]:
 [0 0 0 0 0 ]
 [0 -1 -2 -3 -4 ]
 [0 -2 -4 -6 -8 ]

A1 [1]:
 [0 1 2 3 4 ]
 [1 1 1 1 1 ]
 [2 1 0 -1 -2 ]

A1 [2]:
 [0 2 4 6 8 ]
 [2 3 4 5 6 ]
 [4 4 4 4 4 ]

A1 [3]:
 [0 3 6 9 12 ]
 [3 5 7 9 11 ]
 [6 7 8 9 10 ]
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_Bit.txt`

```
DS1:
 [{0, 0, 0, 0}{3, 0, 1, 1}{2, 0, 2, 2}{1, 0, 3, 3}{0, 0, 0, 0}{3, 0, 1, 1}{2, 0, 2, 2}]
 [{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}]
 [{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}]
 [{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}]
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_BitAttribute.txt`

```
A1:
 [{0, 0, 0, 0}{3, 0, 1, 1}{2, 0, 2, 2}{1, 0, 3, 3}{0, 0, 0, 0}{3, 0, 1, 1}{2, 0, 2, 2}]
 [{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}]
 [{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}]
 [{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}{0, 0, 0, 0}]
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_Commit.txt`

```
Named datatype:  Sensor_Type:
   Class: H5T_COMPOUND
    Serial number
    Location
    Temperature (F)
    Pressure (inHg)
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_Compound.txt`

```
DS1 [0]:
Serial number   : 1153
Location        : Exterior (static)
Temperature (F) : 53.23
Pressure (inHg) : 24.57

DS1 [1]:
Serial number   : 1184
Location        : Intake
Temperature (F) : 55.12
Pressure (inHg) : 22.95

DS1 [2]:
Serial number   : 1027
Location        : Intake manifold
Temperature (F) : 103.55
Pressure (inHg) : 31.23

DS1 [3]:
Serial number   : 1313
Location        : Exhaust manifold
Temperature (F) : 1252.89
Pressure (inHg) : 84.11
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_CompoundAttribute.txt`

```
A1 [0]:
Serial number   : 1153
Location        : Exterior (static)
Temperature (F) : 53.23
Pressure (inHg) : 24.57

A1 [1]:
Serial number   : 1184
Location        : Intake
Temperature (F) : 55.12
Pressure (inHg) : 22.95

A1 [2]:
Serial number   : 1027
Location        : Intake manifold
Temperature (F) : 103.55
Pressure (inHg) : 31.23

A1 [3]:
Serial number   : 1313
Location        : Exhaust manifold
Temperature (F) : 1252.89
Pressure (inHg) : 84.11
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_Float.txt`

```
DS1:
 [ 0.0000 1.0000 2.0000 3.0000 4.0000 5.0000 6.0000]
 [ 2.0000 1.6667 2.4000 3.2857 4.2222 5.1818 6.1538]
 [ 4.0000 2.3333 2.8000 3.5714 4.4444 5.3636 6.3077]
 [ 6.0000 3.0000 3.2000 3.8571 4.6667 5.5455 6.4615]
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_FloatAttribute.txt`

```
A1:
 [ 0.0000 1.0000 2.0000 3.0000 4.0000 5.0000 6.0000]
 [ 2.0000 1.6667 2.4000 3.2857 4.2222 5.1818 6.1538]
 [ 4.0000 2.3333 2.8000 3.5714 4.4444 5.3636 6.3077]
 [ 6.0000 3.0000 3.2000 3.8571 4.6667 5.5455 6.4615]
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_Integer.txt`

```
DS1:
 [ 0 -1 -2 -3 -4 -5 -6]
 [ 0 0 0 0 0 0 0]
 [ 0 1 2 3 4 5 6]
 [ 0 2 4 6 8 10 12]
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_IntegerAttribute.txt`

```
A1:
 [ 0 -1 -2 -3 -4 -5 -6]
 [ 0 0 0 0 0 0 0]
 [ 0 1 2 3 4 5 6]
 [ 0 2 4 6 8 10 12]
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_ObjectReference.txt`

```
DS1[0]:
  ->H5G_GROUP: /G1
DS1[1]:
  ->H5G_DATASET: /DS2
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_ObjectReferenceAttribute.txt`

```
A1[0]:
  ->H5G_GROUP: /G1
A1[1]:
  ->H5G_DATASET: /DS2
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_Opaque.txt`

```
Datatype tag for DS1 is: "Character array"
DS1[0]: OPAQUE0
DS1[1]: OPAQUE1
DS1[2]: OPAQUE2
DS1[3]: OPAQUE3
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_OpaqueAttribute.txt`

```
Datatype tag for A1 is: "Character array"
A1[0]: OPAQUE0
A1[1]: OPAQUE1
A1[2]: OPAQUE2
A1[3]: OPAQUE3
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_RegionReference.txt`

```
DS1[0]:
  -> /DS2: hdf5
DS1[1]:
  -> /DS2: Therowthedog
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_RegionReferenceAttribute.txt`

```
A1[0]:
  -> /DS2: hdf5
A1[1]:
  -> /DS2: Therowthedog
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_String.txt`

```
DS1 [0]: Parting
DS1 [1]: is such
DS1 [2]: sweet
DS1 [3]: sorrow.
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_StringAttribute.txt`

```
DS1 [0]: Parting
DS1 [1]: is such
DS1 [2]: sweet
DS1 [3]: sorrow.
```

### `HDF5Examples/JAVA/H5T/tfiles/110/H5Ex_T_VLString.txt`

```
DS1 [0]: Parting
DS1 [1]: is such
DS1 [2]: sweet
DS1 [3]: sorrow.
```

### `HDF5Examples/JAVA/TUTR/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_JAVA_TUTR Java)

set (CMAKE_VERBOSE_MAKEFILE 1)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (Java_sourcefiles.cmake)

if (WIN32)
  set (CMAKE_JAVA_INCLUDE_FLAG_SEP ";")
else ()
  set (CMAKE_JAVA_INCLUDE_FLAG_SEP ":")
endif ()

set (CMAKE_JAVA_INCLUDE_PATH ".")
foreach (CMAKE_JINCLUDE_PATH ${HDF5_JAVA_INCLUDE_DIRS})
  set (CMAKE_JAVA_INCLUDE_PATH "${CMAKE_JAVA_INCLUDE_PATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${CMAKE_JINCLUDE_PATH}")
endforeach ()
if (Java_VERSION_STRING VERSION_LESS "25.0.0" OR HDF5_PROVIDES_JNI)
  set (CMD_ARGS "-Dhdf.hdf5lib.H5.loadLibraryName=${H5EXAMPLE_JAVA_LIBRARY}$<$<OR:$<CONFIG:Debug>,$<CONFIG:Developer>>:${CMAKE_DEBUG_POSTFIX}>;")
endif ()

set (CMAKE_JAVA_CLASSPATH ".")
foreach (CMAKE_INCLUDE_PATH ${HDF5_JAVA_INCLUDE_DIRS})
  set (CMAKE_JAVA_CLASSPATH "${CMAKE_JAVA_CLASSPATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${CMAKE_INCLUDE_PATH}")
endforeach ()

foreach (HCP_JAR ${HDF5_JAVA_INCLUDE_DIRS})
  get_filename_component (_HCP_FILE ${HCP_JAR} NAME)
  set (HDFJAVA_CLASSJARS "${_HCP_FILE} ${HDFJAVA_CLASSJARS}")
endforeach ()

foreach (example ${HDF_JAVA_EXAMPLES})
  get_filename_component (example_name ${example} NAME_WE)
  file (WRITE ${PROJECT_BINARY_DIR}/${example_name}_Manifest.txt
  "Main-Class: ${example_name}
Class-Path: ${HDFJAVA_CLASSJARS}
"
  )
  add_jar (${EXAMPLE_VARNAME}J_${example_name}
      SOURCES ${example}
      MANIFEST ${PROJECT_BINARY_DIR}/${example_name}_Manifest.txt
  )
  get_target_property (${EXAMPLE_VARNAME}J_${example_name}_JAR_FILE ${EXAMPLE_VARNAME}J_${example_name} JAR_FILE)
  get_target_property (${EXAMPLE_VARNAME}J_${example_name}_CLASSPATH ${EXAMPLE_VARNAME}J_${example_name} CLASSDIR)
  if (H5EXAMPLE_JAVA_LIBRARIES)
    add_dependencies (${EXAMPLE_VARNAME}J_${example_name} ${H5EXAMPLE_JAVA_LIBRARIES})
  endif ()
endforeach ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST resultfile resultcode)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${resultfile}
        COMMAND "${CMAKE_COMMAND}"
            -D "TEST_JAVA=${CMAKE_Java_RUNTIME};${CMAKE_Java_RUNTIME_FLAGS}"
            -D "TEST_PROGRAM=${resultfile}"
            -D "TEST_ARGS:STRING=${ARGN};${CMD_ARGS}"
            -D "TEST_CLASSPATH:STRING=${CMAKE_JAVA_CLASSPATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${${EXAMPLE_VARNAME}J_${resultfile}_JAR_FILE}"
            -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
            -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
            -D "TEST_OUTPUT=${PROJECT_BINARY_DIR}/${resultfile}.out"
            -D "TEST_REFERENCE=${resultfile}.txt"
            -D "TEST_EXPECT=${resultcode}"
            -D "TEST_SKIP_COMPARE=TRUE"
            -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
    )
    if (NOT "${last_test}" STREQUAL "")
      set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${resultfile} PROPERTIES DEPENDS ${last_test})
    endif ()
    set (last_test "${EXAMPLE_VARNAME}_jnative-h5-${resultfile}")
  endmacro ()

  foreach (example ${HDF_JAVA_EXAMPLES})
    get_filename_component (example_name ${example} NAME_WE)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clearall-objects
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${PROJECT_BINARY_DIR}/${example_name}.h5
            ${PROJECT_BINARY_DIR}/${example_name}.out
            ${PROJECT_BINARY_DIR}/${example_name}.out.err
    )
    if (NOT "${last_test}" STREQUAL "")
      set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clearall-objects PROPERTIES DEPENDS ${last_test})
    endif ()
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-copy-objects
        COMMAND    ${CMAKE_COMMAND}
            -E copy_if_different
            ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.txt
            ${PROJECT_BINARY_DIR}/${example_name}.txt
    )
    set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${example_name}-copy-objects PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clearall-objects)
    set (last_test "${EXAMPLE_VARNAME}_jnative-h5-${example_name}-copy-objects")
    ADD_H5_TEST (${example_name} 0)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clean
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${PROJECT_BINARY_DIR}/${example_name}.h5
            ${PROJECT_BINARY_DIR}/${example_name}.out
            ${PROJECT_BINARY_DIR}/${example_name}.out.err
    )
    set_tests_properties (${EXAMPLE_VARNAME}_jnative-h5-${example_name}-clean PROPERTIES DEPENDS ${last_test})
  endforeach ()

endif ()
```

### `HDF5Examples/JAVA/TUTR/HDF5AttributeCreate.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * Copyright by the Board of Trustees of the University of Illinois.         *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

/**
 * <p>
 * Title: HDF Native Package (Java) Example
 * </p>
 * <p>
 * Description: this example shows how to create/read/write HDF attribute using
 * the "HDF Native Package (Java)". The example creates an attribute and, read
 * and write the attribute value:
 *
 * <pre>
 *     "/" (root)
 *             2D 32-bit integer 20x10
 *             (attribute: name="data range", value=[0, 10000])
 * </pre>
 *
 * </p>
 */
public class HDF5AttributeCreate {
    private static String fname    = "HDF5AttributeCreate.h5";
    private static String dsname   = "2D 32-bit integer 20x10";
    private static String attrname = "data range";
    private static long[] dims2D   = {20, 10};

    private static void CreateDatasetAttribute(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long attribute_id = H5I_INVALID_HID();

        // create the file and add groups and dataset into the file
        try {
            createFile(arena);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open file using the default properties.
        try {
            file_id = H5Fopen(arena.allocateFrom(fname), H5F_ACC_RDWR(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open dataset using the default properties.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(dsname), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        long[] attrDims = {2};        // 1D of size two
        int[] attrValue = {0, 10000}; // attribute value

        // Create the data space for the attribute.
        try {
            dataspace_id =
                H5Screate_simple(1, arena.allocateFrom(ValueLayout.JAVA_LONG, attrDims), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create a dataset attribute.
        try {
            if ((dataset_id >= 0) && (dataspace_id >= 0))
                attribute_id = H5Acreate2(dataset_id, arena.allocateFrom(attrname), H5T_STD_I32BE_g(),
                                          dataspace_id, H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        // Write the attribute data.
        try {
            if (attribute_id >= 0) {
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_INT, attrValue);
                H5Awrite(attribute_id, H5T_NATIVE_INT_g(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the attribute.
        try {
            if (attribute_id >= 0)
                H5Aclose(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                attribute_id = H5Aopen_by_name(dataset_id, arena.allocateFrom("."),
                                               arena.allocateFrom(attrname), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Get dataspace and allocate memory for read buffer.
        try {
            if (attribute_id >= 0)
                dataspace_id = H5Aget_space(attribute_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataspace_id >= 0) {
                MemorySegment dimsSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, attrDims);
                H5Sget_simple_extent_dims(dataspace_id, dimsSeg, MemorySegment.NULL);
                // Read back the dimensions
                for (int i = 0; i < attrDims.length; i++) {
                    attrDims[i] = dimsSeg.getAtIndex(ValueLayout.JAVA_LONG, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to two-dimensional arrays (the
        // elements of the dataset.
        int[] attrData = new int[(int)attrDims[0]];

        // Read data.
        try {
            if (attribute_id >= 0) {
                MemorySegment dataSeg = arena.allocate(ValueLayout.JAVA_INT, attrData.length);
                H5Aread(attribute_id, H5T_NATIVE_INT_g(), dataSeg);
                for (int i = 0; i < attrData.length; i++) {
                    attrData[i] = dataSeg.getAtIndex(ValueLayout.JAVA_INT, i);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // print out attribute value
        System.out.println(attrname);
        System.out.println(attrData[0] + "  " + attrData[1]);

        // Close the dataspace.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close to the dataset.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    /**
     * create the file and add groups and dataset into the file, which is the
     * same as javaExample.H5DatasetCreate
     *
     * @see javaExample.HDF5DatasetCreate
     * @throws Exception
     */
    private static void createFile(Arena arena) throws Exception
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(fname), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the data space for the dataset.
        try {
            dataspace_id =
                H5Screate_simple(2, arena.allocateFrom(ValueLayout.JAVA_LONG, dims2D), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset.
        try {
            if ((file_id >= 0) && (dataspace_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(dsname), H5T_STD_I32LE_g(), dataspace_id,
                                        H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // set the data values
        int[] dataIn = new int[20 * 10];
        for (int i = 0; i < 20; i++) {
            for (int j = 0; j < 10; j++) {
                dataIn[i * 10 + j] = i * 100 + j;
            }
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_INT, dataIn);
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            HDF5AttributeCreate.CreateDatasetAttribute(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/TUTR/HDF5DatasetCreate.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * Copyright by the Board of Trustees of the University of Illinois.         *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

/**
 * <p>
 * Title: HDF Native Package (Java) Example
 * </p>
 * <p>
 * Description: this example shows how to create HDF5 datasets using the
 * "HDF Native Package (Java)". The example created the group structure and
 * datasets:
 *
 * <pre>
 *     "/" (root)
 *         integer arrays
 *             2D 32-bit integer 20x10
 *             3D 16-bit integer 20x10x5
 *         float arrays
 *             2D 64-bit double 20x10
 *             3D 32-bit float  20x10x5
 * </pre>
 *
 * </p>
 */
public class HDF5DatasetCreate {
    private static String fname  = "HDF5DatasetCreate.h5";
    private static long[] dims2D = {20, 10};
    private static long[] dims3D = {20, 10, 5};

    private static void CreateDataset(Arena arena)
    {
        long file_id       = H5I_INVALID_HID();
        long group_id1     = H5I_INVALID_HID();
        long group_id2     = H5I_INVALID_HID();
        long dataspace_id1 = H5I_INVALID_HID();
        long dataspace_id2 = H5I_INVALID_HID();
        long dataset_id    = H5I_INVALID_HID();

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(fname), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
            System.err.println("Failed to create file:" + fname);
            return;
        }

        // Create a group in the file.
        try {
            if (file_id >= 0) {
                group_id1 = H5Gcreate2(file_id, arena.allocateFrom("g1"), H5P_DEFAULT(), H5P_DEFAULT(),
                                       H5P_DEFAULT());
                group_id2 = H5Gcreate2(file_id, arena.allocateFrom("g2"), H5P_DEFAULT(), H5P_DEFAULT(),
                                       H5P_DEFAULT());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the data space for the  2D dataset.
        try {
            dataspace_id1 =
                H5Screate_simple(2, arena.allocateFrom(ValueLayout.JAVA_LONG, dims2D), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the data space for the  3D dataset.
        try {
            dataspace_id2 =
                H5Screate_simple(3, arena.allocateFrom(ValueLayout.JAVA_LONG, dims3D), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // create 2D 32-bit (4 bytes) integer dataset of 20 by 10
        try {
            if ((group_id1 >= 0) && (dataspace_id1 >= 0)) {
                dataset_id =
                    H5Dcreate2(group_id1, arena.allocateFrom("2D 32-bit integer 20x10"), H5T_STD_I32LE_g(),
                               dataspace_id1, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                if (dataset_id >= 0)
                    H5Dclose(dataset_id);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // create 3D 8-bit (1 byte) unsigned integer dataset of 20 by 10 by 5
        try {
            if ((group_id1 >= 0) && (dataspace_id2 >= 0)) {
                dataset_id =
                    H5Dcreate2(group_id1, arena.allocateFrom("3D 8-bit unsigned integer 20x10x5"),
                               H5T_STD_U8LE_g(), dataspace_id2, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                if (dataset_id >= 0)
                    H5Dclose(dataset_id);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // create 2D 64-bit (8 bytes) double dataset of 20 by 10
        try {
            if ((group_id2 >= 0) && (dataspace_id1 >= 0)) {
                dataset_id =
                    H5Dcreate2(group_id2, arena.allocateFrom("2D 64-bit double 20x10"), H5T_NATIVE_DOUBLE_g(),
                               dataspace_id1, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                if (dataset_id >= 0)
                    H5Dclose(dataset_id);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // create 3D 32-bit (4 bytes) float dataset of 20 by 10 by 5
        try {
            if ((group_id2 >= 0) && (dataspace_id2 >= 0)) {
                dataset_id = H5Dcreate2(group_id2, arena.allocateFrom("3D 32-bit float  20x10x5"),
                                        H5T_NATIVE_FLOAT_g(), dataspace_id2, H5P_DEFAULT(), H5P_DEFAULT(),
                                        H5P_DEFAULT());
                if (dataset_id >= 0)
                    H5Dclose(dataset_id);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id2 >= 0)
                H5Sclose(dataspace_id2);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (dataspace_id1 >= 0)
                H5Sclose(dataspace_id1);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the groups.
        try {
            if (group_id2 >= 0)
                H5Gclose(group_id2);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (group_id1 >= 0)
                H5Gclose(group_id1);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            HDF5DatasetCreate.CreateDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/TUTR/HDF5DatasetRead.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * Copyright by the Board of Trustees of the University of Illinois.         *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

/**
 * <p>
 * Title: HDF Native Package (Java) Example
 * </p>
 * <p>
 * Description: this example shows how to read/write HDF datasets using the
 * "HDF Native Package (Java)". The example creates an integer dataset, and read
 * and write data values:
 *
 * <pre>
 *     "/" (root)
 *             2D 32-bit integer 20x10
 * </pre>
 *
 * </p>
 */
public class HDF5DatasetRead {
    private static String fname  = "HDF5DatasetRead.h5";
    private static String dsname = "2D 32-bit integer 20x10";
    private static long[] dims2D = {20, 10};

    private static void ReadWriteDataset(Arena arena)
    {
        long file_id    = H5I_INVALID_HID();
        long dataset_id = H5I_INVALID_HID();

        // create the file and add groups and dataset into the file
        try {
            createFile(arena);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open file using the default properties.
        try {
            file_id = H5Fopen(arena.allocateFrom(fname), H5F_ACC_RDWR(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open dataset using the default properties.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(dsname), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to two-dimensional arrays (the
        // elements of the dataset.
        int[][] dataRead = new int[(int)dims2D[0]][(int)(dims2D[1])];

        try {
            if (dataset_id >= 0) {
                // Allocate MemorySegment for reading
                int totalSize         = (int)dims2D[0] * (int)dims2D[1];
                MemorySegment readSeg = arena.allocate(ValueLayout.JAVA_INT, totalSize);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), readSeg);
                // Unflatten 1D MemorySegment to 2D array
                for (int i = 0; i < (int)dims2D[0]; i++)
                    for (int j = 0; j < (int)dims2D[1]; j++)
                        dataRead[i][j] = readSeg.getAtIndex(ValueLayout.JAVA_INT, i * (int)dims2D[1] + j);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // print out the data values
        System.out.println("\n\nOriginal Data Values");
        for (int i = 0; i < 20; i++) {
            System.out.print("\n" + dataRead[i][0]);
            for (int j = 1; j < 10; j++) {
                System.out.print(", " + dataRead[i][j]);
            }
        }

        // change data value and write it to file.
        for (int i = 0; i < 20; i++) {
            for (int j = 0; j < 10; j++) {
                dataRead[i][j]++;
            }
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten 2D array to 1D for MemorySegment
                int totalSize  = (int)dims2D[0] * (int)dims2D[1];
                int[] flatData = new int[totalSize];
                for (int i = 0; i < (int)dims2D[0]; i++)
                    for (int j = 0; j < (int)dims2D[1]; j++)
                        flatData[i * (int)dims2D[1] + j] = dataRead[i][j];
                MemorySegment writeSeg = arena.allocateFrom(ValueLayout.JAVA_INT, flatData);
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), writeSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // reload the data value
        int[][] dataModified = new int[(int)dims2D[0]][(int)(dims2D[1])];

        try {
            if (dataset_id >= 0) {
                // Allocate MemorySegment for reading
                int totalSize             = (int)dims2D[0] * (int)dims2D[1];
                MemorySegment modifiedSeg = arena.allocate(ValueLayout.JAVA_INT, totalSize);
                H5Dread(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), modifiedSeg);
                // Unflatten 1D MemorySegment to 2D array
                for (int i = 0; i < (int)dims2D[0]; i++)
                    for (int j = 0; j < (int)dims2D[1]; j++)
                        dataModified[i][j] =
                            modifiedSeg.getAtIndex(ValueLayout.JAVA_INT, i * (int)dims2D[1] + j);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // print out the modified data values
        System.out.println("\n\nModified Data Values");
        for (int i = 0; i < 20; i++) {
            System.out.print("\n" + dataModified[i][0]);
            for (int j = 1; j < 10; j++) {
                System.out.print(", " + dataModified[i][j]);
            }
        }

        // Close the dataset.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    /**
     * create the file and add groups ans dataset into the file, which is the
     * same as javaExample.H5DatasetCreate
     *
     * @see HDF5DatasetCreate.H5DatasetCreate
     * @throws Exception
     */
    private static void createFile(Arena arena) throws Exception
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(fname), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the data space for the dataset.
        try {
            dataspace_id =
                H5Screate_simple(2, arena.allocateFrom(ValueLayout.JAVA_LONG, dims2D), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset.
        try {
            if ((file_id >= 0) && (dataspace_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(dsname), H5T_STD_I32LE_g(), dataspace_id,
                                        H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // set the data values
        int[] dataIn = new int[20 * 10];
        for (int i = 0; i < 20; i++) {
            for (int j = 0; j < 10; j++) {
                dataIn[i * 10 + j] = i * 100 + j;
            }
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_INT, dataIn);
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            HDF5DatasetRead.ReadWriteDataset(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/TUTR/HDF5FileCreate.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * Copyright by the Board of Trustees of the University of Illinois.         *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

/**
 * <p>
 * Title: HDF Native Package (Java) Example
 * </p>
 * <p>
 * Description: This example shows how to create an empty HDF5 file using the
 * "HDF Native Package (Java)". If the file (H5FileCreate.h5) already exists, it
 * will be truncated to zero length.
 * </p>
 */
public class HDF5FileCreate {
    // The name of the file we'll create.
    private static String fname = "HDF5FileCreate.h5";

    private static void CreateFile(Arena arena)
    {
        long file_id = H5I_INVALID_HID();

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(fname), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
            System.err.println("Failed to create file:" + fname);
            return;
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            HDF5FileCreate.CreateFile(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/TUTR/HDF5FileStructure.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * Copyright by the Board of Trustees of the University of Illinois.         *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

import hdf.hdf5lib.H5;
import hdf.hdf5lib.structs.H5G_info_t;
import hdf.hdf5lib.structs.H5O_token_t;

/**
 * <p>
 * Title: HDF Native Package (Java) Example
 * </p>
 * <p>
 * Description: this example shows how to retrieve HDF file structure using the
 * "HDF Native Package (Java)". The example created the group structure and
 * datasets, and print out the file structure:
 *
 * <pre>
 *     "/" (root)
 *         integer arrays
 *             2D 32-bit integer 20x10
 *             3D unsigned 8-bit integer 20x10x5
 *         float arrays
 *             2D 64-bit double 20x10
 *             3D 32-bit float  20x10x5
 * </pre>
 *
 * </p>
 */
public class HDF5FileStructure {
    private static String fname = "HDF5FileStructure.h5";

    private static void FileStructure(Arena arena)
    {
        long file_id  = H5I_INVALID_HID();
        long group_id = H5I_INVALID_HID();

        // create the file and add groups and dataset into the file
        try {
            createFile(arena);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open file using the default properties.
        try {
            file_id = H5Fopen(arena.allocateFrom(fname), H5F_ACC_RDWR(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open the group, obtaining a new handle.
        try {
            if (file_id >= 0)
                group_id = H5Gopen2(file_id, arena.allocateFrom("/"), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            printGroup(arena, group_id, "/", "");
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the group.
        try {
            if (group_id >= 0)
                H5Gclose(group_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    /**
     * Recursively print a group and its members.
     *
     * @throws Exception
     */
    private static void printGroup(Arena arena, long g_id, String gname, String indent) throws Exception
    {
        if (g_id < 0)
            return;

        MemorySegment ginfo = arena.allocate(org.hdfgroup.javahdf5.H5G_info_t.sizeof());
        H5Gget_info(g_id, ginfo);
        long nlinks = org.hdfgroup.javahdf5.H5G_info_t.nlinks(ginfo);

        String objNames[]       = new String[(int)nlinks];
        int objTypes[]          = new int[(int)nlinks];
        int lnkTypes[]          = new int[(int)nlinks];
        H5O_token_t objTokens[] = new H5O_token_t[(int)nlinks];
        int names_found         = 0;
        try {
            names_found =
                H5.H5Gget_obj_info_all(g_id, null, objNames, objTypes, lnkTypes, objTokens, H5_INDEX_NAME());
        }
        catch (Throwable err) {
            err.printStackTrace();
        }

        indent += "    ";
        for (int i = 0; i < names_found; i++) {
            System.out.println(indent + objNames[i]);
            long group_id = H5I_INVALID_HID();
            if (objTypes[i] == H5O_TYPE_GROUP()) {
                // Open the group, obtaining a new handle.
                try {
                    if (g_id >= 0)
                        group_id = H5Gopen2(g_id, arena.allocateFrom(objNames[i]), H5P_DEFAULT());
                }
                catch (Exception e) {
                    e.printStackTrace();
                }

                if (group_id >= 0)
                    printGroup(arena, group_id, objNames[i], indent);

                // Close the group.
                try {
                    if (group_id >= 0)
                        H5Gclose(group_id);
                }
                catch (Exception e) {
                    e.printStackTrace();
                }
            }
        }
    }

    /**
     * create the file and add groups ans dataset into the file, which is the
     * same as javaExample.H5DatasetCreate
     *
     * @see javaExample.HDF5DatasetCreate
     * @throws Exception
     */
    private static void createFile(Arena arena) throws Exception
    {
        long[] dims2D      = {20, 10};
        long[] dims3D      = {20, 10, 5};
        long file_id       = H5I_INVALID_HID();
        long dataset_id    = H5I_INVALID_HID();
        long dataspace_id1 = H5I_INVALID_HID();
        long dataspace_id2 = H5I_INVALID_HID();
        long group_id1     = H5I_INVALID_HID();
        long group_id2     = H5I_INVALID_HID();

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(fname), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create groups in the file.
        try {
            if (file_id >= 0) {
                group_id1 = H5Gcreate2(file_id, arena.allocateFrom("/integer arrays"), H5P_DEFAULT(),
                                       H5P_DEFAULT(), H5P_DEFAULT());
                group_id1 = H5Gcreate2(file_id, arena.allocateFrom("/float arrays"), H5P_DEFAULT(),
                                       H5P_DEFAULT(), H5P_DEFAULT());
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the data space for the datasets.
        try {
            dataspace_id1 =
                H5Screate_simple(2, arena.allocateFrom(ValueLayout.JAVA_LONG, dims2D), MemorySegment.NULL);
            dataspace_id2 =
                H5Screate_simple(3, arena.allocateFrom(ValueLayout.JAVA_LONG, dims3D), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // create 2D 32-bit (4 bytes) integer dataset of 20 by 10
        try {
            if ((file_id >= 0) && (dataspace_id1 >= 0))
                dataset_id =
                    H5Dcreate2(file_id, arena.allocateFrom("/integer arrays/2D 32-bit integer 20x10"),
                               H5T_STD_I32LE_g(), dataspace_id1, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the dataset.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
            dataset_id = H5I_INVALID_HID();
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // create 3D 8-bit (1 byte) unsigned integer dataset of 20 by 10 by 5
        try {
            if ((file_id >= 0) && (dataspace_id2 >= 0))
                dataset_id = H5Dcreate2(
                    file_id, arena.allocateFrom("/integer arrays/3D 8-bit unsigned integer 20x10x5"),
                    H5T_STD_I64LE_g(), dataspace_id2, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the dataset.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
            dataset_id = H5I_INVALID_HID();
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // create 2D 64-bit (8 bytes) double dataset of 20 by 10
        try {
            if ((file_id >= 0) && (dataspace_id1 >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom("/float arrays/2D 64-bit double 20x10"),
                                        H5T_NATIVE_DOUBLE_g(), dataspace_id1, H5P_DEFAULT(), H5P_DEFAULT(),
                                        H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the dataset.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
            dataset_id = H5I_INVALID_HID();
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // create 3D 32-bit (4 bytes) float dataset of 20 by 10 by 5
        try {
            if ((file_id >= 0) && (dataspace_id2 >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom("/float arrays/3D 32-bit float  20x10x5"),
                                        H5T_NATIVE_FLOAT_g(), dataspace_id2, H5P_DEFAULT(), H5P_DEFAULT(),
                                        H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the dataset.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
            dataset_id = H5I_INVALID_HID();
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the data space.
        try {
            if (dataspace_id1 >= 0)
                H5Sclose(dataspace_id1);
            dataspace_id1 = H5I_INVALID_HID();
            if (dataspace_id2 >= 0)
                H5Sclose(dataspace_id2);
            dataspace_id2 = H5I_INVALID_HID();
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the groups.
        try {
            if (group_id1 >= 0)
                H5Gclose(group_id1);
            if (group_id2 >= 0)
                H5Gclose(group_id2);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            HDF5FileStructure.FileStructure(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/TUTR/HDF5GroupAbsoluteRelativeCreate.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
   Creating groups using absolute and relative names.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class HDF5GroupAbsoluteRelativeCreate {
    private static String FILENAME    = "HDF5GroupAbsoluteRelativeCreate.h5";
    private static String GROUPNAME   = "MyGroup";
    private static String GROUPNAME_A = "GroupA";
    private static String GROUPNAME_B = "GroupB";

    private static void CreateGroupAbsoluteAndRelative(Arena arena)
    {
        long file_id   = H5I_INVALID_HID();
        long group1_id = H5I_INVALID_HID();
        long group2_id = H5I_INVALID_HID();
        long group3_id = H5I_INVALID_HID();

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create a group named "/MyGroup" in the file.
        try {
            if (file_id >= 0)
                group1_id = H5Gcreate2(file_id, arena.allocateFrom("/" + GROUPNAME), H5P_DEFAULT(),
                                       H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create group "Group_A" in group "MyGroup" using absolute name.
        try {
            if (file_id >= 0)
                group2_id = H5Gcreate2(file_id, arena.allocateFrom("/" + GROUPNAME + "/" + GROUPNAME_A),
                                       H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create group "Group_B" in group "MyGroup" using relative name.
        try {
            if (group1_id >= 0)
                group3_id = H5Gcreate2(group1_id, arena.allocateFrom(GROUPNAME_B), H5P_DEFAULT(),
                                       H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the group3.
        try {
            if (group3_id >= 0)
                H5Gclose(group3_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the group2.
        try {
            if (group2_id >= 0)
                H5Gclose(group2_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the group1.
        try {
            if (group1_id >= 0)
                H5Gclose(group1_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {

        try (Arena arena = Arena.ofConfined()) {
            HDF5GroupAbsoluteRelativeCreate.CreateGroupAbsoluteAndRelative(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/TUTR/HDF5GroupCreate.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * Copyright by the Board of Trustees of the University of Illinois.         *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

/**
 * <p>
 * Title: HDF Native Package (Java) Example
 * </p>
 * <p>
 * Description: this example shows how to create HDF5 groups using the
 * "HDF Native Package (Java)". The example created the group structure:
 *
 * <pre>
 *     "/" (root)
 *         g1
 *             g11
 *             g12
 *         g2
 *             g21
 *             g22
 * </pre>
 *
 * </p>
 */
public class HDF5GroupCreate {
    private static String fname = "HDF5GroupCreate.h5";

    private static void CreateGroup(Arena arena)
    {
        long file_id     = H5I_INVALID_HID();
        long subgroup_id = H5I_INVALID_HID();
        long group_id1   = H5I_INVALID_HID();
        long group_id2   = H5I_INVALID_HID();

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(fname), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
            System.err.println("Failed to create file:" + fname);
            return;
        }

        // Create a group in the file.
        try {
            if (file_id >= 0) {
                group_id1 = H5Gcreate2(file_id, arena.allocateFrom("g1"), H5P_DEFAULT(), H5P_DEFAULT(),
                                       H5P_DEFAULT());
                if (group_id1 >= 0) {
                    subgroup_id = H5Gcreate2(group_id1, arena.allocateFrom("g11"), H5P_DEFAULT(),
                                             H5P_DEFAULT(), H5P_DEFAULT());
                    try {
                        if (subgroup_id >= 0)
                            H5Gclose(subgroup_id);
                    }
                    catch (Exception e) {
                        e.printStackTrace();
                    }
                    subgroup_id = H5Gcreate2(group_id1, arena.allocateFrom("g12"), H5P_DEFAULT(),
                                             H5P_DEFAULT(), H5P_DEFAULT());
                    try {
                        if (subgroup_id >= 0)
                            H5Gclose(subgroup_id);
                    }
                    catch (Exception e) {
                        e.printStackTrace();
                    }
                }
                group_id2 = H5Gcreate2(file_id, arena.allocateFrom("g2"), H5P_DEFAULT(), H5P_DEFAULT(),
                                       H5P_DEFAULT());
                if (group_id2 >= 0) {
                    subgroup_id = H5Gcreate2(group_id2, arena.allocateFrom("g21"), H5P_DEFAULT(),
                                             H5P_DEFAULT(), H5P_DEFAULT());
                    try {
                        if (subgroup_id >= 0)
                            H5Gclose(subgroup_id);
                    }
                    catch (Exception e) {
                        e.printStackTrace();
                    }
                    subgroup_id = H5Gcreate2(group_id2, arena.allocateFrom("g22"), H5P_DEFAULT(),
                                             H5P_DEFAULT(), H5P_DEFAULT());
                    try {
                        if (subgroup_id >= 0)
                            H5Gclose(subgroup_id);
                    }
                    catch (Exception e) {
                        e.printStackTrace();
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the groups.
        try {
            if (group_id2 >= 0)
                H5Gclose(group_id2);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (group_id1 >= 0)
                H5Gclose(group_id1);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            HDF5GroupCreate.CreateGroup(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/TUTR/HDF5GroupDatasetCreate.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
    Create two datasets within groups.
 ************************************************************/

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class HDF5GroupDatasetCreate {
    private static String FILENAME     = "HDF5GroupDatasetCreate.h5";
    private static String GROUPNAME    = "MyGroup";
    private static String GROUPNAME_A  = "GroupA";
    private static String DATASETNAME1 = "dset1";
    private static String DATASETNAME2 = "dset2";
    private static final int DIM1_X    = 3;
    private static final int DIM1_Y    = 3;
    private static final int DIM2_X    = 2;
    private static final int DIM2_Y    = 10;

    private static void h5_crtgrpd(Arena arena)
    {
        long file_id       = H5I_INVALID_HID();
        long dataspace_id  = H5I_INVALID_HID();
        long dataset_id    = H5I_INVALID_HID();
        long group_id      = H5I_INVALID_HID();
        long group1_id     = H5I_INVALID_HID();
        long group2_id     = H5I_INVALID_HID();
        int[][] dset1_data = new int[DIM1_X][DIM1_Y];
        int[][] dset2_data = new int[DIM2_X][DIM2_Y];
        long[] dims1       = {DIM1_X, DIM1_Y};
        long[] dims2       = {DIM2_X, DIM2_Y};

        // Initialize the first dataset.
        for (int indx = 0; indx < DIM1_X; indx++)
            for (int jndx = 0; jndx < DIM1_Y; jndx++)
                dset1_data[indx][jndx] = jndx + 1;

        // Initialize the second dataset.
        for (int indx = 0; indx < DIM2_X; indx++)
            for (int jndx = 0; jndx < DIM2_Y; jndx++)
                dset2_data[indx][jndx] = jndx + 1;

        // Create a file.
        try {
            file_id = H5Fcreate(arena.allocateFrom(FILENAME), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
            // Create a group named "/MyGroup" in the file.
            if (file_id >= 0) {
                group1_id = H5Gcreate2(file_id, arena.allocateFrom("/" + GROUPNAME), H5P_DEFAULT(),
                                       H5P_DEFAULT(), H5P_DEFAULT());
                // Create group "Group_A" in group "MyGroup" using absolute name.
                if (group1_id >= 0) {
                    group2_id = H5Gcreate2(file_id, arena.allocateFrom("/" + GROUPNAME + "/" + GROUPNAME_A),
                                           H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
                    if (group2_id >= 0)
                        H5Gclose(group2_id);
                }
                if (group1_id >= 0)
                    H5Gclose(group1_id);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the data space for the first dataset.
        try {
            dataspace_id =
                H5Screate_simple(2, arena.allocateFrom(ValueLayout.JAVA_LONG, dims1), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset in group "MyGroup".
        try {
            if ((file_id >= 0) && (dataspace_id >= 0))
                dataset_id =
                    H5Dcreate2(file_id, arena.allocateFrom("/" + GROUPNAME + "/" + DATASETNAME1),
                               H5T_STD_I32BE_g(), dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the first dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten 2D array to 1D for MemorySegment
                int[] flatData1 = new int[DIM1_X * DIM1_Y];
                for (int i = 0; i < DIM1_X; i++)
                    for (int j = 0; j < DIM1_Y; j++)
                        flatData1[i * DIM1_Y + j] = dset1_data[i][j];
                MemorySegment dataSeg1 = arena.allocateFrom(ValueLayout.JAVA_INT, flatData1);
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg1);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the data space for the first dataset.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
            dataspace_id = H5I_INVALID_HID();
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the first dataset.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
            dataset_id = H5I_INVALID_HID();
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing group of the specified file.
        try {
            if (file_id >= 0)
                group_id =
                    H5Gopen2(file_id, arena.allocateFrom("/" + GROUPNAME + "/" + GROUPNAME_A), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the data space for the second dataset.
        try {
            dataspace_id =
                H5Screate_simple(2, arena.allocateFrom(ValueLayout.JAVA_LONG, dims2), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the second dataset in group "Group_A".
        try {
            if ((group_id >= 0) && (dataspace_id >= 0))
                dataset_id = H5Dcreate2(group_id, arena.allocateFrom(DATASETNAME2), H5T_STD_I32BE_g(),
                                        dataspace_id, H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the second dataset.
        try {
            if (dataset_id >= 0) {
                // Flatten 2D array to 1D for MemorySegment
                int[] flatData2 = new int[DIM2_X * DIM2_Y];
                for (int i = 0; i < DIM2_X; i++)
                    for (int j = 0; j < DIM2_Y; j++)
                        flatData2[i * DIM2_Y + j] = dset2_data[i][j];
                MemorySegment dataSeg2 = arena.allocateFrom(ValueLayout.JAVA_INT, flatData2);
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg2);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the data space for the second dataset.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the second dataset.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the group.
        try {
            if (group_id >= 0)
                H5Gclose(group_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            HDF5GroupDatasetCreate.h5_crtgrpd(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/TUTR/HDF5SubsetSelect.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * Copyright by the Board of Trustees of the University of Illinois.         *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

import static org.hdfgroup.javahdf5.hdf5_h.*;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

/**
 * <p>
 * Title: HDF Native Package (Java) Example
 * </p>
 * <p>
 * Description: this example shows how to select a subset using the
 * "HDF Native Package (Java)". The example creates an integer dataset, and read
 * subset of the dataset:
 *
 * <pre>
 *     "/" (root)
 *             2D 32-bit integer 20x10
 * </pre>
 *
 * The whole 20x10 data set is
 *
 * <pre>
 * 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009
 * 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109
 * 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209
 * 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309
 * 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409
 * 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509
 * 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609
 * 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709
 * 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809
 * 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909
 * 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009
 * 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109
 * 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209
 * 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309
 * 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409
 * 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509
 * 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609
 * 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709
 * 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809
 * 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909
 * </pre>
 *
 * Subset: start=(4, 2), size=(5, 3) and stride=(3, 2). The subset values are:
 *
 * <pre>
 * 1402,1404,1406
 * 1702,1704,1706
 * 2002,2004,2006
 * 2302,2304,2306
 * 2602,2604,2606
 * </pre>
 *
 * </p>
 *
 * @author Peter X. Cao
 * @version 2.4
 */
public class HDF5SubsetSelect {
    private static String fname  = "HDF5SubsetSelect.h5";
    private static String dsname = "2D 32-bit integer 20x10";
    private static long[] dims2D = {20, 10};

    private static void SubsetSelect(Arena arena)
    {
        long file_id      = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();
        long filespace_id = H5I_INVALID_HID();
        long memspace_id  = H5I_INVALID_HID();

        // create the file and add groups and dataset into the file
        try {
            createFile(arena);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open file using the default properties.
        try {
            file_id = H5Fopen(arena.allocateFrom(fname), H5F_ACC_RDWR(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open dataset using the default properties.
        try {
            if (file_id >= 0)
                dataset_id = H5Dopen2(file_id, arena.allocateFrom(dsname), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Allocate array of pointers to two-dimensional arrays (the
        // elements of the dataset.
        int[][] dataRead = new int[5][3];

        // Define and select the hyperslab to use for reading.
        try {
            if (dataset_id >= 0) {
                filespace_id = H5Dget_space(dataset_id);

                long[] start  = {4, 2};
                long[] stride = {3, 2};
                long[] count  = {5, 3};
                long[] block  = null;

                if (filespace_id >= 0) {
                    // Convert arrays to MemorySegments for hyperslab selection
                    MemorySegment startSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, start);
                    MemorySegment strideSeg = arena.allocateFrom(ValueLayout.JAVA_LONG, stride);
                    MemorySegment countSeg  = arena.allocateFrom(ValueLayout.JAVA_LONG, count);
                    H5Sselect_hyperslab(filespace_id, H5S_SELECT_SET(), startSeg, strideSeg, countSeg,
                                        MemorySegment.NULL);

                    memspace_id = H5Screate_simple(2, arena.allocateFrom(ValueLayout.JAVA_LONG, count),
                                                   MemorySegment.NULL);
                    // Read the data using the previously defined hyperslab.
                    if ((dataset_id >= 0) && (filespace_id >= 0) && (memspace_id >= 0)) {
                        // Allocate MemorySegment for reading
                        int totalSize         = 5 * 3;
                        MemorySegment readSeg = arena.allocate(ValueLayout.JAVA_INT, totalSize);
                        H5Dread(dataset_id, H5T_NATIVE_INT_g(), memspace_id, filespace_id, H5P_DEFAULT(),
                                readSeg);
                        // Unflatten 1D MemorySegment to 2D array
                        for (int i = 0; i < 5; i++)
                            for (int j = 0; j < 3; j++)
                                dataRead[i][j] = readSeg.getAtIndex(ValueLayout.JAVA_INT, i * 3 + j);
                    }
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // print out the data values
        System.out.println("\n\nSubset Data Values");
        for (int i = 0; i < 5; i++) {
            System.out.print("\n" + dataRead[i][0]);
            for (int j = 1; j < 3; j++) {
                System.out.print("," + dataRead[i][j]);
            }
        }

        // Close the dataset.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    /**
     * create the file and add groups ans dataset into the file, which is the
     * same as javaExample.H5DatasetCreate
     *
     * @see javaExample.HDF5DatasetCreate
     * @throws Exception
     */
    private static void createFile(Arena arena) throws Exception
    {
        long file_id      = H5I_INVALID_HID();
        long dataspace_id = H5I_INVALID_HID();
        long dataset_id   = H5I_INVALID_HID();

        // Create a new file using default properties.
        try {
            file_id = H5Fcreate(arena.allocateFrom(fname), H5F_ACC_TRUNC(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the data space for the dataset.
        try {
            dataspace_id =
                H5Screate_simple(2, arena.allocateFrom(ValueLayout.JAVA_LONG, dims2D), MemorySegment.NULL);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset.
        try {
            if ((file_id >= 0) && (dataspace_id >= 0))
                dataset_id = H5Dcreate2(file_id, arena.allocateFrom(dsname), H5T_STD_I32LE_g(), dataspace_id,
                                        H5P_DEFAULT(), H5P_DEFAULT(), H5P_DEFAULT());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (dataspace_id >= 0)
                H5Sclose(dataspace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // set the data values
        int[] dataIn = new int[20 * 10];
        for (int i = 0; i < 20; i++) {
            for (int j = 0; j < 10; j++) {
                dataIn[i * 10 + j] = 1000 + i * 100 + j;
            }
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0) {
                MemorySegment dataSeg = arena.allocateFrom(ValueLayout.JAVA_INT, dataIn);
                H5Dwrite(dataset_id, H5T_NATIVE_INT_g(), H5S_ALL(), H5S_ALL(), H5P_DEFAULT(), dataSeg);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        try (Arena arena = Arena.ofConfined()) {
            HDF5SubsetSelect.SubsetSelect(arena);
        }
    }
}
```

### `HDF5Examples/JAVA/TUTR/Java_sourcefiles.cmake`

```cmake
#-----------------------------------------------------------------------------
# Define Sources, one file per application
#-----------------------------------------------------------------------------
set (HDF_JAVA_EXAMPLES
    HDF5FileCreate.java
    HDF5GroupCreate.java
    HDF5DatasetCreate.java
    HDF5AttributeCreate.java
    HDF5DatasetRead.java
    HDF5GroupDatasetCreate.java
    HDF5SubsetSelect.java
    HDF5GroupAbsoluteRelativeCreate.java
    HDF5FileStructure.java
)
```

### `HDF5Examples/JAVA/TUTR/tfiles/110/HDF5AttributeCreate.txt`

```
data range
0  10000
```

### `HDF5Examples/JAVA/TUTR/tfiles/110/HDF5DatasetCreate.txt`

```

```

### `HDF5Examples/JAVA/TUTR/tfiles/110/HDF5DatasetRead.txt`

```


Original Data Values

0, 1, 2, 3, 4, 5, 6, 7, 8, 9
100, 101, 102, 103, 104, 105, 106, 107, 108, 109
200, 201, 202, 203, 204, 205, 206, 207, 208, 209
300, 301, 302, 303, 304, 305, 306, 307, 308, 309
400, 401, 402, 403, 404, 405, 406, 407, 408, 409
500, 501, 502, 503, 504, 505, 506, 507, 508, 509
600, 601, 602, 603, 604, 605, 606, 607, 608, 609
700, 701, 702, 703, 704, 705, 706, 707, 708, 709
800, 801, 802, 803, 804, 805, 806, 807, 808, 809
900, 901, 902, 903, 904, 905, 906, 907, 908, 909
1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009
1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109
1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209
1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309
1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409
1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509
1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609
1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709
1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809
1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909

Modified Data Values

1, 2, 3, 4, 5, 6, 7, 8, 9, 10
101, 102, 103, 104, 105, 106, 107, 108, 109, 110
201, 202, 203, 204, 205, 206, 207, 208, 209, 210
301, 302, 303, 304, 305, 306, 307, 308, 309, 310
401, 402, 403, 404, 405, 406, 407, 408, 409, 410
501, 502, 503, 504, 505, 506, 507, 508, 509, 510
601, 602, 603, 604, 605, 606, 607, 608, 609, 610
701, 702, 703, 704, 705, 706, 707, 708, 709, 710
801, 802, 803, 804, 805, 806, 807, 808, 809, 810
901, 902, 903, 904, 905, 906, 907, 908, 909, 910
1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010
1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110
1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210
1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310
1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410
1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510
1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610
1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710
1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810
1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910
```

### `HDF5Examples/JAVA/TUTR/tfiles/110/HDF5FileCreate.txt`

```

```

### `HDF5Examples/JAVA/TUTR/tfiles/110/HDF5FileStructure.txt`

```
    float arrays
        2D 64-bit double 20x10
        3D 32-bit float  20x10x5
    integer arrays
        2D 32-bit integer 20x10
        3D 8-bit unsigned integer 20x10x5
```

### `HDF5Examples/JAVA/TUTR/tfiles/110/HDF5GroupAbsoluteRelativeCreate.txt`

```

```

### `HDF5Examples/JAVA/TUTR/tfiles/110/HDF5GroupCreate.txt`

```

```

### `HDF5Examples/JAVA/TUTR/tfiles/110/HDF5GroupDatasetCreate.txt`

```

```

### `HDF5Examples/JAVA/TUTR/tfiles/110/HDF5SubsetSelect.txt`

```


Subset Data Values

1402,1404,1406
1702,1704,1706
2002,2004,2006
2302,2304,2306
2602,2604,2606
```

### `HDF5Examples/JAVA/compat/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDFJAVA_COMPAT_EXAMPLES Java)

set_directory_properties(PROPERTIES INCLUDE_DIRECTORIES 
    "${HDFJAVA_LIB_DIR};${JAVA_INCLUDE_PATH};${JAVA_INCLUDE_PATH2}"
)

add_subdirectory (H5D)
add_subdirectory (H5T)
add_subdirectory (H5G)
add_subdirectory (TUTR)
```

### `HDF5Examples/JAVA/compat/H5D/CMakeLists.txt`

```
cmake_minimum_required (VERSION 3.26)
project (HDF5Examples_JAVA_H5D Java)

set (CMAKE_VERBOSE_MAKEFILE 1)

#-----------------------------------------------------------------------------
# Define Sources
#-----------------------------------------------------------------------------
include (Java_sourcefiles.cmake)

if (WIN32)
  set (CMAKE_JAVA_INCLUDE_FLAG_SEP ";")
else ()
  set (CMAKE_JAVA_INCLUDE_FLAG_SEP ":")
endif ()

set (CMAKE_JAVA_INCLUDE_PATH ".")
foreach (CMAKE_JINCLUDE_PATH ${HDF5_JAVA_INCLUDE_DIRS})
  set (CMAKE_JAVA_INCLUDE_PATH "${CMAKE_JAVA_INCLUDE_PATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${CMAKE_JINCLUDE_PATH}")
endforeach ()
if (Java_VERSION_STRING VERSION_LESS "25.0.0" OR HDF5_PROVIDES_JNI)
  set (CMD_ARGS "-Dhdf.hdf5lib.H5.loadLibraryName=${H5EXAMPLE_JAVA_LIBRARY}$<$<OR:$<CONFIG:Debug>,$<CONFIG:Developer>>:${CMAKE_DEBUG_POSTFIX}>;")
endif ()

set (CMAKE_JAVA_CLASSPATH ".")
foreach (CMAKE_INCLUDE_PATH ${HDF5_JAVA_INCLUDE_DIRS})
  set (CMAKE_JAVA_CLASSPATH "${CMAKE_JAVA_CLASSPATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${CMAKE_INCLUDE_PATH}")
endforeach ()

foreach (HCP_JAR ${HDF5_JAVA_INCLUDE_DIRS})
  get_filename_component (_HCP_FILE ${HCP_JAR} NAME)
  set (HDFJAVA_CLASSJARS "${_HCP_FILE} ${HDFJAVA_CLASSJARS}")
endforeach ()

foreach (example ${HDF_JAVA_EXAMPLES})
  get_filename_component (example_name ${example} NAME_WE)
  file (WRITE ${PROJECT_BINARY_DIR}/${example_name}_Manifest.txt
  "Main-Class: ${example_name}
Class-Path: ${HDFJAVA_CLASSJARS}
"
  )
  add_jar (${EXAMPLE_VARNAME}JC_${example_name}
      SOURCES ${example}
      MANIFEST ${PROJECT_BINARY_DIR}/${example_name}_Manifest.txt
  )
  get_target_property (${EXAMPLE_VARNAME}JC_${example_name}_JAR_FILE ${EXAMPLE_VARNAME}JC_${example_name} JAR_FILE)
  get_target_property (${EXAMPLE_VARNAME}JC_${example_name}_CLASSPATH ${EXAMPLE_VARNAME}JC_${example_name} CLASSDIR)
  if (H5EXAMPLE_JAVA_LIBRARIES)
    add_dependencies (${EXAMPLE_VARNAME}JC_${example_name} ${H5EXAMPLE_JAVA_LIBRARIES})
  endif ()
endforeach ()

if (H5EXAMPLE_BUILD_TESTING)
  macro (ADD_H5_TEST resultfile resultcode)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jcompat-h5-${resultfile}
        COMMAND "${CMAKE_COMMAND}"
            -D "TEST_JAVA=${CMAKE_Java_RUNTIME};${CMAKE_Java_RUNTIME_FLAGS}"
            -D "TEST_PROGRAM=${resultfile}"
            -D "TEST_ARGS:STRING=${ARGN};${CMD_ARGS}"
            -D "TEST_CLASSPATH:STRING=${CMAKE_JAVA_CLASSPATH}${CMAKE_JAVA_INCLUDE_FLAG_SEP}${${EXAMPLE_VARNAME}JC_${resultfile}_JAR_FILE}"
            -D "TEST_LIBRARY_DIRECTORY=${CMAKE_TEST_LIB_DIRECTORY}"
            -D "TEST_FOLDER=${PROJECT_BINARY_DIR}"
            -D "TEST_OUTPUT=${PROJECT_BINARY_DIR}/${resultfile}.out"
            -D "TEST_REFERENCE=${resultfile}.txt"
            -D "TEST_EXPECT=${resultcode}"
            -D "TEST_SKIP_COMPARE=TRUE"
            -P "${H5EXAMPLE_RESOURCES_DIR}/runTest.cmake"
    )
    if (NOT "${last_test}" STREQUAL "")
      set_tests_properties (${EXAMPLE_VARNAME}_jcompat-h5-${resultfile} PROPERTIES DEPENDS ${last_test})
    endif ()
    set (last_test "${EXAMPLE_VARNAME}_jcompat-h5-${resultfile}")
  endmacro ()

  foreach (example ${HDF_JAVA_EXAMPLES})
    get_filename_component (example_name ${example} NAME_WE)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jcompat-h5-${example_name}-clearall-objects
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${PROJECT_BINARY_DIR}/${example_name}.h5
            ${PROJECT_BINARY_DIR}/${example_name}.out
            ${PROJECT_BINARY_DIR}/${example_name}.out.err
    )
    if (NOT "${last_test}" STREQUAL "")
      set_tests_properties (${EXAMPLE_VARNAME}_jcompat-h5-${example_name}-clearall-objects PROPERTIES DEPENDS ${last_test})
    endif ()
    add_test (
        NAME ${EXAMPLE_VARNAME}_jcompat-h5-${example_name}-copy-objects
        COMMAND    ${CMAKE_COMMAND}
            -E copy_if_different
            ${PROJECT_SOURCE_DIR}/tfiles/110/${example_name}.txt
            ${PROJECT_BINARY_DIR}/${example_name}.txt
    )
    set_tests_properties (${EXAMPLE_VARNAME}_jcompat-h5-${example_name}-copy-objects PROPERTIES DEPENDS ${EXAMPLE_VARNAME}_jcompat-h5-${example_name}-clearall-objects)
    set (last_test "${EXAMPLE_VARNAME}_jcompat-h5-${example_name}-copy-objects")
    ADD_H5_TEST (${example_name} 0)
    add_test (
        NAME ${EXAMPLE_VARNAME}_jcompat-h5-${example_name}-clean-objects
        COMMAND    ${CMAKE_COMMAND}
            -E remove
            ${PROJECT_BINARY_DIR}/${example_name}.h5
            ${PROJECT_BINARY_DIR}/${example_name}.out
            ${PROJECT_BINARY_DIR}/${example_name}.out.err
    )
    set_tests_properties (${EXAMPLE_VARNAME}_jcompat-h5-${example_name}-clean-objects PROPERTIES DEPENDS ${last_test})
  endforeach ()

endif ()
```

### `HDF5Examples/JAVA/compat/H5D/H5Ex_D_Alloc.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to set the space allocation time
  for a dataset.  The program first creates two datasets,
  one with the default allocation time (late) and one with
  early allocation time, and displays whether each has been
  allocated and their allocation size.  Next, it writes data
  to the datasets, and again displays whether each has been
  allocated and their allocation size.
 ************************************************************/

import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

import hdf.hdf5lib.H5;
import hdf.hdf5lib.HDF5Constants;

public class H5Ex_D_Alloc {
    private static String FILENAME     = "H5Ex_D_Alloc.h5";
    private static String DATASETNAME1 = "DS1";
    private static String DATASETNAME2 = "DS2";
    private static final int DIM_X     = 4;
    private static final int DIM_Y     = 7;
    private static final int FILLVAL   = 99;
    private static final int RANK      = 2;

    // Values for the status of space allocation
    enum H5D_space_status {
        H5D_SPACE_STATUS_ERROR(-1),
        H5D_SPACE_STATUS_NOT_ALLOCATED(0),
        H5D_SPACE_STATUS_PART_ALLOCATED(1),
        H5D_SPACE_STATUS_ALLOCATED(2);
        private static final Map<Integer, H5D_space_status> lookup = new HashMap<Integer, H5D_space_status>();

        static
        {
            for (H5D_space_status s : EnumSet.allOf(H5D_space_status.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5D_space_status(int space_status) { this.code = space_status; }

        public int getCode() { return this.code; }

        public static H5D_space_status get(int code) { return lookup.get(code); }
    }

    private static void allocation()
    {
        long file_id      = HDF5Constants.H5I_INVALID_HID;
        long filespace_id = HDF5Constants.H5I_INVALID_HID;
        long dataset_id1  = HDF5Constants.H5I_INVALID_HID;
        long dataset_id2  = HDF5Constants.H5I_INVALID_HID;
        long dcpl_id      = HDF5Constants.H5I_INVALID_HID;
        long[] dims       = {DIM_X, DIM_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];
        int space_status  = 0;
        long storage_size = 0;

        // Initialize the dataset.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = FILLVAL;

        // Create a file using default properties.
        try {
            file_id = H5.H5Fcreate(FILENAME, HDF5Constants.H5F_ACC_TRUNC, HDF5Constants.H5P_DEFAULT,
                                   HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            filespace_id = H5.H5Screate_simple(RANK, dims, null);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list, and set the chunk size.
        try {
            dcpl_id = H5.H5Pcreate(HDF5Constants.H5P_DATASET_CREATE);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Set the allocation time to "early". This way we can be sure
        // that reading from the dataset immediately after creation will
        // return the fill value.
        try {
            if (dcpl_id >= 0)
                H5.H5Pset_alloc_time(dcpl_id, HDF5Constants.H5D_ALLOC_TIME_EARLY);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        System.out.println("Creating datasets...");
        System.out.println(DATASETNAME1 + " has allocation time H5D_ALLOC_TIME_LATE");
        System.out.println(DATASETNAME2 + " has allocation time H5D_ALLOC_TIME_EARLY");
        System.out.println();

        // Create the dataset using the dataset default creation property list.
        try {
            if ((file_id >= 0) && (filespace_id >= 0))
                dataset_id1 = H5.H5Dcreate(file_id, DATASETNAME1, HDF5Constants.H5T_NATIVE_INT, filespace_id,
                                           HDF5Constants.H5P_DEFAULT, HDF5Constants.H5P_DEFAULT,
                                           HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset using the dataset creation property list.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0))
                dataset_id2 = H5.H5Dcreate(file_id, DATASETNAME2, HDF5Constants.H5T_NATIVE_INT, filespace_id,
                                           HDF5Constants.H5P_DEFAULT, dcpl_id, HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print space status and storage size for dset1.
        try {
            if (dataset_id1 >= 0)
                space_status = H5.H5Dget_space_status(dataset_id1);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (dataset_id1 >= 0)
                storage_size = H5.H5Dget_storage_size(dataset_id1);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        String the_space = " ";
        if (H5D_space_status.get(space_status) != H5D_space_status.H5D_SPACE_STATUS_ALLOCATED)
            the_space += "not ";
        System.out.println("Space for " + DATASETNAME1 + " has" + the_space + "been allocated.");
        System.out.println("Storage size for " + DATASETNAME1 + " is: " + storage_size + " bytes.");

        // Retrieve and print space status and storage size for dset2.
        try {
            if (dataset_id2 >= 0)
                space_status = H5.H5Dget_space_status(dataset_id2);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (dataset_id2 >= 0)
                storage_size = H5.H5Dget_storage_size(dataset_id2);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        the_space = " ";
        if (H5D_space_status.get(space_status) != H5D_space_status.H5D_SPACE_STATUS_ALLOCATED)
            the_space += "not ";
        System.out.println("Space for " + DATASETNAME2 + " has" + the_space + "been allocated.");
        System.out.println("Storage size for " + DATASETNAME2 + " is: " + storage_size + " bytes.");
        System.out.println();

        System.out.println("Writing data...");
        System.out.println();

        // Write the data to the datasets.
        try {
            if (dataset_id1 >= 0)
                H5.H5Dwrite(dataset_id1, HDF5Constants.H5T_NATIVE_INT, HDF5Constants.H5S_ALL,
                            HDF5Constants.H5S_ALL, HDF5Constants.H5P_DEFAULT, dset_data[0]);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (dataset_id2 >= 0)
                H5.H5Dwrite(dataset_id2, HDF5Constants.H5T_NATIVE_INT, HDF5Constants.H5S_ALL,
                            HDF5Constants.H5S_ALL, HDF5Constants.H5P_DEFAULT, dset_data[0]);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print space status and storage size for dset1.
        try {
            if (dataset_id1 >= 0)
                space_status = H5.H5Dget_space_status(dataset_id1);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (dataset_id1 >= 0)
                storage_size = H5.H5Dget_storage_size(dataset_id1);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        the_space = " ";
        if (H5D_space_status.get(space_status) != H5D_space_status.H5D_SPACE_STATUS_ALLOCATED)
            the_space += "not ";
        System.out.println("Space for " + DATASETNAME1 + " has" + the_space + "been allocated.");
        System.out.println("Storage size for " + DATASETNAME1 + " is: " + storage_size + " bytes.");

        // Retrieve and print space status and storage size for dset2.
        try {
            if (dataset_id2 >= 0)
                space_status = H5.H5Dget_space_status(dataset_id2);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        try {
            if (dataset_id2 >= 0)
                storage_size = H5.H5Dget_storage_size(dataset_id2);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        the_space = " ";
        if (H5D_space_status.get(space_status) != H5D_space_status.H5D_SPACE_STATUS_ALLOCATED)
            the_space += "not ";
        System.out.println("Space for " + DATASETNAME2 + " has" + the_space + "been allocated.");
        System.out.println("Storage size for " + DATASETNAME2 + " is: " + storage_size + " bytes.");
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5.H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id1 >= 0)
                H5.H5Dclose(dataset_id1);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id2 >= 0)
                H5.H5Dclose(dataset_id2);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5.H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5.H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args) { H5Ex_D_Alloc.allocation(); }
}
```

### `HDF5Examples/JAVA/compat/H5D/H5Ex_D_Checksum.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write data to a dataset
  using the Fletcher32 checksum filter.  The program first
  checks if the Fletcher32 filter is available, then if it
  is it writes integers to a dataset using Fletcher32, then
  closes the file.  Next, it reopens the file, reads back
  the data, checks if the filter detected an error and
  outputs the type of filter and the maximum value in the
  dataset to the screen.
 ************************************************************/

import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

import hdf.hdf5lib.H5;
import hdf.hdf5lib.HDF5Constants;

public class H5Ex_D_Checksum {
    private static String FILENAME    = "H5Ex_D_Checksum.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 32;
    private static final int DIM_Y    = 64;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 8;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;

    // Values for the status of space allocation
    enum H5Z_filter {
        H5Z_FILTER_ERROR(-1),
        H5Z_FILTER_NONE(0),
        H5Z_FILTER_DEFLATE(1),
        H5Z_FILTER_SHUFFLE(2),
        H5Z_FILTER_FLETCHER32(3),
        H5Z_FILTER_SZIP(4),
        H5Z_FILTER_NBIT(5),
        H5Z_FILTER_SCALEOFFSET(6),
        H5Z_FILTER_RESERVED(256),
        H5Z_FILTER_MAX(65535);
        private static final Map<Integer, H5Z_filter> lookup = new HashMap<Integer, H5Z_filter>();

        static
        {
            for (H5Z_filter s : EnumSet.allOf(H5Z_filter.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5Z_filter(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5Z_filter get(int code) { return lookup.get(code); }
    }

    private static boolean checkFletcher32Filter()
    {
        try {
            int available = H5.H5Zfilter_avail(H5Z_filter.H5Z_FILTER_FLETCHER32.getCode());
            if (available == 0) {
                System.out.println("N-Bit filter not available.");
                return false;
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            int filter_info = H5.H5Zget_filter_info(HDF5Constants.H5Z_FILTER_FLETCHER32);
            if (((filter_info & HDF5Constants.H5Z_FILTER_CONFIG_ENCODE_ENABLED) == 0) ||
                ((filter_info & HDF5Constants.H5Z_FILTER_CONFIG_DECODE_ENABLED) == 0)) {
                System.out.println("N-Bit filter not available for encoding and decoding.");
                return false;
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        return true;
    }

    private static void writeChecksum()
    {
        long file_id      = HDF5Constants.H5I_INVALID_HID;
        long filespace_id = HDF5Constants.H5I_INVALID_HID;
        long dataset_id   = HDF5Constants.H5I_INVALID_HID;
        long dcpl_id      = HDF5Constants.H5I_INVALID_HID;
        long[] dims       = {DIM_X, DIM_Y};
        long[] chunk_dims = {CHUNK_X, CHUNK_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            file_id = H5.H5Fcreate(FILENAME, HDF5Constants.H5F_ACC_TRUNC, HDF5Constants.H5P_DEFAULT,
                                   HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            filespace_id = H5.H5Screate_simple(RANK, dims, null);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list, add the N-Bit filter.
        try {
            dcpl_id = H5.H5Pcreate(HDF5Constants.H5P_DATASET_CREATE);
            if (dcpl_id >= 0) {
                H5.H5Pset_fletcher32(dcpl_id);
                // Set the chunk size.
                H5.H5Pset_chunk(dcpl_id, NDIMS, chunk_dims);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5.H5Dcreate(file_id, DATASETNAME, HDF5Constants.H5T_STD_I32LE, filespace_id,
                                          HDF5Constants.H5P_DEFAULT, dcpl_id, HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0)
                H5.H5Dwrite(dataset_id, HDF5Constants.H5T_NATIVE_INT, HDF5Constants.H5S_ALL,
                            HDF5Constants.H5S_ALL, HDF5Constants.H5P_DEFAULT, dset_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5.H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5.H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5.H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5.H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readChecksum()
    {
        long file_id      = HDF5Constants.H5I_INVALID_HID;
        long dataset_id   = HDF5Constants.H5I_INVALID_HID;
        long dcpl_id      = HDF5Constants.H5I_INVALID_HID;
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open an existing file.
        try {
            file_id = H5.H5Fopen(FILENAME, HDF5Constants.H5F_ACC_RDONLY, HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5.H5Dopen(file_id, DATASETNAME, HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5.H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print the filter type. Here we only retrieve the
        // first filter because we know that we only added one filter.
        try {
            if (dcpl_id >= 0) {
                // Java lib requires a valid filter_name object and cd_values
                int[] flags          = {0};
                long[] cd_nelmts     = {1};
                int[] cd_values      = {0};
                String[] filter_name = {""};
                int[] filter_config  = {0};
                int filter_type      = -1;
                filter_type = H5.H5Pget_filter(dcpl_id, 0, flags, cd_nelmts, cd_values, 120, filter_name,
                                               filter_config);
                System.out.print("Filter type is: ");
                switch (H5Z_filter.get(filter_type)) {
                case H5Z_FILTER_DEFLATE:
                    System.out.println("H5Z_FILTER_DEFLATE");
                    break;
                case H5Z_FILTER_SHUFFLE:
                    System.out.println("H5Z_FILTER_SHUFFLE");
                    break;
                case H5Z_FILTER_FLETCHER32:
                    System.out.println("H5Z_FILTER_FLETCHER32");
                    break;
                case H5Z_FILTER_SZIP:
                    System.out.println("H5Z_FILTER_SZIP");
                    break;
                default:
                    System.out.println("H5Z_FILTER_ERROR");
                }
                System.out.println();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0) {
                int status = H5.H5Dread(dataset_id, HDF5Constants.H5T_NATIVE_INT, HDF5Constants.H5S_ALL,
                                        HDF5Constants.H5S_ALL, HDF5Constants.H5P_DEFAULT, dset_data);
                // Check if the read was successful. Normally we do not perform
                // error checking in these examples for the sake of clarity, but in
                // this case we will make an exception because this is how the
                // fletcher32 checksum filter reports data errors.
                if (status < 0) {
                    System.out.print("Dataset read failed!");
                    try {
                        if (dcpl_id >= 0)
                            H5.H5Pclose(dcpl_id);
                        if (dataset_id >= 0)
                            H5.H5Dclose(dataset_id);
                        if (file_id >= 0)
                            H5.H5Fclose(file_id);
                    }
                    catch (Exception e) {
                        e.printStackTrace();
                    }
                    return;
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Find the maximum value in the dataset, to verify that it was read
        // correctly.
        int max = dset_data[0][0];
        for (int indx = 0; indx < DIM_X; indx++) {
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                if (max < dset_data[indx][jndx])
                    max = dset_data[indx][jndx];
        }
        // Print the maximum value.
        System.out.println("Maximum value in " + DATASETNAME + " is: " + max);

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5.H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5.H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5.H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        // Check if the Fletcher32 filter is available and can be used for
        // both encoding and decoding. Normally we do not perform error
        // checking in these examples for the sake of clarity, but in this
        // case we will make an exception because this filter is an
        // optional part of the hdf5 library.
        // size to be the current size.
        if (H5Ex_D_Checksum.checkFletcher32Filter()) {
            H5Ex_D_Checksum.writeChecksum();
            H5Ex_D_Checksum.readChecksum();
        }
    }
}
```

### `HDF5Examples/JAVA/compat/H5D/H5Ex_D_Chunk.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to create a chunked dataset.  The
  program first writes integers in a hyperslab selection to
  a chunked dataset with dataspace dimensions of DIM_XxDIM_Y
  and chunk size of CHUNK_XxCHUNK_Y, then closes the file.
  Next, it reopens the file, reads back the data, and
  outputs it to the screen.  Finally it reads the data again
  using a different hyperslab selection, and outputs
  the result to the screen.
 ************************************************************/

import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

import hdf.hdf5lib.H5;
import hdf.hdf5lib.HDF5Constants;

public class H5Ex_D_Chunk {
    private static String FILENAME    = "H5Ex_D_Chunk.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 6;
    private static final int DIM_Y    = 8;
    private static final int CHUNK_X  = 4;
    private static final int CHUNK_Y  = 4;
    private static final int RANK     = 2;
    private static final int NDIMS    = 2;

    // Values for the status of space allocation
    enum H5D_layout {
        H5D_LAYOUT_ERROR(-1),
        H5D_COMPACT(0),
        H5D_CONTIGUOUS(1),
        H5D_CHUNKED(2),
        H5D_VIRTUAL(3),
        H5D_NLAYOUTS(4);
        private static final Map<Integer, H5D_layout> lookup = new HashMap<Integer, H5D_layout>();

        static
        {
            for (H5D_layout s : EnumSet.allOf(H5D_layout.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5D_layout(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5D_layout get(int code) { return lookup.get(code); }
    }

    private static void writeChunk()
    {
        long file_id      = HDF5Constants.H5I_INVALID_HID;
        long filespace_id = HDF5Constants.H5I_INVALID_HID;
        long dataset_id   = HDF5Constants.H5I_INVALID_HID;
        long dcpl_id      = HDF5Constants.H5I_INVALID_HID;
        long[] dims       = {DIM_X, DIM_Y};
        long[] chunk_dims = {CHUNK_X, CHUNK_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data to "1", to make it easier to see the selections.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = 1;

        // Print the data to the screen.
        System.out.println("Original Data:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // Create a new file using default properties.
        try {
            file_id = H5.H5Fcreate(FILENAME, HDF5Constants.H5F_ACC_TRUNC, HDF5Constants.H5P_DEFAULT,
                                   HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            filespace_id = H5.H5Screate_simple(RANK, dims, null);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list.
        try {
            dcpl_id = H5.H5Pcreate(HDF5Constants.H5P_DATASET_CREATE);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Set the chunk size.
        try {
            if (dcpl_id >= 0)
                H5.H5Pset_chunk(dcpl_id, NDIMS, chunk_dims);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the chunked dataset.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5.H5Dcreate(file_id, DATASETNAME, HDF5Constants.H5T_STD_I32LE, filespace_id,
                                          HDF5Constants.H5P_DEFAULT, dcpl_id, HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Define and select the first part of the hyperslab selection.
        long[] start  = {0, 0};
        long[] stride = {3, 3};
        long[] count  = {2, 3};
        long[] block  = {2, 2};
        try {
            if ((filespace_id >= 0))
                H5.H5Sselect_hyperslab(filespace_id, HDF5Constants.H5S_SELECT_SET, start, stride, count,
                                       block);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        // Define and select the second part of the hyperslab selection,
        // which is subtracted from the first selection by the use of
        // H5S_SELECT_NOTB
        block[0] = 1;
        block[1] = 1;
        try {
            if ((filespace_id >= 0)) {
                H5.H5Sselect_hyperslab(filespace_id, HDF5Constants.H5S_SELECT_NOTB, start, stride, count,
                                       block);

                // Write the data to the dataset.
                if (dataset_id >= 0)
                    H5.H5Dwrite(dataset_id, HDF5Constants.H5T_NATIVE_INT, HDF5Constants.H5S_ALL, filespace_id,
                                HDF5Constants.H5P_DEFAULT, dset_data);
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5.H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5.H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5.H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5.H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readChunk()
    {
        long file_id      = HDF5Constants.H5I_INVALID_HID;
        long filespace_id = HDF5Constants.H5I_INVALID_HID;
        long dataset_id   = HDF5Constants.H5I_INVALID_HID;
        long dcpl_id      = HDF5Constants.H5I_INVALID_HID;
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open an existing file.
        try {
            file_id = H5.H5Fopen(FILENAME, HDF5Constants.H5F_ACC_RDONLY, HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5.H5Dopen(file_id, DATASETNAME, HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5.H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Print the storage layout.
        try {
            if (dcpl_id >= 0) {
                int layout_type = H5.H5Pget_layout(dcpl_id);
                System.out.print("Storage layout for " + DATASETNAME + " is: ");
                switch (H5D_layout.get(layout_type)) {
                case H5D_COMPACT:
                    System.out.println("H5D_COMPACT");
                    break;
                case H5D_CONTIGUOUS:
                    System.out.println("H5D_CONTIGUOUS");
                    break;
                case H5D_CHUNKED:
                    System.out.println("H5D_CHUNKED");
                    break;
                case H5D_VIRTUAL:
                    System.out.println("H5D_VIRTUAL");
                    break;
                case H5D_LAYOUT_ERROR:
                    break;
                case H5D_NLAYOUTS:
                    break;
                default:
                    break;
                }
                System.out.println();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0)
                H5.H5Dread(dataset_id, HDF5Constants.H5T_NATIVE_INT, HDF5Constants.H5S_ALL,
                           HDF5Constants.H5S_ALL, HDF5Constants.H5P_DEFAULT, dset_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Data as written to disk by hyberslabs:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // Initialize the read array.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = 0;

        // Define and select the hyperslab to use for reading.
        try {
            if (dataset_id >= 0) {
                filespace_id = H5.H5Dget_space(dataset_id);

                long[] start  = {0, 1};
                long[] stride = {4, 4};
                long[] count  = {2, 2};
                long[] block  = {2, 3};

                if (filespace_id >= 0) {
                    H5.H5Sselect_hyperslab(filespace_id, HDF5Constants.H5S_SELECT_SET, start, stride, count,
                                           block);

                    // Read the data using the previously defined hyperslab.
                    if ((dataset_id >= 0) && (filespace_id >= 0))
                        H5.H5Dread(dataset_id, HDF5Constants.H5T_NATIVE_INT, HDF5Constants.H5S_ALL,
                                   filespace_id, HDF5Constants.H5P_DEFAULT, dset_data);
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Data as read from disk by hyberslab:");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5.H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5.H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5.H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5.H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        H5Ex_D_Chunk.writeChunk();
        H5Ex_D_Chunk.readChunk();
    }
}
```

### `HDF5Examples/JAVA/compat/H5D/H5Ex_D_Compact.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write data to a compact
  dataset.  The program first writes integers to a compact
  dataset with dataspace dimensions of DIM_XxDIM_Y, then
  closes the file.  Next, it reopens the file, reads back
  the data, and outputs it to the screen.
 ************************************************************/

import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

import hdf.hdf5lib.H5;
import hdf.hdf5lib.HDF5Constants;

public class H5Ex_D_Compact {
    private static String FILENAME    = "H5Ex_D_Compact.h5";
    private static String DATASETNAME = "DS1";
    private static final int DIM_X    = 4;
    private static final int DIM_Y    = 7;
    private static final int RANK     = 2;

    // Values for the status of space allocation
    enum H5D_layout {
        H5D_LAYOUT_ERROR(-1),
        H5D_COMPACT(0),
        H5D_CONTIGUOUS(1),
        H5D_CHUNKED(2),
        H5D_VIRTUAL(3),
        H5D_NLAYOUTS(4);
        private static final Map<Integer, H5D_layout> lookup = new HashMap<Integer, H5D_layout>();

        static
        {
            for (H5D_layout s : EnumSet.allOf(H5D_layout.class))
                lookup.put(s.getCode(), s);
        }

        private int code;

        H5D_layout(int layout_type) { this.code = layout_type; }

        public int getCode() { return this.code; }

        public static H5D_layout get(int code) { return lookup.get(code); }
    }

    private static void writeCompact()
    {
        long file_id      = HDF5Constants.H5I_INVALID_HID;
        long filespace_id = HDF5Constants.H5I_INVALID_HID;
        long dataset_id   = HDF5Constants.H5I_INVALID_HID;
        long dcpl_id      = HDF5Constants.H5I_INVALID_HID;
        long[] dims       = {DIM_X, DIM_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize data.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            file_id = H5.H5Fcreate(FILENAME, HDF5Constants.H5F_ACC_TRUNC, HDF5Constants.H5P_DEFAULT,
                                   HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            filespace_id = H5.H5Screate_simple(RANK, dims, null);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list.
        try {
            dcpl_id = H5.H5Pcreate(HDF5Constants.H5P_DATASET_CREATE);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Set the layout to compact.
        try {
            if (dcpl_id >= 0)
                H5.H5Pset_layout(dcpl_id, H5D_layout.H5D_COMPACT.getCode());
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset. We will use all default properties for this example.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5.H5Dcreate(file_id, DATASETNAME, HDF5Constants.H5T_STD_I32LE, filespace_id,
                                          HDF5Constants.H5P_DEFAULT, dcpl_id, HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the data to the dataset.
        try {
            if (dataset_id >= 0)
                H5.H5Dwrite(dataset_id, HDF5Constants.H5T_NATIVE_INT, HDF5Constants.H5S_ALL,
                            HDF5Constants.H5S_ALL, HDF5Constants.H5P_DEFAULT, dset_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5.H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5.H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5.H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5.H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readCompact()
    {
        long file_id      = HDF5Constants.H5I_INVALID_HID;
        long filespace_id = HDF5Constants.H5I_INVALID_HID;
        long dataset_id   = HDF5Constants.H5I_INVALID_HID;
        long dcpl_id      = HDF5Constants.H5I_INVALID_HID;
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Open file and dataset using the default properties.
        try {
            file_id = H5.H5Fopen(FILENAME, HDF5Constants.H5F_ACC_RDONLY, HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open an existing dataset.
        try {
            if (file_id >= 0)
                dataset_id = H5.H5Dopen(file_id, DATASETNAME, HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5.H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Print the storage layout.
        try {
            if (dcpl_id >= 0) {
                int layout_type = H5.H5Pget_layout(dcpl_id);
                System.out.print("Storage layout for " + DATASETNAME + " is: ");
                switch (H5D_layout.get(layout_type)) {
                case H5D_COMPACT:
                    System.out.println("H5D_COMPACT");
                    break;
                case H5D_CONTIGUOUS:
                    System.out.println("H5D_CONTIGUOUS");
                    break;
                case H5D_CHUNKED:
                    System.out.println("H5D_CHUNKED");
                    break;
                case H5D_VIRTUAL:
                    System.out.println("H5D_VIRTUAL");
                    break;
                case H5D_LAYOUT_ERROR:
                    break;
                case H5D_NLAYOUTS:
                    break;
                default:
                    break;
                }
                System.out.println();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0)
                H5.H5Dread(dataset_id, HDF5Constants.H5T_NATIVE_INT, HDF5Constants.H5S_ALL,
                           HDF5Constants.H5S_ALL, HDF5Constants.H5P_DEFAULT, dset_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println("Data for " + DATASETNAME + " is: ");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // End access to the dataset and release resources used by it.
        try {
            if (dcpl_id >= 0)
                H5.H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dataset_id >= 0)
                H5.H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (filespace_id >= 0)
                H5.H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5.H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        H5Ex_D_Compact.writeCompact();
        H5Ex_D_Compact.readCompact();
    }
}
```

### `HDF5Examples/JAVA/compat/H5D/H5Ex_D_External.java`

```java
/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 * Copyright by The HDF Group.                                               *
 * All rights reserved.                                                      *
 *                                                                           *
 * This file is part of HDF5.  The full HDF5 copyright notice, including     *
 * terms governing use, modification, and redistribution, is contained in    *
 * the LICENSE file, which can be found at the root of the source code       *
 * distribution tree, or in https://www.hdfgroup.org/licenses.               *
 * If you do not have access to either file, you may request a copy from     *
 * help@hdfgroup.org.                                                        *
 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */

/************************************************************
  This example shows how to read and write data to an
  external dataset.  The program first writes integers to an
  external dataset with dataspace dimensions of DIM_XxDIM_Y,
  then closes the file.  Next, it reopens the file, reads
  back the data, and outputs the name of the external data
  file and the data to the screen.
 ************************************************************/

import hdf.hdf5lib.H5;
import hdf.hdf5lib.HDF5Constants;

public class H5Ex_D_External {
    private static String FILENAME         = "H5Ex_D_External.h5";
    private static String EXTERNALNAME     = "H5Ex_D_External.data";
    private static String DATASETNAME      = "DS1";
    private static final int DIM_X         = 4;
    private static final int DIM_Y         = 7;
    private static final int RANK          = 2;
    private static final int NAME_BUF_SIZE = 32;

    private static void writeExternal()
    {
        long file_id      = HDF5Constants.H5I_INVALID_HID;
        long dcpl_id      = HDF5Constants.H5I_INVALID_HID;
        long filespace_id = HDF5Constants.H5I_INVALID_HID;
        long dataset_id   = HDF5Constants.H5I_INVALID_HID;
        long[] dims       = {DIM_X, DIM_Y};
        int[][] dset_data = new int[DIM_X][DIM_Y];

        // Initialize the dataset.
        for (int indx = 0; indx < DIM_X; indx++)
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                dset_data[indx][jndx] = indx * jndx - jndx;

        // Create a new file using default properties.
        try {
            file_id = H5.H5Fcreate(FILENAME, HDF5Constants.H5F_ACC_TRUNC, HDF5Constants.H5P_DEFAULT,
                                   HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create dataspace. Setting maximum size to NULL sets the maximum
        // size to be the current size.
        try {
            filespace_id = H5.H5Screate_simple(RANK, dims, null);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the dataset creation property list.
        try {
            dcpl_id = H5.H5Pcreate(HDF5Constants.H5P_DATASET_CREATE);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // set the external file.
        try {
            if (dcpl_id >= 0)
                H5.H5Pset_external(dcpl_id, EXTERNALNAME, 0, HDF5Constants.H5F_UNLIMITED);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Create the HDF5Constants.dataset.
        try {
            if ((file_id >= 0) && (filespace_id >= 0) && (dcpl_id >= 0))
                dataset_id = H5.H5Dcreate(file_id, DATASETNAME, HDF5Constants.H5T_STD_I32LE, filespace_id,
                                          HDF5Constants.H5P_DEFAULT, dcpl_id, HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Write the dataset.
        try {
            H5.H5Dwrite(dataset_id, HDF5Constants.H5T_NATIVE_INT, HDF5Constants.H5S_ALL,
                        HDF5Constants.H5S_ALL, HDF5Constants.H5P_DEFAULT, dset_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // End access to the dataset and release resources used by it.
        try {
            if (dataset_id >= 0)
                H5.H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Terminate access to the data space.
        try {
            if (filespace_id >= 0)
                H5.H5Sclose(filespace_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dcpl_id >= 0)
                H5.H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5.H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static void readExternal()
    {
        long file_id      = HDF5Constants.H5I_INVALID_HID;
        long dcpl_id      = HDF5Constants.H5I_INVALID_HID;
        long dataset_id   = HDF5Constants.H5I_INVALID_HID;
        int[][] dset_data = new int[DIM_X][DIM_Y];
        String[] Xname    = new String[1];

        // Open file using the default properties.
        try {
            file_id = H5.H5Fopen(FILENAME, HDF5Constants.H5F_ACC_RDWR, HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Open dataset using the default properties.
        try {
            if (file_id >= 0)
                dataset_id = H5.H5Dopen(file_id, DATASETNAME, HDF5Constants.H5P_DEFAULT);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve the dataset creation property list.
        try {
            if (dataset_id >= 0)
                dcpl_id = H5.H5Dget_create_plist(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Retrieve and print the name of the external file.
        long[] Xsize = new long[NAME_BUF_SIZE];
        try {
            if (dcpl_id >= 0)
                H5.H5Pget_external(dcpl_id, 0, Xsize.length, Xname, Xsize);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        System.out.println(DATASETNAME + " is stored in file: " + Xname[0]);

        // Read the data using the default properties.
        try {
            if (dataset_id >= 0)
                H5.H5Dread(dataset_id, HDF5Constants.H5T_NATIVE_INT, HDF5Constants.H5S_ALL,
                           HDF5Constants.H5S_ALL, HDF5Constants.H5P_DEFAULT, dset_data);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Output the data to the screen.
        System.out.println(DATASETNAME + ":");
        for (int indx = 0; indx < DIM_X; indx++) {
            System.out.print(" [ ");
            for (int jndx = 0; jndx < DIM_Y; jndx++)
                System.out.print(dset_data[indx][jndx] + " ");
            System.out.println("]");
        }
        System.out.println();

        // Close the dataset.
        try {
            if (dataset_id >= 0)
                H5.H5Dclose(dataset_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        try {
            if (dcpl_id >= 0)
                H5.H5Pclose(dcpl_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

        // Close the file.
        try {
            if (file_id >= 0)
                H5.H5Fclose(file_id);
        }
        catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args)
    {
        H5Ex_D_External.writeExternal();
        H5Ex_D_External.readExternal();
    }
}
```
