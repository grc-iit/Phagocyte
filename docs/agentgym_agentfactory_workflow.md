# AgentGym + AgentFactory Workflow: Phagocyte + Generator Pipeline

## Table of Contents
- [Overview](#overview)
- [Complete System Architecture](#complete-system-architecture)
- [AgentFactory Workflow](#agentfactory-workflow)
- [AgentGym Training Pipeline](#agentgym-training-pipeline)
- [Phagocyte + Generator Dataflow](#phagocyte--generator-dataflow)
- [Agent Evolution Lifecycle](#agent-evolution-lifecycle)
- [Integration Architecture](#integration-architecture)

---

## Overview

This document provides detailed workflow and dataflow diagrams for the **AgentGym** and **AgentFactory** systems within the Phagocyte + Generator pipeline. It illustrates how autonomous agents are created, trained, evolved, and deployed to execute the complete RAG pipeline from research to fine-tuning.

**Key Systems:**
- ğŸ­ **AgentFactory**: Creates and manages specialized agents
- ğŸ‹ï¸ **AgentGym**: Training framework for agent evolution
- ğŸ”¬ **Phagocyte**: RAG pipeline (Research â†’ Parser â†’ Ingestor â†’ Processor)
- ğŸ§¬ **Generator**: Synthetic data generation (QA + CoT + Tool-Use)

---

## Complete System Architecture

### High-Level System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              AGENT FACTORY                                   â”‚
â”‚                         (Central Management System)                          â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Agent Registry â”‚  â”‚ Task Scheduler â”‚  â”‚ Performance    â”‚               â”‚
â”‚  â”‚ - Templates    â”‚  â”‚ - Queue        â”‚  â”‚ Monitor        â”‚               â”‚
â”‚  â”‚ - Instances    â”‚  â”‚ - Priority     â”‚  â”‚ - Metrics      â”‚               â”‚
â”‚  â”‚ - Capabilities â”‚  â”‚ - Dependencies â”‚  â”‚ - Optimization â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    AGENT CREATION & DEPLOYMENT                       â”‚  â”‚
â”‚  â”‚                                                                      â”‚  â”‚
â”‚  â”‚  create_agent() â†’ configure() â†’ train() â†’ evaluate() â†’ deploy()    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             AGENT TEMPLATES                                  â”‚
â”‚                       (Specialized Agent Blueprints)                         â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Research   â”‚  â”‚ Parser     â”‚  â”‚ Ingestor   â”‚  â”‚ Processor  â”‚          â”‚
â”‚  â”‚ Agent      â”‚  â”‚ Agent      â”‚  â”‚ Agent      â”‚  â”‚ Agent      â”‚          â”‚
â”‚  â”‚            â”‚  â”‚            â”‚  â”‚            â”‚  â”‚            â”‚          â”‚
â”‚  â”‚ â€¢ Gemini   â”‚  â”‚ â€¢ DOI      â”‚  â”‚ â€¢ PDFâ†’MD   â”‚  â”‚ â€¢ Chunk    â”‚          â”‚
â”‚  â”‚ â€¢ Citationsâ”‚  â”‚ â€¢ arXiv    â”‚  â”‚ â€¢ Web      â”‚  â”‚ â€¢ Embed    â”‚          â”‚
â”‚  â”‚ â€¢ Reports  â”‚  â”‚ â€¢ Batch    â”‚  â”‚ â€¢ Images   â”‚  â”‚ â€¢ LanceDB  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚         â†“                â†“                â†“                â†“                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚ Generator  â”‚  â”‚ Evaluator  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Agent      â”‚  â”‚ Agent      â”‚         â”‚ Orchestrator   â”‚                â”‚
â”‚  â”‚            â”‚  â”‚            â”‚         â”‚ Agent          â”‚                â”‚
â”‚  â”‚ â€¢ QA Gen   â”‚  â”‚ â€¢ Quality  â”‚         â”‚                â”‚                â”‚
â”‚  â”‚ â€¢ CoT      â”‚  â”‚ â€¢ Metrics  â”‚         â”‚ â€¢ Coordinates  â”‚                â”‚
â”‚  â”‚ â€¢ Tool-Use â”‚  â”‚ â€¢ Feedback â”‚         â”‚ â€¢ Full Pipelineâ”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            AGENTGYM TRAINING                                 â”‚
â”‚                        (Evolution & Improvement)                             â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    4-ENVIRONMENT TRAINING SYSTEM                     â”‚  â”‚
â”‚  â”‚                                                                      â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚
â”‚  â”‚  â”‚ ResearchEnv  â”‚  â”‚ ParserEnv    â”‚  â”‚ IngestorEnv  â”‚             â”‚  â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚             â”‚  â”‚
â”‚  â”‚  â”‚ - Deep mode  â”‚  â”‚ - Batch DL   â”‚  â”‚ - PDFâ†’MD     â”‚             â”‚  â”‚
â”‚  â”‚  â”‚ - Quick mode â”‚  â”‚ - DOI lookup â”‚  â”‚ - Image VLM  â”‚             â”‚  â”‚
â”‚  â”‚  â”‚ - Quality    â”‚  â”‚ - Verify     â”‚  â”‚ - Filter TOC â”‚             â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚  â”‚
â”‚  â”‚                                                                      â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚  â”‚
â”‚  â”‚  â”‚ProcessorEnv  â”‚  â”‚ GeneratorEnv â”‚                                â”‚  â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚                                â”‚  â”‚
â”‚  â”‚  â”‚ - Chunk      â”‚  â”‚ - QA Gen     â”‚                                â”‚  â”‚
â”‚  â”‚  â”‚ - Embed      â”‚  â”‚ - CoT Gen    â”‚                                â”‚  â”‚
â”‚  â”‚  â”‚ - Quality    â”‚  â”‚ - Tool Gen   â”‚                                â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    AGENTEVOL (Evolution Method)                      â”‚  â”‚
â”‚  â”‚                                                                      â”‚  â”‚
â”‚  â”‚  Behavioral Cloning â†’ Exploration â†’ Self-Critique â†’ Improvement     â”‚  â”‚
â”‚  â”‚         â†“                  â†“              â†“              â†“           â”‚  â”‚
â”‚  â”‚   Expert Demos      Try Actions    Analyze Results  Update Policy   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PHAGOCYTE + GENERATOR PIPELINE                       â”‚
â”‚                            (Production Execution)                            â”‚
â”‚                                                                              â”‚
â”‚  Research â†’ Parser â†’ Ingestor â†’ Processor â†’ LanceDB â†’ Generator â†’ Training â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## AgentFactory Workflow

### 1. Agent Creation Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       AGENT FACTORY: CREATE AGENT                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Request:   â”‚
â”‚ "Create parser  â”‚
â”‚  agent"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. SELECT TEMPLATE                                         â”‚
â”‚                                                            â”‚
â”‚  factory.get_template("parser")                           â”‚
â”‚                                                            â”‚
â”‚  Returns: ParserAgent blueprint with:                     â”‚
â”‚  - Action space: [parse_refs, batch_download, verify]    â”‚
â”‚  - Reward function: download_success_rate                â”‚
â”‚  - State encoding: papers_found, errors, status          â”‚
â”‚  - LLM model: gpt-4o-mini                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. INSTANTIATE AGENT                                       â”‚
â”‚                                                            â”‚
â”‚  agent = ParserAgent(                                     â”‚
â”‚      name="parser_v1",                                    â”‚
â”‚      model="gpt-4o-mini",                                 â”‚
â”‚      environment=ParserEnv()                              â”‚
â”‚  )                                                        â”‚
â”‚                                                            â”‚
â”‚  Agent receives:                                          â”‚
â”‚  - Unique ID: agent_parser_20260122_001                  â”‚
â”‚  - Memory buffer: []                                      â”‚
â”‚  - Performance tracker: {}                                â”‚
â”‚  - Strategy library: {}                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. LOAD TRAINING DATA                                      â”‚
â”‚                                                            â”‚
â”‚  trajectories = load_expert_demos("parser")               â”‚
â”‚                                                            â”‚
â”‚  Loads: 100 expert parser execution trajectories          â”‚
â”‚  Format: (instruction, thought, action, observation,       â”‚
â”‚           reward, success)                                â”‚
â”‚                                                            â”‚
â”‚  Example trajectory:                                      â”‚
â”‚  {                                                        â”‚
â”‚    "instruction": "Download papers from research report", â”‚
â”‚    "thought": "Extract references then batch download",   â”‚
â”‚    "action": "parse refs report.md --export-batch",       â”‚
â”‚    "observation": "Extracted 45 refs, created batch.json",â”‚
â”‚    "reward": 0.95,                                        â”‚
â”‚    "success": true                                        â”‚
â”‚  }                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. BEHAVIORAL CLONING (Initial Training)                  â”‚
â”‚                                                            â”‚
â”‚  agent.train(                                             â”‚
â”‚      trajectories=expert_demos,                           â”‚
â”‚      method="behavioral_cloning",                         â”‚
â”‚      epochs=10                                            â”‚
â”‚  )                                                        â”‚
â”‚                                                            â”‚
â”‚  Training process:                                        â”‚
â”‚  â€¢ Agent learns to imitate expert actions                â”‚
â”‚  â€¢ Learns: situation â†’ action mappings                   â”‚
â”‚  â€¢ Builds initial policy network                         â”‚
â”‚  â€¢ Validation accuracy: 87%                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. REGISTER AGENT                                          â”‚
â”‚                                                            â”‚
â”‚  factory.register(agent)                                  â”‚
â”‚                                                            â”‚
â”‚  Agent added to:                                          â”‚
â”‚  â€¢ Active agent pool                                      â”‚
â”‚  â€¢ Capability matrix                                      â”‚
â”‚  â€¢ Task assignment queue                                  â”‚
â”‚  â€¢ Performance monitoring dashboard                       â”‚
â”‚                                                            â”‚
â”‚  Status: READY FOR DEPLOYMENT                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… Agent Ready  â”‚
â”‚ for tasks       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Agent Task Execution Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     AGENT FACTORY: TASK EXECUTION                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Task Request:   â”‚
â”‚ "Parse research â”‚
â”‚  report and     â”‚
â”‚  download papersâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. TASK ROUTING                                            â”‚
â”‚                                                            â”‚
â”‚  factory.assign_task(                                     â”‚
â”‚      task_type="parser",                                  â”‚
â”‚      description="Extract and download papers",           â”‚
â”‚      input_file="research_report.md"                      â”‚
â”‚  )                                                        â”‚
â”‚                                                            â”‚
â”‚  Factory analyzes:                                        â”‚
â”‚  â€¢ Task type: PARSER                                      â”‚
â”‚  â€¢ Required capabilities: ref_extraction, batch_download â”‚
â”‚  â€¢ Priority: HIGH                                         â”‚
â”‚  â€¢ Dependencies: None                                     â”‚
â”‚                                                            â”‚
â”‚  Selects: parser_v1 (best performance: 92%)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. AGENT EXECUTION LOOP                                    â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Step 1: OBSERVE                              â”‚        â”‚
â”‚  â”‚                                              â”‚        â”‚
â”‚  â”‚ observation = env.reset(task)                â”‚        â”‚
â”‚  â”‚ â†’ "File: research_report.md, 45 citations"  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                   â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Step 2: THINK                                â”‚        â”‚
â”‚  â”‚                                              â”‚        â”‚
â”‚  â”‚ thought = agent.think(observation)           â”‚        â”‚
â”‚  â”‚ â†’ "I should extract references first,        â”‚        â”‚
â”‚  â”‚    then batch download with concurrency=3    â”‚        â”‚
â”‚  â”‚    to avoid rate limits"                     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                   â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Step 3: ACT                                  â”‚        â”‚
â”‚  â”‚                                              â”‚        â”‚
â”‚  â”‚ action = agent.act(thought, available_actions)â”‚       â”‚
â”‚  â”‚ â†’ "parse refs research_report.md             â”‚        â”‚
â”‚  â”‚     --export-batch"                          â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                   â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Step 4: EXECUTE                              â”‚        â”‚
â”‚  â”‚                                              â”‚        â”‚
â”‚  â”‚ result = env.step(action)                    â”‚        â”‚
â”‚  â”‚ â†’ "Extracted 45 refs, created batch.json"    â”‚        â”‚
â”‚  â”‚ â†’ reward = 0.95                              â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                   â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Step 5: REMEMBER                             â”‚        â”‚
â”‚  â”‚                                              â”‚        â”‚
â”‚  â”‚ agent.remember({                             â”‚        â”‚
â”‚  â”‚   observation, thought, action, reward       â”‚        â”‚
â”‚  â”‚ })                                           â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                   â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Step 6: CONTINUE OR FINISH                   â”‚        â”‚
â”‚  â”‚                                              â”‚        â”‚
â”‚  â”‚ if done: break                               â”‚        â”‚
â”‚  â”‚ else: go to Step 1                           â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. TASK COMPLETION                                         â”‚
â”‚                                                            â”‚
â”‚  Results:                                                 â”‚
â”‚  â€¢ Status: SUCCESS                                        â”‚
â”‚  â€¢ Papers extracted: 45                                   â”‚
â”‚  â€¢ Batch file created: batch.json                        â”‚
â”‚  â€¢ Reward: 0.95                                          â”‚
â”‚  â€¢ Time: 8.3s                                            â”‚
â”‚                                                            â”‚
â”‚  factory.log_task_completion(agent_id, task_id, metrics) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… Task Done    â”‚
â”‚ Results logged  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Multi-Agent Coordination Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  AGENT FACTORY: PIPELINE ORCHESTRATION               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Request:   â”‚
â”‚ "Execute full   â”‚
â”‚  Phagocyte      â”‚
â”‚  pipeline for   â”‚
â”‚  HDF5 topic"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ORCHESTRATOR AGENT TAKES CONTROL                                    â”‚
â”‚                                                                     â”‚
â”‚  orchestrator = factory.get_agent("orchestrator")                  â”‚
â”‚  orchestrator.execute_pipeline(topic="HDF5 best practices")       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: RESEARCH                                                   â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ orchestrator.delegate_task(                  â”‚                 â”‚
â”‚  â”‚   agent_type="research",                     â”‚                 â”‚
â”‚  â”‚   task="Research HDF5 best practices",       â”‚                 â”‚
â”‚  â”‚   mode="directed"                            â”‚                 â”‚
â”‚  â”‚ )                                            â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                        â†“                                            â”‚
â”‚  research_agent executes:                                          â”‚
â”‚  â€¢ Queries Gemini with HDF5 queries                               â”‚
â”‚  â€¢ Gathers 45 citations                                           â”‚
â”‚  â€¢ Generates research report                                      â”‚
â”‚  â€¢ Returns: research_report.md                                    â”‚
â”‚                                                                     â”‚
â”‚  Status: âœ… SUCCESS (reward: 0.92)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: PARSE & DOWNLOAD                                          â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ orchestrator.delegate_task(                  â”‚                 â”‚
â”‚  â”‚   agent_type="parser",                       â”‚                 â”‚
â”‚  â”‚   task="Extract refs and download papers",   â”‚                 â”‚
â”‚  â”‚   input="research_report.md"                 â”‚                 â”‚
â”‚  â”‚ )                                            â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                        â†“                                            â”‚
â”‚  parser_agent executes:                                            â”‚
â”‚  â€¢ Extracts 45 references â†’ batch.json                            â”‚
â”‚  â€¢ Downloads papers with concurrency=3                            â”‚
â”‚  â€¢ Success: 38/42 papers (4 paywalled)                           â”‚
â”‚  â€¢ Returns: ./papers/ directory                                   â”‚
â”‚                                                                     â”‚
â”‚  Status: âœ… PARTIAL SUCCESS (reward: 0.85)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: INGEST & CONVERT                                          â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ orchestrator.delegate_task(                  â”‚                 â”‚
â”‚  â”‚   agent_type="ingestor",                     â”‚                 â”‚
â”‚  â”‚   task="Convert PDFs to markdown with VLM",  â”‚                 â”‚
â”‚  â”‚   input_dir="./papers/",                     â”‚                 â”‚
â”‚  â”‚   options=["--describe-images"]              â”‚                 â”‚
â”‚  â”‚ )                                            â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                        â†“                                            â”‚
â”‚  ingestor_agent executes:                                          â”‚
â”‚  â€¢ Converts 38 PDFs â†’ Markdown                                    â”‚
â”‚  â€¢ Describes 127 figures with VLM                                 â”‚
â”‚  â€¢ Filters out TOC pages                                          â”‚
â”‚  â€¢ Returns: ./markdown/ directory                                 â”‚
â”‚                                                                     â”‚
â”‚  Status: âœ… SUCCESS (reward: 0.95)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: PROCESS & EMBED                                           â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ orchestrator.delegate_task(                  â”‚                 â”‚
â”‚  â”‚   agent_type="processor",                    â”‚                 â”‚
â”‚  â”‚   task="Chunk and embed for RAG",            â”‚                 â”‚
â”‚  â”‚   input_dir="./markdown/",                   â”‚                 â”‚
â”‚  â”‚   profile="medium"                           â”‚                 â”‚
â”‚  â”‚ )                                            â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                        â†“                                            â”‚
â”‚  processor_agent executes:                                         â”‚
â”‚  â€¢ Creates 7,690 text chunks                                      â”‚
â”‚  â€¢ Creates 446 code chunks                                        â”‚
â”‚  â€¢ Creates 135 image chunks                                       â”‚
â”‚  â€¢ Embeds with BAAI/bge-base-en-v1.5                            â”‚
â”‚  â€¢ Stores in LanceDB                                              â”‚
â”‚  â€¢ Returns: database_path                                         â”‚
â”‚                                                                     â”‚
â”‚  Status: âœ… SUCCESS (reward: 0.98)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 5: GENERATE TRAINING DATA                                    â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ orchestrator.delegate_task(                  â”‚                 â”‚
â”‚  â”‚   agent_type="generator",                    â”‚                 â”‚
â”‚  â”‚   task="Generate QA + CoT training data",    â”‚                 â”‚
â”‚  â”‚   input_db="./lancedb/",                     â”‚                 â”‚
â”‚  â”‚   n_pairs=500                                â”‚                 â”‚
â”‚  â”‚ )                                            â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                        â†“                                            â”‚
â”‚  generator_agent executes:                                         â”‚
â”‚  â€¢ Generates 500 QA pairs (Instruction Backtranslation)          â”‚
â”‚  â€¢ Enhances 500 with CoT reasoning                                â”‚
â”‚  â€¢ Curates with LLM-as-Judge (keeps 350)                         â”‚
â”‚  â€¢ Formats as JSONL for fine-tuning                              â”‚
â”‚  â€¢ Returns: training_data.jsonl                                   â”‚
â”‚                                                                     â”‚
â”‚  Status: âœ… SUCCESS (reward: 0.94)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PIPELINE COMPLETION                                                 â”‚
â”‚                                                                     â”‚
â”‚  orchestrator.summarize_results()                                  â”‚
â”‚                                                                     â”‚
â”‚  Results:                                                          â”‚
â”‚  âœ… Research: 45 citations, quality 0.92                          â”‚
â”‚  âœ… Parser: 38/42 papers (90.5% success)                          â”‚
â”‚  âœ… Ingestor: 38 markdowns, 127 image descriptions                â”‚
â”‚  âœ… Processor: 8,271 total chunks in LanceDB                      â”‚
â”‚  âœ… Generator: 350 high-quality training examples                 â”‚
â”‚                                                                     â”‚
â”‚  Average reward: 0.928                                             â”‚
â”‚  Total time: 1,847 seconds (~31 minutes)                          â”‚
â”‚  Success rate: 100%                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… Pipeline     â”‚
â”‚ Complete!       â”‚
â”‚ Ready for       â”‚
â”‚ fine-tuning     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## AgentGym Training Pipeline

### 1. Training Data Collection Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AGENTGYM: TRAJECTORY COLLECTION                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SOURCE 1: EXPERT DEMONSTRATIONS                                     â”‚
â”‚                                                                     â”‚
â”‚  Human experts execute Phagocyte pipeline manually                 â”‚
â”‚  All commands, decisions, and outcomes are logged                  â”‚
â”‚                                                                     â”‚
â”‚  Example logged trajectory:                                        â”‚
â”‚  {                                                                 â”‚
â”‚    "trajectory_id": "expert_001",                                  â”‚
â”‚    "expert": "human_researcher",                                   â”‚
â”‚    "environment": "processor",                                     â”‚
â”‚    "steps": [                                                      â”‚
â”‚      {                                                             â”‚
â”‚        "observation": "1000 markdown files in ./docs/",            â”‚
â”‚        "thought": "Large dataset, use medium quality for balance", â”‚
â”‚        "action": "phagocyte process run ./docs/                    â”‚
â”‚                   --text-profile medium",                          â”‚
â”‚        "result": "7690 chunks created, search quality: 0.89",      â”‚
â”‚        "reward": 0.89,                                             â”‚
â”‚        "timestamp": "2026-01-15T10:23:45"                          â”‚
â”‚      }                                                             â”‚
â”‚    ],                                                              â”‚
â”‚    "success": true,                                                â”‚
â”‚    "total_reward": 0.89                                            â”‚
â”‚  }                                                                 â”‚
â”‚                                                                     â”‚
â”‚  Collected: 500 expert trajectories across 4 environments          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SOURCE 2: AUTOMATED LOGGING                                         â”‚
â”‚                                                                     â”‚
â”‚  Phagocyte CLI automatically logs all executions                   â”‚
â”‚  MCP servers record tool usage patterns                            â”‚
â”‚                                                                     â”‚
â”‚  Logged data includes:                                             â”‚
â”‚  â€¢ Command executed                                                â”‚
â”‚  â€¢ Parameters used                                                 â”‚
â”‚  â€¢ Execution time                                                  â”‚
â”‚  â€¢ Success/failure                                                 â”‚
â”‚  â€¢ Error messages                                                  â”‚
â”‚  â€¢ Output quality metrics                                          â”‚
â”‚                                                                     â”‚
â”‚  Collected: 2,000 automated execution logs                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SOURCE 3: SYNTHETIC EXPLORATION                                     â”‚
â”‚                                                                     â”‚
â”‚  Base agents (trained on experts) explore environments             â”‚
â”‚  Try different action combinations                                 â”‚
â”‚  Learn from successes and failures                                 â”‚
â”‚                                                                     â”‚
â”‚  Exploration strategies:                                           â”‚
â”‚  â€¢ Random action selection (10%)                                   â”‚
â”‚  â€¢ Epsilon-greedy (80%)                                            â”‚
â”‚  â€¢ Curiosity-driven (10%)                                          â”‚
â”‚                                                                     â”‚
â”‚  Collected: 5,000 exploration trajectories                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TRAJECTORY DATASET                                                  â”‚
â”‚                                                                     â”‚
â”‚  Total trajectories: 7,500                                         â”‚
â”‚  â€¢ Expert: 500 (high quality)                                      â”‚
â”‚  â€¢ Automated: 2,000 (real usage patterns)                          â”‚
â”‚  â€¢ Exploration: 5,000 (diverse experiences)                        â”‚
â”‚                                                                     â”‚
â”‚  Split:                                                            â”‚
â”‚  â€¢ Training: 6,000 (80%)                                           â”‚
â”‚  â€¢ Validation: 750 (10%)                                           â”‚
â”‚  â€¢ Test: 750 (10%)                                                 â”‚
â”‚                                                                     â”‚
â”‚  Storage: trajectories.jsonl (450 MB)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… Dataset      â”‚
â”‚ Ready for       â”‚
â”‚ training        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. AgentEvol Training Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENTGYM: AGENTEVOL TRAINING                      â”‚
â”‚                  (3-Stage Self-Improvement System)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: BEHAVIORAL CLONING (Imitation Learning)                   â”‚
â”‚                                                                     â”‚
â”‚  Goal: Agent learns to imitate expert demonstrations               â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Training Process:                           â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚ for trajectory in expert_trajectories:      â”‚                  â”‚
â”‚  â”‚     for step in trajectory:                 â”‚                  â”‚
â”‚  â”‚         observation = step.observation      â”‚                  â”‚
â”‚  â”‚         expert_action = step.action         â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚         # Predict action                    â”‚                  â”‚
â”‚  â”‚         predicted_action = agent(observation)â”‚                 â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚         # Calculate loss                    â”‚                  â”‚
â”‚  â”‚         loss = cross_entropy(               â”‚                  â”‚
â”‚  â”‚             predicted_action,               â”‚                  â”‚
â”‚  â”‚             expert_action                   â”‚                  â”‚
â”‚  â”‚         )                                   â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚         # Update agent                      â”‚                  â”‚
â”‚  â”‚         agent.update(loss)                  â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                     â”‚
â”‚  Training metrics:                                                 â”‚
â”‚  â€¢ Epochs: 10                                                      â”‚
â”‚  â€¢ Training accuracy: 87%                                          â”‚
â”‚  â€¢ Validation accuracy: 82%                                        â”‚
â”‚  â€¢ Time: 3.5 hours                                                 â”‚
â”‚                                                                     â”‚
â”‚  Result: Agent can execute basic Phagocyte operations              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: ENVIRONMENTAL EXPLORATION                                  â”‚
â”‚                                                                     â”‚
â”‚  Goal: Agent explores environments to discover new strategies       â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Exploration Loop:                           â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚ for episode in range(1000):                 â”‚                  â”‚
â”‚  â”‚     env = random_choice(environments)       â”‚                  â”‚
â”‚  â”‚     observation = env.reset(random_task())  â”‚                  â”‚
â”‚  â”‚     done = False                            â”‚                  â”‚
â”‚  â”‚     trajectory = []                         â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚     while not done:                         â”‚                  â”‚
â”‚  â”‚         # Agent decides action              â”‚                  â”‚
â”‚  â”‚         thought = agent.think(observation)  â”‚                  â”‚
â”‚  â”‚         action = agent.act(thought)         â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚         # Execute in environment            â”‚                  â”‚
â”‚  â”‚         result = env.step(action)           â”‚                  â”‚
â”‚  â”‚         observation = result.observation    â”‚                  â”‚
â”‚  â”‚         reward = result.reward              â”‚                  â”‚
â”‚  â”‚         done = result.done                  â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚         # Record experience                 â”‚                  â”‚
â”‚  â”‚         trajectory.append({                 â”‚                  â”‚
â”‚  â”‚             thought, action, observation,   â”‚                  â”‚
â”‚  â”‚             reward                          â”‚                  â”‚
â”‚  â”‚         })                                  â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚     # Store trajectory                      â”‚                  â”‚
â”‚  â”‚     if sum(rewards) > threshold:            â”‚                  â”‚
â”‚  â”‚         successful_trajectories.append(     â”‚                  â”‚
â”‚  â”‚             trajectory                      â”‚                  â”‚
â”‚  â”‚         )                                   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                     â”‚
â”‚  Exploration statistics:                                           â”‚
â”‚  â€¢ Episodes: 1,000                                                 â”‚
â”‚  â€¢ Success rate: 68%                                               â”‚
â”‚  â€¢ New strategies discovered: 23                                   â”‚
â”‚  â€¢ Time: 12 hours                                                  â”‚
â”‚                                                                     â”‚
â”‚  Result: Agent discovers optimal strategies beyond expert demos    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: SELF-CRITIQUE & IMPROVEMENT                               â”‚
â”‚                                                                     â”‚
â”‚  Goal: Agent analyzes its performance and improves                 â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Self-Improvement Loop:                      â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚ # Analyze performance                       â”‚                  â”‚
â”‚  â”‚ failures = [t for t in trajectories         â”‚                  â”‚
â”‚  â”‚             if t.reward < 0.5]              â”‚                  â”‚
â”‚  â”‚ successes = [t for t in trajectories        â”‚                  â”‚
â”‚  â”‚              if t.reward > 0.8]             â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚ # Identify patterns                         â”‚                  â”‚
â”‚  â”‚ for failure in failures:                    â”‚                  â”‚
â”‚  â”‚     # What went wrong?                      â”‚                  â”‚
â”‚  â”‚     critique = agent.self_critique(failure) â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚     # Find similar successful case          â”‚                  â”‚
â”‚  â”‚     similar_success = find_similar(         â”‚                  â”‚
â”‚  â”‚         failure, successes                  â”‚                  â”‚
â”‚  â”‚     )                                       â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚     # Learn the difference                  â”‚                  â”‚
â”‚  â”‚     improvement = contrast(                 â”‚                  â”‚
â”‚  â”‚         failure, similar_success            â”‚                  â”‚
â”‚  â”‚     )                                       â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚     # Update policy                         â”‚                  â”‚
â”‚  â”‚     agent.update_policy(improvement)        â”‚                  â”‚
â”‚  â”‚                                             â”‚                  â”‚
â”‚  â”‚ # Validate improvements                     â”‚                  â”‚
â”‚  â”‚ new_success_rate = evaluate(agent)          â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                     â”‚
â”‚  Self-critique examples:                                           â”‚
â”‚                                                                     â”‚
â”‚  Failure:                                                          â”‚
â”‚  "Used --text-profile high on 10,000 files â†’ timeout"             â”‚
â”‚  Critique:                                                         â”‚
â”‚  "Should use lower profile for large datasets"                    â”‚
â”‚  Improvement:                                                      â”‚
â”‚  "If file_count > 5000, use medium profile"                       â”‚
â”‚                                                                     â”‚
â”‚  Improvement metrics:                                              â”‚
â”‚  â€¢ Success rate: 68% â†’ 84%                                         â”‚
â”‚  â€¢ Average reward: 0.71 â†’ 0.87                                     â”‚
â”‚  â€¢ Error rate: 32% â†’ 16%                                           â”‚
â”‚  â€¢ Time: 8 hours                                                   â”‚
â”‚                                                                     â”‚
â”‚  Result: Agent significantly outperforms initial behavior          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FINAL EVALUATION                                                    â”‚
â”‚                                                                     â”‚
â”‚  Benchmark tasks: 100 held-out test scenarios                      â”‚
â”‚                                                                     â”‚
â”‚  Performance comparison:                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Agent Version   â”‚ Success â”‚ Avg Time â”‚ Quality  â”‚             â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚
â”‚  â”‚ Baseline (BC)   â”‚ 72%     â”‚ 145s     â”‚ 0.78     â”‚             â”‚
â”‚  â”‚ After Explore   â”‚ 84%     â”‚ 128s     â”‚ 0.87     â”‚             â”‚
â”‚  â”‚ After Critique  â”‚ 92%     â”‚ 118s     â”‚ 0.93     â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                     â”‚
â”‚  Key improvements:                                                 â”‚
â”‚  âœ… 20% increase in success rate                                   â”‚
â”‚  âœ… 19% faster execution                                           â”‚
â”‚  âœ… 19% higher quality outputs                                     â”‚
â”‚  âœ… Better error handling                                          â”‚
â”‚  âœ… Adaptive strategy selection                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… Agent Evolvedâ”‚
â”‚ Ready for       â”‚
â”‚ deployment      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Multi-Environment Training Schedule

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                AGENTGYM: CONCURRENT TRAINING SCHEDULE                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Week 1-2: Initial Training (Behavioral Cloning)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ResearchEnv        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  (67% complete)   â”‚
â”‚ ParserEnv          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  (80% complete)   â”‚
â”‚ IngestorEnv        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  (50% complete)   â”‚
â”‚ ProcessorEnv       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (100% complete)  â”‚
â”‚ GeneratorEnv       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  (40% complete)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Week 3-6: Environmental Exploration
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Day 1: ResearchEnv      â†’ 50 episodes                      â”‚
â”‚ Day 2: ParserEnv        â†’ 50 episodes                      â”‚
â”‚ Day 3: IngestorEnv      â†’ 50 episodes                      â”‚
â”‚ Day 4: ProcessorEnv     â†’ 50 episodes                      â”‚
â”‚ Day 5: GeneratorEnv     â†’ 50 episodes                      â”‚
â”‚ Day 6: Integration      â†’ 20 full pipeline runs            â”‚
â”‚ Day 7: Analysis & Rest                                     â”‚
â”‚                                                            â”‚
â”‚ Repeat for 4 weeks â†’ Total: 1,000 episodes per env        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Week 7-8: Self-Critique & Improvement
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analyze failures      â†’ Identify patterns                  â”‚
â”‚ Compare with successes â†’ Learn improvements                â”‚
â”‚ Update policies       â†’ Refine strategies                  â”‚
â”‚ Re-evaluate          â†’ Measure improvements                â”‚
â”‚ Fine-tune            â†’ Polish performance                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Week 9-10: Cross-Environment Transfer Learning
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Train on Environment A                                     â”‚
â”‚ â†“                                                          â”‚
â”‚ Evaluate on Environment B (zero-shot transfer)             â”‚
â”‚ â†“                                                          â”‚
â”‚ Fine-tune on Environment B                                 â”‚
â”‚ â†“                                                          â”‚
â”‚ Result: Generalist agent across all environments          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Final Result: 5 specialized agents + 1 generalist orchestrator
```

---

## Phagocyte + Generator Dataflow

### Complete Data Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PHAGOCYTE + GENERATOR: COMPLETE DATAFLOW                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: Research Topic
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODULE 1: RESEARCHER                                          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ INPUT:  Topic string (e.g., "HDF5 best practices")     â”‚  â”‚
â”‚ â”‚ AGENT:  ResearchAgent (trained on ResearchEnv)         â”‚  â”‚
â”‚ â”‚ PROCESS:                                                â”‚  â”‚
â”‚ â”‚   1. Formulate search queries                           â”‚  â”‚
â”‚ â”‚   2. Query Gemini API                                   â”‚  â”‚
â”‚ â”‚   3. Extract citations                                  â”‚  â”‚
â”‚ â”‚   4. Generate research report                           â”‚  â”‚
â”‚ â”‚ OUTPUT: research_report.md + citations.json             â”‚  â”‚
â”‚ â”‚ METRICS:                                                â”‚  â”‚
â”‚ â”‚   - Citations found: 45                                 â”‚  â”‚
â”‚ â”‚   - Quality score: 0.92                                 â”‚  â”‚
â”‚ â”‚   - Time: 87s                                           â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODULE 2: PARSER                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ INPUT:  research_report.md                              â”‚  â”‚
â”‚ â”‚ AGENT:  ParserAgent (trained on ParserEnv)             â”‚  â”‚
â”‚ â”‚ PROCESS:                                                â”‚  â”‚
â”‚ â”‚   1. Extract references from report                     â”‚  â”‚
â”‚ â”‚      â†’ regex patterns, DOI detection                    â”‚  â”‚
â”‚ â”‚   2. Validate against CrossRef/arXiv                    â”‚  â”‚
â”‚ â”‚   3. Create batch download file                         â”‚  â”‚
â”‚ â”‚   4. Download papers (concurrent=3)                     â”‚  â”‚
â”‚ â”‚      â†’ DOI resolver, arXiv API, PDF downloads           â”‚  â”‚
â”‚ â”‚ OUTPUT: papers/ (38 PDFs)                               â”‚  â”‚
â”‚ â”‚ METRICS:                                                â”‚  â”‚
â”‚ â”‚   - Refs extracted: 45                                  â”‚  â”‚
â”‚ â”‚   - Papers downloaded: 38/42 (90.5%)                    â”‚  â”‚
â”‚ â”‚   - Failed (paywall): 4                                 â”‚  â”‚
â”‚ â”‚   - Time: 245s                                          â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODULE 3: INGESTOR                                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ INPUT:  papers/ (38 PDFs)                               â”‚  â”‚
â”‚ â”‚ AGENT:  IngestorAgent (trained on IngestorEnv)         â”‚  â”‚
â”‚ â”‚ PROCESS:                                                â”‚  â”‚
â”‚ â”‚   1. Convert PDF â†’ Markdown (PyMuPDF)                   â”‚  â”‚
â”‚ â”‚      â†’ Extract text, tables, equations                  â”‚  â”‚
â”‚ â”‚   2. Extract images from PDFs                           â”‚  â”‚
â”‚ â”‚      â†’ Save as PNG files                                â”‚  â”‚
â”‚ â”‚   3. Generate image descriptions (VLM)                  â”‚  â”‚
â”‚ â”‚      â†’ Gemini Vision API                                â”‚  â”‚
â”‚ â”‚   4. Filter TOC pages (heuristics)                      â”‚  â”‚
â”‚ â”‚      â†’ Detect page numbers, "Contents" headers          â”‚  â”‚
â”‚ â”‚   5. Clean markdown formatting                          â”‚  â”‚
â”‚ â”‚ OUTPUT: markdown/ (38 .md files + 127 images)           â”‚  â”‚
â”‚ â”‚ METRICS:                                                â”‚  â”‚
â”‚ â”‚   - PDFs converted: 38                                  â”‚  â”‚
â”‚ â”‚   - Images described: 127                               â”‚  â”‚
â”‚ â”‚   - TOC pages filtered: 76                              â”‚  â”‚
â”‚ â”‚   - Avg quality: 0.95                                   â”‚  â”‚
â”‚ â”‚   - Time: 412s                                          â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODULE 4: PROCESSOR                                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ INPUT:  markdown/ (38 .md files)                        â”‚  â”‚
â”‚ â”‚ AGENT:  ProcessorAgent (trained on ProcessorEnv)       â”‚  â”‚
â”‚ â”‚ PROCESS:                                                â”‚  â”‚
â”‚ â”‚   1. Chunk text (semantic + AST-aware)                  â”‚  â”‚
â”‚ â”‚      â†’ LangChain splitters, AST for code                â”‚  â”‚
â”‚ â”‚      â€¢ Text: RecursiveCharacterTextSplitter             â”‚  â”‚
â”‚ â”‚      â€¢ Code: AST-aware Python/Java splitters            â”‚  â”‚
â”‚ â”‚      â€¢ Images: Individual records                       â”‚  â”‚
â”‚ â”‚   2. Generate embeddings                                â”‚  â”‚
â”‚ â”‚      â†’ BAAI/bge-base-en-v1.5 (768 dims)                 â”‚  â”‚
â”‚ â”‚      â†’ Batch size: 32, workers: 4                       â”‚  â”‚
â”‚ â”‚   3. Store in LanceDB                                   â”‚  â”‚
â”‚ â”‚      â†’ 3 tables: text_chunks_clean, code_chunks, images â”‚  â”‚
â”‚ â”‚   4. Build vector index (IVF-PQ)                        â”‚  â”‚
â”‚ â”‚ OUTPUT: lancedb/ database                               â”‚  â”‚
â”‚ â”‚ DATA STRUCTURE:                                         â”‚  â”‚
â”‚ â”‚   Table: text_chunks_clean (7,690 rows)                 â”‚  â”‚
â”‚ â”‚     - id: str                                           â”‚  â”‚
â”‚ â”‚     - text: str                                         â”‚  â”‚
â”‚ â”‚     - embedding: vector[768]                            â”‚  â”‚
â”‚ â”‚     - metadata: {file, chunk_idx, token_count}          â”‚  â”‚
â”‚ â”‚   Table: code_chunks (446 rows)                         â”‚  â”‚
â”‚ â”‚     - id: str                                           â”‚  â”‚
â”‚ â”‚     - code: str                                         â”‚  â”‚
â”‚ â”‚     - language: str                                     â”‚  â”‚
â”‚ â”‚     - embedding: vector[768]                            â”‚  â”‚
â”‚ â”‚   Table: images (135 rows)                              â”‚  â”‚
â”‚ â”‚     - id: str                                           â”‚  â”‚
â”‚ â”‚     - image_path: str                                   â”‚  â”‚
â”‚ â”‚     - description: str (from VLM)                       â”‚  â”‚
â”‚ â”‚     - embedding: vector[768]                            â”‚  â”‚
â”‚ â”‚ METRICS:                                                â”‚  â”‚
â”‚ â”‚   - Text chunks: 7,690                                  â”‚  â”‚
â”‚ â”‚   - Code chunks: 446                                    â”‚  â”‚
â”‚ â”‚   - Image chunks: 135                                   â”‚  â”‚
â”‚ â”‚   - Total: 8,271 chunks                                 â”‚  â”‚
â”‚ â”‚   - Database size: 1.2 GB                               â”‚  â”‚
â”‚ â”‚   - Avg search latency: 23ms                            â”‚  â”‚
â”‚ â”‚   - Time: 628s                                          â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODULE 5: GENERATOR (QA Pipeline)                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ INPUT:  lancedb/ (8,271 chunks)                         â”‚  â”‚
â”‚ â”‚ AGENT:  GeneratorAgent (trained on GeneratorEnv)       â”‚  â”‚
â”‚ â”‚ PROCESS:                                                â”‚  â”‚
â”‚ â”‚   1. Sample chunks from LanceDB                         â”‚  â”‚
â”‚ â”‚      â†’ Random sampling: 500 text chunks                 â”‚  â”‚
â”‚ â”‚      â†’ Stratified by file and topic                     â”‚  â”‚
â”‚ â”‚   2. Generate QA pairs (Instruction Backtranslation)    â”‚  â”‚
â”‚ â”‚      â†’ LLM: Ollama granite4:latest                      â”‚  â”‚
â”‚ â”‚      â†’ Method: Given answer (chunk), generate question  â”‚  â”‚
â”‚ â”‚      â†’ Parallel workers: 4                              â”‚  â”‚
â”‚ â”‚      â†’ Prompt:                                          â”‚  â”‚
â”‚ â”‚        "Given this document excerpt, generate a         â”‚  â”‚
â”‚ â”‚         question that this excerpt answers..."          â”‚  â”‚
â”‚ â”‚   3. Format output                                      â”‚  â”‚
â”‚ â”‚      â†’ JSONL format for fine-tuning                     â”‚  â”‚
â”‚ â”‚ OUTPUT: qa_pairs.jsonl (500 examples)                   â”‚  â”‚
â”‚ â”‚ SAMPLE OUTPUT:                                          â”‚  â”‚
â”‚ â”‚   {                                                     â”‚  â”‚
â”‚ â”‚     "question": "What is the purpose of H5Fcreate?",    â”‚  â”‚
â”‚ â”‚     "answer": "H5Fcreate creates a new HDF5 file...",   â”‚  â”‚
â”‚ â”‚     "source_chunk_id": "chunk_1234",                    â”‚  â”‚
â”‚ â”‚     "metadata": {                                       â”‚  â”‚
â”‚ â”‚       "file": "hdf5_tutorial.md",                       â”‚  â”‚
â”‚ â”‚       "chunk_idx": 42                                   â”‚  â”‚
â”‚ â”‚     }                                                   â”‚  â”‚
â”‚ â”‚   }                                                     â”‚  â”‚
â”‚ â”‚ METRICS:                                                â”‚  â”‚
â”‚ â”‚   - Chunks processed: 500                               â”‚  â”‚
â”‚ â”‚   - QA pairs generated: 500                             â”‚  â”‚
â”‚ â”‚   - Avg question length: 18 tokens                      â”‚  â”‚
â”‚ â”‚   - Avg answer length: 87 tokens                        â”‚  â”‚
â”‚ â”‚   - Time: 342s                                          â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODULE 6: GENERATOR (CoT Pipeline)                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ INPUT:  qa_pairs.jsonl (500 examples)                   â”‚  â”‚
â”‚ â”‚ AGENT:  GeneratorAgent (CoT mode)                       â”‚  â”‚
â”‚ â”‚ PROCESS:                                                â”‚  â”‚
â”‚ â”‚   1. Load QA pairs                                      â”‚  â”‚
â”‚ â”‚   2. Enhance with CoT reasoning                         â”‚  â”‚
â”‚ â”‚      â†’ LLM: Ollama granite4:latest                      â”‚  â”‚
â”‚ â”‚      â†’ Method: Add step-by-step reasoning               â”‚  â”‚
â”‚ â”‚      â†’ Prompt:                                          â”‚  â”‚
â”‚ â”‚        "Add step-by-step reasoning to answer            â”‚  â”‚
â”‚ â”‚         this question..."                               â”‚  â”‚
â”‚ â”‚   3. Format with reasoning                              â”‚  â”‚
â”‚ â”‚ OUTPUT: qa_pairs_cot.jsonl (500 examples)               â”‚  â”‚
â”‚ â”‚ SAMPLE OUTPUT:                                          â”‚  â”‚
â”‚ â”‚   {                                                     â”‚  â”‚
â”‚ â”‚     "question": "What is the purpose of H5Fcreate?",    â”‚  â”‚
â”‚ â”‚     "reasoning": [                                      â”‚  â”‚
â”‚ â”‚       "1. H5Fcreate is an HDF5 API function",           â”‚  â”‚
â”‚ â”‚       "2. It operates on HDF5 files",                   â”‚  â”‚
â”‚ â”‚       "3. The 'create' suggests file creation",         â”‚  â”‚
â”‚ â”‚       "4. It likely initializes a new file"             â”‚  â”‚
â”‚ â”‚     ],                                                  â”‚  â”‚
â”‚ â”‚     "answer": "H5Fcreate creates a new HDF5 file...",   â”‚  â”‚
â”‚ â”‚     "cot_method": "enhancement"                         â”‚  â”‚
â”‚ â”‚   }                                                     â”‚  â”‚
â”‚ â”‚ METRICS:                                                â”‚  â”‚
â”‚ â”‚   - QA pairs enhanced: 500                              â”‚  â”‚
â”‚ â”‚   - Avg reasoning steps: 4.2                            â”‚  â”‚
â”‚ â”‚   - Avg reasoning length: 156 tokens                    â”‚  â”‚
â”‚ â”‚   - Time: 487s                                          â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODULE 7: GENERATOR (Curation with LLM-as-Judge)             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ INPUT:  qa_pairs_cot.jsonl (500 examples)               â”‚  â”‚
â”‚ â”‚ AGENT:  EvaluatorAgent                                  â”‚  â”‚
â”‚ â”‚ PROCESS:                                                â”‚  â”‚
â”‚ â”‚   1. Load enhanced QA pairs                             â”‚  â”‚
â”‚ â”‚   2. Score each example (DEITA-style)                   â”‚  â”‚
â”‚ â”‚      â†’ LLM: Claude Sonnet 3.5                           â”‚  â”‚
â”‚ â”‚      â†’ Dimensions:                                      â”‚  â”‚
â”‚ â”‚        â€¢ Accuracy (0-10)                                â”‚  â”‚
â”‚ â”‚        â€¢ Clarity (0-10)                                 â”‚  â”‚
â”‚ â”‚        â€¢ Completeness (0-10)                            â”‚  â”‚
â”‚ â”‚      â†’ Overall score: avg(dimensions)                   â”‚  â”‚
â”‚ â”‚   3. Filter by threshold                                â”‚  â”‚
â”‚ â”‚      â†’ Keep examples with score â‰¥ 7.0                   â”‚  â”‚
â”‚ â”‚   4. Format for fine-tuning                             â”‚  â”‚
â”‚ â”‚      â†’ ChatML format                                    â”‚  â”‚
â”‚ â”‚ OUTPUT: training_data.jsonl (350 examples)              â”‚  â”‚
â”‚ â”‚ SAMPLE OUTPUT:                                          â”‚  â”‚
â”‚ â”‚   {                                                     â”‚  â”‚
â”‚ â”‚     "messages": [                                       â”‚  â”‚
â”‚ â”‚       {                                                 â”‚  â”‚
â”‚ â”‚         "role": "user",                                 â”‚  â”‚
â”‚ â”‚         "content": "What is the purpose of H5Fcreate?"  â”‚  â”‚
â”‚ â”‚       },                                                â”‚  â”‚
â”‚ â”‚       {                                                 â”‚  â”‚
â”‚ â”‚         "role": "assistant",                            â”‚  â”‚
â”‚ â”‚         "content": "Let me explain step by step:\n      â”‚  â”‚
â”‚ â”‚                     1. H5Fcreate is an HDF5 API...",    â”‚  â”‚
â”‚ â”‚       }                                                 â”‚  â”‚
â”‚ â”‚     ],                                                  â”‚  â”‚
â”‚ â”‚     "quality_scores": {                                 â”‚  â”‚
â”‚ â”‚       "accuracy": 9.2,                                  â”‚  â”‚
â”‚ â”‚       "clarity": 8.8,                                   â”‚  â”‚
â”‚ â”‚       "completeness": 9.0                               â”‚  â”‚
â”‚ â”‚     },                                                  â”‚  â”‚
â”‚ â”‚     "overall_score": 9.0                                â”‚  â”‚
â”‚ â”‚   }                                                     â”‚  â”‚
â”‚ â”‚ METRICS:                                                â”‚  â”‚
â”‚ â”‚   - Examples scored: 500                                â”‚  â”‚
â”‚ â”‚   - High quality (â‰¥7.0): 350 (70%)                      â”‚  â”‚
â”‚ â”‚   - Average score: 7.8                                  â”‚  â”‚
â”‚ â”‚   - Filtered out: 150 (30%)                             â”‚  â”‚
â”‚ â”‚   - Time: 623s                                          â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FINAL OUTPUT: Fine-Tuning Dataset                             â”‚
â”‚                                                               â”‚
â”‚ training_data.jsonl                                           â”‚
â”‚ â”œâ”€â”€ 350 high-quality examples                                â”‚
â”‚ â”œâ”€â”€ ChatML format                                             â”‚
â”‚ â”œâ”€â”€ Chain-of-Thought reasoning                               â”‚
â”‚ â”œâ”€â”€ Source-grounded answers                                   â”‚
â”‚ â””â”€â”€ Ready for fine-tuning                                     â”‚
â”‚                                                               â”‚
â”‚ Use with:                                                     â”‚
â”‚ â€¢ Ollama: ollama create hdf5-expert -f Modelfile             â”‚
â”‚ â€¢ Unsloth: train_lora.py --data training_data.jsonl          â”‚
â”‚ â€¢ HuggingFace: trainer.train()                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PIPELINE SUMMARY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Module          Input               Output                  Time
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Researcher      Topic              Research report          87s
Parser          Report             38 PDFs                  245s
Ingestor        38 PDFs            38 Markdowns + images    412s
Processor       38 Markdowns       8,271 chunks (LanceDB)   628s
Generator-QA    8,271 chunks       500 QA pairs             342s
Generator-CoT   500 QA             500 + reasoning          487s
Generator-Judge 500 + reasoning    350 high-quality         623s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL                                                        2,824s
                                                            (~47 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

---

## Agent Evolution Lifecycle

### From Creation to Mastery

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       AGENT LIFECYCLE: BIRTH TO MASTERY              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHASE 1: BIRTH (Day 1)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ factory.create_agent("processor")                                   â”‚
â”‚                                                                     â”‚
â”‚ Agent State:                                                        â”‚
â”‚ â€¢ Knowledge: None (blank slate)                                     â”‚
â”‚ â€¢ Experience: 0 trajectories                                        â”‚
â”‚ â€¢ Success Rate: 0%                                                  â”‚
â”‚ â€¢ Capabilities: Template-defined action space                       â”‚
â”‚                                                                     â”‚
â”‚ Status: CREATED âœ…                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
PHASE 2: CHILDHOOD - Behavioral Cloning (Week 1-2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ agent.train(expert_trajectories, method="behavioral_cloning")       â”‚
â”‚                                                                     â”‚
â”‚ Learning:                                                           â”‚
â”‚ â€¢ Watches 500 expert demonstrations                                â”‚
â”‚ â€¢ Learns basic patterns: "When X, do Y"                            â”‚
â”‚ â€¢ Builds initial policy network                                    â”‚
â”‚ â€¢ Can execute simple tasks                                          â”‚
â”‚                                                                     â”‚
â”‚ Agent State After Training:                                         â”‚
â”‚ â€¢ Knowledge: Basic task execution                                   â”‚
â”‚ â€¢ Experience: 500 expert demos (observed)                           â”‚
â”‚ â€¢ Success Rate: 72% (validation)                                    â”‚
â”‚ â€¢ Capabilities: Can execute standard Phagocyte operations           â”‚
â”‚                                                                     â”‚
â”‚ Example learned behavior:                                           â”‚
â”‚ IF observation contains "large dataset" (>5000 files)               â”‚
â”‚ THEN action = "process --text-profile medium"                       â”‚
â”‚                                                                     â”‚
â”‚ Status: TRAINED (NOVICE) âœ…                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
PHASE 3: ADOLESCENCE - Environmental Exploration (Week 3-6)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ agentgym.explore(agent, environments, episodes=1000)                â”‚
â”‚                                                                     â”‚
â”‚ Exploration Activities:                                             â”‚
â”‚ Week 3: ResearchEnv (250 episodes)                                 â”‚
â”‚   â€¢ Tries different research modes                                  â”‚
â”‚   â€¢ Learns: Deep research for academic, quick for general           â”‚
â”‚   â€¢ Discovers: Citation quality correlates with report depth        â”‚
â”‚                                                                     â”‚
â”‚ Week 4: ParserEnv (250 episodes)                                   â”‚
â”‚   â€¢ Experiments with concurrency levels                             â”‚
â”‚   â€¢ Learns: concurrency=3 optimal for most cases                    â”‚
â”‚   â€¢ Discovers: Batch verification prevents duplicate downloads      â”‚
â”‚                                                                     â”‚
â”‚ Week 5: IngestorEnv (250 episodes)                                 â”‚
â”‚   â€¢ Tests different VLM settings                                    â”‚
â”‚   â€¢ Learns: Image descriptions improve RAG quality                  â”‚
â”‚   â€¢ Discovers: TOC filtering saves 15% processing time              â”‚
â”‚                                                                     â”‚
â”‚ Week 6: ProcessorEnv (250 episodes)                                â”‚
â”‚   â€¢ Tries embedding quality vs speed tradeoffs                      â”‚
â”‚   â€¢ Learns: Medium profile best for most datasets                   â”‚
â”‚   â€¢ Discovers: Incremental mode 3x faster for updates               â”‚
â”‚                                                                     â”‚
â”‚ Agent State After Exploration:                                      â”‚
â”‚ â€¢ Knowledge: Optimal strategies for different scenarios             â”‚
â”‚ â€¢ Experience: 1,500 trajectories (500 expert + 1000 self-generated) â”‚
â”‚ â€¢ Success Rate: 84%                                                 â”‚
â”‚ â€¢ Capabilities: Adaptive strategy selection                         â”‚
â”‚                                                                     â”‚
â”‚ Status: EXPERIENCED (INTERMEDIATE) âœ…                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
PHASE 4: ADULTHOOD - Self-Critique & Mastery (Week 7-8)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ agentgym.self_improve(agent, method="critique")                     â”‚
â”‚                                                                     â”‚
â”‚ Self-Improvement Process:                                           â”‚
â”‚                                                                     â”‚
â”‚ 1. Analyze Failures (150 failed trajectories)                       â”‚
â”‚    Common failure patterns identified:                              â”‚
â”‚    â€¢ Pattern A: Timeout on large datasets with high profile         â”‚
â”‚      Solution: Use medium/low profile for datasets >5000 files      â”‚
â”‚    â€¢ Pattern B: Out-of-memory with too many workers                 â”‚
â”‚      Solution: workers = min(4, cpu_count / 2)                      â”‚
â”‚    â€¢ Pattern C: Download failures due to rate limits                â”‚
â”‚      Solution: Add exponential backoff, reduce concurrency          â”‚
â”‚                                                                     â”‚
â”‚ 2. Contrast with Successes (1200 successful trajectories)           â”‚
â”‚    Key success factors:                                             â”‚
â”‚    â€¢ Adaptive parameter selection based on dataset size             â”‚
â”‚    â€¢ Error recovery with retries                                    â”‚
â”‚    â€¢ Resource monitoring and adjustment                             â”‚
â”‚                                                                     â”‚
â”‚ 3. Update Policy                                                    â”‚
â”‚    Before: Fixed strategies                                         â”‚
â”‚    After: Context-aware decision making                             â”‚
â”‚                                                                     â”‚
â”‚    Example improvement:                                             â”‚
â”‚    OLD: Always use --text-profile high                              â”‚
â”‚    NEW: if files < 1000: high                                       â”‚
â”‚         elif files < 5000: medium                                   â”‚
â”‚         else: low (then verify quality, upgrade if needed)          â”‚
â”‚                                                                     â”‚
â”‚ Agent State After Self-Critique:                                    â”‚
â”‚ â€¢ Knowledge: Advanced optimization techniques                       â”‚
â”‚ â€¢ Experience: 1,500 analyzed + 200 improvement iterations           â”‚
â”‚ â€¢ Success Rate: 92%                                                 â”‚
â”‚ â€¢ Capabilities: Proactive error prevention, adaptive optimization   â”‚
â”‚                                                                     â”‚
â”‚ Status: EXPERT âœ…                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
PHASE 5: MASTERY - Generalization & Transfer (Week 9-10)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ agentgym.transfer_learning(agent, target_envs=[all_environments])   â”‚
â”‚                                                                     â”‚
â”‚ Transfer Learning:                                                  â”‚
â”‚ â€¢ Processor skills â†’ Applied to Research (resource management)      â”‚
â”‚ â€¢ Parser error handling â†’ Applied to Ingestor (retry logic)         â”‚
â”‚ â€¢ Research quality metrics â†’ Applied to Generator (output quality)  â”‚
â”‚                                                                     â”‚
â”‚ Cross-Environment Performance:                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚ â”‚ Environment  â”‚ Trained  â”‚ Transfer â”‚ Fine-tuneâ”‚                 â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                 â”‚
â”‚ â”‚ Research     â”‚ 89%      â”‚ 76%      â”‚ 91%      â”‚                 â”‚
â”‚ â”‚ Parser       â”‚ 92%      â”‚ 78%      â”‚ 93%      â”‚                 â”‚
â”‚ â”‚ Ingestor     â”‚ 90%      â”‚ 74%      â”‚ 92%      â”‚                 â”‚
â”‚ â”‚ Processor    â”‚ 94%      â”‚ 80%      â”‚ 95%      â”‚                 â”‚
â”‚ â”‚ Generator    â”‚ 87%      â”‚ 72%      â”‚ 89%      â”‚                 â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                     â”‚
â”‚ Agent State After Transfer Learning:                                â”‚
â”‚ â€¢ Knowledge: Generalized principles across all modules              â”‚
â”‚ â€¢ Experience: 2,000 diverse trajectories                            â”‚
â”‚ â€¢ Success Rate: 93% (average across all environments)               â”‚
â”‚ â€¢ Capabilities: True generalist, can handle any Phagocyte task      â”‚
â”‚                                                                     â”‚
â”‚ Status: MASTER âœ…                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
PHASE 6: CONTINUOUS IMPROVEMENT (Ongoing)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Production deployment with continuous learning                      â”‚
â”‚                                                                     â”‚
â”‚ â€¢ Every execution logged                                            â”‚
â”‚ â€¢ Performance monitored                                             â”‚
â”‚ â€¢ New strategies discovered                                         â”‚
â”‚ â€¢ Policy updated nightly                                            â”‚
â”‚ â€¢ Knowledge shared across agent fleet                               â”‚
â”‚                                                                     â”‚
â”‚ Monthly Evolution Metrics:                                          â”‚
â”‚ Month 1: 93% â†’ 94% (+1%)                                           â”‚
â”‚ Month 2: 94% â†’ 95% (+1%)                                           â”‚
â”‚ Month 3: 95% â†’ 96% (+1%)                                           â”‚
â”‚ Month 6: 96% â†’ 97% (+1%)                                           â”‚
â”‚ Month 12: 97% â†’ 98% (+1%)                                          â”‚
â”‚                                                                     â”‚
â”‚ Status: EVOLVING â™¾ï¸                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FINAL AGENT CAPABILITIES (After 12 months):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… Success Rate: 98%
âœ… Avg Task Time: 40% faster than baseline
âœ… Error Rate: 2%
âœ… Handles edge cases gracefully
âœ… Proactive optimization
âœ… Self-debugging
âœ… Knowledge transfer across tasks
âœ… Continuous self-improvement
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Integration Architecture

### How All Systems Work Together

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               COMPLETE INTEGRATION: FACTORY + GYM + PHAGOCYTE + GENERATOR    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CONTROL LAYER                             â”‚
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚               AGENT FACTORY                          â”‚    â”‚
â”‚  â”‚  â€¢ Creates and manages all agents                    â”‚    â”‚
â”‚  â”‚  â€¢ Routes tasks to appropriate agents                â”‚    â”‚
â”‚  â”‚  â€¢ Monitors performance                              â”‚    â”‚
â”‚  â”‚  â€¢ Orchestrates multi-agent workflows                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRAINING LAYER                              â”‚
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                  AGENTGYM                            â”‚    â”‚
â”‚  â”‚  â€¢ Trains agents in safe environments                â”‚    â”‚
â”‚  â”‚  â€¢ Provides trajectory datasets                      â”‚    â”‚
â”‚  â”‚  â€¢ Implements AgentEvol (self-improvement)           â”‚    â”‚
â”‚  â”‚  â€¢ Benchmarks agent performance                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                          â†“ â†‘                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚         ENVIRONMENT WRAPPERS                        â”‚     â”‚
â”‚  â”‚  â€¢ ResearchEnv, ParserEnv, IngestorEnv             â”‚     â”‚
â”‚  â”‚  â€¢ ProcessorEnv, GeneratorEnv                       â”‚     â”‚
â”‚  â”‚  â€¢ Provide gym-like interfaces                      â”‚     â”‚
â”‚  â”‚  â€¢ Calculate rewards                                â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   EXECUTION LAYER                              â”‚
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚            PHAGOCYTE MODULES                         â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚  â”‚Research â”‚ â”‚ Parser  â”‚ â”‚ Ingestor â”‚ â”‚Processor â”‚ â”‚    â”‚
â”‚  â”‚  â”‚  CLI    â”‚ â”‚   CLI   â”‚ â”‚   CLI    â”‚ â”‚   CLI    â”‚ â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚    â”‚
â”‚  â”‚  â”‚      GENERATOR MODULE            â”‚               â”‚    â”‚
â”‚  â”‚  â”‚  â€¢ QA Generation                 â”‚               â”‚    â”‚
â”‚  â”‚  â”‚  â€¢ CoT Enhancement                â”‚               â”‚    â”‚
â”‚  â”‚  â”‚  â€¢ Tool-Use Generation           â”‚               â”‚    â”‚
â”‚  â”‚  â”‚  â€¢ LLM-as-Judge Curation         â”‚               â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STORAGE LAYER                               â”‚
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  LanceDB   â”‚  â”‚ Trajectory â”‚  â”‚  Training Data     â”‚      â”‚
â”‚  â”‚            â”‚  â”‚  Store     â”‚  â”‚  (.jsonl)          â”‚      â”‚
â”‚  â”‚ â€¢ Text     â”‚  â”‚            â”‚  â”‚                    â”‚      â”‚
â”‚  â”‚ â€¢ Code     â”‚  â”‚ â€¢ Expert   â”‚  â”‚ â€¢ QA pairs         â”‚      â”‚
â”‚  â”‚ â€¢ Images   â”‚  â”‚ â€¢ Agent    â”‚  â”‚ â€¢ CoT examples     â”‚      â”‚
â”‚  â”‚            â”‚  â”‚ â€¢ Metrics  â”‚  â”‚ â€¢ Tool-use data    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DATA FLOW EXAMPLE: Complete Research â†’ Fine-Tuning Pipeline
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. USER REQUEST
   â†“
   "Build HDF5 training dataset"

2. ORCHESTRATOR AGENT (AgentFactory)
   â†“
   Analyzes request â†’ Creates execution plan
   Plan: Research â†’ Parse â†’ Ingest â†’ Process â†’ Generate

3. DELEGATE TO SPECIALIZED AGENTS
   â†“
   ResearchAgent â†’ ParserAgent â†’ IngestorAgent â†’ ProcessorAgent â†’ GeneratorAgent

4. EACH AGENT EXECUTES IN ITS ENVIRONMENT
   â†“
   â€¢ Observes state
   â€¢ Generates thought
   â€¢ Selects action
   â€¢ Executes via Phagocyte CLI
   â€¢ Records trajectory
   â€¢ Learns from experience

5. GENERATOR PRODUCES TRAINING DATA
   â†“
   LanceDB (8,271 chunks) â†’ 500 QA â†’ 500 CoT â†’ 350 curated examples

6. AGENTGYM LEARNS FROM EXECUTION
   â†“
   â€¢ All trajectories stored
   â€¢ Success/failure analyzed
   â€¢ Agent policies updated
   â€¢ Knowledge shared across agent fleet

7. CONTINUOUS IMPROVEMENT
   â†“
   Next execution is faster, smarter, more reliable
```

---

## Command Reference

### AgentFactory Commands

```bash
# Create new agent
python -m agent_factory create --type processor --model gpt-4o-mini

# Train agent
python -m agent_factory train --agent-id processor_v1 \
    --trajectories ./data/expert_demos.jsonl \
    --method behavioral_cloning \
    --epochs 10

# Deploy agent
python -m agent_factory deploy --agent-id processor_v1 --environment production

# Execute task
python -m agent_factory execute --agent-type orchestrator \
    --task "Research HDF5 and build RAG database" \
    --output-dir ./output/

# Monitor performance
python -m agent_factory monitor --agent-id processor_v1 --metrics success_rate,avg_time

# List all agents
python -m agent_factory list

# Get agent info
python -m agent_factory info --agent-id processor_v1
```

### AgentGym Commands

```bash
# Initialize training environment
python -m agentgym init --workspace ./phagocyte_workspace

# Collect expert trajectories
python -m agentgym collect --environment processor \
    --episodes 100 \
    --expert human \
    --output ./data/expert_processor.jsonl

# Train agent (behavioral cloning)
python -m agentgym train --agent processor_v1 \
    --data ./data/expert_processor.jsonl \
    --method behavioral_cloning \
    --epochs 10

# Agent exploration
python -m agentgym explore --agent processor_v1 \
    --environment ProcessorEnv \
    --episodes 1000 \
    --strategy epsilon_greedy

# Self-improvement (AgentEvol)
python -m agentgym evolve --agent processor_v1 \
    --method self_critique \
    --iterations 50

# Evaluate agent
python -m agentgym evaluate --agent processor_v1 \
    --benchmark ./data/test_tasks.jsonl \
    --metrics success_rate,efficiency,quality

# Transfer learning
python -m agentgym transfer --source-agent processor_v1 \
    --target-env ResearchEnv \
    --fine-tune-episodes 200
```

### Phagocyte + Generator Pipeline

```bash
# Full automated pipeline (orchestrator agent)
python -m agent_factory pipeline --topic "HDF5 best practices" \
    --output-dir ./output/ \
    --n-training-examples 500

# This internally executes:
# 1. phagocyte research "HDF5 best practices" --mode directed
# 2. phagocyte parse refs research_report.md --export-batch
# 3. phagocyte parse batch batch.json --concurrent 3
# 4. phagocyte ingest batch ./papers/ --describe-images
# 5. phagocyte process run ./markdown/ --text-profile medium
# 6. uv run generator generate ./lancedb/ -o qa_pairs.json --n-pairs 500
# 7. uv run generator cot enhance qa_pairs.json -o qa_cot.json
# 8. uv run generator curate qa_cot.json -o training_data.jsonl --min-score 7.0
```

---

## Summary

This workflow diagram illustrates how **AgentFactory**, **AgentGym**, **Phagocyte**, and **Generator** integrate into a complete autonomous research-to-fine-tuning pipeline:

1. **AgentFactory** creates and manages specialized agents for each module
2. **AgentGym** trains agents through behavioral cloning, exploration, and self-critique
3. **Phagocyte modules** execute RAG pipeline tasks (research â†’ parse â†’ ingest â†’ process)
4. **Generator** produces high-quality training data (QA + CoT + Tool-Use)
5. **Agents continuously improve** through experience and self-evaluation

**Key Innovation**: The system evolves autonomously, becoming more efficient and capable over time without manual intervention.
